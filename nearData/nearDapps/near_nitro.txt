*GitHub Repository "near/nitro"*

'''--- .github/ISSUE_TEMPLATE/BOUNTY.yml ---
name: "Simple Bounty"
description: "Use this template to create a HEROES Simple Bounty via Github bot"
title: "Bounty: "
labels: ["bounty"]
assignees: heroes-bot-test
body:
  - type: markdown
    attributes:
      value: |
        Hi! Let's set up your bounty! Please don't change the template - @heroes-bot-test won't be able to help you.

  - type: dropdown
    id: type
    attributes:
      label: What talent are you looking for?
      options:
        - Marketing
        - Development
        - Design
        - Other
        - Content
        - Research
        - Audit

  - type: textarea
    id: description
    attributes:
      label: What you need to be done?

  - type: dropdown
    id: tags
    attributes:
      label: Tags
      description: Add tags that match the topic of the work
      multiple: true
      options:
        - API
        - Blockchain
        - Community
        - CSS
        - DAO
        - dApp
        - DeFi
        - Design
        - Documentation
        - HTML
        - Javascript
        - NFT
        - React
        - Rust
        - Smart contract
        - Typescript
        - UI/UX
        - web3
        - Translation
        - Illustration
        - Branding
        - Copywriting
        - Blogging
        - Editing
        - Video Creation
        - Social Media
        - Graphic Design
        - Transcription
        - Product Design
        - Artificial Intelligence
        - Quality Assurance
        - Risk Assessment
        - Security Audit
        - Bug Bounty
        - Code Review
        - Blockchain Security
        - Smart Contract Testing
        - Penetration Testing
        - Vulnerability Assessment
        - BOS
        - News
        - Hackathon
        - NEARCON2023
        - NEARWEEK

  - type: input
    id: deadline
    attributes:
      label: Deadline
      description: "Set a deadline for your bounty. Please enter the date in format: DD.MM.YYYY"
      placeholder: "19.05.2027"

  - type: dropdown
    id: currencyType
    attributes:
      label: Currency
      description: What is the currency you want to pay?
      options:
        - USDC.e
        - USDT.e
        - DAI
        - wNEAR
        - USDt
        - XP
        - marmaj
        - NEKO
        - JUMP
        - USDC
        - NEARVIDIA
      default: 0
    validations:
      required: true

  - type: input
    id: currencyAmount
    attributes:
      label: Amount
      description: How much it will be cost?

  - type: markdown
    attributes:
      value: "## Advanced settings"

  - type: checkboxes
    id: kyc
    attributes:
      label: KYC
      description: "Use HEROES' KYC Verification, only applicants who passed HEROES' KYC can apply and work on this bounty!"
      options:
        - label: Use KYC Verification

  - type: markdown
    attributes:
      value: |
        ### This cannot be changed once the bounty is live!

'''
'''--- .github/ISSUE_TEMPLATE/bug_report.md ---
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Additional context**
Add any other context about the problem here.

'''
'''--- .github/ISSUE_TEMPLATE/feature_request.md ---
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: ''
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.

'''
'''--- .github/codeql/codeql-config.yml ---
name: "Nitro CodeQL config"

'''
'''--- .github/workflows/arbitrator-ci.yml ---
name: Arbitrator CI
run-name: Arbitrator CI triggered from @${{ github.actor }} of ${{ github.head_ref }}

on:
  workflow_dispatch:
  merge_group:
  pull_request:
    paths:
      - 'arbitrator/**'
      - 'contracts'
      - '.github/workflows/arbitrator-ci.yml'
      - 'Makefile'
  push:
    branches:
      - master

env:
  RUST_BACKTRACE: 1
  RUSTFLAGS: -Dwarnings
  WABT_VERSION: 1.0.32

jobs:
  coverage:
    name: Run Arbitrator tests
    runs-on: ubuntu-8
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install Ubuntu dependencies
        run: |
          sudo add-apt-repository -y ppa:ethereum/ethereum
          sudo apt-get update && sudo apt-get install -y \
            build-essential cmake ethereum lld-14 libudev-dev
          sudo ln -s /usr/bin/wasm-ld-14 /usr/local/bin/wasm-ld

      - name: Install go
        uses: actions/setup-go@v4
        with:
          go-version: 1.20.x

      - name: Setup nodejs
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'yarn'
          cache-dependency-path: '**/yarn.lock'

      - name: Install rust stable
        uses: dtolnay/rust-toolchain@stable
        with:
          components: 'llvm-tools-preview, rustfmt, clippy'
          targets: 'wasm32-wasi, wasm32-unknown-unknown'

      - name: Install grcov
        uses: jaxxstorm/action-install-gh-release@v1.10.0
        with:
          repo: mozilla/grcov
          tag: v0.8.18
          extension: "\\.bz2"
          cache: enable

      - name: Cache Rust intermediate build products
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            arbitrator/target/
            arbitrator/wasm-libraries/target/
          key: ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-full-${{ hashFiles('arbitrator/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-full-
            ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-

      - name: Cache wabt build
        id: cache-wabt
        uses: actions/cache@v3
        with:
          path: ~/wabt-prefix
          key: ${{ runner.os }}-wabt-${{ env.WABT_VERSION }}

      - name: Install latest wabt
        if: steps.cache-wabt.outputs.cache-hit != 'true'
        run: |
          cd "$(mktemp -d)"
          git clone --recursive -b "$WABT_VERSION" https://github.com/WebAssembly/wabt .
          mkdir build
          cd build
          mkdir -p ~/wabt-prefix
          cmake .. -DCMAKE_INSTALL_PREFIX="$HOME/wabt-prefix"
          make -j
          make install

      - name: Cache cbrotli
        uses: actions/cache@v3
        id: cache-cbrotli
        with:
          path: |
            target/include/brotli/
            target/lib-wasm/
            target/lib/libbrotlicommon-static.a
            target/lib/libbrotlienc-static.a
            target/lib/libbrotlidec-static.a
          key: ${{ runner.os }}-brotli-3-${{ hashFiles('scripts/build-brotli.sh') }}-${{ hashFiles('.github/workflows/arbitrator-ci.yaml') }}
          restore-keys: ${{ runner.os }}-brotli-2-

      - name: Build cbrotli-local
        if: steps.cache-cbrotli.outputs.cache-hit != 'true'
        run: ./scripts/build-brotli.sh -l

      - name: Setup emsdk
        if: steps.cache-cbrotli.outputs.cache-hit != 'true'
        uses: mymindstorm/setup-emsdk@v12
        with:
          # Make sure to set a version number!
          version: 3.1.6
          # This is the name of the cache folder.
          # The cache folder will be placed in the build directory,
          #  so make sure it doesn't conflict with anything!
          actions-cache-folder: 'emsdk-cache'
          no-cache: true

      - name: Build cbrotli-wasm
        if: steps.cache-cbrotli.outputs.cache-hit != 'true'
        run: ./scripts/build-brotli.sh -w

      - name: Add wabt to path
        run: echo "$HOME/wabt-prefix/bin" >> "$GITHUB_PATH"

      - name: Make arbitrator libraries
        run: make -j wasm-ci-build

      - name: Enable rust instrumentation
        run: |
          echo LLVM_PROFILE_FILE="your_name-%p-%m.profraw" >> $GITHUB_ENV
          echo "CARGO_INCREMENTAL=0" >> $GITHUB_ENV
          echo RUSTFLAGS="-Cinstrument-coverage" >> $GITHUB_ENV
          echo RUSTDOCFLAGS="-Cpanic=abort" >> $GITHUB_ENV

      - name: Clippy check
        run: cargo clippy --all --manifest-path arbitrator/Cargo.toml -- -D warnings

      - name: Run rust tests
        run: cargo test --all --manifest-path arbitrator/Cargo.toml

      - name: Rustfmt
        run: cargo fmt --all --manifest-path arbitrator/Cargo.toml -- --check

      - name: Make proofs from test cases
        run: make -j test-gen-proofs

      - name: Create code-coverage files
        run: |
          grcov . --binary-path arbitrator/target/release/ -s . -t lcov --branch --ignore-not-existing --ignore "/*" -o lcov.info

      - name: Upload to codecov.io
        uses: codecov/codecov-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./lcov.info
          fail_ci_if_error: true
          verbose: false

      - name: Start geth server
        run: |
          geth --dev --http --http.port 8545 &
          sleep 2

      - name: Run proof validation tests
        run: |
          npm install --global yarn
          cd contracts
          yarn install
          yarn build
          yarn hardhat --network localhost test test/prover/*.ts

'''
'''--- .github/workflows/arbitrator-skip-ci.yml ---
name: Arbitrator skip CI
run-name: Arbitrator skip CI triggered from @${{ github.actor }} of ${{ github.head_ref }}

on:
  merge_group:
  pull_request:
    paths-ignore:
      - 'arbitrator/**'
      - 'contracts/src/osp/**'
      - 'contracts/src/mock/**'
      - 'contracts/test/**'
      - 'contracts/hardhat.config.ts'
      - 'Makefile'

jobs:
  coverage:
    name: Run Arbitrator tests
    runs-on: ubuntu-latest
    steps:
      - name: Do nothing
        run: echo "doing nothing"

'''
'''--- .github/workflows/ci.yml ---
name: Go tests CI
run-name: Go tests CI triggered from @${{ github.actor }} of ${{ github.head_ref }}

on:
  workflow_dispatch:
  merge_group:
  pull_request:
  push:
    branches:
      - master
      - develop

jobs:
  test:
    name: Go Tests
    runs-on: ubuntu-8

    # Creates a redis container for redis tests
    services:
      redis:
        image: redis
        ports:
            - 6379:6379

    strategy:
      fail-fast: false
      matrix:
        test-mode: [defaults, race, challenge]

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Install dependencies
        run: sudo apt update && sudo apt install -y wabt gotestsum

      - name: Setup nodejs
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'yarn'
          cache-dependency-path: '**/yarn.lock'

      - name: Install go
        uses: actions/setup-go@v4
        with:
          go-version: 1.20.x

      - name: Install wasm-ld
        run: |
          sudo apt-get update && sudo apt-get install -y lld-14
          sudo ln -s /usr/bin/wasm-ld-14 /usr/local/bin/wasm-ld

      - name: Install rust stable
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: 'wasm32-unknown-unknown, wasm32-wasi'

      - name: Cache Build Products
        uses: actions/cache@v3
        with:
          path: |
            ~/go/pkg/mod
            ~/.cache/go-build
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}-${{ matrix.test-mode }}
          restore-keys: ${{ runner.os }}-go-

      - name: Cache Rust Build Products
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry/
            ~/.cargo/git/
            arbitrator/target/
            arbitrator/wasm-libraries/target/
            arbitrator/wasm-libraries/soft-float/SoftFloat/build
            target/etc/initial-machine-cache/
          key: ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-min-${{ hashFiles('arbitrator/Cargo.lock') }}-${{ matrix.test-mode }}
          restore-keys: ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-

      - name: Cache cbrotli
        uses: actions/cache@v3
        id: cache-cbrotli
        with:
          path: |
            target/include/brotli/
            target/lib-wasm/
            target/lib/libbrotlicommon-static.a
            target/lib/libbrotlienc-static.a
            target/lib/libbrotlidec-static.a
          key: ${{ runner.os }}-brotli-${{ hashFiles('scripts/build-brotli.sh') }}-${{ hashFiles('.github/workflows/arbitrator-ci.yaml') }}-${{ matrix.test-mode }}
          restore-keys: ${{ runner.os }}-brotli-

      - name: Build cbrotli-local
        if: steps.cache-cbrotli.outputs.cache-hit != 'true'
        run: ./scripts/build-brotli.sh -l

      - name: Build cbrotli-wasm in docker
        if: steps.cache-cbrotli.outputs.cache-hit != 'true'
        run: ./scripts/build-brotli.sh -w -d

      - name: Build
        run: make build test-go-deps -j

      - name: Build all lint dependencies
        run: make -j build-node-deps

      - name: Lint
        uses: golangci/golangci-lint-action@v3
        with:
          version: latest
          skip-pkg-cache: true
      - name: Custom Lint
        run: |
          go run ./linter/koanf ./...
          go run ./linter/pointercheck ./...

      - name: Set environment variables
        run: |
          mkdir -p target/tmp/deadbeefbee
          echo "TMPDIR=$(pwd)/target/tmp/deadbeefbee" >> "$GITHUB_ENV"
          echo "GOMEMLIMIT=6GiB" >> "$GITHUB_ENV"
          echo "GOGC=80" >> "$GITHUB_ENV"

      - name: run tests without race detection
        if: matrix.test-mode == 'defaults'
        run: |
          packages=`go list ./...`
          gotestsum --format short-verbose --packages="$packages" --rerun-fails=1 -- -coverprofile=coverage.txt -covermode=atomic -coverpkg=./...,./go-ethereum/...

      - name: run tests with race detection
        if: matrix.test-mode == 'race'
        run:  |
          packages=`go list ./...`
          gotestsum --format short-verbose --packages="$packages" --rerun-fails=1 -- -race

      - name: run redis tests
        if: matrix.test-mode == 'defaults'
        run: TEST_REDIS=redis://localhost:6379/0 gotestsum --format short-verbose -- -p 1 -run TestRedis ./arbnode/... ./system_tests/... -coverprofile=coverage-redis.txt -covermode=atomic -coverpkg=./...

      - name: run challenge tests
        if: matrix.test-mode == 'challenge'
        run:  |
          packages=`go list ./...`
          gotestsum --format short-verbose --packages="$packages" --rerun-fails=1 -- ./... -coverprofile=coverage.txt -covermode=atomic -coverpkg=./...,./go-ethereum/... -tags=challengetest -run=TestChallenge

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v2
        if: matrix.test-mode == 'defaults'
        with:
          fail_ci_if_error: false
          files: ./coverage.txt,./coverage-redis.txt
          verbose: false
          token: ${{ secrets.CODECOV_TOKEN }}

'''
'''--- .github/workflows/codeql-analysis.yml ---
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL"

on:
  push:
    branches: [ "master" ]
  merge_group:
    branches: [ "master" ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ "master" ]
  schedule:
    - cron: '18 21 * * 5'

jobs:
  analyze:
    name: Analyze
    if: github.repository == 'OffchainLabs/nitro' # don't run in any forks without "Advanced Security" enabled
    runs-on: ubuntu-8
    permissions:
      actions: read
      contents: read
      security-events: write
    env:
      WABT_VERSION: 1.0.32

    strategy:
      fail-fast: false
      matrix:
        language: [ 'go' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        submodules: true

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # Details on CodeQL's query packs refer to : https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality
        config-file: ./.github/codeql/codeql-config.yml

    - name: Setup nodejs
      uses: actions/setup-node@v3
      with:
        node-version: '16'
        cache: 'yarn'
        cache-dependency-path: '**/yarn.lock'

    - name: Install go
      uses: actions/setup-go@v4
      with:
        go-version: 1.20.x

    - name: Install rust stable
      uses: dtolnay/rust-toolchain@stable

    - name: Cache Rust Build Products
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry/
          ~/.cargo/git/
          arbitrator/target/
          arbitrator/wasm-libraries/target/
          arbitrator/wasm-libraries/soft-float/SoftFloat/build
          target/etc/initial-machine-cache/
        key: ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-min-${{ hashFiles('arbitrator/Cargo.lock') }}
        restore-keys: ${{ runner.os }}-cargo-${{ steps.install-rust.outputs.rustc_hash }}-

    - name: Cache wabt build
      id: cache-wabt
      uses: actions/cache@v3
      with:
        path: ~/wabt-prefix
        key: ${{ runner.os }}-wabt-codeql-${{ env.WABT_VERSION }}

    - name: Cache cbrotli
      uses: actions/cache@v3
      id: cache-cbrotli
      with:
        path: |
          target/include/brotli/
          target/lib-wasm/
          target/lib/libbrotlicommon-static.a
          target/lib/libbrotlienc-static.a
          target/lib/libbrotlidec-static.a
        key: ${{ runner.os }}-brotli-3a-${{ hashFiles('scripts/build-brotli.sh') }}-${{ hashFiles('.github/workflows/arbitrator-ci.yaml') }}
        restore-keys: ${{ runner.os }}-brotli-

    - name: Build cbrotli-local
      if: steps.cache-cbrotli.outputs.cache-hit != 'true'
      run: ./scripts/build-brotli.sh -l

    - name: Cache Build Products
      uses: actions/cache@v3
      with:
        path: |
          ~/go/pkg/mod
          ~/.cache/go-build
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        restore-keys: ${{ runner.os }}-go-

    - name: Build all lint dependencies
      run: make -j build-node-deps

    # â„¹ï¸ Command-line programs to run using the OS shell.
    # ðŸ“š See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

    #   If the Autobuild fails above, remove it and uncomment the following three lines.
    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

    # - run: |
    #   echo "Run, Build Application using script"
    #   ./location_of_script_within_repo/buildscript.sh

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2

'''
'''--- .github/workflows/docker.yml ---
name: Docker build CI
run-name: Docker build CI triggered from @${{ github.actor }} of ${{ github.head_ref }}

on:
  workflow_dispatch:
  merge_group:
  pull_request:
  push:
    branches:
      - master
      - develop

jobs:
  docker:
    name: Docker build
    runs-on: ubuntu-8
    services:
      # local registery
      registry:
        image: registry:2
        ports:
          - 5000:5000

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: network=host

      - name: Cache Docker layers
        uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ hashFiles('Dockerfile') }}
          restore-keys: ${{ runner.os }}-buildx-

      - name: Build nitro-node docker
        uses: docker/build-push-action@v5
        with:
          target: nitro-node
          push: true
          context: .
          tags: localhost:5000/nitro-node:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Build nitro-node-dev docker
        uses: docker/build-push-action@v5
        with:
          target: nitro-node-dev
          push: true
          context: .
          tags: localhost:5000/nitro-node-dev:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Start background nitro-testnode
        shell: bash
        run: |
          cd nitro-testnode
          ./test-node.bash --init --dev &
      
      - name: Wait for rpc to come up
        shell: bash
        run: |
          ${{ github.workspace }}/.github/workflows/waitForNitro.sh
  
      - name: Print WAVM module root
        id: module-root
        run: |
          # Unfortunately, `docker cp` seems to always result in a "permission denied"
          # We work around this by piping a tarball through stdout
          docker run --rm --entrypoint tar localhost:5000/nitro-node-dev:latest -cf - target/machines/latest | tar xf -
          module_root="$(cat "target/machines/latest/module-root.txt")"
          echo "name=module-root=$module_root" >> $GITHUB_STATE
          echo -e "\x1b[1;34mWAVM module root:\x1b[0m $module_root"

      - name: Upload WAVM machine as artifact
        uses: actions/upload-artifact@v3
        with:
          name: wavm-machine-${{ steps.module-root.outputs.module-root }}
          path: target/machines/latest/*
          if-no-files-found: error

      - name: Move cache
        # Temp fix
        # https://github.com/docker/build-push-action/issues/252
        # https://github.com/moby/buildkit/issues/1896
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

      - name: Clear cache on failure
        if: failure()
        run: |
          keys=(${{ runner.os }}-buildx- ${{ runner.os }}-buildx-${{ hashFiles('Dockerfile') }})
          for key in "${keys[@]}"; do
            curl -X DELETE -H "Accept: application/vnd.github.v3+json" -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" "https://api.github.com/repos/${{ github.repository }}/actions/caches/$key"
          done

'''
'''--- .github/workflows/waitForNitro.sh ---
#!/bin/bash
# poll the nitro endpoint until we get a 0 return code or 30mins have passed, in that case exit 1
timeout_time=$(($(date +%s) + 1800))

while (( $(date +%s) <= timeout_time )); do
  if curl -X POST -H 'Content-Type: application/json' -d '{"jsonrpc":"2.0","id":45678,"method":"eth_chainId","params":[]}' 'http://localhost:8547'; then
    exit 0
  else
    sleep 20
  fi
done

exit 1
'''
'''--- .golangci.yml ---
# golangci-lint configuration

run:
  skip-dirs:
    - go-ethereum
    - fastcache

  timeout: 10m

issues:
  exclude-rules:
    - path: _test\.go
      linters:
        - staticcheck

linters:
  enable:
    - asciicheck  # check for non-ascii characters
    - errorlint   # enure error wrapping is safely done
    - gocritic    # check for certain simplifications
    - gofmt       # ensure code is formatted
    - gosec       # check for security concerns
    - nilerr      # ensure errors aren't mishandled
    - staticcheck # check for suspicious constructs
    - unused      # check for unused constructs

linters-settings:
  errcheck:
    # report when type assertions aren't checked for errors as in
    #         a := b.(MyStruct)
    #
    check-type-assertions: true

  gocritic:
    disabled-tags:
      - experimental
      - opinionated

    disabled-checks:
      - ifElseChain
      - assignOp
      - unlambda
      - exitAfterDefer

  gosec:
    excludes:
      - G404  # checks that random numbers are securely generated

  govet:
    enable-all: true
    disable:
      - shadow
      - fieldalignment

'''
'''--- .nitro-tag.txt ---

'''
'''--- README.md ---
<br />
<p align="center">
  <a href="https://arbitrum.io/">
    <img src="https://arbitrum.io/assets/arbitrum/logo_color.png" alt="Logo" width="80" height="80">
  </a>

  <h3 align="center">Arbitrum Nitro</h3>

  <p align="center">
    <a href="https://developer.arbitrum.io/"><strong>Next Generation Ethereum L2 Technology Â»</strong></a>
    <br />
  </p>
</p>

## About Arbitrum Nitro

<img src="https://arbitrum.io/assets/arbitrum/logo_color.png" alt="Logo" width="80" height="80">

Nitro is the latest iteration of the Arbitrum technology. It is a fully integrated, complete
layer 2 optimistic rollup system, including fraud proofs, the sequencer, the token bridges, 
advanced calldata compression, and more.

See the live docs-site [here](https://developer.arbitrum.io/) (or [here](https://github.com/OffchainLabs/arbitrum-docs) for markdown docs source.)

See [here](./audits) for security audit reports.

The Nitro stack is built on several innovations. At its core is a new prover, which can do Arbitrumâ€™s classic 
interactive fraud proofs over WASM code. That means the L2 Arbitrum engine can be written and compiled using 
standard languages and tools, replacing the custom-designed language and compiler used in previous Arbitrum
versions. In normal execution, 
validators and nodes run the Nitro engine compiled to native code, switching to WASM if a fraud proof is needed. 
We compile the core of Geth, the EVM engine that practically defines the Ethereum standard, right into Arbitrum. 
So the previous custom-built EVM emulator is replaced by Geth, the most popular and well-supported Ethereum client.

The last piece of the stack is a slimmed-down version of our ArbOS component, rewritten in Go, which provides the 
rest of whatâ€™s needed to run an L2 chain: things like cross-chain communication, and a new and improved batching 
and compression system to minimize L1 costs.

Essentially, Nitro runs Geth at layer 2 on top of Ethereum, and can prove fraud over the core engine of Geth 
compiled to WASM.

Arbitrum One successfully migrated from the Classic Arbitrum stack onto Nitro on 8/31/22. (See [state migration](https://developer.arbitrum.io/migration/state-migration) and [dapp migration](https://developer.arbitrum.io/migration/dapp_migration) for more info).

## License

We currently have Nitro [licensed](./LICENSE) under a Business Source License, similar to our friends at Uniswap and Aave, with an "Additional Use Grant" to ensure that everyone can have full comfort using and running nodes on all public Arbitrum chains.

## Contact

Discord - [Arbitrum](https://discord.com/invite/5KE54JwyTs)

Twitter: [Arbitrum](https://twitter.com/arbitrum)

'''
'''--- arbcompress/compress_cgo.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build !js
// +build !js

package arbcompress

/*
#cgo CFLAGS: -g -Wall -I${SRCDIR}/../target/include/
#cgo LDFLAGS: ${SRCDIR}/../target/lib/libbrotlidec-static.a ${SRCDIR}/../target/lib/libbrotlienc-static.a ${SRCDIR}/../target/lib/libbrotlicommon-static.a -lm
#include "brotli/encode.h"
#include "brotli/decode.h"
*/
import "C"
import (
	"fmt"
)

func Decompress(input []byte, maxSize int) ([]byte, error) {
	outbuf := make([]byte, maxSize)
	outsize := C.size_t(maxSize)
	var ptr *C.uint8_t
	if len(input) > 0 {
		ptr = (*C.uint8_t)(&input[0])
	}
	res := C.BrotliDecoderDecompress(C.size_t(len(input)), ptr, &outsize, (*C.uint8_t)(&outbuf[0]))
	if res != 1 {
		return nil, fmt.Errorf("failed decompression: %d", res)
	}
	if int(outsize) > maxSize {
		return nil, fmt.Errorf("result too large: %d", outsize)
	}
	return outbuf[:outsize], nil
}

func compressLevel(input []byte, level int) ([]byte, error) {
	maxOutSize := compressedBufferSizeFor(len(input))
	outbuf := make([]byte, maxOutSize)
	outSize := C.size_t(maxOutSize)
	var inputPtr *C.uint8_t
	if len(input) > 0 {
		inputPtr = (*C.uint8_t)(&input[0])
	}
	res := C.BrotliEncoderCompress(C.int(level), C.BROTLI_DEFAULT_WINDOW, C.BROTLI_MODE_GENERIC,
		C.size_t(len(input)), inputPtr, &outSize, (*C.uint8_t)(&outbuf[0]))
	if res != 1 {
		return nil, fmt.Errorf("failed compression: %d", res)
	}
	return outbuf[:outSize], nil
}

func CompressWell(input []byte) ([]byte, error) {
	return compressLevel(input, LEVEL_WELL)
}

'''
'''--- arbcompress/compress_common.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbcompress

const LEVEL_WELL = 11
const WINDOW_SIZE = 22 // BROTLI_DEFAULT_WINDOW

func compressedBufferSizeFor(length int) int {
	return length + (length>>10)*8 + 64 // actual limit is: length + (length >> 14) * 4 + 6
}

func CompressLevel(input []byte, level int) ([]byte, error) {
	return compressLevel(input, level)
}

'''
'''--- arbcompress/compress_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbcompress

import (
	"bytes"
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func testDecompress(t *testing.T, compressed, decompressed []byte) {
	res, err := Decompress(compressed, len(decompressed)*2+64)
	if err != nil {
		t.Fatal(err)
	}
	if !bytes.Equal(res, decompressed) {
		t.Fatal("results differ ", res, " vs. ", decompressed)
	}
}

func testCompressDecompress(t *testing.T, data []byte) {
	compressedWell, err := CompressWell(data)
	if err != nil {
		t.Fatal(err)
	}
	testDecompress(t, compressedWell, data)

	compressedFast, err := CompressLevel(data, 0)
	if err != nil {
		t.Fatal(err)
	}
	testDecompress(t, compressedFast, data)
}

func TestArbCompress(t *testing.T) {
	asciiData := []byte("This is a long and repetitive string. Yadda yadda yadda yadda yadda. The quick brown fox jumped over the lazy dog.")
	for i := 0; i < 8; i++ {
		asciiData = append(asciiData, asciiData...)
	}
	testCompressDecompress(t, asciiData)

	source := testhelpers.NewPseudoRandomDataSource(t, 0)
	randData := source.GetData(2500)
	testCompressDecompress(t, randData)

	// test empty data:
	testCompressDecompress(t, []byte{})
}

'''
'''--- arbcompress/compress_wasm.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build js
// +build js

package arbcompress

import (
	"fmt"
)

func brotliCompress(inBuf []byte, outBuf []byte, level int, windowSize int) int64

func brotliDecompress(inBuf []byte, outBuf []byte) int64

func Decompress(input []byte, maxSize int) ([]byte, error) {
	outBuf := make([]byte, maxSize)
	outLen := brotliDecompress(input, outBuf)
	if outLen < 0 {
		return nil, fmt.Errorf("failed decompression")
	}
	return outBuf[:outLen], nil
}

func compressLevel(input []byte, level int) ([]byte, error) {
	maxOutSize := compressedBufferSizeFor(len(input))
	outBuf := make([]byte, maxOutSize)
	outLen := brotliCompress(input, outBuf, level, WINDOW_SIZE)
	if outLen < 0 {
		return nil, fmt.Errorf("failed compression")
	}
	return outBuf[:outLen], nil
}

'''
'''--- arbitrator/Cargo.toml ---
[workspace]
members = [
        "arbutil",
        "prover",
        "jit",
]

[profile.release]
debug = true

'''
'''--- arbitrator/arbutil/Cargo.toml ---
[package]
name = "arbutil"
version = "0.1.0"
edition = "2021"

[dependencies]
digest = "0.10.7"
num_enum = "0.7.0"
sha2 = "0.10.7"
sha3 = "0.10.8"

'''
'''--- arbitrator/arbutil/src/color.rs ---
// Copyright 2020-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

#![allow(dead_code)]

use std::fmt::{Debug, Display};

pub const BLUE: &str = "\x1b[34;1m";
pub const DIM: &str = "\x1b[2m";
pub const GREY: &str = "\x1b[0;0m\x1b[90m";
pub const MINT: &str = "\x1b[38;5;48;1m";
pub const PINK: &str = "\x1b[38;5;161;1m";
pub const RED: &str = "\x1b[31;1m";
pub const CLEAR: &str = "\x1b[0;0m";
pub const WHITE: &str = "\x1b[0;1m";
pub const YELLOW: &str = "\x1b[33;1m";

pub trait Color {
    fn color(&self, color: &str) -> String;

    fn blue(&self) -> String;
    fn dim(&self) -> String;
    fn clear(&self) -> String;
    fn grey(&self) -> String;
    fn mint(&self) -> String;
    fn pink(&self) -> String;
    fn red(&self) -> String;
    fn white(&self) -> String;
    fn yellow(&self) -> String;
}

#[rustfmt::skip]
impl<T> Color for T where T: Display {

    fn color(&self, color: &str) -> String {
        format!("{}{}{}", color, self, CLEAR)
    }

    fn blue(&self)   -> String { self.color(BLUE)   }
    fn dim(&self)    -> String { self.color(DIM)    }
    fn clear(&self)  -> String { self.color(CLEAR)  }
    fn grey(&self)   -> String { self.color(GREY)   }
    fn mint(&self)   -> String { self.color(MINT)   }
    fn pink(&self)   -> String { self.color(PINK)   }
    fn red(&self)    -> String { self.color(RED)    }
    fn white(&self)  -> String { self.color(WHITE)  }
    fn yellow(&self) -> String { self.color(YELLOW) }
}

pub fn when<T: Display>(cond: bool, text: T, when_color: &str) -> String {
    match cond {
        true => text.color(when_color),
        false => format!("{text}"),
    }
}

pub trait DebugColor {
    fn debug_color(&self, color: &str) -> String;

    fn debug_blue(&self) -> String;
    fn debug_dim(&self) -> String;
    fn debug_clear(&self) -> String;
    fn debug_grey(&self) -> String;
    fn debug_mint(&self) -> String;
    fn debug_pink(&self) -> String;
    fn debug_red(&self) -> String;
    fn debug_white(&self) -> String;
    fn debug_yellow(&self) -> String;
}

#[rustfmt::skip]
impl<T> DebugColor for T where T: Debug {

    fn debug_color(&self, color: &str) -> String {
        format!("{}{:?}{}", color, self, CLEAR)
    }

    fn debug_blue(&self)   -> String { self.debug_color(BLUE)   }
    fn debug_dim(&self)    -> String { self.debug_color(DIM)    }
    fn debug_clear(&self)  -> String { self.debug_color(CLEAR)  }
    fn debug_grey(&self)   -> String { self.debug_color(GREY)   }
    fn debug_mint(&self)   -> String { self.debug_color(MINT)   }
    fn debug_pink(&self)   -> String { self.debug_color(PINK)   }
    fn debug_red(&self)    -> String { self.debug_color(RED)    }
    fn debug_white(&self)  -> String { self.debug_color(WHITE)  }
    fn debug_yellow(&self) -> String { self.debug_color(YELLOW) }
}

'''
'''--- arbitrator/arbutil/src/format.rs ---
// Copyright 2022-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

use crate::color::Color;
use std::fmt::Display;

#[must_use]
pub fn commas<T, U>(items: U) -> String
where
    T: Display,
    U: IntoIterator<Item = T>,
{
    let items: Vec<_> = items.into_iter().map(|x| format!("{x}")).collect();
    items.join(&", ".grey())
}

'''
'''--- arbitrator/arbutil/src/lib.rs ---
// Copyright 2022-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

pub mod color;
pub mod format;
mod types;

pub use color::{Color, DebugColor};
pub use types::PreimageType;

'''
'''--- arbitrator/arbutil/src/types.rs ---
// Copyright 2022-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

use digest::Digest;
use num_enum::{IntoPrimitive, TryFromPrimitive};

// These values must be kept in sync with `arbutil/preimage_type.go`,
// and the if statement in `contracts/src/osp/OneStepProverHostIo.sol` (search for "UNKNOWN_PREIMAGE_TYPE").
#[derive(
    Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, TryFromPrimitive, IntoPrimitive,
)]
#[repr(u8)]
pub enum PreimageType {
    Keccak256,
    Sha2_256,
}

impl PreimageType {
    pub fn hash(&self, preimage: &[u8]) -> [u8; 32] {
        match self {
            Self::Keccak256 => sha3::Keccak256::digest(preimage).into(),
            Self::Sha2_256 => sha2::Sha256::digest(preimage).into(),
        }
    }
}

'''
'''--- arbitrator/cbindgen.toml ---
language = "C"

'''
'''--- arbitrator/jit/Cargo.toml ---
[package]
name = "jit"
version = "0.1.0"
edition = "2021"

[dependencies]
arbutil = { path = "../arbutil/" }
wasmer = "3.1.0"
wasmer-compiler-cranelift = "3.1.0"
wasmer-compiler-llvm = { version = "3.1.0", optional = true }
eyre = "0.6.5"
parking_lot = "0.12.1"
rand = { version = "0.8.4", default-features = false }
rand_pcg = { version = "0.3.1", default-features = false }
thiserror = "1.0.33"
hex = "0.4.3"
structopt = "0.3.26"
sha3 = "0.9.1"
libc = "0.2.132"
ouroboros = "0.16.0"

[features]
llvm = ["dep:wasmer-compiler-llvm"]

'''
'''--- arbitrator/jit/build.rs ---
fn main() {
    // Tell Cargo that if the given file changes, to rerun this build script.
    println!("cargo:rustc-link-search=../target/lib/");
    println!("cargo:rustc-link-lib=static=brotlienc-static");
    println!("cargo:rustc-link-lib=static=brotlidec-static");
    println!("cargo:rustc-link-lib=static=brotlicommon-static");
}

'''
'''--- arbitrator/jit/programs/print/main.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

func main() {
	// what follows is a heartfelt poem about spiders, compilers,
	// and the language that brings them together

	println("itsy bitsy spider /\\oo/\\ made 407614 lines ðŸŽµ")
	println("that's because it's golang and there's a huge runtime ðŸŽµ")
	println("there's more than just 4 printlns since we might reflect ðŸŽµ")
	println("that's the state of golang, what would you expect? ðŸŽµ")
}

'''
'''--- arbitrator/jit/programs/time/main.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import "time"

func main() {
	println("What time is it??")
	println(time.Now().Nanosecond())
	println(time.Now().Nanosecond())
	println(time.Now().Nanosecond())
}

'''
'''--- arbitrator/jit/src/arbcompress.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{gostack::GoStack, machine::WasmEnvMut};

extern "C" {
    pub fn BrotliDecoderDecompress(
        encoded_size: usize,
        encoded_buffer: *const u8,
        decoded_size: *mut usize,
        decoded_buffer: *mut u8,
    ) -> u32;

    pub fn BrotliEncoderCompress(
        quality: u32,
        lgwin: u32,
        mode: u32,
        input_size: usize,
        input_buffer: *const u8,
        encoded_size: *mut usize,
        encoded_buffer: *mut u8,
    ) -> u32;
}

const BROTLI_MODE_GENERIC: u32 = 0;
const BROTLI_RES_SUCCESS: u32 = 1;

pub fn brotli_compress(mut env: WasmEnvMut, sp: u32) {
    let (sp, _) = GoStack::new(sp, &mut env);

    //(inBuf []byte, outBuf []byte, level int, windowSize int) int
    let in_buf_ptr = sp.read_u64(0);
    let in_buf_len = sp.read_u64(1);
    let out_buf_ptr = sp.read_u64(3);
    let out_buf_len = sp.read_u64(4);
    let level = sp.read_u64(6) as u32;
    let windowsize = sp.read_u64(7) as u32;
    let output_arg = 8;

    let in_slice = sp.read_slice(in_buf_ptr, in_buf_len);
    let mut output = vec![0u8; out_buf_len as usize];
    let mut output_len = out_buf_len as usize;

    let res = unsafe {
        BrotliEncoderCompress(
            level,
            windowsize,
            BROTLI_MODE_GENERIC,
            in_buf_len as usize,
            in_slice.as_ptr(),
            &mut output_len,
            output.as_mut_ptr(),
        )
    };

    if (res != BROTLI_RES_SUCCESS) || (output_len as u64 > out_buf_len) {
        sp.write_u64(output_arg, u64::MAX);
        return;
    }
    sp.write_slice(out_buf_ptr, &output[..output_len]);
    sp.write_u64(output_arg, output_len as u64);
}

pub fn brotli_decompress(mut env: WasmEnvMut, sp: u32) {
    let (sp, _) = GoStack::new(sp, &mut env);

    //(inBuf []byte, outBuf []byte) int
    let in_buf_ptr = sp.read_u64(0);
    let in_buf_len = sp.read_u64(1);
    let out_buf_ptr = sp.read_u64(3);
    let out_buf_len = sp.read_u64(4);
    let output_arg = 6;

    let in_slice = sp.read_slice(in_buf_ptr, in_buf_len);
    let mut output = vec![0u8; out_buf_len as usize];
    let mut output_len = out_buf_len as usize;

    let res = unsafe {
        BrotliDecoderDecompress(
            in_buf_len as usize,
            in_slice.as_ptr(),
            &mut output_len,
            output.as_mut_ptr(),
        )
    };

    if (res != BROTLI_RES_SUCCESS) || (output_len as u64 > out_buf_len) {
        sp.write_u64(output_arg, u64::MAX);
        return;
    }
    sp.write_slice(out_buf_ptr, &output[..output_len]);
    sp.write_u64(output_arg, output_len as u64);
}

'''
'''--- arbitrator/jit/src/color.rs ---
// Copyright 2020-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

#![allow(dead_code)]

use std::fmt;

pub const RED: &str = "\x1b[31;1m";
pub const BLUE: &str = "\x1b[34;1m";
pub const YELLOW: &str = "\x1b[33;1m";
pub const PINK: &str = "\x1b[38;5;161;1m";
pub const MINT: &str = "\x1b[38;5;48;1m";
pub const GREY: &str = "\x1b[90m";
pub const RESET: &str = "\x1b[0;0m";

pub const LIME: &str = "\x1b[38;5;119;1m";
pub const LAVENDER: &str = "\x1b[38;5;183;1m";
pub const MAROON: &str = "\x1b[38;5;124;1m";
pub const ORANGE: &str = "\x1b[38;5;202;1m";

pub fn color<S: fmt::Display>(color: &str, text: S) -> String {
    format!("{}{}{}", color, text, RESET)
}

/// Colors text red.
pub fn red<S: fmt::Display>(text: S) -> String {
    color(RED, text)
}

/// Colors text blue.
pub fn blue<S: fmt::Display>(text: S) -> String {
    color(BLUE, text)
}

/// Colors text yellow.
pub fn yellow<S: fmt::Display>(text: S) -> String {
    color(YELLOW, text)
}

/// Colors text pink.
pub fn pink<S: fmt::Display>(text: S) -> String {
    color(PINK, text)
}

/// Colors text grey.
pub fn grey<S: fmt::Display>(text: S) -> String {
    color(GREY, text)
}

/// Colors text lavender.
pub fn lavender<S: fmt::Display>(text: S) -> String {
    color(LAVENDER, text)
}

/// Colors text mint.
pub fn mint<S: fmt::Display>(text: S) -> String {
    color(MINT, text)
}

/// Colors text lime.
pub fn lime<S: fmt::Display>(text: S) -> String {
    color(LIME, text)
}

/// Colors text orange.
pub fn orange<S: fmt::Display>(text: S) -> String {
    color(ORANGE, text)
}

/// Colors text maroon.
pub fn maroon<S: fmt::Display>(text: S) -> String {
    color(MAROON, text)
}

/// Color a bool one of two colors depending on its value.
pub fn color_if(cond: bool, true_color: &str, false_color: &str) -> String {
    match cond {
        true => color(true_color, &format!("{cond}")),
        false => color(false_color, &format!("{cond}")),
    }
}

/// Color a bool if true
pub fn when<S: fmt::Display>(cond: bool, text: S, when_color: &str) -> String {
    match cond {
        true => color(when_color, text),
        false => format!("{text}"),
    }
}

'''
'''--- arbitrator/jit/src/gostack.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

#![allow(clippy::useless_transmute)]

use crate::{
    machine::{WasmEnv, WasmEnvMut},
    syscall::JsValue,
};

use ouroboros::self_referencing;
use rand_pcg::Pcg32;
use wasmer::{AsStoreRef, Memory, MemoryView, StoreRef, WasmPtr};

use std::collections::{BTreeSet, BinaryHeap};

#[self_referencing]
struct MemoryViewContainer {
    memory: Memory,
    #[borrows(memory)]
    #[covariant]
    view: MemoryView<'this>,
}

impl MemoryViewContainer {
    fn create(env: &WasmEnvMut<'_>) -> Self {
        // this func exists to properly constrain the closure's type
        fn closure<'a>(
            store: &'a StoreRef,
        ) -> impl (for<'b> FnOnce(&'b Memory) -> MemoryView<'b>) + 'a {
            move |memory: &Memory| memory.view(&store)
        }

        let store = env.as_store_ref();
        let memory = env.data().memory.clone().unwrap();
        let view_builder = closure(&store);
        MemoryViewContainerBuilder {
            memory,
            view_builder,
        }
        .build()
    }

    fn view(&self) -> &MemoryView {
        self.borrow_view()
    }
}

pub struct GoStack {
    start: u32,
    memory: MemoryViewContainer,
}

#[allow(dead_code)]
impl GoStack {
    pub fn new<'a, 'b: 'a>(start: u32, env: &'a mut WasmEnvMut<'b>) -> (Self, &'a mut WasmEnv) {
        let memory = MemoryViewContainer::create(env);
        let sp = Self { start, memory };
        (sp, env.data_mut())
    }

    pub fn simple(start: u32, env: &WasmEnvMut<'_>) -> Self {
        let memory = MemoryViewContainer::create(env);
        Self { start, memory }
    }

    pub fn shift_start(&mut self, offset: u32) {
        self.start += offset;
    }

    fn view(&self) -> &MemoryView {
        self.memory.view()
    }

    /// Returns the memory size, in bytes.
    /// note: wasmer measures memory in 65536-byte pages.
    pub fn memory_size(&self) -> u64 {
        self.view().size().0 as u64 * 65536
    }

    pub fn relative_offset(&self, arg: u32) -> u32 {
        (arg + 1) * 8
    }

    fn offset(&self, arg: u32) -> u32 {
        self.start + self.relative_offset(arg)
    }

    pub fn read_u8(&self, arg: u32) -> u8 {
        self.read_u8_ptr(self.offset(arg))
    }

    pub fn read_u32(&self, arg: u32) -> u32 {
        self.read_u32_ptr(self.offset(arg))
    }

    pub fn read_u64(&self, arg: u32) -> u64 {
        self.read_u64_ptr(self.offset(arg))
    }

    pub fn read_u8_ptr(&self, ptr: u32) -> u8 {
        let ptr: WasmPtr<u8> = WasmPtr::new(ptr);
        ptr.deref(self.view()).read().unwrap()
    }

    pub fn read_u32_ptr(&self, ptr: u32) -> u32 {
        let ptr: WasmPtr<u32> = WasmPtr::new(ptr);
        ptr.deref(self.view()).read().unwrap()
    }

    pub fn read_u64_ptr(&self, ptr: u32) -> u64 {
        let ptr: WasmPtr<u64> = WasmPtr::new(ptr);
        ptr.deref(self.view()).read().unwrap()
    }

    pub fn write_u8(&self, arg: u32, x: u8) {
        self.write_u8_ptr(self.offset(arg), x);
    }

    pub fn write_u32(&self, arg: u32, x: u32) {
        self.write_u32_ptr(self.offset(arg), x);
    }

    pub fn write_u64(&self, arg: u32, x: u64) {
        self.write_u64_ptr(self.offset(arg), x);
    }

    pub fn write_u8_ptr(&self, ptr: u32, x: u8) {
        let ptr: WasmPtr<u8> = WasmPtr::new(ptr);
        ptr.deref(self.view()).write(x).unwrap();
    }

    pub fn write_u32_ptr(&self, ptr: u32, x: u32) {
        let ptr: WasmPtr<u32> = WasmPtr::new(ptr);
        ptr.deref(self.view()).write(x).unwrap();
    }

    pub fn write_u64_ptr(&self, ptr: u32, x: u64) {
        let ptr: WasmPtr<u64> = WasmPtr::new(ptr);
        ptr.deref(self.view()).write(x).unwrap();
    }

    pub fn read_slice(&self, ptr: u64, len: u64) -> Vec<u8> {
        u32::try_from(ptr).expect("Go pointer not a u32"); // kept for consistency
        let len = u32::try_from(len).expect("length isn't a u32") as usize;
        let mut data = vec![0; len];
        self.view().read(ptr, &mut data).expect("failed to read");
        data
    }

    pub fn write_slice(&self, ptr: u64, src: &[u8]) {
        u32::try_from(ptr).expect("Go pointer not a u32");
        self.view().write(ptr, src).unwrap();
    }

    pub fn read_value_slice(&self, mut ptr: u64, len: u64) -> Vec<JsValue> {
        let mut values = Vec::new();
        for _ in 0..len {
            let p = u32::try_from(ptr).expect("Go pointer not a u32");
            values.push(JsValue::new(self.read_u64_ptr(p)));
            ptr += 8;
        }
        values
    }
}

pub struct GoRuntimeState {
    /// An increasing clock used when Go asks for time, measured in nanoseconds
    pub time: u64,
    /// The amount of time advanced each check. Currently 10 milliseconds
    pub time_interval: u64,
    /// The state of Go's timeouts
    pub timeouts: TimeoutState,
    /// Deterministic source of random data
    pub rng: Pcg32,
}

impl Default for GoRuntimeState {
    fn default() -> Self {
        Self {
            time: 0,
            time_interval: 10_000_000,
            timeouts: TimeoutState::default(),
            rng: Pcg32::new(0xcafef00dd15ea5e5, 0xa02bdbf7bb3c0a7),
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct TimeoutInfo {
    pub time: u64,
    pub id: u32,
}

impl Ord for TimeoutInfo {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        other
            .time
            .cmp(&self.time)
            .then_with(|| other.id.cmp(&self.id))
    }
}

impl PartialOrd for TimeoutInfo {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

#[derive(Default, Debug)]
pub struct TimeoutState {
    /// Contains tuples of (time, id)
    pub times: BinaryHeap<TimeoutInfo>,
    pub pending_ids: BTreeSet<u32>,
    pub next_id: u32,
}

'''
'''--- arbitrator/jit/src/machine.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{
    arbcompress, gostack::GoRuntimeState, runtime, socket, syscall, syscall::JsRuntimeState,
    wavmio, wavmio::Bytes32, Opts,
};

use arbutil::{Color, PreimageType};
use eyre::{bail, Result, WrapErr};
use sha3::{Digest, Keccak256};
use thiserror::Error;
use wasmer::{
    imports, CompilerConfig, Function, FunctionEnv, FunctionEnvMut, Instance, Memory, Module,
    RuntimeError, Store, TypedFunction,
};
use wasmer_compiler_cranelift::Cranelift;

use std::{
    collections::BTreeMap,
    fs::File,
    io::{self, Write},
    io::{BufReader, BufWriter, ErrorKind, Read},
    net::TcpStream,
    time::Instant,
};

pub fn create(opts: &Opts, env: WasmEnv) -> (Instance, FunctionEnv<WasmEnv>, Store) {
    let file = &opts.binary;

    let wasm = match std::fs::read(file) {
        Ok(wasm) => wasm,
        Err(err) => panic!("failed to read {}: {err}", file.to_string_lossy()),
    };

    let mut store = match opts.cranelift {
        true => {
            let mut compiler = Cranelift::new();
            compiler.canonicalize_nans(true);
            compiler.enable_verifier();
            Store::new(compiler)
        }
        false => {
            #[cfg(not(feature = "llvm"))]
            panic!("Please rebuild with the \"llvm\" feature for LLVM support");
            #[cfg(feature = "llvm")]
            {
                let mut compiler = wasmer_compiler_llvm::LLVM::new();
                compiler.canonicalize_nans(true);
                compiler.opt_level(wasmer_compiler_llvm::LLVMOptLevel::Aggressive);
                compiler.enable_verifier();
                Store::new(compiler)
            }
        }
    };

    let module = match Module::new(&store, wasm) {
        Ok(module) => module,
        Err(err) => panic!("{}", err),
    };

    let func_env = FunctionEnv::new(&mut store, env);
    macro_rules! native {
        ($func:expr) => {
            Function::new_typed(&mut store, $func)
        };
    }
    macro_rules! func {
        ($func:expr) => {
            Function::new_typed_with_env(&mut store, &func_env, $func)
        };
    }

    let imports = imports! {
        "go" => {
            "debug" => native!(runtime::go_debug),

            "runtime.resetMemoryDataView" => native!(runtime::reset_memory_data_view),
            "runtime.wasmExit" => func!(runtime::wasm_exit),
            "runtime.wasmWrite" => func!(runtime::wasm_write),
            "runtime.nanotime1" => func!(runtime::nanotime1),
            "runtime.walltime" => func!(runtime::walltime),
            "runtime.walltime1" => func!(runtime::walltime1),
            "runtime.scheduleTimeoutEvent" => func!(runtime::schedule_timeout_event),
            "runtime.clearTimeoutEvent" => func!(runtime::clear_timeout_event),
            "runtime.getRandomData" => func!(runtime::get_random_data),

            "syscall/js.finalizeRef" => func!(syscall::js_finalize_ref),
            "syscall/js.stringVal" => func!(syscall::js_string_val),
            "syscall/js.valueGet" => func!(syscall::js_value_get),
            "syscall/js.valueSet" => func!(syscall::js_value_set),
            "syscall/js.valueDelete" => func!(syscall::js_value_delete),
            "syscall/js.valueIndex" => func!(syscall::js_value_index),
            "syscall/js.valueSetIndex" => func!(syscall::js_value_set_index),
            "syscall/js.valueCall" => func!(syscall::js_value_call),
            "syscall/js.valueInvoke" => func!(syscall::js_value_invoke),
            "syscall/js.valueNew" => func!(syscall::js_value_new),
            "syscall/js.valueLength" => func!(syscall::js_value_length),
            "syscall/js.valuePrepareString" => func!(syscall::js_value_prepare_string),
            "syscall/js.valueLoadString" => func!(syscall::js_value_load_string),
            "syscall/js.valueInstanceOf" => func!(syscall::js_value_instance_of),
            "syscall/js.copyBytesToGo" => func!(syscall::js_copy_bytes_to_go),
            "syscall/js.copyBytesToJS" => func!(syscall::js_copy_bytes_to_js),

            "github.com/offchainlabs/nitro/wavmio.getGlobalStateBytes32" => func!(wavmio::get_global_state_bytes32),
            "github.com/offchainlabs/nitro/wavmio.setGlobalStateBytes32" => func!(wavmio::set_global_state_bytes32),
            "github.com/offchainlabs/nitro/wavmio.getGlobalStateU64" => func!(wavmio::get_global_state_u64),
            "github.com/offchainlabs/nitro/wavmio.setGlobalStateU64" => func!(wavmio::set_global_state_u64),
            "github.com/offchainlabs/nitro/wavmio.readInboxMessage" => func!(wavmio::read_inbox_message),
            "github.com/offchainlabs/nitro/wavmio.readDelayedInboxMessage" => func!(wavmio::read_delayed_inbox_message),
            "github.com/offchainlabs/nitro/wavmio.resolvePreImage" => {
                #[allow(deprecated)] // we're just keeping this around until we no longer need to validate old replay binaries
                {
                    func!(wavmio::resolve_keccak_preimage)
                }
            },
            "github.com/offchainlabs/nitro/wavmio.resolveTypedPreimage" => func!(wavmio::resolve_typed_preimage),

            "github.com/offchainlabs/nitro/arbcompress.brotliCompress" => func!(arbcompress::brotli_compress),
            "github.com/offchainlabs/nitro/arbcompress.brotliDecompress" => func!(arbcompress::brotli_decompress),
        },
    };

    let instance = match Instance::new(&mut store, &module, &imports) {
        Ok(instance) => instance,
        Err(err) => panic!("Failed to create instance: {}", err.red()),
    };
    let memory = match instance.exports.get_memory("mem") {
        Ok(memory) => memory.clone(),
        Err(err) => panic!("Failed to get memory: {}", err.red()),
    };
    let resume = match instance.exports.get_typed_function(&store, "resume") {
        Ok(resume) => resume,
        Err(err) => panic!("Failed to get the {} func: {}", "resume".red(), err.red()),
    };
    let getsp = match instance.exports.get_typed_function(&store, "getsp") {
        Ok(getsp) => getsp,
        Err(err) => panic!("Failed to get the {} func: {}", "getsp".red(), err.red()),
    };

    let env = func_env.as_mut(&mut store);
    env.memory = Some(memory);
    env.exports.resume = Some(resume);
    env.exports.get_stack_pointer = Some(getsp);
    (instance, func_env, store)
}

#[derive(Error, Debug)]
pub enum Escape {
    #[error("program exited with status code `{0}`")]
    Exit(u32),
    #[error("jit failed with `{0}`")]
    Failure(String),
    #[error("hostio failed with `{0}`")]
    HostIO(String),
    #[error("hostio socket failed with `{0}`")]
    SocketError(#[from] io::Error),
}

pub type MaybeEscape = Result<(), Escape>;

impl Escape {
    pub fn exit(code: u32) -> MaybeEscape {
        Err(Self::Exit(code))
    }

    pub fn hostio<S: std::convert::AsRef<str>>(message: S) -> MaybeEscape {
        Err(Self::HostIO(message.as_ref().to_string()))
    }

    pub fn failure<S: std::convert::AsRef<str>>(message: S) -> MaybeEscape {
        Err(Self::Failure(message.as_ref().to_string()))
    }
}

impl From<RuntimeError> for Escape {
    fn from(outcome: RuntimeError) -> Self {
        match outcome.downcast() {
            Ok(escape) => escape,
            Err(outcome) => Escape::Failure(format!("unknown runtime error: {outcome}")),
        }
    }
}

pub type WasmEnvMut<'a> = FunctionEnvMut<'a, WasmEnv>;
pub type Inbox = BTreeMap<u64, Vec<u8>>;
pub type Preimages = BTreeMap<PreimageType, BTreeMap<[u8; 32], Vec<u8>>>;

#[derive(Default)]
pub struct WasmEnv {
    /// Mechanism for reading and writing the module's memory
    pub memory: Option<Memory>,
    /// Go's general runtime state
    pub go_state: GoRuntimeState,
    /// The state of Go's js runtime
    pub js_state: JsRuntimeState,
    /// An ordered list of the 8-byte globals
    pub small_globals: [u64; 2],
    /// An ordered list of the 32-byte globals
    pub large_globals: [Bytes32; 2],
    /// An oracle allowing the prover to reverse keccak256
    pub preimages: Preimages,
    /// The sequencer inbox's messages
    pub sequencer_messages: Inbox,
    /// The delayed inbox's messages
    pub delayed_messages: Inbox,
    /// The purpose and connections of this process
    pub process: ProcessEnv,
    /// The exported funcs callable in hostio
    pub exports: WasmEnvFuncs,
}

impl WasmEnv {
    pub fn cli(opts: &Opts) -> Result<Self> {
        let mut env = WasmEnv::default();
        env.process.forks = opts.forks;
        env.process.debug = opts.debug;

        let mut inbox_position = opts.inbox_position;
        let mut delayed_position = opts.delayed_inbox_position;

        for path in &opts.inbox {
            let mut msg = vec![];
            File::open(path)?.read_to_end(&mut msg)?;
            env.sequencer_messages.insert(inbox_position, msg);
            inbox_position += 1;
        }
        for path in &opts.delayed_inbox {
            let mut msg = vec![];
            File::open(path)?.read_to_end(&mut msg)?;
            env.delayed_messages.insert(delayed_position, msg);
            delayed_position += 1;
        }

        if let Some(path) = &opts.preimages {
            let mut file = BufReader::new(File::open(path)?);
            let mut preimages = Vec::new();
            let filename = path.to_string_lossy();
            loop {
                let mut size_buf = [0u8; 8];
                match file.read_exact(&mut size_buf) {
                    Ok(()) => {}
                    Err(err) if err.kind() == ErrorKind::UnexpectedEof => break,
                    Err(err) => bail!("Failed to parse {filename}: {}", err),
                }
                let size = u64::from_le_bytes(size_buf) as usize;
                let mut buf = vec![0u8; size];
                file.read_exact(&mut buf)?;
                preimages.push(buf);
            }
            let keccak_preimages = env.preimages.entry(PreimageType::Keccak256).or_default();
            for preimage in preimages {
                let mut hasher = Keccak256::new();
                hasher.update(&preimage);
                let hash = hasher.finalize().into();
                keccak_preimages.insert(hash, preimage);
            }
        }

        fn parse_hex(arg: &Option<String>, name: &str) -> Result<Bytes32> {
            match arg {
                Some(arg) => {
                    let mut arg = arg.as_str();
                    if arg.starts_with("0x") {
                        arg = &arg[2..];
                    }
                    let mut bytes32 = Bytes32::default();
                    hex::decode_to_slice(arg, &mut bytes32)
                        .wrap_err_with(|| format!("failed to parse {} contents", name))?;
                    Ok(bytes32)
                }
                None => Ok(Bytes32::default()),
            }
        }

        let last_block_hash = parse_hex(&opts.last_block_hash, "--last-block-hash")?;
        let last_send_root = parse_hex(&opts.last_send_root, "--last-send-root")?;
        env.small_globals = [opts.inbox_position, opts.position_within_message];
        env.large_globals = [last_block_hash, last_send_root];
        Ok(env)
    }

    pub fn send_results(&mut self, error: Option<String>) {
        let writer = match &mut self.process.socket {
            Some((writer, _)) => writer,
            None => return,
        };

        macro_rules! check {
            ($expr:expr) => {{
                if let Err(comms_error) = $expr {
                    eprintln!("Failed to send results to Go: {comms_error}");
                    panic!("Communication failure");
                }
            }};
        }

        if let Some(error) = error {
            check!(socket::write_u8(writer, socket::FAILURE));
            check!(socket::write_bytes(writer, &error.into_bytes()));
            check!(writer.flush());
            return;
        }

        check!(socket::write_u8(writer, socket::SUCCESS));
        check!(socket::write_u64(writer, self.small_globals[0]));
        check!(socket::write_u64(writer, self.small_globals[1]));
        check!(socket::write_bytes32(writer, &self.large_globals[0]));
        check!(socket::write_bytes32(writer, &self.large_globals[1]));
        check!(writer.flush());
    }
}

pub struct ProcessEnv {
    /// Whether to create child processes to handle execution
    pub forks: bool,
    /// Whether to print debugging info
    pub debug: bool,
    /// Mechanism for asking for preimages and returning results
    pub socket: Option<(BufWriter<TcpStream>, BufReader<TcpStream>)>,
    /// A timestamp that helps with printing at various moments
    pub timestamp: Instant,
    /// Whether the machine has reached the first wavmio instruction
    pub reached_wavmio: bool,
}

impl Default for ProcessEnv {
    fn default() -> Self {
        Self {
            forks: false,
            debug: false,
            socket: None,
            timestamp: Instant::now(),
            reached_wavmio: false,
        }
    }
}

#[derive(Default)]
pub struct WasmEnvFuncs {
    /// Calls `resume` from the go runtime
    pub resume: Option<TypedFunction<(), ()>>,
    /// Calls `getsp` from the go runtime
    pub get_stack_pointer: Option<TypedFunction<(), i32>>,
}

'''
'''--- arbitrator/jit/src/main.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::machine::{Escape, WasmEnv};

use arbutil::{color, Color};
use structopt::StructOpt;
use wasmer::Value;

use std::path::PathBuf;

mod arbcompress;
mod gostack;
mod machine;
mod runtime;
mod socket;
mod syscall;
mod test;
mod wavmio;

#[derive(StructOpt)]
#[structopt(name = "jit-prover")]
pub struct Opts {
    #[structopt(short, long)]
    binary: PathBuf,
    #[structopt(long, default_value = "0")]
    inbox_position: u64,
    #[structopt(long, default_value = "0")]
    delayed_inbox_position: u64,
    #[structopt(long, default_value = "0")]
    position_within_message: u64,
    #[structopt(long)]
    last_block_hash: Option<String>,
    #[structopt(long)]
    last_send_root: Option<String>,
    #[structopt(long)]
    inbox: Vec<PathBuf>,
    #[structopt(long)]
    delayed_inbox: Vec<PathBuf>,
    #[structopt(long)]
    preimages: Option<PathBuf>,
    #[structopt(long)]
    cranelift: bool,
    #[structopt(long)]
    forks: bool,
    #[structopt(long)]
    debug: bool,
}

fn main() {
    let opts = Opts::from_args();

    let env = match WasmEnv::cli(&opts) {
        Ok(env) => env,
        Err(err) => panic!("{}", err),
    };

    let (instance, env, mut store) = machine::create(&opts, env);

    let main = instance.exports.get_function("run").unwrap();
    let outcome = main.call(&mut store, &[Value::I32(0), Value::I32(0)]);
    let escape = match outcome {
        Ok(outcome) => {
            println!("Go returned values {:?}", outcome);
            None
        }
        Err(outcome) => {
            let trace = outcome.trace();
            if !trace.is_empty() {
                println!("backtrace:");
            }
            for frame in trace {
                let module = frame.module_name();
                let name = frame.function_name().unwrap_or("??");
                println!("  in {} of {}", name.red(), module.red());
            }
            Some(Escape::from(outcome))
        }
    };

    let env = env.as_mut(&mut store);
    let user = env.process.socket.is_none();
    let time = format!("{}ms", env.process.timestamp.elapsed().as_millis());
    let time = color::when(user, time, color::PINK);
    let hash = color::when(user, hex::encode(env.large_globals[0]), color::PINK);
    let (success, message) = match escape {
        Some(Escape::Exit(0)) => (true, format!("Completed in {time} with hash {hash}.")),
        Some(Escape::Exit(x)) => (false, format!("Failed in {time} with exit code {x}.")),
        Some(Escape::Failure(err)) => (false, format!("Jit failed with {err} in {time}.")),
        Some(Escape::HostIO(err)) => (false, format!("Hostio failed with {err} in {time}.")),
        Some(Escape::SocketError(err)) => (false, format!("Socket failed with {err} in {time}.")),
        None => (false, "Machine exited prematurely".to_owned()),
    };

    if opts.debug {
        println!("{message}");
    }

    let error = match success {
        true => None,
        false => Some(message),
    };

    env.send_results(error);
}

// require a usize be at least 32 bits wide
#[cfg(not(any(target_pointer_width = "32", target_pointer_width = "64")))]
compile_error!(
    "Unsupported target pointer width (only 32 bit and 64 bit architectures are supported)"
);

'''
'''--- arbitrator/jit/src/runtime.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{
    gostack::{GoStack, TimeoutInfo},
    machine::{Escape, MaybeEscape, WasmEnvMut},
};

use rand::RngCore;

use std::io::Write;

pub fn go_debug(x: u32) {
    println!("go debug: {x}")
}

pub fn reset_memory_data_view(_: u32) {}

pub fn wasm_exit(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, _) = GoStack::new(sp, &mut env);
    Escape::exit(sp.read_u32(0))
}

pub fn wasm_write(mut env: WasmEnvMut, sp: u32) {
    let (sp, _) = GoStack::new(sp, &mut env);
    let fd = sp.read_u64(0);
    let ptr = sp.read_u64(1);
    let len = sp.read_u32(2);
    let buf = sp.read_slice(ptr, len.into());
    if fd == 2 {
        let stderr = std::io::stderr();
        let mut stderr = stderr.lock();
        stderr.write_all(&buf).unwrap();
    } else {
        let stdout = std::io::stdout();
        let mut stdout = stdout.lock();
        stdout.write_all(&buf).unwrap();
    }
}

pub fn nanotime1(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    env.go_state.time += env.go_state.time_interval;
    sp.write_u64(0, env.go_state.time);
}

pub fn walltime(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    env.go_state.time += env.go_state.time_interval;
    sp.write_u64(0, env.go_state.time / 1_000_000_000);
    sp.write_u32(1, (env.go_state.time % 1_000_000_000) as u32);
}

pub fn walltime1(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    env.go_state.time += env.go_state.time_interval;
    sp.write_u64(0, env.go_state.time / 1_000_000_000);
    sp.write_u64(1, env.go_state.time % 1_000_000_000);
}

pub fn schedule_timeout_event(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    let mut time = sp.read_u64(0);
    time = time.saturating_mul(1_000_000); // milliseconds to nanoseconds
    time = time.saturating_add(env.go_state.time); // add the current time to the delay

    let timeouts = &mut env.go_state.timeouts;
    let id = timeouts.next_id;
    timeouts.next_id += 1;
    timeouts.times.push(TimeoutInfo { time, id });
    timeouts.pending_ids.insert(id);

    sp.write_u32(1, id);
}

pub fn clear_timeout_event(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);

    let id = sp.read_u32(0);
    if !env.go_state.timeouts.pending_ids.remove(&id) {
        eprintln!("Go attempting to clear not pending timeout event {id}");
    }
}

pub fn get_random_data(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);

    let mut ptr = u32::try_from(sp.read_u64(0)).expect("Go getRandomData pointer not a u32");
    let mut len = sp.read_u64(1);
    while len >= 4 {
        let next = env.go_state.rng.next_u32();
        sp.write_u32_ptr(ptr, next);
        ptr += 4;
        len -= 4;
    }
    if len > 0 {
        let mut rem = env.go_state.rng.next_u32();
        for _ in 0..len {
            sp.write_u8_ptr(ptr, rem as u8);
            ptr += 1;
            rem >>= 8;
        }
    }
}

'''
'''--- arbitrator/jit/src/socket.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use std::{
    io,
    io::{BufReader, BufWriter, Read, Write},
    net::TcpStream,
};

use crate::wavmio::Bytes32;

pub const SUCCESS: u8 = 0x0;
pub const FAILURE: u8 = 0x1;
pub const ANOTHER: u8 = 0x3;
pub const READY: u8 = 0x4;

pub fn read_u8<T: Read>(reader: &mut BufReader<T>) -> Result<u8, io::Error> {
    let mut buf = [0; 1];
    reader.read_exact(&mut buf).map(|_| u8::from_be_bytes(buf))
}

pub fn read_u64<T: Read>(reader: &mut BufReader<T>) -> Result<u64, io::Error> {
    let mut buf = [0; 8];
    reader.read_exact(&mut buf).map(|_| u64::from_be_bytes(buf))
}

pub fn read_bytes32<T: Read>(reader: &mut BufReader<T>) -> Result<Bytes32, io::Error> {
    let mut buf = Bytes32::default();
    reader.read_exact(&mut buf).map(|_| buf)
}

pub fn read_bytes<T: Read>(reader: &mut BufReader<T>) -> Result<Vec<u8>, io::Error> {
    let size = read_u64(reader)?;
    let mut buf = vec![0; size as usize];
    reader.read_exact(&mut buf)?;
    Ok(buf)
}

pub fn write_u8(writer: &mut BufWriter<TcpStream>, data: u8) -> Result<(), io::Error> {
    let buf = [data; 1];
    writer.write_all(&buf)
}

pub fn write_u64(writer: &mut BufWriter<TcpStream>, data: u64) -> Result<(), io::Error> {
    let buf = data.to_be_bytes();
    writer.write_all(&buf)
}

pub fn write_bytes32(writer: &mut BufWriter<TcpStream>, data: &Bytes32) -> Result<(), io::Error> {
    writer.write_all(data)
}

pub fn write_bytes(writer: &mut BufWriter<TcpStream>, data: &[u8]) -> Result<(), io::Error> {
    write_u64(writer, data.len() as u64)?;
    writer.write_all(data)
}

'''
'''--- arbitrator/jit/src/syscall.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{
    gostack::GoStack,
    machine::{Escape, MaybeEscape, WasmEnv, WasmEnvMut},
};

use arbutil::Color;
use rand::RngCore;
use wasmer::AsStoreMut;

use std::{collections::BTreeMap, io::Write};

const ZERO_ID: u32 = 1;
const NULL_ID: u32 = 2;
const GLOBAL_ID: u32 = 5;
const GO_ID: u32 = 6;

const OBJECT_ID: u32 = 100;
const ARRAY_ID: u32 = 101;
const PROCESS_ID: u32 = 102;
const FS_ID: u32 = 103;
const UINT8_ARRAY_ID: u32 = 104;
const CRYPTO_ID: u32 = 105;
const DATE_ID: u32 = 106;

const FS_CONSTANTS_ID: u32 = 200;

const DYNAMIC_OBJECT_ID_BASE: u32 = 10000;

#[derive(Default)]
pub struct JsRuntimeState {
    /// A collection of js objects
    pool: DynamicObjectPool,
    /// The event Go will execute next
    pub pending_event: Option<PendingEvent>,
}

#[derive(Clone, Default, Debug)]
struct DynamicObjectPool {
    objects: BTreeMap<u32, DynamicObject>,
    free_ids: Vec<u32>,
}

impl DynamicObjectPool {
    fn insert(&mut self, object: DynamicObject) -> u32 {
        let id = self
            .free_ids
            .pop()
            .unwrap_or(DYNAMIC_OBJECT_ID_BASE + self.objects.len() as u32);
        self.objects.insert(id, object);
        id
    }

    fn get(&self, id: u32) -> Option<&DynamicObject> {
        self.objects.get(&id)
    }

    fn get_mut(&mut self, id: u32) -> Option<&mut DynamicObject> {
        self.objects.get_mut(&id)
    }

    fn remove(&mut self, id: u32) -> Option<DynamicObject> {
        let res = self.objects.remove(&id);
        if res.is_some() {
            self.free_ids.push(id);
        }
        res
    }
}

#[derive(Debug, Clone)]
enum DynamicObject {
    Uint8Array(Vec<u8>),
    FunctionWrapper(JsValue, JsValue),
    PendingEvent(PendingEvent),
    ValueArray(Vec<GoValue>),
    Date,
}

#[derive(Clone, Debug)]
pub struct PendingEvent {
    pub id: JsValue,
    pub this: JsValue,
    pub args: Vec<GoValue>,
}

#[derive(Clone, Copy, Debug, PartialEq)]
pub enum JsValue {
    Undefined,
    Number(f64),
    Ref(u32),
}

impl JsValue {
    fn assume_num_or_object(self) -> GoValue {
        match self {
            JsValue::Undefined => GoValue::Undefined,
            JsValue::Number(x) => GoValue::Number(x),
            JsValue::Ref(x) => GoValue::Object(x),
        }
    }

    /// Creates a JS runtime value from its native 64-bit floating point representation.
    /// The JS runtime stores handles to references in the NaN bits.
    /// Native 0 is the value called "undefined", and actual 0 is a special-cased NaN.
    /// Anything else that's not a NaN is the Number class.
    pub fn new(repr: u64) -> Self {
        if repr == 0 {
            return Self::Undefined;
        }
        let float = f64::from_bits(repr);
        if float.is_nan() && repr != f64::NAN.to_bits() {
            let id = repr as u32;
            if id == ZERO_ID {
                return Self::Number(0.);
            }
            return Self::Ref(id);
        }
        Self::Number(float)
    }
}

#[derive(Clone, Copy, Debug)]
#[allow(dead_code)]
pub enum GoValue {
    Undefined,
    Number(f64),
    Null,
    Object(u32),
    String(u32),
    Symbol(u32),
    Function(u32),
}

impl GoValue {
    fn encode(self) -> u64 {
        let (ty, id): (u32, u32) = match self {
            GoValue::Undefined => return 0,
            GoValue::Number(mut f) => {
                // Canonicalize NaNs so they don't collide with other value types
                if f.is_nan() {
                    f = f64::NAN;
                }
                if f == 0. {
                    // Zeroes are encoded differently for some reason
                    (0, ZERO_ID)
                } else {
                    return f.to_bits();
                }
            }
            GoValue::Null => (0, NULL_ID),
            GoValue::Object(x) => (1, x),
            GoValue::String(x) => (2, x),
            GoValue::Symbol(x) => (3, x),
            GoValue::Function(x) => (4, x),
        };
        // Must not be all zeroes, otherwise it'd collide with a real NaN
        assert!(ty != 0 || id != 0, "GoValue must not be empty");
        f64::NAN.to_bits() | (u64::from(ty) << 32) | u64::from(id)
    }
}

fn get_field(env: &mut WasmEnv, source: u32, field: &[u8]) -> GoValue {
    use DynamicObject::*;

    if let Some(source) = env.js_state.pool.get(source) {
        return match (source, field) {
            (PendingEvent(event), b"id" | b"this") => event.id.assume_num_or_object(),
            (PendingEvent(event), b"args") => {
                let args = ValueArray(event.args.clone());
                let id = env.js_state.pool.insert(args);
                GoValue::Object(id)
            }
            _ => {
                let field = String::from_utf8_lossy(field);
                eprintln!(
                    "Go trying to access unimplemented unknown JS value {:?} field {field}",
                    source
                );
                GoValue::Undefined
            }
        };
    }

    match (source, field) {
        (GLOBAL_ID, b"Object") => GoValue::Function(OBJECT_ID),
        (GLOBAL_ID, b"Array") => GoValue::Function(ARRAY_ID),
        (GLOBAL_ID, b"process") => GoValue::Object(PROCESS_ID),
        (GLOBAL_ID, b"fs") => GoValue::Object(FS_ID),
        (GLOBAL_ID, b"Uint8Array") => GoValue::Function(UINT8_ARRAY_ID),
        (GLOBAL_ID, b"crypto") => GoValue::Object(CRYPTO_ID),
        (GLOBAL_ID, b"Date") => GoValue::Object(DATE_ID),
        (GLOBAL_ID, b"fetch") => GoValue::Undefined, // Triggers a code path in Go for a fake network impl
        (FS_ID, b"constants") => GoValue::Object(FS_CONSTANTS_ID),
        (
            FS_CONSTANTS_ID,
            b"O_WRONLY" | b"O_RDWR" | b"O_CREAT" | b"O_TRUNC" | b"O_APPEND" | b"O_EXCL",
        ) => GoValue::Number(-1.),
        (GO_ID, b"_pendingEvent") => match &mut env.js_state.pending_event {
            Some(event) => {
                let event = PendingEvent(event.clone());
                let id = env.js_state.pool.insert(event);
                GoValue::Object(id)
            }
            None => GoValue::Null,
        },
        _ => {
            let field = String::from_utf8_lossy(field);
            eprintln!("Go trying to access unimplemented unknown JS value {source} field {field}");
            GoValue::Undefined
        }
    }
}

pub fn js_finalize_ref(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    let pool = &mut env.js_state.pool;

    let val = JsValue::new(sp.read_u64(0));
    match val {
        JsValue::Ref(x) if x < DYNAMIC_OBJECT_ID_BASE => {}
        JsValue::Ref(x) => {
            if pool.remove(x).is_none() {
                eprintln!("Go trying to finalize unknown ref {}", x);
            }
        }
        val => eprintln!("Go trying to finalize {:?}", val),
    }
}

pub fn js_value_get(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    let source = JsValue::new(sp.read_u64(0));
    let field_ptr = sp.read_u64(1);
    let field_len = sp.read_u64(2);
    let field = sp.read_slice(field_ptr, field_len);
    let value = match source {
        JsValue::Ref(id) => get_field(env, id, &field),
        val => {
            let field = String::from_utf8_lossy(&field);
            eprintln!("Go trying to read field {:?} . {field}", val);
            GoValue::Null
        }
    };
    sp.write_u64(3, value.encode());
}

pub fn js_value_set(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    use JsValue::*;

    let source = JsValue::new(sp.read_u64(0));
    let field_ptr = sp.read_u64(1);
    let field_len = sp.read_u64(2);
    let new_value = JsValue::new(sp.read_u64(3));
    let field = sp.read_slice(field_ptr, field_len);
    if source == Ref(GO_ID) && &field == b"_pendingEvent" && new_value == Ref(NULL_ID) {
        env.js_state.pending_event = None;
        return;
    }
    if let Ref(id) = source {
        let source = env.js_state.pool.get(id);
        if let Some(DynamicObject::PendingEvent(_)) = source {
            if field == b"result" {
                return;
            }
        }
    }
    let field = String::from_utf8_lossy(&field);
    eprintln!(
        "Go attempted to set unsupported value {:?} field {field} to {:?}",
        source, new_value,
    );
}

pub fn js_value_index(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);

    macro_rules! fail {
        ($text:expr $(,$args:expr)*) => {{
            eprintln!($text $(,$args)*);
            return sp.write_u64(2, GoValue::Null.encode());
        }};
    }

    let source = match JsValue::new(sp.read_u64(0)) {
        JsValue::Ref(x) => env.js_state.pool.get(x),
        val => fail!("Go attempted to index into {:?}", val),
    };
    let index = match u32::try_from(sp.read_u64(1)) {
        Ok(index) => index as usize,
        Err(err) => fail!("{:?}", err),
    };
    let value = match source {
        Some(DynamicObject::Uint8Array(x)) => x.get(index).map(|x| GoValue::Number(*x as f64)),
        Some(DynamicObject::ValueArray(x)) => x.get(index).cloned(),
        _ => fail!("Go attempted to index into unsupported value {:?}", source),
    };
    let Some(value) = value else {
        fail!("Go indexing out of bounds into {:?} index {index}", source)
    };
    sp.write_u64(2, value.encode());
}

pub fn js_value_call(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let Some(resume) = env.data().exports.resume.clone() else {
        return Escape::failure(format!("wasmer failed to bind {}", "resume".red()));
    };
    let Some(get_stack_pointer) = env.data().exports.get_stack_pointer.clone() else {
        return Escape::failure(format!("wasmer failed to bind {}", "getsp".red()));
    };
    let sp = GoStack::simple(sp, &env);
    let data = env.data_mut();
    let rng = &mut data.go_state.rng;
    let pool = &mut data.js_state.pool;
    use JsValue::*;

    let object = JsValue::new(sp.read_u64(0));
    let method_name_ptr = sp.read_u64(1);
    let method_name_len = sp.read_u64(2);
    let method_name = sp.read_slice(method_name_ptr, method_name_len);
    let args_ptr = sp.read_u64(3);
    let args_len = sp.read_u64(4);
    let args = sp.read_value_slice(args_ptr, args_len);
    let name = String::from_utf8_lossy(&method_name);

    macro_rules! fail {
        ($text:expr $(,$args:expr)*) => {{
            eprintln!($text $(,$args)*);
            sp.write_u64(6, GoValue::Null.encode());
            sp.write_u8(7, 1);
            return Ok(())
        }};
    }

    let value = match (object, method_name.as_slice()) {
        (Ref(GO_ID), b"_makeFuncWrapper") => {
            let arg = match args.get(0) {
                Some(arg) => arg,
                None => fail!(
                    "Go trying to call Go._makeFuncWrapper with bad args {:?}",
                    args
                ),
            };
            let ref_id = pool.insert(DynamicObject::FunctionWrapper(*arg, object));
            GoValue::Function(ref_id)
        }
        (Ref(FS_ID), b"write") => {
            // ignore any args after the 6th, and slice no more than than the number of args we have
            let args_len = std::cmp::min(6, args.len());

            match &args.as_slice()[..args_len] {
                &[Number(fd), Ref(buf_id), Number(offset), Number(length), Ref(NULL_ID), Ref(callback_id)] =>
                {
                    let buf = match pool.get(buf_id) {
                        Some(DynamicObject::Uint8Array(x)) => x,
                        x => fail!("Go trying to call fs.write with bad buffer {:?}", x),
                    };
                    let (func_id, this) = match pool.get(callback_id) {
                        Some(DynamicObject::FunctionWrapper(f, t)) => (f, t),
                        x => fail!("Go trying to call fs.write with bad buffer {:?}", x),
                    };

                    let mut offset = offset as usize;
                    let mut length = length as usize;
                    if offset > buf.len() {
                        eprintln!(
                            "Go trying to call fs.write with offset {offset} >= buf.len() {length}"
                        );
                        offset = buf.len();
                    }
                    if offset + length > buf.len() {
                        eprintln!(
                            "Go trying to call fs.write with offset {offset} + length {length} >= buf.len() {}",
                            buf.len(),
                        );
                        length = buf.len() - offset;
                    }
                    if fd == 1. {
                        let stdout = std::io::stdout();
                        let mut stdout = stdout.lock();
                        stdout.write_all(&buf[offset..(offset + length)]).unwrap();
                    } else if fd == 2. {
                        let stderr = std::io::stderr();
                        let mut stderr = stderr.lock();
                        stderr.write_all(&buf[offset..(offset + length)]).unwrap();
                    } else {
                        eprintln!("Go trying to write to unknown FD {}", fd);
                    }

                    data.js_state.pending_event = Some(PendingEvent {
                        id: *func_id,
                        this: *this,
                        args: vec![
                            GoValue::Null,                  // no error
                            GoValue::Number(length as f64), // amount written
                        ],
                    });

                    // recursively call into wasmer
                    let mut store = env.as_store_mut();
                    resume.call(&mut store)?;

                    // the stack pointer has changed, so we'll need to write our return results elsewhere
                    let pointer = get_stack_pointer.call(&mut store)? as u32;
                    sp.write_u64_ptr(pointer + sp.relative_offset(6), GoValue::Null.encode());
                    sp.write_u8_ptr(pointer + sp.relative_offset(7), 1);
                    return Ok(());
                }
                _ => fail!("Go trying to call fs.write with bad args {:?}", args),
            }
        }
        (Ref(CRYPTO_ID), b"getRandomValues") => {
            let name = "crypto.getRandomValues";

            let id = match args.get(0) {
                Some(Ref(x)) => x,
                _ => fail!("Go trying to call {name} with bad args {:?}", args),
            };

            let buf = match pool.get_mut(*id) {
                Some(DynamicObject::Uint8Array(buf)) => buf,
                Some(x) => fail!("Go trying to call {name} on bad object {:?}", x),
                None => fail!("Go trying to call {name} on unknown reference {id}"),
            };

            rng.fill_bytes(buf.as_mut_slice());
            GoValue::Undefined
        }
        (Ref(obj_id), _) => {
            let value = match pool.get(obj_id) {
                Some(value) => value,
                None => fail!("Go trying to call method {name} for unknown object - id {obj_id}"),
            };
            match value {
                DynamicObject::Date => GoValue::Number(0.0),
                _ => fail!("Go trying to call unknown method {name} for date object"),
            }
        }
        _ => fail!("Go trying to call unknown method {:?} . {name}", object),
    };

    sp.write_u64(6, value.encode());
    sp.write_u8(7, 1);
    Ok(())
}

pub fn js_value_new(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    let pool = &mut env.js_state.pool;

    let class = sp.read_u32(0);
    let args_ptr = sp.read_u64(1);
    let args_len = sp.read_u64(2);
    let args = sp.read_value_slice(args_ptr, args_len);
    match class {
        UINT8_ARRAY_ID => match args.get(0) {
            Some(JsValue::Number(size)) => {
                let id = pool.insert(DynamicObject::Uint8Array(vec![0; *size as usize]));
                sp.write_u64(4, GoValue::Object(id).encode());
                sp.write_u8(5, 1);
                return;
            }
            _ => eprintln!(
                "Go attempted to construct Uint8Array with bad args: {:?}",
                args,
            ),
        },
        DATE_ID => {
            let id = pool.insert(DynamicObject::Date);
            sp.write_u64(4, GoValue::Object(id).encode());
            sp.write_u8(5, 1);
            return;
        }
        _ => eprintln!("Go trying to construct unimplemented JS value {class}"),
    }
    sp.write_u64(4, GoValue::Null.encode());
    sp.write_u8(5, 0);
}

pub fn js_value_length(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);

    let source = match JsValue::new(sp.read_u64(0)) {
        JsValue::Ref(x) => env.js_state.pool.get(x),
        _ => None,
    };
    let length = match source {
        Some(DynamicObject::Uint8Array(x)) => x.len(),
        Some(DynamicObject::ValueArray(x)) => x.len(),
        _ => {
            eprintln!(
                "Go attempted to get length of unsupported value {:?}",
                source,
            );
            0
        }
    };
    sp.write_u64(1, length as u64);
}

pub fn js_copy_bytes_to_go(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);
    let dest_ptr = sp.read_u64(0);
    let dest_len = sp.read_u64(1);
    let src_val = JsValue::new(sp.read_u64(3));

    match src_val {
        JsValue::Ref(src_id) => match env.js_state.pool.get_mut(src_id) {
            Some(DynamicObject::Uint8Array(buf)) => {
                let src_len = buf.len() as u64;
                if src_len != dest_len {
                    eprintln!(
                        "Go copying bytes from JS source length {src_len} to Go dest length {dest_len}",
                    );
                }
                let len = std::cmp::min(src_len, dest_len) as usize;
                sp.write_slice(dest_ptr, &buf[..len]);
                sp.write_u64(4, GoValue::Number(len as f64).encode());
                sp.write_u8(5, 1);
                return;
            }
            source => {
                eprintln!(
                    "Go trying to copy bytes from unsupported source {:?}",
                    source,
                );
            }
        },
        _ => eprintln!("Go trying to copy bytes from {:?}", src_val),
    }

    sp.write_u8(5, 0);
}

pub fn js_copy_bytes_to_js(mut env: WasmEnvMut, sp: u32) {
    let (sp, env) = GoStack::new(sp, &mut env);

    match JsValue::new(sp.read_u64(0)) {
        JsValue::Ref(dest_id) => {
            let src_ptr = sp.read_u64(1);
            let src_len = sp.read_u64(2);

            match env.js_state.pool.get_mut(dest_id) {
                Some(DynamicObject::Uint8Array(buf)) => {
                    let dest_len = buf.len() as u64;
                    if buf.len() as u64 != src_len {
                        eprintln!(
                            "Go copying bytes from Go source length {src_len} to JS dest length {dest_len}",
                        );
                    }
                    let len = std::cmp::min(src_len, dest_len) as usize;

                    // Slightly inefficient as this allocates a new temporary buffer
                    let data = sp.read_slice(src_ptr, len as u64);
                    buf[..len].copy_from_slice(&data);
                    sp.write_u64(4, GoValue::Number(len as f64).encode());
                    sp.write_u8(5, 1);
                    return;
                }
                dest => eprintln!("Go trying to copy bytes into unsupported target {:?}", dest),
            }
        }
        value => eprintln!("Go trying to copy bytes into {:?}", value),
    }

    sp.write_u64(4, GoValue::Null.encode());
    sp.write_u8(5, 0);
}

macro_rules! unimpl_js {
    ($($f:ident),* $(,)?) => {
        $(
            #[no_mangle]
            pub fn $f(_: WasmEnvMut, _: u32) {
                unimplemented!("Go JS interface {} not supported", stringify!($f));
            }
        )*
    }
}

unimpl_js!(
    js_string_val,
    js_value_set_index,
    js_value_prepare_string,
    js_value_load_string,
    js_value_delete,
    js_value_invoke,
    js_value_instance_of,
);

'''
'''--- arbitrator/jit/src/test.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

#![cfg(test)]

use wasmer::{imports, Instance, Module, Store, Value};

#[test]
fn test_crate() -> eyre::Result<()> {
    // Adapted from https://docs.rs/wasmer/3.1.0/wasmer/index.html

    let source = std::fs::read("programs/pure/main.wat")?;

    let mut store = Store::default();
    let module = Module::new(&store, source)?;
    let imports = imports! {};
    let instance = Instance::new(&mut store, &module, &imports)?;

    let add_one = instance.exports.get_function("add_one")?;
    let result = add_one.call(&mut store, &[Value::I32(42)])?;
    assert_eq!(result[0], Value::I32(43));
    Ok(())
}

'''
'''--- arbitrator/jit/src/wavmio.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{
    gostack::GoStack,
    machine::{Escape, Inbox, MaybeEscape, WasmEnv, WasmEnvMut},
    socket,
};

use arbutil::{Color, PreimageType};
use std::{
    io,
    io::{BufReader, BufWriter, ErrorKind},
    net::TcpStream,
    time::Instant,
};

pub type Bytes32 = [u8; 32];

pub fn get_global_state_bytes32(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let global = sp.read_u64(0) as u32 as usize;
    let out_ptr = sp.read_u64(1);
    let mut out_len = sp.read_u64(2) as usize;
    if out_len < 32 {
        eprintln!("Go trying to read block hash into {out_len} bytes long buffer");
    } else {
        out_len = 32;
    }

    let global = match env.large_globals.get(global) {
        Some(global) => global,
        None => return Escape::hostio("global read out of bounds in wavmio.getGlobalStateBytes32"),
    };
    sp.write_slice(out_ptr, &global[..out_len]);
    Ok(())
}

pub fn set_global_state_bytes32(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let global = sp.read_u64(0) as u32 as usize;
    let src_ptr = sp.read_u64(1);
    let src_len = sp.read_u64(2);
    if src_len != 32 {
        eprintln!("Go trying to set 32-byte global with a {src_len} bytes long buffer");
        return Ok(());
    }

    let slice = sp.read_slice(src_ptr, src_len);
    let slice = &slice.try_into().unwrap();
    match env.large_globals.get_mut(global) {
        Some(global) => *global = *slice,
        None => {
            return Escape::hostio("global write out of bounds in wavmio.setGlobalStateBytes32")
        }
    }
    Ok(())
}

pub fn get_global_state_u64(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let global = sp.read_u64(0) as u32 as usize;
    match env.small_globals.get(global) {
        Some(global) => sp.write_u64(1, *global),
        None => return Escape::hostio("global read out of bounds in wavmio.getGlobalStateU64"),
    }
    Ok(())
}

pub fn set_global_state_u64(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let global = sp.read_u64(0) as u32 as usize;
    match env.small_globals.get_mut(global) {
        Some(global) => *global = sp.read_u64(1),
        None => return Escape::hostio("global write out of bounds in wavmio.setGlobalStateU64"),
    }
    Ok(())
}

pub fn read_inbox_message(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let inbox = &env.sequencer_messages;
    inbox_message_impl(&sp, inbox, "wavmio.readInboxMessage")
}

pub fn read_delayed_inbox_message(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    ready_hostio(env)?;

    let inbox = &env.delayed_messages;
    inbox_message_impl(&sp, inbox, "wavmio.readDelayedInboxMessage")
}

/// Reads an inbox message
/// note: the order of the checks is very important.
fn inbox_message_impl(sp: &GoStack, inbox: &Inbox, name: &str) -> MaybeEscape {
    let msg_num = sp.read_u64(0);
    let offset = sp.read_u64(1);
    let out_ptr = sp.read_u64(2);
    let out_len = sp.read_u64(3);
    if out_len != 32 {
        eprintln!("Go trying to read inbox message with out len {out_len} in {name}");
        sp.write_u64(5, 0);
        return Ok(());
    }

    macro_rules! error {
        ($text:expr $(,$args:expr)*) => {{
            let text = format!($text $(,$args)*);
            return Escape::hostio(&text)
        }};
    }

    let message = match inbox.get(&msg_num) {
        Some(message) => message,
        None => error!("missing inbox message {msg_num} in {name}"),
    };

    if out_ptr + 32 > sp.memory_size() {
        error!("unknown message type in {name}");
    }
    let offset = match u32::try_from(offset) {
        Ok(offset) => offset as usize,
        Err(_) => error!("bad offset {offset} in {name}"),
    };

    let len = std::cmp::min(32, message.len().saturating_sub(offset));
    let read = message.get(offset..(offset + len)).unwrap_or_default();
    sp.write_slice(out_ptr, read);
    sp.write_u64(5, read.len() as u64);
    Ok(())
}

#[deprecated] // we're just keeping this around until we no longer need to validate old replay binaries
pub fn resolve_keccak_preimage(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (sp, env) = GoStack::new(sp, &mut env);
    resolve_preimage_impl(env, sp, 0, "wavmio.ResolvePreImage")
}

pub fn resolve_typed_preimage(mut env: WasmEnvMut, sp: u32) -> MaybeEscape {
    let (mut sp, env) = GoStack::new(sp, &mut env);
    let preimage_type = sp.read_u8(0);
    sp.shift_start(8); // to account for the preimage type being the first slot
    resolve_preimage_impl(env, sp, preimage_type, "wavmio.ResolveTypedPreimage")
}

pub fn resolve_preimage_impl(
    env: &mut WasmEnv,
    sp: GoStack,
    preimage_type: u8,
    name: &str,
) -> MaybeEscape {
    let hash_ptr = sp.read_u64(0);
    let hash_len = sp.read_u64(1);
    let offset = sp.read_u64(3);
    let out_ptr = sp.read_u64(4);
    let out_len = sp.read_u64(5);
    if hash_len != 32 || out_len != 32 {
        eprintln!("Go trying to resolve pre image with hash len {hash_len} and out len {out_len}");
        sp.write_u64(7, 0);
        return Ok(());
    }

    let Ok(preimage_type) = preimage_type.try_into() else {
        eprintln!("Go trying to resolve pre image with unknown type {preimage_type}");
        sp.write_u64(7, 0);
        return Ok(());
    };

    macro_rules! error {
        ($text:expr $(,$args:expr)*) => {{
            let text = format!($text $(,$args)*);
            return Escape::hostio(&text)
        }};
    }

    let hash = sp.read_slice(hash_ptr, hash_len);
    let hash: &[u8; 32] = &hash.try_into().unwrap();
    let hash_hex = hex::encode(hash);

    let Some(preimage) = env.preimages.get(&preimage_type).and_then(|m| m.get(hash)) else {
        error!("Missing requested preimage for preimage type {preimage_type:?} hash {hash_hex} in {name}");
    };

    let offset = match u32::try_from(offset) {
        Ok(offset) => offset as usize,
        Err(_) => error!("bad offset {offset} in {name}"),
    };

    let len = std::cmp::min(32, preimage.len().saturating_sub(offset));
    let read = preimage.get(offset..(offset + len)).unwrap_or_default();
    sp.write_slice(out_ptr, read);
    sp.write_u64(7, read.len() as u64);
    Ok(())
}

fn ready_hostio(env: &mut WasmEnv) -> MaybeEscape {
    let debug = env.process.debug;

    if !env.process.reached_wavmio {
        if debug {
            let time = format!("{}ms", env.process.timestamp.elapsed().as_millis());
            println!("Created the machine in {}.", time.pink());
        }
        env.process.timestamp = Instant::now();
        env.process.reached_wavmio = true;
    }

    if !env.process.forks {
        return Ok(());
    }

    unsafe {
        libc::signal(libc::SIGCHLD, libc::SIG_IGN); // avoid making zombies
    }

    let stdin = io::stdin();
    let mut address = String::new();

    loop {
        if let Err(error) = stdin.read_line(&mut address) {
            return match error.kind() {
                ErrorKind::UnexpectedEof => Escape::exit(0),
                error => Escape::hostio(format!("Error reading stdin: {error}")),
            };
        }

        address.pop(); // pop the newline
        if address.is_empty() {
            return Ok(());
        }
        if debug {
            println!("Child will connect to {address}");
        }

        unsafe {
            match libc::fork() {
                -1 => return Escape::hostio("Failed to fork"),
                0 => break,                   // we're the child process
                _ => address = String::new(), // we're the parent process
            }
        }
    }

    env.process.timestamp = Instant::now();
    if debug {
        println!("Connecting to {address}");
    }
    let socket = TcpStream::connect(&address)?;
    socket.set_nodelay(true)?;

    let mut reader = BufReader::new(socket.try_clone()?);
    let stream = &mut reader;

    let inbox_position = socket::read_u64(stream)?;
    let position_within_message = socket::read_u64(stream)?;
    let last_block_hash = socket::read_bytes32(stream)?;
    let last_send_root = socket::read_bytes32(stream)?;

    env.small_globals = [inbox_position, position_within_message];
    env.large_globals = [last_block_hash, last_send_root];

    while socket::read_u8(stream)? == socket::ANOTHER {
        let position = socket::read_u64(stream)?;
        let message = socket::read_bytes(stream)?;
        env.sequencer_messages.insert(position, message);
    }
    while socket::read_u8(stream)? == socket::ANOTHER {
        let position = socket::read_u64(stream)?;
        let message = socket::read_bytes(stream)?;
        env.delayed_messages.insert(position, message);
    }

    let preimage_types = socket::read_u64(stream)?;
    for _ in 0..preimage_types {
        let preimage_ty = PreimageType::try_from(socket::read_u8(stream)?)
            .map_err(|e| Escape::Failure(e.to_string()))?;
        let map = env.preimages.entry(preimage_ty).or_default();
        let preimage_count = socket::read_u64(stream)?;
        for _ in 0..preimage_count {
            let hash = socket::read_bytes32(stream)?;
            let preimage = socket::read_bytes(stream)?;
            map.insert(hash, preimage);
        }
    }

    if socket::read_u8(stream)? != socket::READY {
        return Escape::hostio("failed to parse global state");
    }

    let writer = BufWriter::new(socket);
    env.process.socket = Some((writer, reader));
    env.process.forks = false;
    Ok(())
}

'''
'''--- arbitrator/prover/Cargo.toml ---
[package]
name = "prover"
version = "0.1.0"
edition = "2018"
publish = false

[dependencies]
bincode = "1.3.3"
brotli2 = "0.3.2"
digest = "0.9.0"
eyre = "0.6.5"
fnv = "1.0.7"
hex = "0.4.3"
libc = "0.2.108"
nom = "7.0.0"
nom-leb128 = "0.2.0"
num = "0.4"
rayon = "1.5.1"
rustc-demangle = "0.1.21"
serde = { version = "1.0.130", features = ["derive", "rc"] }
serde_json = "1.0.67"
sha3 = "0.9.1"
static_assertions = "1.1.0"
structopt = "0.3.23"
wasmparser = "0.84.0"
wat = "1.0.56"
serde_with = "1.12.1"
lazy_static = "1.4.0"
smallvec = { version = "1.10.0", features = ["serde"] }
arbutil = { path = "../arbutil/" }

[lib]
name = "prover"
crate-type = ["staticlib","lib"]

'''
'''--- arbitrator/prover/fuzz/Cargo.toml ---
[package]
name = "prover-fuzz"
version = "0.0.0"
authors = ["Automatically generated"]
publish = false
edition = "2018"

[package.metadata]
cargo-fuzz = true

[dependencies]
lazy_static = "1.4.0"
libfuzzer-sys = "0.4"
eyre = "0.6.8"
tokio = { version = "1.18.5", features = ["rt", "rt-multi-thread"] }
serde = { version = "1.0.137", features = ["derive"] }
hex = "0.4.3"
evm = "0.36.0"
serde_json = "1.0.81"
primitive-types = "0.11.1"
rayon = "1.5.1"

[dependencies.prover]
path = ".."

# Prevent this from interfering with workspaces
[workspace]
members = ["."]

[[bin]]
name = "osp"
path = "fuzz_targets/osp.rs"
test = false
doc = false

'''
'''--- arbitrator/prover/fuzz/fuzz_targets/osp.rs ---
#![no_main]
use evm::{
    backend::MemoryAccount,
    executor::stack::{self as evm_stack, StackSubstateMetadata},
};
use eyre::{bail, Result};
use libfuzzer_sys::fuzz_target;
use primitive_types::{H160, U256};
use prover::{
    binary,
    machine::{GlobalState, Machine},
    utils::Bytes32,
    wavm::Opcode,
};
use serde::Deserialize;
use std::{collections::BTreeMap, fs::File, rc::Rc};

const MAX_STEPS: u64 = 200;
const DEBUG: bool = false;
const EVM_CONFIG: evm::Config = evm::Config::london();
const MAX_OSP_GAS: u64 = 15_000_000;

#[derive(Deserialize)]
#[serde(rename_all = "camelCase")]
struct ContractInfo {
    deployed_bytecode: String,
}

fn get_contract_deployed_bytecode(contract: &str) -> Vec<u8> {
    let f = File::open(format!(
        "../../../contracts/build/contracts/src/osp/{0}.sol/{0}.json",
        contract,
    ))
    .expect("Failed to read contract JSON");
    let info: ContractInfo = serde_json::from_reader(f).expect("Failed to parse contract JSON");
    hex::decode(&info.deployed_bytecode[2..]).unwrap()
}

lazy_static::lazy_static! {
    static ref OSP_PREFIX: Vec<u8> = {
        let mut data = Vec::new();
        data.extend(hex::decode("2fae8811").unwrap()); // function selector
        data.extend([0; 32]); // maxInboxMessagesRead
        data.extend([0; 32]); // sequencerInbox
        data.extend([0; 32]); // delayedInbox
        data
    };
    static ref EVM_VICINITY: evm::backend::MemoryVicinity = {
        evm::backend::MemoryVicinity {
            gas_price: Default::default(),
            origin: Default::default(),
            chain_id: Default::default(),
            block_hashes: Default::default(),
            block_number: Default::default(),
            block_coinbase: Default::default(),
            block_timestamp: Default::default(),
            block_difficulty: Default::default(),
            block_gas_limit: Default::default(),
            block_base_fee_per_gas: Default::default(),
        }
    };
    static ref OSP_ENTRY_ADDRESS: H160 = H160::repeat_byte(1);
    static ref OSP_ENTRY_CODE: Vec<u8> = get_contract_deployed_bytecode("OneStepProofEntry");
    static ref EVM_BACKEND: evm::backend::MemoryBackend<'static> = {
        const CONTRACTS: &[&str] = &[
            "OneStepProofEntry",
            "OneStepProver0",
            "OneStepProverMemory",
            "OneStepProverMath",
            "OneStepProverHostIo",
        ];
        let mut state = BTreeMap::new();
        for (i, contract) in CONTRACTS.iter().enumerate() {
            let mut account = MemoryAccount::default();
            if i == 0 {
                account.code = OSP_ENTRY_CODE.clone();
                // Put the other provers' addresses in the entry contract's storage
                for i in 1..CONTRACTS.len() {
                    let mut key = [0u8; 32];
                    key[31] = i as u8 - 1;
                    account.storage.insert(key.into(), H160::repeat_byte(i as u8 + 1).into());
                }
            } else {
                account.code = get_contract_deployed_bytecode(contract);
            }
            state.insert(H160::repeat_byte(i as u8 + 1), account);
        }
        evm::backend::MemoryBackend::new(&*EVM_VICINITY, state)
    };
}

thread_local! {
    static OSP_ENTRY_CODE_RC: Rc<Vec<u8>> = Rc::new(OSP_ENTRY_CODE.clone());
}

fn make_evm_executor() -> evm_stack::StackExecutor<
    'static,
    'static,
    evm_stack::MemoryStackState<'static, 'static, evm::backend::MemoryBackend<'static>>,
    (),
> {
    let stack = evm_stack::MemoryStackState::new(
        StackSubstateMetadata::new(MAX_OSP_GAS, &EVM_CONFIG),
        &*EVM_BACKEND,
    );
    evm_stack::StackExecutor::new_with_precompiles(stack, &EVM_CONFIG, &())
}

fn test_proof(
    before_hash: Bytes32,
    steps: u64,
    opcode: Option<Opcode>,
    proof: Vec<u8>,
    after_hash: Bytes32,
) -> Result<()> {
    let mut data = OSP_PREFIX.clone();
    data.extend([0u8; (32 - 8)]);
    data.extend(steps.to_be_bytes());
    data.extend(before_hash);
    let proof_offset = data.len() + 32 - 4;
    data.extend([0u8; (32 - 8)]);
    data.extend(proof_offset.to_be_bytes());
    data.extend([0u8; (32 - 8)]);
    data.extend(proof.len().to_be_bytes());
    data.extend(&proof);
    if proof.len() % 32 != 0 {
        data.extend(std::iter::repeat(0).take(32 - (proof.len() % 32)));
    }
    if DEBUG {
        println!("Proving {:?} with {}", opcode, hex::encode(&data));
    }
    let code = OSP_ENTRY_CODE_RC.with(|code| code.clone());
    let context = evm::Context {
        address: *OSP_ENTRY_ADDRESS,
        caller: H160::default(),
        apparent_value: U256::default(),
    };
    let mut runtime = evm::Runtime::new(code, Rc::new(data), context, &EVM_CONFIG);
    let mut handler = make_evm_executor();
    let res = match runtime.run(&mut handler) {
        evm::Capture::Exit(res) => res,
        evm::Capture::Trap(_) => bail!("hit trap executing EVM"),
    };
    match res {
        evm::ExitReason::Succeed(_) => {
            let result = runtime.machine().return_value();
            if result.as_slice() != *after_hash {
                bail!(
                    "executing {:?} expecting after hash {} but got {}",
                    opcode,
                    after_hash,
                    hex::encode(result),
                );
            }
        }
        evm::ExitReason::Revert(_) => {
            let result = runtime.machine().return_value();
            if result.is_empty() {
                bail!("execution reverted");
            } else if result.len() >= 64
                && result[..31].iter().all(|b| *b == 0)
                && result[31] == 32
                && result[32..48].iter().all(|b| *b == 0)
            {
                bail!(
                    "execution reverted: {} ({})",
                    String::from_utf8_lossy(&result[64..]),
                    hex::encode(&result),
                );
            } else {
                bail!("execution reverted: {}", hex::encode(&result));
            }
        }
        evm::ExitReason::Error(err) => {
            bail!("EVM hit error: {:?}", err);
        }
        evm::ExitReason::Fatal(err) => {
            bail!("EVM hit fatal error: {:?}", err);
        }
    }
    Ok(())
}

fn fuzz_impl(data: &[u8]) -> Result<()> {
    let wavm_binary = binary::parse(data)?;
    let mut mach = Machine::from_binaries(
        &[],
        wavm_binary,
        true,
        true,
        false,
        GlobalState::default(),
        Default::default(),
        prover::machine::get_empty_preimage_resolver(),
    )?;
    let mut last_hash = mach.hash();
    while mach.get_steps() <= MAX_STEPS {
        let proof = mach.serialize_proof();
        let op = mach.get_next_instruction().map(|i| i.opcode);
        if DEBUG {
            println!("Executing {:?} with stack {:?}", op, mach.get_data_stack());
        }
        mach.step_n(1).expect("Failed to execute machine step");
        let new_hash = mach.hash();
        test_proof(last_hash, mach.get_steps(), op, proof, new_hash)
            .expect("Failed to validate proof");
        if new_hash == last_hash {
            break;
        }
        last_hash = new_hash;
    }
    Ok(())
}

static CONFIGURE_RAYON: std::sync::Once = std::sync::Once::new();

fuzz_target!(|data: &[u8]| {
    CONFIGURE_RAYON.call_once(|| {
        std::env::set_var("RAYON_NUM_THREADS", "1"); // in case a different version of rayon is loaded
        rayon::ThreadPoolBuilder::new()
            .num_threads(1)
            .build_global()
            .expect("Failed to configure global Rayon thread pool");
    });
    if let Err(err) = fuzz_impl(data) {
        if DEBUG {
            eprintln!("Non-critical error: {}", err);
        }
    }
});

'''
'''--- arbitrator/prover/src/binary.rs ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

use crate::value::{ArbValueType, FunctionType, IntegerValType, Value as LirValue};
use eyre::{bail, ensure, Result};
use fnv::FnvHashMap as HashMap;
use nom::{
    branch::alt,
    bytes::complete::tag,
    combinator::{all_consuming, map, value},
    sequence::{preceded, tuple},
};
use serde::{Deserialize, Serialize};
use std::{convert::TryInto, hash::Hash, str::FromStr};
use wasmparser::{
    Data, Element, Export, Global, Import, MemoryType, Name, NameSectionReader, Naming, Operator,
    Parser, Payload, TableType, TypeDef,
};

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FloatType {
    F32,
    F64,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FloatUnOp {
    Abs,
    Neg,
    Ceil,
    Floor,
    Trunc,
    Nearest,
    Sqrt,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FloatBinOp {
    Add,
    Sub,
    Mul,
    Div,
    Min,
    Max,
    CopySign,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FloatRelOp {
    Eq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FloatInstruction {
    UnOp(FloatType, FloatUnOp),
    BinOp(FloatType, FloatBinOp),
    RelOp(FloatType, FloatRelOp),
    /// The bools represent (saturating, signed)
    TruncIntOp(IntegerValType, FloatType, bool, bool),
    ConvertIntOp(FloatType, IntegerValType, bool),
    F32DemoteF64,
    F64PromoteF32,
}

impl FloatInstruction {
    pub fn signature(&self) -> FunctionType {
        match *self {
            FloatInstruction::UnOp(t, _) => FunctionType::new(vec![t.into()], vec![t.into()]),
            FloatInstruction::BinOp(t, _) => FunctionType::new(vec![t.into(); 2], vec![t.into()]),
            FloatInstruction::RelOp(t, _) => {
                FunctionType::new(vec![t.into(); 2], vec![ArbValueType::I32])
            }
            FloatInstruction::TruncIntOp(i, f, ..) => {
                FunctionType::new(vec![f.into()], vec![i.into()])
            }
            FloatInstruction::ConvertIntOp(f, i, _) => {
                FunctionType::new(vec![i.into()], vec![f.into()])
            }
            FloatInstruction::F32DemoteF64 => {
                FunctionType::new(vec![ArbValueType::F64], vec![ArbValueType::F32])
            }
            FloatInstruction::F64PromoteF32 => {
                FunctionType::new(vec![ArbValueType::F32], vec![ArbValueType::F64])
            }
        }
    }
}

impl FromStr for FloatInstruction {
    type Err = String;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        type IResult<'a, T> = nom::IResult<&'a str, T, nom::error::Error<&'a str>>;

        fn parse_fp_type(s: &str) -> IResult<FloatType> {
            alt((
                value(FloatType::F32, tag("f32")),
                value(FloatType::F64, tag("f64")),
            ))(s)
        }

        fn parse_signedness(s: &str) -> IResult<bool> {
            alt((value(true, tag("s")), value(false, tag("u"))))(s)
        }

        fn parse_int_type(s: &str) -> IResult<IntegerValType> {
            alt((
                value(IntegerValType::I32, tag("i32")),
                value(IntegerValType::I64, tag("i64")),
            ))(s)
        }

        fn parse_un_op(s: &str) -> IResult<FloatUnOp> {
            alt((
                value(FloatUnOp::Abs, tag("abs")),
                value(FloatUnOp::Neg, tag("neg")),
                value(FloatUnOp::Ceil, tag("ceil")),
                value(FloatUnOp::Floor, tag("floor")),
                value(FloatUnOp::Trunc, tag("trunc")),
                value(FloatUnOp::Nearest, tag("nearest")),
                value(FloatUnOp::Sqrt, tag("sqrt")),
            ))(s)
        }

        fn parse_bin_op(s: &str) -> IResult<FloatBinOp> {
            alt((
                value(FloatBinOp::Add, tag("add")),
                value(FloatBinOp::Sub, tag("sub")),
                value(FloatBinOp::Mul, tag("mul")),
                value(FloatBinOp::Div, tag("div")),
                value(FloatBinOp::Min, tag("min")),
                value(FloatBinOp::Max, tag("max")),
                value(FloatBinOp::CopySign, tag("copysign")),
            ))(s)
        }

        fn parse_rel_op(s: &str) -> IResult<FloatRelOp> {
            alt((
                value(FloatRelOp::Eq, tag("eq")),
                value(FloatRelOp::Ne, tag("ne")),
                value(FloatRelOp::Lt, tag("lt")),
                value(FloatRelOp::Gt, tag("gt")),
                value(FloatRelOp::Le, tag("le")),
                value(FloatRelOp::Ge, tag("ge")),
            ))(s)
        }

        let inst = alt((
            map(
                all_consuming(tuple((parse_fp_type, tag("_"), parse_un_op))),
                |(t, _, o)| FloatInstruction::UnOp(t, o),
            ),
            map(
                all_consuming(tuple((parse_fp_type, tag("_"), parse_bin_op))),
                |(t, _, o)| FloatInstruction::BinOp(t, o),
            ),
            map(
                all_consuming(tuple((parse_fp_type, tag("_"), parse_rel_op))),
                |(t, _, o)| FloatInstruction::RelOp(t, o),
            ),
            map(
                all_consuming(tuple((
                    parse_int_type,
                    alt((
                        value(true, tag("_trunc_sat_")),
                        value(false, tag("_trunc_")),
                    )),
                    parse_fp_type,
                    tag("_"),
                    parse_signedness,
                ))),
                |(i, sat, f, _, s)| FloatInstruction::TruncIntOp(i, f, sat, s),
            ),
            map(
                all_consuming(tuple((
                    parse_fp_type,
                    tag("_convert_"),
                    parse_int_type,
                    tag("_"),
                    parse_signedness,
                ))),
                |(f, _, i, _, s)| FloatInstruction::ConvertIntOp(f, i, s),
            ),
            value(
                FloatInstruction::F32DemoteF64,
                all_consuming(tag("f32_demote_f64")),
            ),
            value(
                FloatInstruction::F64PromoteF32,
                all_consuming(tag("f64_promote_f32")),
            ),
        ));

        let res = preceded(tag("wavm__"), inst)(s);

        res.map(|(_, i)| i).map_err(|e| e.to_string())
    }
}

pub fn op_as_const(op: Operator) -> Result<LirValue> {
    match op {
        Operator::I32Const { value } => Ok(LirValue::I32(value as u32)),
        Operator::I64Const { value } => Ok(LirValue::I64(value as u64)),
        Operator::F32Const { value } => Ok(LirValue::F32(f32::from_bits(value.bits()))),
        Operator::F64Const { value } => Ok(LirValue::F64(f64::from_bits(value.bits()))),
        _ => bail!("Opcode is not a constant"),
    }
}

#[derive(Clone, Debug, Default)]
pub struct Code<'a> {
    pub locals: Vec<Local>,
    pub expr: Vec<Operator<'a>>,
}

#[derive(Clone, Debug)]
pub struct Local {
    pub index: u32,
    pub value: ArbValueType,
}

#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize)]
pub struct NameCustomSection {
    pub module: String,
    pub functions: HashMap<u32, String>,
}

#[derive(Clone, Default)]
pub struct WasmBinary<'a> {
    pub types: Vec<FunctionType>,
    pub imports: Vec<Import<'a>>,
    pub functions: Vec<u32>,
    pub tables: Vec<TableType>,
    pub memories: Vec<MemoryType>,
    pub globals: Vec<Global<'a>>,
    pub exports: Vec<Export<'a>>,
    pub start: Option<u32>,
    pub elements: Vec<Element<'a>>,
    pub codes: Vec<Code<'a>>,
    pub datas: Vec<Data<'a>>,
    pub names: NameCustomSection,
}

pub fn parse(input: &[u8]) -> eyre::Result<WasmBinary<'_>> {
    let features = wasmparser::WasmFeatures {
        mutable_global: true,
        saturating_float_to_int: true,
        sign_extension: true,
        reference_types: false,
        multi_value: true,
        bulk_memory: true, // not all ops supported yet
        simd: false,
        relaxed_simd: false,
        threads: false,
        tail_call: false,
        deterministic_only: false,
        multi_memory: false,
        exceptions: false,
        memory64: false,
        extended_const: false,
        component_model: false,
    };
    wasmparser::Validator::new_with_features(features).validate_all(input)?;
    let sections: Vec<_> = Parser::new(0).parse_all(input).collect::<Result<_, _>>()?;

    let mut binary = WasmBinary::default();

    for mut section in sections {
        use Payload::*;

        macro_rules! process {
            ($dest:expr, $source:expr) => {{
                for _ in 0..$source.get_count() {
                    let item = $source.read()?;
                    $dest.push(item.into())
                }
            }};
        }

        match &mut section {
            TypeSection(type_section) => {
                for _ in 0..type_section.get_count() {
                    let TypeDef::Func(ty) = type_section.read()?;
                    binary.types.push(ty.try_into()?);
                }
            }
            CodeSectionEntry(codes) => {
                let mut code = Code::default();
                let mut locals = codes.get_locals_reader()?;
                let mut ops = codes.get_operators_reader()?;

                let mut index = 0;

                for _ in 0..locals.get_count() {
                    let (count, value) = locals.read()?;
                    for _ in 0..count {
                        code.locals.push(Local {
                            index,
                            value: value.try_into()?,
                        });
                        index += 1;
                    }
                }
                while !ops.eof() {
                    code.expr.push(ops.read()?);
                }

                binary.codes.push(code);
            }
            ImportSection(imports) => process!(binary.imports, imports),
            FunctionSection(functions) => process!(binary.functions, functions),
            TableSection(tables) => process!(binary.tables, tables),
            MemorySection(memories) => process!(binary.memories, memories),
            GlobalSection(globals) => process!(binary.globals, globals),
            ExportSection(exports) => process!(binary.exports, exports),
            StartSection { func, .. } => binary.start = Some(*func),
            ElementSection(elements) => process!(binary.elements, elements),
            DataSection(datas) => process!(binary.datas, datas),
            CodeSectionStart { .. } => {}
            CustomSection {
                name,
                data_offset,
                data,
                ..
            } => {
                if *name != "name" {
                    continue;
                }

                let mut name_reader = NameSectionReader::new(data, *data_offset)?;

                while !name_reader.eof() {
                    match name_reader.read()? {
                        Name::Module(name) => binary.names.module = name.get_name()?.to_owned(),
                        Name::Function(namemap) => {
                            let mut map_reader = namemap.get_map()?;
                            for _ in 0..map_reader.get_count() {
                                let Naming { index, name } = map_reader.read()?;
                                binary.names.functions.insert(index, name.to_owned());
                            }
                        }
                        _ => {}
                    }
                }
            }
            Version { num, .. } => ensure!(*num == 1, "wasm format version not supported {}", num),
            UnknownSection { id, .. } => bail!("unsupported unknown section type {}", id),
            End(_offset) => {}
            x => bail!("unsupported section type {:?}", x),
        }
    }

    Ok(binary)
}

'''
'''--- arbitrator/prover/src/host.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

#![allow(clippy::vec_init_then_push)]

use crate::{
    binary, host,
    machine::{Function, InboxIdentifier},
    utils,
    value::{ArbValueType, FunctionType},
    wavm::{wasm_to_wavm, Instruction, Opcode},
};
use arbutil::{Color, PreimageType};
use eyre::{bail, ErrReport, Result};
use lazy_static::lazy_static;
use std::{collections::HashMap, str::FromStr};

/// Represents the internal hostio functions a module may have.
#[derive(Clone, Copy)]
#[repr(u64)]
pub enum InternalFunc {
    WavmCallerLoad8,
    WavmCallerLoad32,
    WavmCallerStore8,
    WavmCallerStore32,
    MemoryFill,
    MemoryCopy,
}

impl InternalFunc {
    pub fn ty(&self) -> FunctionType {
        use ArbValueType::*;
        use InternalFunc::*;
        match self {
            WavmCallerLoad8 | WavmCallerLoad32 => FunctionType::new(vec![I32], vec![I32]),
            WavmCallerStore8 | WavmCallerStore32 => FunctionType::new(vec![I32, I32], vec![]),
            MemoryFill | MemoryCopy => FunctionType::new(vec![I32, I32, I32], vec![]),
        }
    }
}

/// Represents the internal hostio functions a module may have.
pub enum Hostio {
    WavmCallerLoad8,
    WavmCallerLoad32,
    WavmCallerStore8,
    WavmCallerStore32,
    WavmGetGlobalStateBytes32,
    WavmSetGlobalStateBytes32,
    WavmGetGlobalStateU64,
    WavmSetGlobalStateU64,
    WavmReadKeccakPreimage,
    WavmReadSha256Preimage,
    WavmReadInboxMessage,
    WavmReadDelayedInboxMessage,
    WavmHaltAndSetFinished,
}

impl FromStr for Hostio {
    type Err = ErrReport;

    fn from_str(s: &str) -> Result<Self> {
        let (module, name) = utils::split_import(s)?;

        use Hostio::*;
        Ok(match (module, name) {
            ("env", "wavm_caller_load8") => WavmCallerLoad8,
            ("env", "wavm_caller_load32") => WavmCallerLoad32,
            ("env", "wavm_caller_store8") => WavmCallerStore8,
            ("env", "wavm_caller_store32") => WavmCallerStore32,
            ("env", "wavm_get_globalstate_bytes32") => WavmGetGlobalStateBytes32,
            ("env", "wavm_set_globalstate_bytes32") => WavmSetGlobalStateBytes32,
            ("env", "wavm_get_globalstate_u64") => WavmGetGlobalStateU64,
            ("env", "wavm_set_globalstate_u64") => WavmSetGlobalStateU64,
            ("env", "wavm_read_keccak_256_preimage") => WavmReadKeccakPreimage,
            ("env", "wavm_read_sha2_256_preimage") => WavmReadSha256Preimage,
            ("env", "wavm_read_inbox_message") => WavmReadInboxMessage,
            ("env", "wavm_read_delayed_inbox_message") => WavmReadDelayedInboxMessage,
            ("env", "wavm_halt_and_set_finished") => WavmHaltAndSetFinished,
            _ => bail!("no such hostio {} in {}", name.red(), module.red()),
        })
    }
}

impl Hostio {
    pub fn ty(&self) -> FunctionType {
        use ArbValueType::*;
        use Hostio::*;

        macro_rules! func {
            () => {
                FunctionType::default()
            };
            ([$($args:expr),*]) => {
                FunctionType::new(vec![$($args),*], vec![])
            };
            ([$($args:expr),*], [$($outs:expr),*]) => {
                FunctionType::new(vec![$($args),*], vec![$($outs),*])
            };
        }

        #[rustfmt::skip]
        let ty = match self {
            WavmCallerLoad8             => InternalFunc::WavmCallerLoad8.ty(),
            WavmCallerLoad32            => InternalFunc::WavmCallerLoad32.ty(),
            WavmCallerStore8            => InternalFunc::WavmCallerStore8.ty(),
            WavmCallerStore32           => InternalFunc::WavmCallerStore32.ty(),
            WavmGetGlobalStateBytes32   => func!([I32, I32]),
            WavmSetGlobalStateBytes32   => func!([I32, I32]),
            WavmGetGlobalStateU64       => func!([I32], [I64]),
            WavmSetGlobalStateU64       => func!([I32, I64]),
            WavmReadKeccakPreimage      => func!([I32, I32], [I32]),
            WavmReadSha256Preimage      => func!([I32, I32], [I32]),
            WavmReadInboxMessage        => func!([I64, I32, I32], [I32]),
            WavmReadDelayedInboxMessage => func!([I64, I32, I32], [I32]),
            WavmHaltAndSetFinished      => func!(),
        };
        ty
    }

    pub fn body(&self) -> Vec<Instruction> {
        let mut body = vec![];

        macro_rules! opcode {
            ($opcode:expr) => {
                body.push(Instruction::simple($opcode))
            };
            ($opcode:expr, $value:expr) => {
                body.push(Instruction::with_data($opcode, $value as u64))
            };
        }

        use Hostio::*;
        use Opcode::*;
        match self {
            WavmCallerLoad8 => {
                opcode!(LocalGet, 0);
                opcode!(CallerModuleInternalCall, InternalFunc::WavmCallerLoad8);
            }
            WavmCallerLoad32 => {
                opcode!(LocalGet, 0);
                opcode!(CallerModuleInternalCall, InternalFunc::WavmCallerLoad32);
            }
            WavmCallerStore8 => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(CallerModuleInternalCall, InternalFunc::WavmCallerStore8);
            }
            WavmCallerStore32 => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(CallerModuleInternalCall, InternalFunc::WavmCallerStore32);
            }
            WavmGetGlobalStateBytes32 => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(GetGlobalStateBytes32);
            }
            WavmSetGlobalStateBytes32 => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(SetGlobalStateBytes32);
            }
            WavmGetGlobalStateU64 => {
                opcode!(LocalGet, 0);
                opcode!(GetGlobalStateU64);
            }
            WavmSetGlobalStateU64 => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(SetGlobalStateU64);
            }
            WavmReadKeccakPreimage => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(ReadPreImage, PreimageType::Keccak256);
            }
            WavmReadSha256Preimage => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(ReadPreImage, PreimageType::Sha2_256);
            }
            WavmReadInboxMessage => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(LocalGet, 2);
                opcode!(ReadInboxMessage, InboxIdentifier::Sequencer);
            }
            WavmReadDelayedInboxMessage => {
                opcode!(LocalGet, 0);
                opcode!(LocalGet, 1);
                opcode!(LocalGet, 2);
                opcode!(ReadInboxMessage, InboxIdentifier::Delayed);
            }
            WavmHaltAndSetFinished => {
                opcode!(HaltAndSetFinished);
            }
        }
        body
    }
}

pub fn get_impl(module: &str, name: &str) -> Result<Function> {
    let hostio: Hostio = format!("{module}__{name}").parse()?;

    let append = |code: &mut Vec<Instruction>| {
        code.extend(hostio.body());
        Ok(())
    };
    Function::new(&[], append, hostio.ty(), &[])
}

/// Adds internal functions to a module.
/// Note: the order of the functions must match that of the `InternalFunc` enum
pub fn new_internal_funcs() -> Vec<Function> {
    use ArbValueType::*;
    use InternalFunc::*;
    use Opcode::*;

    fn code_func(code: Vec<Instruction>, ty: FunctionType) -> Function {
        let mut wavm = vec![Instruction::simple(InitFrame)];
        wavm.extend(code);
        wavm.push(Instruction::simple(Return));
        Function::new_from_wavm(wavm, ty, vec![])
    }

    fn op_func(opcode: Opcode, func: InternalFunc) -> Function {
        code_func(vec![Instruction::simple(opcode)], func.ty())
    }

    let mut funcs = vec![];
    let mut add_func = |func, internal| {
        assert_eq!(funcs.len(), internal as usize);
        funcs.push(func)
    };
    let mut add_op_func = |opcode, internal| add_func(op_func(opcode, internal), internal);

    // order matters!
    add_op_func(
        MemoryLoad {
            ty: I32,
            bytes: 1,
            signed: false,
        },
        WavmCallerLoad8,
    );
    add_op_func(
        MemoryLoad {
            ty: I32,
            bytes: 4,
            signed: false,
        },
        WavmCallerLoad32,
    );
    add_op_func(MemoryStore { ty: I32, bytes: 1 }, WavmCallerStore8);
    add_op_func(MemoryStore { ty: I32, bytes: 4 }, WavmCallerStore32);

    let [memory_fill, memory_copy] = (*BULK_MEMORY_FUNCS).clone();
    add_func(memory_fill, MemoryFill);
    add_func(memory_copy, MemoryCopy);
    funcs
}

lazy_static! {
    static ref BULK_MEMORY_FUNCS: [Function; 2] = {
        use host::InternalFunc::*;

        let data = include_bytes!("bulk_memory.wat");
        let wasm = wat::parse_bytes(data).expect("failed to parse bulk_memory.wat");
        let bin = binary::parse(&wasm).expect("failed to parse bulk_memory.wasm");
        let types = [MemoryFill.ty(), MemoryCopy.ty()];
        let names = ["memory_fill", "memory_copy"];

        [0, 1].map(|i| {
            let code = &bin.codes[i];
            let name = bin.names.functions.get(&(i as u32)).unwrap();
            let ty = &bin.types[bin.functions[i] as usize];
            assert_eq!(ty, &types[i]);
            assert_eq!(name, names[i]);

            let func = Function::new(
                &code.locals,
                |wasm| wasm_to_wavm(
                    &code.expr,
                    wasm,
                    &HashMap::default(), // impls don't use floating point
                    &[],                // impls don't make calls
                    &[ty.clone()],      // only type needed is the func itself
                    0,                  // -----------------------------------
                    0,                  // impls don't use other internals
                ),
                ty.clone(),
                &[] // impls don't make calls
            );
            func.expect("failed to create bulk memory func")
        })
    };
}

'''
'''--- arbitrator/prover/src/lib.rs ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

#![allow(clippy::missing_safety_doc, clippy::too_many_arguments)]

pub mod binary;
mod host;
pub mod machine;
/// cbindgen:ignore
mod memory;
mod merkle;
mod reinterpret;
pub mod utils;
pub mod value;
pub mod wavm;

use crate::machine::{argument_data_to_inbox, Machine};
use arbutil::PreimageType;
use eyre::Result;
use machine::{get_empty_preimage_resolver, GlobalState, MachineStatus, PreimageResolver};
use sha3::{Digest, Keccak256};
use static_assertions::const_assert_eq;
use std::{
    ffi::CStr,
    os::raw::{c_char, c_int},
    path::Path,
    sync::{
        atomic::{self, AtomicU8},
        Arc,
    },
};
use utils::{Bytes32, CBytes};

#[repr(C)]
#[derive(Clone, Copy)]
pub struct CByteArray {
    pub ptr: *const u8,
    pub len: usize,
}

#[repr(C)]
#[derive(Clone, Copy)]
pub struct RustByteArray {
    pub ptr: *mut u8,
    pub len: usize,
    pub capacity: usize,
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_load_machine(
    binary_path: *const c_char,
    library_paths: *const *const c_char,
    library_paths_size: isize,
) -> *mut Machine {
    match arbitrator_load_machine_impl(binary_path, library_paths, library_paths_size) {
        Ok(mach) => mach,
        Err(err) => {
            eprintln!("Error loading binary: {}", err);
            std::ptr::null_mut()
        }
    }
}

unsafe fn arbitrator_load_machine_impl(
    binary_path: *const c_char,
    library_paths: *const *const c_char,
    library_paths_size: isize,
) -> Result<*mut Machine> {
    let binary_path = cstr_to_string(binary_path);
    let binary_path = Path::new(&binary_path);

    let mut libraries = vec![];
    for i in 0..library_paths_size {
        let path = cstr_to_string(*(library_paths.offset(i)));
        libraries.push(Path::new(&path).to_owned());
    }

    let mach = Machine::from_paths(
        &libraries,
        binary_path,
        true,
        false,
        false,
        Default::default(),
        Default::default(),
        get_empty_preimage_resolver(),
    )?;
    Ok(Box::into_raw(Box::new(mach)))
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_load_wavm_binary(binary_path: *const c_char) -> *mut Machine {
    let binary_path = cstr_to_string(binary_path);
    let binary_path = Path::new(&binary_path);
    match Machine::new_from_wavm(binary_path) {
        Ok(mach) => Box::into_raw(Box::new(mach)),
        Err(err) => {
            eprintln!("Error loading binary: {}", err);
            std::ptr::null_mut()
        }
    }
}

unsafe fn cstr_to_string(c_str: *const c_char) -> String {
    CStr::from_ptr(c_str).to_string_lossy().into_owned()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_free_machine(mach: *mut Machine) {
    drop(Box::from_raw(mach));
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_clone_machine(mach: *mut Machine) -> *mut Machine {
    let new_mach = (*mach).clone();
    Box::into_raw(Box::new(new_mach))
}

/// Go doesn't have this functionality builtin for whatever reason. Uses relaxed ordering.
#[no_mangle]
pub unsafe extern "C" fn atomic_u8_store(ptr: *mut u8, contents: u8) {
    (*(ptr as *mut AtomicU8)).store(contents, atomic::Ordering::Relaxed);
}

fn err_to_c_string(err: eyre::Report) -> *mut libc::c_char {
    let err = format!("{:#}", err);
    unsafe {
        let buf = libc::malloc(err.len() + 1);
        if buf.is_null() {
            panic!("Failed to allocate memory for error string");
        }
        std::ptr::copy_nonoverlapping(err.as_ptr(), buf as *mut u8, err.len());
        *(buf.add(err.len()) as *mut u8) = 0;
        buf as *mut libc::c_char
    }
}

/// Runs the machine while the condition variable is zero. May return early if num_steps is hit.
/// Returns a c string error (freeable with libc's free) on error, or nullptr on success.
#[no_mangle]
pub unsafe extern "C" fn arbitrator_step(
    mach: *mut Machine,
    num_steps: u64,
    condition: *const u8,
) -> *mut libc::c_char {
    let mach = &mut *mach;
    let condition = &*(condition as *const AtomicU8);
    let mut remaining_steps = num_steps;
    while condition.load(atomic::Ordering::Relaxed) == 0 {
        if remaining_steps == 0 || mach.is_halted() {
            break;
        }
        let stepping = std::cmp::min(remaining_steps, 1_000_000);
        match mach.step_n(stepping) {
            Ok(()) => {}
            Err(err) => return err_to_c_string(err),
        }
        remaining_steps -= stepping;
    }
    std::ptr::null_mut()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_add_inbox_message(
    mach: *mut Machine,
    inbox_identifier: u64,
    index: u64,
    data: CByteArray,
) -> c_int {
    let mach = &mut *mach;
    if let Some(identifier) = argument_data_to_inbox(inbox_identifier) {
        let slice = std::slice::from_raw_parts(data.ptr, data.len);
        let data = slice.to_vec();
        mach.add_inbox_msg(identifier, index, data);
        0
    } else {
        1
    }
}

/// Like arbitrator_step, but stops early if it hits a host io operation.
/// Returns a c string error (freeable with libc's free) on error, or nullptr on success.
#[no_mangle]
pub unsafe extern "C" fn arbitrator_step_until_host_io(
    mach: *mut Machine,
    condition: *const u8,
) -> *mut libc::c_char {
    let mach = &mut *mach;
    let condition = &*(condition as *const AtomicU8);
    while condition.load(atomic::Ordering::Relaxed) == 0 {
        for _ in 0..1_000_000 {
            if mach.is_halted() {
                return std::ptr::null_mut();
            }
            if mach.next_instruction_is_host_io() {
                return std::ptr::null_mut();
            }
            match mach.step_n(1) {
                Ok(()) => {}
                Err(err) => return err_to_c_string(err),
            }
        }
    }
    std::ptr::null_mut()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_serialize_state(
    mach: *const Machine,
    path: *const c_char,
) -> c_int {
    let mach = &*mach;
    let res = CStr::from_ptr(path)
        .to_str()
        .map_err(eyre::Report::from)
        .and_then(|path| mach.serialize_state(path));
    if let Err(err) = res {
        eprintln!("Failed to serialize machine state: {}", err);
        1
    } else {
        0
    }
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_deserialize_and_replace_state(
    mach: *mut Machine,
    path: *const c_char,
) -> c_int {
    let mach = &mut *mach;
    let res = CStr::from_ptr(path)
        .to_str()
        .map_err(eyre::Report::from)
        .and_then(|path| mach.deserialize_and_replace_state(path));
    if let Err(err) = res {
        eprintln!("Failed to deserialize machine state: {}", err);
        1
    } else {
        0
    }
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_get_num_steps(mach: *const Machine) -> u64 {
    (*mach).get_steps()
}

pub const ARBITRATOR_MACHINE_STATUS_RUNNING: u8 = 0;
pub const ARBITRATOR_MACHINE_STATUS_FINISHED: u8 = 1;
pub const ARBITRATOR_MACHINE_STATUS_ERRORED: u8 = 2;
pub const ARBITRATOR_MACHINE_STATUS_TOO_FAR: u8 = 3;

// Unfortunately, cbindgen doesn't support constants with non-literal values, so we assert that they're correct.
const_assert_eq!(
    ARBITRATOR_MACHINE_STATUS_RUNNING,
    MachineStatus::Running as u8,
);
const_assert_eq!(
    ARBITRATOR_MACHINE_STATUS_FINISHED,
    MachineStatus::Finished as u8,
);
const_assert_eq!(
    ARBITRATOR_MACHINE_STATUS_ERRORED,
    MachineStatus::Errored as u8,
);
const_assert_eq!(
    ARBITRATOR_MACHINE_STATUS_TOO_FAR,
    MachineStatus::TooFar as u8,
);

/// Returns one of ARBITRATOR_MACHINE_STATUS_*
#[no_mangle]
pub unsafe extern "C" fn arbitrator_get_status(mach: *const Machine) -> u8 {
    (*mach).get_status() as u8
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_global_state(mach: *mut Machine) -> GlobalState {
    (*mach).get_global_state()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_set_global_state(mach: *mut Machine, gs: GlobalState) {
    (*mach).set_global_state(gs);
}

#[repr(C)]
pub struct ResolvedPreimage {
    pub ptr: *mut u8,
    pub len: isize, // negative if not found
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_set_preimage_resolver(
    mach: *mut Machine,
    resolver: unsafe extern "C" fn(u64, u8, *const u8) -> ResolvedPreimage,
) {
    (*mach).set_preimage_resolver(Arc::new(
        move |context: u64, ty: PreimageType, hash: Bytes32| -> Option<CBytes> {
            let res = resolver(context, ty.into(), hash.as_ptr());
            if res.len < 0 {
                return None;
            }
            let data = CBytes::from_raw_parts(res.ptr, res.len as usize);
            let have_hash = Keccak256::digest(&data);
            if have_hash.as_slice() != *hash {
                panic!(
                    "Resolved incorrect data for hash {}: got {}",
                    hash,
                    hex::encode(data),
                );
            }
            Some(data)
        },
    ) as PreimageResolver);
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_set_context(mach: *mut Machine, context: u64) {
    (*mach).set_context(context);
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_hash(mach: *mut Machine) -> utils::Bytes32 {
    (*mach).hash()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_module_root(mach: *mut Machine) -> utils::Bytes32 {
    (*mach).get_modules_root()
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_gen_proof(mach: *mut Machine) -> RustByteArray {
    let mut proof = (*mach).serialize_proof();
    let ret = RustByteArray {
        ptr: proof.as_mut_ptr(),
        len: proof.len(),
        capacity: proof.capacity(),
    };
    std::mem::forget(proof);
    ret
}

#[no_mangle]
pub unsafe extern "C" fn arbitrator_free_proof(proof: RustByteArray) {
    drop(Vec::from_raw_parts(proof.ptr, proof.len, proof.capacity))
}

'''
'''--- arbitrator/prover/src/machine.rs ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

use crate::{
    binary::{parse, FloatInstruction, Local, NameCustomSection, WasmBinary},
    host,
    memory::Memory,
    merkle::{Merkle, MerkleType},
    reinterpret::{ReinterpretAsSigned, ReinterpretAsUnsigned},
    utils::{file_bytes, Bytes32, CBytes, RemoteTableType},
    value::{ArbValueType, FunctionType, IntegerValType, ProgramCounter, Value},
    wavm::{
        pack_cross_module_call, unpack_cross_module_call, wasm_to_wavm, FloatingPointImpls,
        IBinOpType, IRelOpType, IUnOpType, Instruction, Opcode,
    },
};
use arbutil::{Color, PreimageType};
use digest::Digest;
use eyre::{bail, ensure, eyre, Result, WrapErr};
use fnv::FnvHashMap as HashMap;
use num::{traits::PrimInt, Zero};
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use serde_with::serde_as;
use sha3::Keccak256;
use smallvec::SmallVec;
use std::{
    borrow::Cow,
    convert::{TryFrom, TryInto},
    fmt::{self, Display},
    fs::File,
    io::{BufReader, BufWriter, Write},
    num::Wrapping,
    path::{Path, PathBuf},
    sync::Arc,
};
use wasmparser::{DataKind, ElementItem, ElementKind, ExternalKind, Operator, TableType, TypeRef};

fn hash_call_indirect_data(table: u32, ty: &FunctionType) -> Bytes32 {
    let mut h = Keccak256::new();
    h.update("Call indirect:");
    h.update((table as u64).to_be_bytes());
    h.update(ty.hash());
    h.finalize().into()
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum InboxIdentifier {
    Sequencer = 0,
    Delayed,
}

pub fn argument_data_to_inbox(argument_data: u64) -> Option<InboxIdentifier> {
    match argument_data {
        0x0 => Some(InboxIdentifier::Sequencer),
        0x1 => Some(InboxIdentifier::Delayed),
        _ => None,
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Function {
    code: Vec<Instruction>,
    ty: FunctionType,
    #[serde(skip)]
    code_merkle: Merkle,
    local_types: Vec<ArbValueType>,
}

impl Function {
    pub fn new<F: FnOnce(&mut Vec<Instruction>) -> Result<()>>(
        locals: &[Local],
        add_body: F,
        func_ty: FunctionType,
        module_types: &[FunctionType],
    ) -> Result<Function> {
        let mut locals_with_params = func_ty.inputs.clone();
        locals_with_params.extend(locals.iter().map(|x| x.value));

        let mut insts = Vec::new();
        let empty_local_hashes = locals_with_params
            .iter()
            .cloned()
            .map(Value::default_of_type)
            .map(Value::hash)
            .collect::<Vec<_>>();
        insts.push(Instruction {
            opcode: Opcode::InitFrame,
            argument_data: 0,
            proving_argument_data: Some(Merkle::new(MerkleType::Value, empty_local_hashes).root()),
        });
        // Fill in parameters
        for i in (0..func_ty.inputs.len()).rev() {
            insts.push(Instruction {
                opcode: Opcode::LocalSet,
                argument_data: i as u64,
                proving_argument_data: None,
            });
        }

        add_body(&mut insts)?;
        insts.push(Instruction::simple(Opcode::Return));

        // Insert missing proving argument data
        for inst in insts.iter_mut() {
            if inst.opcode == Opcode::CallIndirect {
                let (table, ty) = crate::wavm::unpack_call_indirect(inst.argument_data);
                let ty = &module_types[usize::try_from(ty).unwrap()];
                inst.proving_argument_data = Some(hash_call_indirect_data(table, ty));
            }
        }

        Ok(Function::new_from_wavm(insts, func_ty, locals_with_params))
    }

    pub fn new_from_wavm(
        code: Vec<Instruction>,
        ty: FunctionType,
        local_types: Vec<ArbValueType>,
    ) -> Function {
        assert!(
            u32::try_from(code.len()).is_ok(),
            "Function instruction count doesn't fit in a u32",
        );
        let code_merkle = Merkle::new(
            MerkleType::Instruction,
            code.par_iter().map(|i| i.hash()).collect(),
        );

        Function {
            code,
            ty,
            code_merkle,
            local_types,
        }
    }

    fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Function:");
        h.update(self.code_merkle.root());
        h.finalize().into()
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
struct StackFrame {
    return_ref: Value,
    locals: SmallVec<[Value; 16]>,
    caller_module: u32,
    caller_module_internals: u32,
}

impl StackFrame {
    fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Stack frame:");
        h.update(self.return_ref.hash());
        h.update(
            Merkle::new(
                MerkleType::Value,
                self.locals.iter().map(|v| v.hash()).collect(),
            )
            .root(),
        );
        h.update(self.caller_module.to_be_bytes());
        h.update(self.caller_module_internals.to_be_bytes());
        h.finalize().into()
    }

    fn serialize_for_proof(&self) -> Vec<u8> {
        let mut data = Vec::new();
        data.extend(self.return_ref.serialize_for_proof());
        data.extend(
            Merkle::new(
                MerkleType::Value,
                self.locals.iter().map(|v| v.hash()).collect(),
            )
            .root(),
        );
        data.extend(self.caller_module.to_be_bytes());
        data.extend(self.caller_module_internals.to_be_bytes());
        data
    }
}

#[derive(Clone, Debug, Serialize, Deserialize)]
struct TableElement {
    func_ty: FunctionType,
    val: Value,
}

impl Default for TableElement {
    fn default() -> Self {
        TableElement {
            func_ty: FunctionType::default(),
            val: Value::RefNull,
        }
    }
}

impl TableElement {
    fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Table element:");
        h.update(self.func_ty.hash());
        h.update(self.val.hash());
        h.finalize().into()
    }
}

#[serde_as]
#[derive(Clone, Debug, Serialize, Deserialize)]
struct Table {
    #[serde(with = "RemoteTableType")]
    ty: TableType,
    elems: Vec<TableElement>,
    #[serde(skip)]
    elems_merkle: Merkle,
}

impl Table {
    fn serialize_for_proof(&self) -> Result<Vec<u8>> {
        let mut data = vec![ArbValueType::try_from(self.ty.element_type)?.serialize()];
        data.extend((self.elems.len() as u64).to_be_bytes());
        data.extend(self.elems_merkle.root());
        Ok(data)
    }

    fn hash(&self) -> Result<Bytes32> {
        let mut h = Keccak256::new();
        h.update("Table:");
        h.update([ArbValueType::try_from(self.ty.element_type)?.serialize()]);
        h.update((self.elems.len() as u64).to_be_bytes());
        h.update(self.elems_merkle.root());
        Ok(h.finalize().into())
    }
}

#[derive(Clone, Debug)]
struct AvailableImport {
    ty: FunctionType,
    module: u32,
    func: u32,
}

#[derive(Clone, Debug, Default, Serialize, Deserialize)]
struct Module {
    globals: Vec<Value>,
    memory: Memory,
    tables: Vec<Table>,
    #[serde(skip)]
    tables_merkle: Merkle,
    funcs: Arc<Vec<Function>>,
    #[serde(skip)]
    funcs_merkle: Arc<Merkle>,
    types: Arc<Vec<FunctionType>>,
    internals_offset: u32,
    names: Arc<NameCustomSection>,
    host_call_hooks: Arc<Vec<Option<(String, String)>>>,
    start_function: Option<u32>,
    func_types: Arc<Vec<FunctionType>>,
    exports: Arc<HashMap<String, u32>>,
}

impl Module {
    fn from_binary(
        bin: &WasmBinary,
        available_imports: &HashMap<String, AvailableImport>,
        floating_point_impls: &FloatingPointImpls,
        allow_hostapi: bool,
    ) -> Result<Module> {
        let mut code = Vec::new();
        let mut func_type_idxs: Vec<u32> = Vec::new();
        let mut memory = Memory::default();
        let mut exports = HashMap::default();
        let mut tables = Vec::new();
        let mut host_call_hooks = Vec::new();
        for import in &bin.imports {
            if let TypeRef::Func(ty) = import.ty {
                let mut qualified_name = format!("{}__{}", import.module, import.name);
                qualified_name = qualified_name.replace(&['/', '.'] as &[char], "_");
                let have_ty = &bin.types[ty as usize];
                let func;
                if let Some(import) = available_imports.get(&qualified_name) {
                    ensure!(
                        &import.ty == have_ty,
                        "Import has different function signature than host function. Expected {:?} but got {:?}",
                        import.ty, have_ty,
                    );
                    let wavm = vec![
                        Instruction::simple(Opcode::InitFrame),
                        Instruction::with_data(
                            Opcode::CrossModuleCall,
                            pack_cross_module_call(import.module, import.func),
                        ),
                        Instruction::simple(Opcode::Return),
                    ];
                    func = Function::new_from_wavm(wavm, import.ty.clone(), Vec::new());
                } else {
                    func = host::get_impl(import.module, import.name)?;
                    ensure!(
                        &func.ty == have_ty,
                        "Import has different function signature than host function. Expected {:?} but got {:?}",
                        func.ty, have_ty,
                    );
                    ensure!(
                        allow_hostapi,
                        "Calling hostapi directly is not allowed. Function {}",
                        import.name,
                    );
                }
                func_type_idxs.push(ty);
                code.push(func);
                host_call_hooks.push(Some((import.module.into(), import.name.into())));
            } else {
                bail!("Unsupport import kind {:?}", import);
            }
        }
        func_type_idxs.extend(bin.functions.iter());

        let internals = host::new_internal_funcs();
        let internals_offset = (code.len() + bin.codes.len()) as u32;
        let internals_types = internals.iter().map(|f| f.ty.clone());

        let mut types = bin.types.clone();
        let mut func_types: Vec<_> = func_type_idxs
            .iter()
            .map(|i| types[*i as usize].clone())
            .collect();

        func_types.extend(internals_types.clone());
        types.extend(internals_types);

        for c in &bin.codes {
            let idx = code.len();
            let func_ty = func_types[idx].clone();
            code.push(Function::new(
                &c.locals,
                |code| {
                    wasm_to_wavm(
                        &c.expr,
                        code,
                        floating_point_impls,
                        &func_types,
                        &types,
                        func_type_idxs[idx],
                        internals_offset,
                    )
                },
                func_ty.clone(),
                &types,
            )?);
        }
        code.extend(internals);
        ensure!(
            code.len() < (1usize << 31),
            "Module function count must be under 2^31",
        );

        ensure!(
            bin.memories.len() <= 1,
            "Multiple memories are not supported"
        );
        if let Some(limits) = bin.memories.get(0) {
            let page_size = Memory::PAGE_SIZE;
            let initial = limits.initial; // validate() checks this is less than max::u32
            let allowed = u32::MAX as u64 / Memory::PAGE_SIZE - 1; // we require the size remain *below* 2^32

            let max_size = match limits.maximum {
                Some(pages) => u64::min(allowed, pages),
                _ => allowed,
            };
            if initial > max_size {
                bail!(
                    "Memory inits to a size larger than its max: {} vs {}",
                    limits.initial,
                    max_size
                );
            }
            let size = initial * page_size;

            memory = Memory::new(size as usize, max_size);
        }

        let mut globals = vec![];
        for global in &bin.globals {
            let mut init = global.init_expr.get_operators_reader();

            let value = match (init.read()?, init.read()?, init.eof()) {
                (op, Operator::End, true) => crate::binary::op_as_const(op)?,
                _ => bail!("Non-constant global initializer"),
            };
            globals.push(value);
        }

        for export in &bin.exports {
            if let ExternalKind::Func = export.kind {
                exports.insert(export.name.to_owned(), export.index);
            }
        }

        for data in &bin.datas {
            let (memory_index, mut init) = match data.kind {
                DataKind::Active {
                    memory_index,
                    init_expr,
                } => (memory_index, init_expr.get_operators_reader()),
                _ => continue,
            };
            ensure!(
                memory_index == 0,
                "Attempted to write to nonexistant memory"
            );

            let offset = match (init.read()?, init.read()?, init.eof()) {
                (Operator::I32Const { value }, Operator::End, true) => value as usize,
                x => bail!("Non-constant element segment offset expression {:?}", x),
            };
            if !matches!(
                offset.checked_add(data.data.len()),
                Some(x) if (x as u64) <= memory.size(),
            ) {
                bail!(
                    "Out-of-bounds data memory init with offset {} and size {}",
                    offset,
                    data.data.len(),
                );
            }
            memory.set_range(offset, data.data);
        }

        for table in &bin.tables {
            tables.push(Table {
                elems: vec![TableElement::default(); usize::try_from(table.initial).unwrap()],
                ty: *table,
                elems_merkle: Merkle::default(),
            });
        }

        for elem in &bin.elements {
            let (t, mut init) = match elem.kind {
                ElementKind::Active {
                    table_index,
                    init_expr,
                } => (table_index, init_expr.get_operators_reader()),
                _ => continue,
            };
            let offset = match (init.read()?, init.read()?, init.eof()) {
                (Operator::I32Const { value }, Operator::End, true) => value as usize,
                x => bail!("Non-constant element segment offset expression {:?}", x),
            };
            let table = match tables.get_mut(t as usize) {
                Some(t) => t,
                None => bail!("Element segment for non-exsistent table {}", t),
            };
            let expected_ty = table.ty.element_type;
            ensure!(
                expected_ty == elem.ty,
                "Element type expected to be of table type {:?} but of type {:?}",
                expected_ty,
                elem.ty
            );

            let mut contents = vec![];
            let mut item_reader = elem.items.get_items_reader()?;
            for _ in 0..item_reader.get_count() {
                let item = item_reader.read()?;
                let index = match item {
                    ElementItem::Func(index) => index,
                    ElementItem::Expr(_) => {
                        bail!("Non-constant element initializers are not supported")
                    }
                };
                let func_ty = func_types[index as usize].clone();
                contents.push(TableElement {
                    val: Value::FuncRef(index),
                    func_ty,
                })
            }

            let len = contents.len();
            ensure!(
                offset.saturating_add(len) <= table.elems.len(),
                "Out of bounds element segment at offset {} and length {} for table of length {}",
                offset,
                len,
                table.elems.len(),
            );
            table.elems[offset..][..len].clone_from_slice(&contents);
        }

        let tables_hashes: Result<_, _> = tables.iter().map(Table::hash).collect();

        Ok(Module {
            memory,
            globals,
            tables_merkle: Merkle::new(MerkleType::Table, tables_hashes?),
            tables,
            funcs_merkle: Arc::new(Merkle::new(
                MerkleType::Function,
                code.iter().map(|f| f.hash()).collect(),
            )),
            funcs: Arc::new(code),
            types: Arc::new(types.to_owned()),
            internals_offset,
            names: Arc::new(bin.names.to_owned()),
            host_call_hooks: Arc::new(host_call_hooks),
            start_function: bin.start,
            func_types: Arc::new(func_types),
            exports: Arc::new(exports),
        })
    }

    fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Module:");
        h.update(
            Merkle::new(
                MerkleType::Value,
                self.globals.iter().map(|v| v.hash()).collect(),
            )
            .root(),
        );
        h.update(self.memory.hash());
        h.update(self.tables_merkle.root());
        h.update(self.funcs_merkle.root());
        h.update(self.internals_offset.to_be_bytes());
        h.finalize().into()
    }

    fn serialize_for_proof(&self, mem_merkle: &Merkle) -> Vec<u8> {
        let mut data = Vec::new();

        data.extend(
            Merkle::new(
                MerkleType::Value,
                self.globals.iter().map(|v| v.hash()).collect(),
            )
            .root(),
        );

        data.extend(self.memory.size().to_be_bytes());
        data.extend(self.memory.max_size.to_be_bytes());
        data.extend(mem_merkle.root());

        data.extend(self.tables_merkle.root());
        data.extend(self.funcs_merkle.root());

        data.extend(self.internals_offset.to_be_bytes());

        data
    }
}

// Globalstate holds:
// bytes32 - last_block_hash
// bytes32 - send_root
// uint64 - inbox_position
// uint64 - position_within_message
pub const GLOBAL_STATE_BYTES32_NUM: usize = 2;
pub const GLOBAL_STATE_U64_NUM: usize = 2;

#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize)]
#[repr(C)]
pub struct GlobalState {
    pub bytes32_vals: [Bytes32; GLOBAL_STATE_BYTES32_NUM],
    pub u64_vals: [u64; GLOBAL_STATE_U64_NUM],
}

impl GlobalState {
    fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Global state:");
        for item in self.bytes32_vals {
            h.update(item)
        }
        for item in self.u64_vals {
            h.update(item.to_be_bytes())
        }
        h.finalize().into()
    }

    fn serialize(&self) -> Vec<u8> {
        let mut data = Vec::new();
        for item in self.bytes32_vals {
            data.extend(item)
        }
        for item in self.u64_vals {
            data.extend(item.to_be_bytes())
        }
        data
    }
}

#[derive(Serialize)]
pub struct ProofInfo {
    pub before: String,
    pub proof: String,
    pub after: String,
}

impl ProofInfo {
    pub fn new(before: String, proof: String, after: String) -> Self {
        Self {
            before,
            proof,
            after,
        }
    }
}

/// cbindgen:ignore
#[derive(Clone, Copy, Debug, PartialEq, Eq, Serialize, Deserialize)]
#[repr(u8)]
pub enum MachineStatus {
    Running,
    Finished,
    Errored,
    TooFar,
}

impl Display for MachineStatus {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Running => write!(f, "running"),
            Self::Finished => write!(f, "finished"),
            Self::Errored => write!(f, "errored"),
            Self::TooFar => write!(f, "too far"),
        }
    }
}

#[derive(Clone, Serialize, Deserialize)]
pub struct ModuleState<'a> {
    globals: Cow<'a, Vec<Value>>,
    memory: Cow<'a, Memory>,
}

#[derive(Serialize, Deserialize)]
pub struct MachineState<'a> {
    steps: u64, // Not part of machine hash
    status: MachineStatus,
    value_stack: Cow<'a, Vec<Value>>,
    internal_stack: Cow<'a, Vec<Value>>,
    frame_stack: Cow<'a, Vec<StackFrame>>,
    modules: Vec<ModuleState<'a>>,
    global_state: GlobalState,
    pc: ProgramCounter,
    stdio_output: Cow<'a, Vec<u8>>,
    initial_hash: Bytes32,
}

pub type PreimageResolver = Arc<dyn Fn(u64, PreimageType, Bytes32) -> Option<CBytes> + Send + Sync>;

/// Wraps a preimage resolver to provide an easier API
/// and cache the last preimage retrieved.
#[derive(Clone)]
struct PreimageResolverWrapper {
    resolver: PreimageResolver,
    last_resolved: Option<(Bytes32, CBytes)>,
}

impl fmt::Debug for PreimageResolverWrapper {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "resolver...")
    }
}

impl PreimageResolverWrapper {
    pub fn new(resolver: PreimageResolver) -> PreimageResolverWrapper {
        PreimageResolverWrapper {
            resolver,
            last_resolved: None,
        }
    }

    pub fn get(&mut self, context: u64, ty: PreimageType, hash: Bytes32) -> Option<&[u8]> {
        // TODO: this is unnecessarily complicated by the rust borrow checker.
        // This will probably be simplifiable when Polonius is shipped.
        if matches!(&self.last_resolved, Some(r) if r.0 != hash) {
            self.last_resolved = None;
        }
        match &mut self.last_resolved {
            Some(resolved) => Some(&resolved.1),
            x => {
                let data = (self.resolver)(context, ty, hash)?;
                Some(&x.insert((hash, data)).1)
            }
        }
    }

    pub fn get_const(&self, context: u64, ty: PreimageType, hash: Bytes32) -> Option<CBytes> {
        if let Some(resolved) = &self.last_resolved {
            if resolved.0 == hash {
                return Some(resolved.1.clone());
            }
        }
        (self.resolver)(context, ty, hash)
    }
}

#[derive(Clone, Debug)]
pub struct Machine {
    steps: u64, // Not part of machine hash
    status: MachineStatus,
    value_stack: Vec<Value>,
    internal_stack: Vec<Value>,
    frame_stack: Vec<StackFrame>,
    modules: Vec<Module>,
    modules_merkle: Option<Merkle>,
    global_state: GlobalState,
    pc: ProgramCounter,
    stdio_output: Vec<u8>,
    inbox_contents: HashMap<(InboxIdentifier, u64), Vec<u8>>,
    first_too_far: u64, // Not part of machine hash
    preimage_resolver: PreimageResolverWrapper,
    initial_hash: Bytes32,
    context: u64,
}

fn hash_stack<I, D>(stack: I, prefix: &str) -> Bytes32
where
    I: IntoIterator<Item = D>,
    D: AsRef<[u8]>,
{
    let mut hash = Bytes32::default();
    for item in stack.into_iter() {
        let mut h = Keccak256::new();
        h.update(prefix);
        h.update(item.as_ref());
        h.update(hash);
        hash = h.finalize().into();
    }
    hash
}

fn hash_value_stack(stack: &[Value]) -> Bytes32 {
    hash_stack(stack.iter().map(|v| v.hash()), "Value stack:")
}

fn hash_stack_frame_stack(frames: &[StackFrame]) -> Bytes32 {
    hash_stack(frames.iter().map(|f| f.hash()), "Stack frame stack:")
}

#[must_use]
fn prove_window<T, F, D, G>(items: &[T], stack_hasher: F, encoder: G) -> Vec<u8>
where
    F: Fn(&[T]) -> Bytes32,
    D: AsRef<[u8]>,
    G: Fn(&T) -> D,
{
    let mut data = Vec::with_capacity(33);
    if items.is_empty() {
        data.extend(Bytes32::default());
        data.push(0);
    } else {
        let last_idx = items.len() - 1;
        data.extend(stack_hasher(&items[..last_idx]));
        data.push(1);
        data.extend(encoder(&items[last_idx]).as_ref());
    }
    data
}

#[must_use]
fn prove_stack<T, F, D, G>(
    items: &[T],
    proving_depth: usize,
    stack_hasher: F,
    encoder: G,
) -> Vec<u8>
where
    F: Fn(&[T]) -> Bytes32,
    D: AsRef<[u8]>,
    G: Fn(&T) -> D,
{
    let mut data = Vec::with_capacity(33);
    let unproven_stack_depth = items.len().saturating_sub(proving_depth);
    data.extend(stack_hasher(&items[..unproven_stack_depth]));
    data.extend(Bytes32::from(items.len() - unproven_stack_depth));
    for val in &items[unproven_stack_depth..] {
        data.extend(encoder(val).as_ref());
    }
    data
}

#[must_use]
fn exec_ibin_op<T>(a: T, b: T, op: IBinOpType) -> Option<T>
where
    Wrapping<T>: ReinterpretAsSigned,
    T: Zero,
{
    let a = Wrapping(a);
    let b = Wrapping(b);
    if matches!(
        op,
        IBinOpType::DivS | IBinOpType::DivU | IBinOpType::RemS | IBinOpType::RemU,
    ) && b.is_zero()
    {
        return None;
    }
    let res = match op {
        IBinOpType::Add => a + b,
        IBinOpType::Sub => a - b,
        IBinOpType::Mul => a * b,
        IBinOpType::DivS => (a.cast_signed() / b.cast_signed()).cast_unsigned(),
        IBinOpType::DivU => a / b,
        IBinOpType::RemS => (a.cast_signed() % b.cast_signed()).cast_unsigned(),
        IBinOpType::RemU => a % b,
        IBinOpType::And => a & b,
        IBinOpType::Or => a | b,
        IBinOpType::Xor => a ^ b,
        IBinOpType::Shl => a << b.cast_usize(),
        IBinOpType::ShrS => (a.cast_signed() >> b.cast_usize()).cast_unsigned(),
        IBinOpType::ShrU => a >> b.cast_usize(),
        IBinOpType::Rotl => a.rotl(b.cast_usize()),
        IBinOpType::Rotr => a.rotr(b.cast_usize()),
    };
    Some(res.0)
}

#[must_use]
fn exec_iun_op<T>(a: T, op: IUnOpType) -> u32
where
    T: PrimInt,
{
    match op {
        IUnOpType::Clz => a.leading_zeros(),
        IUnOpType::Ctz => a.trailing_zeros(),
        IUnOpType::Popcnt => a.count_ones(),
    }
}

fn exec_irel_op<T>(a: T, b: T, op: IRelOpType) -> Value
where
    T: Ord,
{
    let res = match op {
        IRelOpType::Eq => a == b,
        IRelOpType::Ne => a != b,
        IRelOpType::Lt => a < b,
        IRelOpType::Gt => a > b,
        IRelOpType::Le => a <= b,
        IRelOpType::Ge => a >= b,
    };
    Value::I32(res as u32)
}

pub fn get_empty_preimage_resolver() -> PreimageResolver {
    Arc::new(|_, _, _| None) as _
}

impl Machine {
    pub const MAX_STEPS: u64 = 1 << 43;

    pub fn from_paths(
        library_paths: &[PathBuf],
        binary_path: &Path,
        language_support: bool,
        always_merkleize: bool,
        allow_hostapi_from_main: bool,
        global_state: GlobalState,
        inbox_contents: HashMap<(InboxIdentifier, u64), Vec<u8>>,
        preimage_resolver: PreimageResolver,
    ) -> Result<Machine> {
        let bin_source = file_bytes(binary_path)?;
        let bin = parse(&bin_source)
            .wrap_err_with(|| format!("failed to validate WASM binary at {:?}", binary_path))?;
        let mut libraries = vec![];
        let mut lib_sources = vec![];
        for path in library_paths {
            let error_message = format!("failed to validate WASM binary at {:?}", path);
            lib_sources.push((file_bytes(path)?, error_message));
        }
        for (source, error_message) in &lib_sources {
            let library = parse(source).wrap_err_with(|| error_message.clone())?;
            libraries.push(library);
        }
        Self::from_binaries(
            &libraries,
            bin,
            language_support,
            always_merkleize,
            allow_hostapi_from_main,
            global_state,
            inbox_contents,
            preimage_resolver,
        )
    }

    pub fn from_binaries(
        libraries: &[WasmBinary<'_>],
        bin: WasmBinary<'_>,
        runtime_support: bool,
        always_merkleize: bool,
        allow_hostapi_from_main: bool,
        global_state: GlobalState,
        inbox_contents: HashMap<(InboxIdentifier, u64), Vec<u8>>,
        preimage_resolver: PreimageResolver,
    ) -> Result<Machine> {
        use ArbValueType::*;

        // `modules` starts out with the entrypoint module, which will be initialized later
        let mut modules = vec![Module::default()];
        let mut available_imports = HashMap::default();
        let mut floating_point_impls = HashMap::default();

        for export in &bin.exports {
            if let ExternalKind::Func = export.kind {
                if let Some(ty_idx) = usize::try_from(export.index)
                    .unwrap()
                    .checked_sub(bin.imports.len())
                {
                    let ty = bin.functions[ty_idx];
                    let ty = &bin.types[usize::try_from(ty).unwrap()];
                    let module = u32::try_from(modules.len() + libraries.len()).unwrap();
                    available_imports.insert(
                        format!("env__wavm_guest_call__{}", export.name),
                        AvailableImport {
                            ty: ty.clone(),
                            module,
                            func: export.index,
                        },
                    );
                }
            }
        }

        for lib in libraries {
            let module = Module::from_binary(lib, &available_imports, &floating_point_impls, true)?;
            for (name, &func) in &*module.exports {
                let ty = module.func_types[func as usize].clone();
                available_imports.insert(
                    name.clone(),
                    AvailableImport {
                        module: modules.len() as u32,
                        func,
                        ty: ty.clone(),
                    },
                );
                if let Ok(op) = name.parse::<FloatInstruction>() {
                    let mut sig = op.signature();
                    // wavm codegen takes care of effecting this type change at callsites
                    for ty in sig.inputs.iter_mut().chain(sig.outputs.iter_mut()) {
                        if *ty == F32 {
                            *ty = I32;
                        } else if *ty == F64 {
                            *ty = I64;
                        }
                    }
                    ensure!(
                        ty == sig,
                        "Wrong type for floating point impl {:?} expecting {:?} but got {:?}",
                        name,
                        sig,
                        ty
                    );
                    floating_point_impls.insert(op, (modules.len() as u32, func));
                }
            }
            modules.push(module);
        }

        // Shouldn't be necessary, but to safe, don't allow the main binary to import its own guest calls
        available_imports.retain(|_, i| i.module as usize != modules.len());
        modules.push(Module::from_binary(
            &bin,
            &available_imports,
            &floating_point_impls,
            allow_hostapi_from_main,
        )?);

        // Build the entrypoint module
        let mut entrypoint = Vec::new();
        macro_rules! entry {
            ($opcode:ident) => {
                entrypoint.push(Instruction::simple(Opcode::$opcode));
            };
            ($opcode:ident, $value:expr) => {
                entrypoint.push(Instruction::with_data(Opcode::$opcode, $value));
            };
            ($opcode:ident ($inside:expr)) => {
                entrypoint.push(Instruction::simple(Opcode::$opcode($inside)));
            };
            (@cross, $module:expr, $func:expr) => {
                entrypoint.push(Instruction::with_data(
                    Opcode::CrossModuleCall,
                    pack_cross_module_call($module, $func),
                ));
            };
        }
        for (i, module) in modules.iter().enumerate() {
            if let Some(s) = module.start_function {
                ensure!(
                    module.func_types[s as usize] == FunctionType::default(),
                    "Start function takes inputs or outputs",
                );
                entry!(@cross, u32::try_from(i).unwrap(), s);
            }
        }
        let main_module_idx = modules.len() - 1;
        let main_module = &modules[main_module_idx];

        // Rust support
        let rust_fn = "__main_void";
        if let Some(&f) = main_module.exports.get(rust_fn).filter(|_| runtime_support) {
            let expected_type = FunctionType::new(vec![], vec![I32]);
            ensure!(
                main_module.func_types[f as usize] == expected_type,
                "Main function doesn't match expected signature of [] -> [ret]",
            );
            entry!(@cross, u32::try_from(main_module_idx).unwrap(), f);
            entry!(Drop);
            entry!(HaltAndSetFinished);
        }

        // Go support
        if let Some(&f) = main_module.exports.get("run").filter(|_| runtime_support) {
            let mut expected_type = FunctionType::default();
            expected_type.inputs.push(I32); // argc
            expected_type.inputs.push(I32); // argv
            ensure!(
                main_module.func_types[f as usize] == expected_type,
                "Run function doesn't match expected signature of [argc, argv]",
            );
            // Go's flags library panics if the argument list is empty.
            // To pass in the program name argument, we need to put it in memory.
            // The Go linker guarantees a section of memory starting at byte 4096 is available for this purpose.
            // https://github.com/golang/go/blob/252324e879e32f948d885f787decf8af06f82be9/misc/wasm/wasm_exec.js#L520
            // These memory stores also assume that the Go module's memory is large enough to begin with.
            // That's also handled by the Go compiler. Go 1.17.5 in the compilation of the arbitrator go test case
            // initializes its memory to 272 pages long (about 18MB), much larger than the required space.
            let free_memory_base = 4096;
            let name_str_ptr = free_memory_base;
            let argv_ptr = name_str_ptr + 8;
            ensure!(
                main_module.internals_offset != 0,
                "Main module doesn't have internals"
            );
            let main_module_idx = u32::try_from(main_module_idx).unwrap();
            let main_module_store32 = main_module.internals_offset + 3;

            // Write "js\0" to name_str_ptr, to match what the actual JS environment does
            entry!(I32Const, name_str_ptr);
            entry!(I32Const, 0x736a); // b"js\0"
            entry!(@cross, main_module_idx, main_module_store32);
            entry!(I32Const, name_str_ptr + 4);
            entry!(I32Const, 0);
            entry!(@cross, main_module_idx, main_module_store32);

            // Write name_str_ptr to argv_ptr
            entry!(I32Const, argv_ptr);
            entry!(I32Const, name_str_ptr);
            entry!(@cross, main_module_idx, main_module_store32);
            entry!(I32Const, argv_ptr + 4);
            entry!(I32Const, 0);
            entry!(@cross, main_module_idx, main_module_store32);

            // Launch main with an argument count of 1 and argv_ptr
            entry!(I32Const, 1);
            entry!(I32Const, argv_ptr);
            entry!(@cross, main_module_idx, f);
            if let Some(i) = available_imports.get("wavm__go_after_run") {
                ensure!(
                    i.ty == FunctionType::default(),
                    "Resume function has non-empty function signature",
                );
                entry!(@cross, i.module, i.func);
            }
        }

        let entrypoint_types = vec![FunctionType::default()];
        let mut entrypoint_names = NameCustomSection {
            module: "entry".into(),
            functions: HashMap::default(),
        };
        entrypoint_names
            .functions
            .insert(0, "wavm_entrypoint".into());
        let entrypoint_funcs = vec![Function::new(
            &[],
            |code| {
                code.extend(entrypoint);
                Ok(())
            },
            FunctionType::default(),
            &entrypoint_types,
        )?];
        let entrypoint = Module {
            globals: Vec::new(),
            memory: Memory::default(),
            tables: Vec::new(),
            tables_merkle: Merkle::default(),
            funcs_merkle: Arc::new(Merkle::new(
                MerkleType::Function,
                entrypoint_funcs.iter().map(Function::hash).collect(),
            )),
            funcs: Arc::new(entrypoint_funcs),
            types: Arc::new(entrypoint_types),
            names: Arc::new(entrypoint_names),
            internals_offset: 0,
            host_call_hooks: Arc::new(Vec::new()),
            start_function: None,
            func_types: Arc::new(vec![FunctionType::default()]),
            exports: Arc::new(HashMap::default()),
        };
        modules[0] = entrypoint;

        ensure!(
            u32::try_from(modules.len()).is_ok(),
            "module count doesn't fit in a u32",
        );

        // Merkleize things if requested
        for module in &mut modules {
            for table in module.tables.iter_mut() {
                table.elems_merkle = Merkle::new(
                    MerkleType::TableElement,
                    table.elems.iter().map(TableElement::hash).collect(),
                );
            }

            let tables_hashes: Result<_, _> = module.tables.iter().map(Table::hash).collect();
            module.tables_merkle = Merkle::new(MerkleType::Table, tables_hashes?);

            if always_merkleize {
                module.memory.cache_merkle_tree();
            }
        }
        let mut modules_merkle = None;
        if always_merkleize {
            modules_merkle = Some(Merkle::new(
                MerkleType::Module,
                modules.iter().map(Module::hash).collect(),
            ));
        }

        // find the first inbox index that's out of bounds
        let first_too_far = inbox_contents
            .iter()
            .filter(|((kind, _), _)| kind == &InboxIdentifier::Sequencer)
            .map(|((_, index), _)| *index + 1)
            .max()
            .unwrap_or(0);

        let mut mach = Machine {
            status: MachineStatus::Running,
            steps: 0,
            value_stack: vec![Value::RefNull, Value::I32(0), Value::I32(0)],
            internal_stack: Vec::new(),
            frame_stack: Vec::new(),
            modules,
            modules_merkle,
            global_state,
            pc: ProgramCounter::default(),
            stdio_output: Vec::new(),
            inbox_contents,
            first_too_far,
            preimage_resolver: PreimageResolverWrapper::new(preimage_resolver),
            initial_hash: Bytes32::default(),
            context: 0,
        };
        mach.initial_hash = mach.hash();
        Ok(mach)
    }

    pub fn new_from_wavm(wavm_binary: &Path) -> Result<Machine> {
        let f = BufReader::new(File::open(wavm_binary)?);
        let decompressor = brotli2::read::BrotliDecoder::new(f);
        let mut modules: Vec<Module> = bincode::deserialize_from(decompressor)?;
        for module in modules.iter_mut() {
            for table in module.tables.iter_mut() {
                table.elems_merkle = Merkle::new(
                    MerkleType::TableElement,
                    table.elems.iter().map(TableElement::hash).collect(),
                );
            }
            let tables: Result<_> = module.tables.iter().map(Table::hash).collect();
            module.tables_merkle = Merkle::new(MerkleType::Table, tables?);

            let funcs =
                Arc::get_mut(&mut module.funcs).expect("Multiple copies of module functions");
            for func in funcs.iter_mut() {
                func.code_merkle = Merkle::new(
                    MerkleType::Instruction,
                    func.code.par_iter().map(|i| i.hash()).collect(),
                );
            }
            module.funcs_merkle = Arc::new(Merkle::new(
                MerkleType::Function,
                module.funcs.iter().map(Function::hash).collect(),
            ));
        }
        let mut mach = Machine {
            status: MachineStatus::Running,
            steps: 0,
            value_stack: vec![Value::RefNull, Value::I32(0), Value::I32(0)],
            internal_stack: Vec::new(),
            frame_stack: Vec::new(),
            modules,
            modules_merkle: None,
            global_state: Default::default(),
            pc: ProgramCounter::default(),
            stdio_output: Vec::new(),
            inbox_contents: Default::default(),
            first_too_far: 0,
            preimage_resolver: PreimageResolverWrapper::new(get_empty_preimage_resolver()),
            initial_hash: Bytes32::default(),
            context: 0,
        };
        mach.initial_hash = mach.hash();
        Ok(mach)
    }

    pub fn serialize_binary<P: AsRef<Path>>(&self, path: P) -> Result<()> {
        ensure!(
            self.hash() == self.initial_hash,
            "serialize_binary can only be called on initial machine",
        );
        let mut f = File::create(path)?;
        let mut compressor = brotli2::write::BrotliEncoder::new(BufWriter::new(&mut f), 9);
        bincode::serialize_into(&mut compressor, &self.modules)?;
        compressor.flush()?;
        drop(compressor);
        f.sync_data()?;
        Ok(())
    }

    pub fn serialize_state<P: AsRef<Path>>(&self, path: P) -> Result<()> {
        let mut f = File::create(path)?;
        let mut writer = BufWriter::new(&mut f);
        let modules = self
            .modules
            .iter()
            .map(|m| ModuleState {
                globals: Cow::Borrowed(&m.globals),
                memory: Cow::Borrowed(&m.memory),
            })
            .collect();
        let state = MachineState {
            steps: self.steps,
            status: self.status,
            value_stack: Cow::Borrowed(&self.value_stack),
            internal_stack: Cow::Borrowed(&self.internal_stack),
            frame_stack: Cow::Borrowed(&self.frame_stack),
            modules,
            global_state: self.global_state.clone(),
            pc: self.pc,
            stdio_output: Cow::Borrowed(&self.stdio_output),
            initial_hash: self.initial_hash,
        };
        bincode::serialize_into(&mut writer, &state)?;
        writer.flush()?;
        drop(writer);
        f.sync_data()?;
        Ok(())
    }

    // Requires that this is the same base machine. If this returns an error, it has not mutated `self`.
    pub fn deserialize_and_replace_state<P: AsRef<Path>>(&mut self, path: P) -> Result<()> {
        let reader = BufReader::new(File::open(path)?);
        let new_state: MachineState = bincode::deserialize_from(reader)?;
        if self.initial_hash != new_state.initial_hash {
            bail!(
                "attempted to load deserialize machine with initial hash {} into machine with initial hash {}",
                new_state.initial_hash, self.initial_hash,
            );
        }
        assert_eq!(self.modules.len(), new_state.modules.len());

        // Start mutating the machine. We must not return an error past this point.
        for (module, new_module_state) in self.modules.iter_mut().zip(new_state.modules.into_iter())
        {
            module.globals = new_module_state.globals.into_owned();
            module.memory = new_module_state.memory.into_owned();
        }
        self.steps = new_state.steps;
        self.status = new_state.status;
        self.value_stack = new_state.value_stack.into_owned();
        self.internal_stack = new_state.internal_stack.into_owned();
        self.frame_stack = new_state.frame_stack.into_owned();
        self.global_state = new_state.global_state;
        self.pc = new_state.pc;
        self.stdio_output = new_state.stdio_output.into_owned();
        Ok(())
    }

    pub fn start_merkle_caching(&mut self) {
        for module in &mut self.modules {
            module.memory.cache_merkle_tree();
        }
        self.modules_merkle = Some(Merkle::new(
            MerkleType::Module,
            self.modules.iter().map(Module::hash).collect(),
        ));
    }

    pub fn stop_merkle_caching(&mut self) {
        self.modules_merkle = None;
        for module in &mut self.modules {
            module.memory.merkle = None;
        }
    }

    pub fn jump_into_function(&mut self, func: &str, mut args: Vec<Value>) {
        let frame_args = [Value::RefNull, Value::I32(0), Value::I32(0)];
        args.extend(frame_args);
        self.value_stack = args;

        let module = self.modules.last().expect("no module");
        let export = module.exports.iter().find(|x| x.0 == func);
        let export = export
            .unwrap_or_else(|| panic!("func {} not found", func))
            .1;

        self.frame_stack.clear();
        self.internal_stack.clear();

        self.pc = ProgramCounter {
            module: (self.modules.len() - 1).try_into().unwrap(),
            func: *export,
            inst: 0,
        };
        self.status = MachineStatus::Running;
        self.steps = 0;
    }

    pub fn get_final_result(&self) -> Result<Vec<Value>> {
        if !self.frame_stack.is_empty() {
            bail!(
                "machine has not successfully computed a final result {:?}",
                self.status
            )
        }
        Ok(self.value_stack.clone())
    }

    pub fn get_next_instruction(&self) -> Option<Instruction> {
        if self.is_halted() {
            return None;
        }
        self.modules[self.pc.module()].funcs[self.pc.func()]
            .code
            .get(self.pc.inst())
            .cloned()
    }

    pub fn next_instruction_is_host_io(&self) -> bool {
        self.get_next_instruction()
            .map(|i| i.opcode.is_host_io())
            .unwrap_or(true)
    }

    pub fn get_pc(&self) -> Option<ProgramCounter> {
        if self.is_halted() {
            return None;
        }
        Some(self.pc)
    }

    fn test_next_instruction(func: &Function, pc: &ProgramCounter) {
        debug_assert!(func.code.len() > pc.inst.try_into().unwrap());
    }

    pub fn get_steps(&self) -> u64 {
        self.steps
    }

    pub fn step_n(&mut self, n: u64) -> Result<()> {
        if self.is_halted() {
            return Ok(());
        }
        let mut module = &mut self.modules[self.pc.module()];
        let mut func = &module.funcs[self.pc.func()];

        macro_rules! flush_module {
            () => {
                if let Some(merkle) = self.modules_merkle.as_mut() {
                    merkle.set(self.pc.module(), module.hash());
                }
            };
        }
        macro_rules! error {
            () => {{
                self.status = MachineStatus::Errored;
                break;
            }};
        }

        for _ in 0..n {
            self.steps += 1;
            if self.steps == Self::MAX_STEPS {
                error!();
            }
            let inst = func.code[self.pc.inst()];
            self.pc.inst += 1;
            match inst.opcode {
                Opcode::Unreachable => error!(),
                Opcode::Nop => {}
                Opcode::InitFrame => {
                    let caller_module_internals = self.value_stack.pop().unwrap().assume_u32();
                    let caller_module = self.value_stack.pop().unwrap().assume_u32();
                    let return_ref = self.value_stack.pop().unwrap();
                    self.frame_stack.push(StackFrame {
                        return_ref,
                        locals: func
                            .local_types
                            .iter()
                            .cloned()
                            .map(Value::default_of_type)
                            .collect(),
                        caller_module,
                        caller_module_internals,
                    });
                    if let Some(hook) = module
                        .host_call_hooks
                        .get(self.pc.func())
                        .and_then(|h| h.as_ref())
                    {
                        if let Err(err) = Self::host_call_hook(
                            &self.value_stack,
                            module,
                            &mut self.stdio_output,
                            &hook.0,
                            &hook.1,
                        ) {
                            eprintln!(
                                "Failed to process host call hook for host call {:?} {:?}: {}",
                                hook.0, hook.1, err,
                            );
                        }
                    }
                }
                Opcode::ArbitraryJump => {
                    self.pc.inst = inst.argument_data as u32;
                    Machine::test_next_instruction(func, &self.pc);
                }
                Opcode::ArbitraryJumpIf => {
                    let x = self.value_stack.pop().unwrap();
                    if !x.is_i32_zero() {
                        self.pc.inst = inst.argument_data as u32;
                        Machine::test_next_instruction(func, &self.pc);
                    }
                }
                Opcode::Return => {
                    let frame = self.frame_stack.pop().unwrap();
                    match frame.return_ref {
                        Value::RefNull => error!(),
                        Value::InternalRef(pc) => {
                            let changing_module = pc.module != self.pc.module;
                            if changing_module {
                                flush_module!();
                            }
                            self.pc = pc;
                            if changing_module {
                                module = &mut self.modules[self.pc.module()];
                            }
                            func = &module.funcs[self.pc.func()];
                        }
                        v => bail!("attempted to return into an invalid reference: {:?}", v),
                    }
                }
                Opcode::Call => {
                    let current_frame = self.frame_stack.last().unwrap();
                    self.value_stack.push(Value::InternalRef(self.pc));
                    self.value_stack
                        .push(Value::I32(current_frame.caller_module));
                    self.value_stack
                        .push(Value::I32(current_frame.caller_module_internals));
                    self.pc.func = inst.argument_data as u32;
                    self.pc.inst = 0;
                    func = &module.funcs[self.pc.func()];
                }
                Opcode::CrossModuleCall => {
                    flush_module!();
                    self.value_stack.push(Value::InternalRef(self.pc));
                    self.value_stack.push(Value::I32(self.pc.module));
                    self.value_stack.push(Value::I32(module.internals_offset));
                    let (call_module, call_func) = unpack_cross_module_call(inst.argument_data);
                    self.pc.module = call_module;
                    self.pc.func = call_func;
                    self.pc.inst = 0;
                    module = &mut self.modules[self.pc.module()];
                    func = &module.funcs[self.pc.func()];
                }
                Opcode::CallerModuleInternalCall => {
                    self.value_stack.push(Value::InternalRef(self.pc));
                    self.value_stack.push(Value::I32(self.pc.module));
                    self.value_stack.push(Value::I32(module.internals_offset));

                    let current_frame = self.frame_stack.last().unwrap();
                    if current_frame.caller_module_internals > 0 {
                        let func_idx = u32::try_from(inst.argument_data)
                            .ok()
                            .and_then(|o| current_frame.caller_module_internals.checked_add(o))
                            .expect("Internal call function index overflow");
                        flush_module!();
                        self.pc.module = current_frame.caller_module;
                        self.pc.func = func_idx;
                        self.pc.inst = 0;
                        module = &mut self.modules[self.pc.module()];
                        func = &module.funcs[self.pc.func()];
                    } else {
                        // The caller module has no internals
                        error!();
                    }
                }
                Opcode::CallIndirect => {
                    let (table, ty) = crate::wavm::unpack_call_indirect(inst.argument_data);
                    let idx = match self.value_stack.pop() {
                        Some(Value::I32(i)) => usize::try_from(i).unwrap(),
                        x => bail!(
                            "WASM validation failed: top of stack before call_indirect is {:?}",
                            x,
                        ),
                    };
                    let ty = &module.types[usize::try_from(ty).unwrap()];
                    let elems = &module.tables[usize::try_from(table).unwrap()].elems;
                    if let Some(elem) = elems.get(idx).filter(|e| &e.func_ty == ty) {
                        match elem.val {
                            Value::FuncRef(call_func) => {
                                let current_frame = self.frame_stack.last().unwrap();
                                self.value_stack.push(Value::InternalRef(self.pc));
                                self.value_stack
                                    .push(Value::I32(current_frame.caller_module));
                                self.value_stack
                                    .push(Value::I32(current_frame.caller_module_internals));
                                self.pc.func = call_func;
                                self.pc.inst = 0;
                                func = &module.funcs[self.pc.func()];
                            }
                            Value::RefNull => error!(),
                            v => bail!("invalid table element value {:?}", v),
                        }
                    } else {
                        error!();
                    }
                }
                Opcode::LocalGet => {
                    let val = self.frame_stack.last().unwrap().locals[inst.argument_data as usize];
                    self.value_stack.push(val);
                }
                Opcode::LocalSet => {
                    let val = self.value_stack.pop().unwrap();
                    self.frame_stack.last_mut().unwrap().locals[inst.argument_data as usize] = val;
                }
                Opcode::GlobalGet => {
                    self.value_stack
                        .push(module.globals[inst.argument_data as usize]);
                }
                Opcode::GlobalSet => {
                    let val = self.value_stack.pop().unwrap();
                    module.globals[inst.argument_data as usize] = val;
                }
                Opcode::MemoryLoad { ty, bytes, signed } => {
                    let base = match self.value_stack.pop() {
                        Some(Value::I32(x)) => x,
                        x => bail!(
                            "WASM validation failed: top of stack before memory load is {:?}",
                            x,
                        ),
                    };
                    if let Some(idx) = inst.argument_data.checked_add(base.into()) {
                        let val = module.memory.get_value(idx, ty, bytes, signed);
                        if let Some(val) = val {
                            self.value_stack.push(val);
                        } else {
                            error!();
                        }
                    } else {
                        error!();
                    }
                }
                Opcode::MemoryStore { ty: _, bytes } => {
                    let val = match self.value_stack.pop() {
                        Some(Value::I32(x)) => x.into(),
                        Some(Value::I64(x)) => x,
                        Some(Value::F32(x)) => x.to_bits().into(),
                        Some(Value::F64(x)) => x.to_bits(),
                        x => bail!(
                            "WASM validation failed: attempted to memory store type {:?}",
                            x,
                        ),
                    };
                    let base = match self.value_stack.pop() {
                        Some(Value::I32(x)) => x,
                        x => bail!(
                            "WASM validation failed: attempted to memory store with index type {:?}",
                            x,
                        ),
                    };
                    if let Some(idx) = inst.argument_data.checked_add(base.into()) {
                        if !module.memory.store_value(idx, val, bytes) {
                            error!();
                        }
                    } else {
                        error!();
                    }
                }
                Opcode::I32Const => {
                    self.value_stack.push(Value::I32(inst.argument_data as u32));
                }
                Opcode::I64Const => {
                    self.value_stack.push(Value::I64(inst.argument_data));
                }
                Opcode::F32Const => {
                    self.value_stack
                        .push(Value::F32(f32::from_bits(inst.argument_data as u32)));
                }
                Opcode::F64Const => {
                    self.value_stack
                        .push(Value::F64(f64::from_bits(inst.argument_data)));
                }
                Opcode::I32Eqz => {
                    let val = self.value_stack.pop().unwrap();
                    self.value_stack.push(Value::I32(val.is_i32_zero() as u32));
                }
                Opcode::I64Eqz => {
                    let val = self.value_stack.pop().unwrap();
                    self.value_stack.push(Value::I32(val.is_i64_zero() as u32));
                }
                Opcode::IRelOp(t, op, signed) => {
                    let vb = self.value_stack.pop();
                    let va = self.value_stack.pop();
                    match t {
                        IntegerValType::I32 => {
                            if let (Some(Value::I32(a)), Some(Value::I32(b))) = (va, vb) {
                                if signed {
                                    self.value_stack.push(exec_irel_op(a as i32, b as i32, op));
                                } else {
                                    self.value_stack.push(exec_irel_op(a, b, op));
                                }
                            } else {
                                bail!("WASM validation failed: wrong types for i32relop");
                            }
                        }
                        IntegerValType::I64 => {
                            if let (Some(Value::I64(a)), Some(Value::I64(b))) = (va, vb) {
                                if signed {
                                    self.value_stack.push(exec_irel_op(a as i64, b as i64, op));
                                } else {
                                    self.value_stack.push(exec_irel_op(a, b, op));
                                }
                            } else {
                                bail!("WASM validation failed: wrong types for i64relop");
                            }
                        }
                    }
                }
                Opcode::Drop => {
                    self.value_stack.pop().unwrap();
                }
                Opcode::Select => {
                    let selector_zero = self.value_stack.pop().unwrap().is_i32_zero();
                    let val2 = self.value_stack.pop().unwrap();
                    let val1 = self.value_stack.pop().unwrap();
                    if selector_zero {
                        self.value_stack.push(val2);
                    } else {
                        self.value_stack.push(val1);
                    }
                }
                Opcode::MemorySize => {
                    let pages = u32::try_from(module.memory.size() / Memory::PAGE_SIZE)
                        .expect("Memory pages grew past a u32");
                    self.value_stack.push(Value::I32(pages));
                }
                Opcode::MemoryGrow => {
                    let old_size = module.memory.size();
                    let adding_pages = match self.value_stack.pop() {
                        Some(Value::I32(x)) => x,
                        v => bail!("WASM validation failed: bad value for memory.grow {:?}", v),
                    };
                    let page_size = Memory::PAGE_SIZE;
                    let max_size = module.memory.max_size * page_size;

                    let new_size = (|| {
                        let adding_size = u64::from(adding_pages).checked_mul(page_size)?;
                        let new_size = old_size.checked_add(adding_size)?;
                        if new_size <= max_size {
                            Some(new_size)
                        } else {
                            None
                        }
                    })();
                    if let Some(new_size) = new_size {
                        module.memory.resize(usize::try_from(new_size).unwrap());
                        // Push the old number of pages
                        let old_pages = u32::try_from(old_size / page_size).unwrap();
                        self.value_stack.push(Value::I32(old_pages));
                    } else {
                        // Push -1
                        self.value_stack.push(Value::I32(u32::MAX));
                    }
                }
                Opcode::IUnOp(w, op) => {
                    let va = self.value_stack.pop();
                    match w {
                        IntegerValType::I32 => {
                            if let Some(Value::I32(a)) = va {
                                self.value_stack.push(Value::I32(exec_iun_op(a, op)));
                            } else {
                                bail!("WASM validation failed: wrong types for i32unop");
                            }
                        }
                        IntegerValType::I64 => {
                            if let Some(Value::I64(a)) = va {
                                self.value_stack.push(Value::I64(exec_iun_op(a, op) as u64));
                            } else {
                                bail!("WASM validation failed: wrong types for i64unop");
                            }
                        }
                    }
                }
                Opcode::IBinOp(w, op) => {
                    let vb = self.value_stack.pop();
                    let va = self.value_stack.pop();
                    match w {
                        IntegerValType::I32 => {
                            if let (Some(Value::I32(a)), Some(Value::I32(b))) = (va, vb) {
                                if op == IBinOpType::DivS
                                    && (a as i32) == i32::MIN
                                    && (b as i32) == -1
                                {
                                    error!();
                                }
                                let value = match exec_ibin_op(a, b, op) {
                                    Some(value) => value,
                                    None => error!(),
                                };
                                self.value_stack.push(Value::I32(value))
                            } else {
                                bail!("WASM validation failed: wrong types for i32binop");
                            }
                        }
                        IntegerValType::I64 => {
                            if let (Some(Value::I64(a)), Some(Value::I64(b))) = (va, vb) {
                                if op == IBinOpType::DivS
                                    && (a as i64) == i64::MIN
                                    && (b as i64) == -1
                                {
                                    error!();
                                }
                                let value = match exec_ibin_op(a, b, op) {
                                    Some(value) => value,
                                    None => error!(),
                                };
                                self.value_stack.push(Value::I64(value))
                            } else {
                                bail!("WASM validation failed: wrong types for i64binop");
                            }
                        }
                    }
                }
                Opcode::I32WrapI64 => {
                    let x = match self.value_stack.pop() {
                        Some(Value::I64(x)) => x,
                        v => bail!(
                            "WASM validation failed: wrong type for i32.wrapi64: {:?}",
                            v,
                        ),
                    };
                    self.value_stack.push(Value::I32(x as u32));
                }
                Opcode::I64ExtendI32(signed) => {
                    let x: u32 = self.value_stack.pop().unwrap().assume_u32();
                    let x64 = match signed {
                        true => x as i32 as i64 as u64,
                        false => x as u64,
                    };
                    self.value_stack.push(Value::I64(x64));
                }
                Opcode::Reinterpret(dest, source) => {
                    let val = match self.value_stack.pop() {
                        Some(Value::I32(x)) if source == ArbValueType::I32 => {
                            assert_eq!(dest, ArbValueType::F32, "Unsupported reinterpret");
                            Value::F32(f32::from_bits(x))
                        }
                        Some(Value::I64(x)) if source == ArbValueType::I64 => {
                            assert_eq!(dest, ArbValueType::F64, "Unsupported reinterpret");
                            Value::F64(f64::from_bits(x))
                        }
                        Some(Value::F32(x)) if source == ArbValueType::F32 => {
                            assert_eq!(dest, ArbValueType::I32, "Unsupported reinterpret");
                            Value::I32(x.to_bits())
                        }
                        Some(Value::F64(x)) if source == ArbValueType::F64 => {
                            assert_eq!(dest, ArbValueType::I64, "Unsupported reinterpret");
                            Value::I64(x.to_bits())
                        }
                        v => bail!("bad reinterpret: val {:?} source {:?}", v, source),
                    };
                    self.value_stack.push(val);
                }
                Opcode::I32ExtendS(b) => {
                    let mut x = self.value_stack.pop().unwrap().assume_u32();
                    let mask = (1u32 << b) - 1;
                    x &= mask;
                    if x & (1 << (b - 1)) != 0 {
                        x |= !mask;
                    }
                    self.value_stack.push(Value::I32(x));
                }
                Opcode::I64ExtendS(b) => {
                    let mut x = self.value_stack.pop().unwrap().assume_u64();
                    let mask = (1u64 << b) - 1;
                    x &= mask;
                    if x & (1 << (b - 1)) != 0 {
                        x |= !mask;
                    }
                    self.value_stack.push(Value::I64(x));
                }
                Opcode::MoveFromStackToInternal => {
                    self.internal_stack.push(self.value_stack.pop().unwrap());
                }
                Opcode::MoveFromInternalToStack => {
                    self.value_stack.push(self.internal_stack.pop().unwrap());
                }
                Opcode::Dup => {
                    let val = self.value_stack.last().cloned().unwrap();
                    self.value_stack.push(val);
                }
                Opcode::GetGlobalStateBytes32 => {
                    let ptr = self.value_stack.pop().unwrap().assume_u32();
                    let idx = self.value_stack.pop().unwrap().assume_u32() as usize;
                    if idx >= self.global_state.bytes32_vals.len()
                        || !module
                            .memory
                            .store_slice_aligned(ptr.into(), &*self.global_state.bytes32_vals[idx])
                    {
                        error!();
                    }
                }
                Opcode::SetGlobalStateBytes32 => {
                    let ptr = self.value_stack.pop().unwrap().assume_u32();
                    let idx = self.value_stack.pop().unwrap().assume_u32() as usize;
                    if idx >= self.global_state.bytes32_vals.len() {
                        error!();
                    } else if let Some(hash) = module.memory.load_32_byte_aligned(ptr.into()) {
                        self.global_state.bytes32_vals[idx] = hash;
                    } else {
                        error!();
                    }
                }
                Opcode::GetGlobalStateU64 => {
                    let idx = self.value_stack.pop().unwrap().assume_u32() as usize;
                    if idx >= self.global_state.u64_vals.len() {
                        error!();
                    } else {
                        self.value_stack
                            .push(Value::I64(self.global_state.u64_vals[idx]));
                    }
                }
                Opcode::SetGlobalStateU64 => {
                    let val = self.value_stack.pop().unwrap().assume_u64();
                    let idx = self.value_stack.pop().unwrap().assume_u32() as usize;
                    if idx >= self.global_state.u64_vals.len() {
                        error!();
                    } else {
                        self.global_state.u64_vals[idx] = val
                    }
                }
                Opcode::ReadPreImage => {
                    let offset = self.value_stack.pop().unwrap().assume_u32();
                    let ptr = self.value_stack.pop().unwrap().assume_u32();
                    let preimage_ty = PreimageType::try_from(u8::try_from(inst.argument_data)?)?;
                    if let Some(hash) = module.memory.load_32_byte_aligned(ptr.into()) {
                        if let Some(preimage) =
                            self.preimage_resolver.get(self.context, preimage_ty, hash)
                        {
                            let offset = usize::try_from(offset).unwrap();
                            let len = std::cmp::min(32, preimage.len().saturating_sub(offset));
                            let read = preimage.get(offset..(offset + len)).unwrap_or_default();
                            let success = module.memory.store_slice_aligned(ptr.into(), read);
                            assert!(success, "Failed to write to previously read memory");
                            self.value_stack.push(Value::I32(len as u32));
                        } else {
                            eprintln!(
                                "{} for hash {}",
                                "Missing requested preimage".red(),
                                hash.red(),
                            );
                            self.eprint_backtrace();
                            bail!("missing requested preimage for hash {}", hash);
                        }
                    } else {
                        error!();
                    }
                }
                Opcode::ReadInboxMessage => {
                    let offset = self.value_stack.pop().unwrap().assume_u32();
                    let ptr = self.value_stack.pop().unwrap().assume_u32();
                    let msg_num = self.value_stack.pop().unwrap().assume_u64();
                    let inbox_identifier =
                        argument_data_to_inbox(inst.argument_data).expect("Bad inbox indentifier");
                    if let Some(message) = self.inbox_contents.get(&(inbox_identifier, msg_num)) {
                        if ptr as u64 + 32 > module.memory.size() {
                            error!();
                        } else {
                            let offset = usize::try_from(offset).unwrap();
                            let len = std::cmp::min(32, message.len().saturating_sub(offset));
                            let read = message.get(offset..(offset + len)).unwrap_or_default();
                            if module.memory.store_slice_aligned(ptr.into(), read) {
                                self.value_stack.push(Value::I32(len as u32));
                            } else {
                                error!();
                            }
                        }
                    } else {
                        let delayed = inbox_identifier == InboxIdentifier::Delayed;
                        if msg_num < self.first_too_far || delayed {
                            eprintln!("{} {msg_num}", "Missing inbox message".red());
                            self.eprint_backtrace();
                            bail!(
                                "missing inbox message {msg_num} of {}",
                                self.first_too_far - 1
                            );
                        }
                        self.status = MachineStatus::TooFar;
                        break;
                    }
                }
                Opcode::HaltAndSetFinished => {
                    self.status = MachineStatus::Finished;
                    break;
                }
            }
        }
        flush_module!();
        if self.is_halted() && !self.stdio_output.is_empty() {
            // If we halted, print out any trailing output that didn't have a newline.
            println!(
                "{} {}",
                "WASM says:".yellow(),
                String::from_utf8_lossy(&self.stdio_output),
            );
            self.stdio_output.clear();
        }
        Ok(())
    }

    fn host_call_hook(
        value_stack: &[Value],
        module: &Module,
        stdio_output: &mut Vec<u8>,
        module_name: &str,
        name: &str,
    ) -> Result<()> {
        macro_rules! pull_arg {
            ($offset:expr, $t:ident) => {
                value_stack
                    .get(value_stack.len().wrapping_sub($offset + 1))
                    .and_then(|v| match v {
                        Value::$t(x) => Some(*x),
                        _ => None,
                    })
                    .ok_or_else(|| eyre!("exit code not on top of stack"))?
            };
        }
        macro_rules! read_u32_ptr {
            ($ptr:expr) => {
                module
                    .memory
                    .get_u32($ptr.into())
                    .ok_or_else(|| eyre!("pointer out of bounds"))?
            };
        }
        macro_rules! read_bytes_segment {
            ($ptr:expr, $size:expr) => {
                module
                    .memory
                    .get_range($ptr as usize, $size as usize)
                    .ok_or_else(|| eyre!("bytes segment out of bounds"))?
            };
        }
        match (module_name, name) {
            ("wasi_snapshot_preview1", "proc_exit") | ("env", "exit") => {
                let exit_code = pull_arg!(0, I32);
                if exit_code != 0 {
                    println!(
                        "\x1b[31mWASM exiting\x1b[0m with exit code \x1b[31m{}\x1b[0m",
                        exit_code,
                    );
                }
                Ok(())
            }
            ("wasi_snapshot_preview1", "fd_write") => {
                let fd = pull_arg!(3, I32);
                if fd != 1 && fd != 2 {
                    // Not stdout or stderr, ignore
                    return Ok(());
                }
                let iovecs_ptr = pull_arg!(2, I32);
                let iovecs_len = pull_arg!(1, I32);
                for offset in 0..iovecs_len {
                    let offset = offset.wrapping_mul(8);
                    let data_ptr_ptr = iovecs_ptr.wrapping_add(offset);
                    let data_size_ptr = data_ptr_ptr.wrapping_add(4);

                    let data_ptr = read_u32_ptr!(data_ptr_ptr);
                    let data_size = read_u32_ptr!(data_size_ptr);
                    stdio_output.extend_from_slice(read_bytes_segment!(data_ptr, data_size));
                }
                while let Some(mut idx) = stdio_output.iter().position(|&c| c == b'\n') {
                    println!(
                        "\x1b[33mWASM says:\x1b[0m {}",
                        String::from_utf8_lossy(&stdio_output[..idx]),
                    );
                    if stdio_output.get(idx + 1) == Some(&b'\r') {
                        idx += 1;
                    }
                    *stdio_output = stdio_output.split_off(idx + 1);
                }
                Ok(())
            }
            _ => Ok(()),
        }
    }

    pub fn is_halted(&self) -> bool {
        self.status != MachineStatus::Running
    }

    pub fn get_status(&self) -> MachineStatus {
        self.status
    }

    fn get_modules_merkle(&self) -> Cow<Merkle> {
        if let Some(merkle) = &self.modules_merkle {
            Cow::Borrowed(merkle)
        } else {
            Cow::Owned(Merkle::new(
                MerkleType::Module,
                self.modules.iter().map(Module::hash).collect(),
            ))
        }
    }

    pub fn get_modules_root(&self) -> Bytes32 {
        self.get_modules_merkle().root()
    }

    pub fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        match self.status {
            MachineStatus::Running => {
                h.update(b"Machine running:");
                h.update(hash_value_stack(&self.value_stack));
                h.update(hash_value_stack(&self.internal_stack));
                h.update(hash_stack_frame_stack(&self.frame_stack));
                h.update(self.global_state.hash());
                h.update(self.pc.module.to_be_bytes());
                h.update(self.pc.func.to_be_bytes());
                h.update(self.pc.inst.to_be_bytes());
                h.update(self.get_modules_root());
            }
            MachineStatus::Finished => {
                h.update("Machine finished:");
                h.update(self.global_state.hash());
            }
            MachineStatus::Errored => {
                h.update("Machine errored:");
            }
            MachineStatus::TooFar => {
                h.update("Machine too far:");
            }
        }
        h.finalize().into()
    }

    pub fn serialize_proof(&self) -> Vec<u8> {
        // Could be variable, but not worth it yet
        const STACK_PROVING_DEPTH: usize = 3;

        let mut data = vec![self.status as u8];

        data.extend(prove_stack(
            &self.value_stack,
            STACK_PROVING_DEPTH,
            hash_value_stack,
            |v| v.serialize_for_proof(),
        ));

        data.extend(prove_stack(
            &self.internal_stack,
            1,
            hash_value_stack,
            |v| v.serialize_for_proof(),
        ));

        data.extend(prove_window(
            &self.frame_stack,
            hash_stack_frame_stack,
            StackFrame::serialize_for_proof,
        ));

        data.extend(self.global_state.hash());

        data.extend(self.pc.module.to_be_bytes());
        data.extend(self.pc.func.to_be_bytes());
        data.extend(self.pc.inst.to_be_bytes());
        let mod_merkle = self.get_modules_merkle();
        data.extend(mod_merkle.root());

        // End machine serialization, serialize module

        let module = &self.modules[self.pc.module()];
        let mem_merkle = module.memory.merkelize();
        data.extend(module.serialize_for_proof(&mem_merkle));

        // Prove module is in modules merkle tree

        data.extend(
            mod_merkle
                .prove(self.pc.module())
                .expect("Failed to prove module"),
        );

        if self.is_halted() {
            return data;
        }

        // Begin next instruction proof

        let func = &module.funcs[self.pc.func()];
        data.extend(func.code[self.pc.inst()].serialize_for_proof());
        data.extend(
            func.code_merkle
                .prove(self.pc.inst())
                .expect("Failed to prove against code merkle"),
        );
        data.extend(
            module
                .funcs_merkle
                .prove(self.pc.func())
                .expect("Failed to prove against function merkle"),
        );

        // End next instruction proof, begin instruction specific serialization

        if let Some(next_inst) = func.code.get(self.pc.inst()) {
            if matches!(
                next_inst.opcode,
                Opcode::GetGlobalStateBytes32
                    | Opcode::SetGlobalStateBytes32
                    | Opcode::GetGlobalStateU64
                    | Opcode::SetGlobalStateU64
            ) {
                data.extend(self.global_state.serialize());
            }
            if matches!(next_inst.opcode, Opcode::LocalGet | Opcode::LocalSet) {
                let locals = &self.frame_stack.last().unwrap().locals;
                let idx = next_inst.argument_data as usize;
                data.extend(locals[idx].serialize_for_proof());
                let locals_merkle =
                    Merkle::new(MerkleType::Value, locals.iter().map(|v| v.hash()).collect());
                data.extend(
                    locals_merkle
                        .prove(idx)
                        .expect("Out of bounds local access"),
                );
            } else if matches!(next_inst.opcode, Opcode::GlobalGet | Opcode::GlobalSet) {
                let idx = next_inst.argument_data as usize;
                data.extend(module.globals[idx].serialize_for_proof());
                let locals_merkle = Merkle::new(
                    MerkleType::Value,
                    module.globals.iter().map(|v| v.hash()).collect(),
                );
                data.extend(
                    locals_merkle
                        .prove(idx)
                        .expect("Out of bounds global access"),
                );
            } else if matches!(
                next_inst.opcode,
                Opcode::MemoryLoad { .. } | Opcode::MemoryStore { .. },
            ) {
                let is_store = matches!(next_inst.opcode, Opcode::MemoryStore { .. });
                // this isn't really a bool -> int, it's determining an offset based on a bool
                #[allow(clippy::bool_to_int_with_if)]
                let stack_idx_offset = if is_store {
                    // The index is one item below the top stack item for a memory store
                    1
                } else {
                    0
                };
                let base = match self
                    .value_stack
                    .get(self.value_stack.len() - 1 - stack_idx_offset)
                {
                    Some(Value::I32(x)) => *x,
                    x => panic!("WASM validation failed: memory index type is {:?}", x),
                };
                if let Some(mut idx) = u64::from(base)
                    .checked_add(next_inst.argument_data)
                    .and_then(|x| usize::try_from(x).ok())
                {
                    // Prove the leaf this index is in, and the next one, if they are within the memory's size.
                    idx /= Memory::LEAF_SIZE;
                    data.extend(module.memory.get_leaf_data(idx));
                    data.extend(mem_merkle.prove(idx).unwrap_or_default());
                    // Now prove the next leaf too, in case it's accessed.
                    let next_leaf_idx = idx.saturating_add(1);
                    data.extend(module.memory.get_leaf_data(next_leaf_idx));
                    let second_mem_merkle = if is_store {
                        // For stores, prove the second merkle against a state after the first leaf is set.
                        // This state also happens to have the second leaf set, but that's irrelevant.
                        let mut copy = self.clone();
                        copy.step_n(1)
                            .expect("Failed to step machine forward for proof");
                        copy.modules[self.pc.module()]
                            .memory
                            .merkelize()
                            .into_owned()
                    } else {
                        mem_merkle.into_owned()
                    };
                    data.extend(second_mem_merkle.prove(next_leaf_idx).unwrap_or_default());
                }
            } else if next_inst.opcode == Opcode::CallIndirect {
                let (table, ty) = crate::wavm::unpack_call_indirect(next_inst.argument_data);
                let idx = match self.value_stack.last() {
                    Some(Value::I32(i)) => *i,
                    x => panic!(
                        "WASM validation failed: top of stack before call_indirect is {:?}",
                        x,
                    ),
                };
                let ty = &module.types[usize::try_from(ty).unwrap()];
                data.extend((table as u64).to_be_bytes());
                data.extend(ty.hash());
                let table_usize = usize::try_from(table).unwrap();
                let table = &module.tables[table_usize];
                data.extend(
                    table
                        .serialize_for_proof()
                        .expect("failed to serialize table"),
                );
                data.extend(
                    module
                        .tables_merkle
                        .prove(table_usize)
                        .expect("Failed to prove tables merkle"),
                );
                let idx_usize = usize::try_from(idx).unwrap();
                if let Some(elem) = table.elems.get(idx_usize) {
                    data.extend(elem.func_ty.hash());
                    data.extend(elem.val.serialize_for_proof());
                    data.extend(
                        table
                            .elems_merkle
                            .prove(idx_usize)
                            .expect("Failed to prove elements merkle"),
                    );
                }
            } else if matches!(
                next_inst.opcode,
                Opcode::GetGlobalStateBytes32 | Opcode::SetGlobalStateBytes32,
            ) {
                let ptr = self.value_stack.last().unwrap().assume_u32();
                if let Some(mut idx) = usize::try_from(ptr).ok().filter(|x| x % 32 == 0) {
                    // Prove the leaf this index is in
                    idx /= Memory::LEAF_SIZE;
                    data.extend(module.memory.get_leaf_data(idx));
                    data.extend(mem_merkle.prove(idx).unwrap_or_default());
                }
            } else if matches!(
                next_inst.opcode,
                Opcode::ReadPreImage | Opcode::ReadInboxMessage,
            ) {
                let ptr = self
                    .value_stack
                    .get(self.value_stack.len() - 2)
                    .unwrap()
                    .assume_u32();
                if let Some(mut idx) = usize::try_from(ptr).ok().filter(|x| x % 32 == 0) {
                    // Prove the leaf this index is in
                    idx /= Memory::LEAF_SIZE;
                    let prev_data = module.memory.get_leaf_data(idx);
                    data.extend(prev_data);
                    data.extend(mem_merkle.prove(idx).unwrap_or_default());
                    if next_inst.opcode == Opcode::ReadPreImage {
                        let hash = Bytes32(prev_data);
                        let preimage_ty = PreimageType::try_from(
                            u8::try_from(next_inst.argument_data)
                                .expect("ReadPreImage argument data is out of range for a u8"),
                        )
                        .expect("Invalid preimage type in ReadPreImage argument data");
                        let preimage =
                            match self
                                .preimage_resolver
                                .get_const(self.context, preimage_ty, hash)
                            {
                                Some(b) => b,
                                None => panic!("Missing requested preimage for hash {}", hash),
                            };
                        data.push(0); // preimage proof type
                        data.extend(preimage);
                    } else if next_inst.opcode == Opcode::ReadInboxMessage {
                        let msg_idx = self
                            .value_stack
                            .get(self.value_stack.len() - 3)
                            .unwrap()
                            .assume_u64();
                        let inbox_identifier = argument_data_to_inbox(next_inst.argument_data)
                            .expect("Bad inbox indentifier");
                        if let Some(msg_data) =
                            self.inbox_contents.get(&(inbox_identifier, msg_idx))
                        {
                            data.push(0); // inbox proof type
                            data.extend(msg_data);
                        }
                    } else {
                        panic!("Should never ever get here");
                    }
                }
            }
        }

        data
    }

    pub fn get_data_stack(&self) -> &[Value] {
        &self.value_stack
    }

    pub fn get_global_state(&self) -> GlobalState {
        self.global_state.clone()
    }

    pub fn set_global_state(&mut self, gs: GlobalState) {
        self.global_state = gs;
    }

    pub fn set_preimage_resolver(&mut self, resolver: PreimageResolver) {
        self.preimage_resolver.resolver = resolver;
    }

    pub fn set_context(&mut self, context: u64) {
        self.context = context;
    }

    pub fn add_inbox_msg(&mut self, identifier: InboxIdentifier, index: u64, data: Vec<u8>) {
        self.inbox_contents.insert((identifier, index), data);
        if index >= self.first_too_far && identifier == InboxIdentifier::Sequencer {
            self.first_too_far = index + 1
        }
    }

    pub fn get_module_names(&self, module: usize) -> Option<&NameCustomSection> {
        self.modules.get(module).map(|m| &*m.names)
    }

    pub fn get_backtrace(&self) -> Vec<(String, String, usize)> {
        let mut res = Vec::new();
        let mut push_pc = |pc: ProgramCounter| {
            let names = &self.modules[pc.module()].names;
            let func = names
                .functions
                .get(&pc.func)
                .cloned()
                .unwrap_or_else(|| format!("{}", pc.func));
            let mut module = names.module.clone();
            if module.is_empty() {
                module = format!("{}", pc.module);
            }
            res.push((module, func, pc.inst()));
        };
        push_pc(self.pc);
        for frame in self.frame_stack.iter().rev() {
            if let Value::InternalRef(pc) = frame.return_ref {
                push_pc(pc);
            }
        }
        res
    }

    pub fn eprint_backtrace(&self) {
        eprintln!("Backtrace:");
        for (module, func, pc) in self.get_backtrace() {
            let func = rustc_demangle::demangle(&func);
            eprintln!("  {} {} @ {}", module, func.mint(), pc.blue());
        }
    }
}

'''
'''--- arbitrator/prover/src/main.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use arbutil::{format, Color, DebugColor, PreimageType};
use eyre::{Context, Result};
use fnv::{FnvHashMap as HashMap, FnvHashSet as HashSet};
use prover::{
    machine::{GlobalState, InboxIdentifier, Machine, MachineStatus, PreimageResolver, ProofInfo},
    utils::{Bytes32, CBytes},
    wavm::Opcode,
};
use std::sync::Arc;
use std::{convert::TryInto, io::BufWriter};
use std::{
    fs::File,
    io::{BufReader, ErrorKind, Read, Write},
    path::{Path, PathBuf},
};
use structopt::StructOpt;

#[derive(StructOpt)]
#[structopt(name = "arbitrator-prover")]
struct Opts {
    binary: PathBuf,
    #[structopt(short, long)]
    libraries: Vec<PathBuf>,
    #[structopt(short, long)]
    output: Option<PathBuf>,
    #[structopt(short = "b", long)]
    proving_backoff: bool,
    #[structopt(long)]
    allow_hostapi: bool,
    #[structopt(long)]
    inbox_add_stub_headers: bool,
    #[structopt(long)]
    always_merkleize: bool,
    /// profile output instead of generting proofs
    #[structopt(short = "p", long)]
    profile_run: bool,
    /// simple summary of hot opcodes
    #[structopt(long)]
    profile_sum_opcodes: bool,
    /// simple summary of hot functions
    #[structopt(long)]
    profile_sum_funcs: bool,
    /// profile written in "folded" format (use as input for e.g. inferno-flamegraph)
    #[structopt(long)]
    profile_output: Option<PathBuf>,
    #[structopt(short = "i", long, default_value = "1")]
    proving_interval: u64,
    #[structopt(short = "s", long, default_value = "0")]
    proving_start: u64,
    #[structopt(long, default_value = "0")]
    delayed_inbox_position: u64,
    #[structopt(long, default_value = "0")]
    inbox_position: u64,
    #[structopt(long, default_value = "0")]
    position_within_message: u64,
    #[structopt(long)]
    last_block_hash: Option<String>,
    #[structopt(long)]
    last_send_root: Option<String>,
    #[structopt(long)]
    inbox: Vec<PathBuf>,
    #[structopt(long)]
    delayed_inbox: Vec<PathBuf>,
    #[structopt(long)]
    preimages: Option<PathBuf>,
    /// Require that the machine end in the Finished state
    #[structopt(long)]
    require_success: bool,
    /// Generate WAVM binary, until host io state, and module root and exit
    #[structopt(long)]
    generate_binaries: Option<PathBuf>,
    #[structopt(long)]
    skip_until_host_io: bool,
    #[structopt(long)]
    max_steps: Option<u64>,
}

fn file_with_stub_header(path: &Path, headerlength: usize) -> Result<Vec<u8>> {
    let mut msg = vec![0u8; headerlength];
    File::open(path).unwrap().read_to_end(&mut msg)?;
    Ok(msg)
}

fn decode_hex_arg(arg: &Option<String>, name: &str) -> Result<Bytes32> {
    if let Some(arg) = arg {
        let mut arg = arg.as_str();
        if arg.starts_with("0x") {
            arg = &arg[2..];
        }
        let mut bytes32 = Bytes32::default();
        hex::decode_to_slice(arg, &mut bytes32.0)
            .wrap_err_with(|| format!("failed to parse {} contents", name))?;
        Ok(bytes32)
    } else {
        Ok(Bytes32::default())
    }
}

#[derive(Debug, Default, Copy, Clone, PartialEq, Eq)]
struct SimpleProfile {
    count: u64,
    total_cycles: u64,
    local_cycles: u64,
}

const INBOX_HEADER_LEN: usize = 40; // also in test-case's host-io.rs & contracts's OneStepProverHostIo.sol
const DELAYED_HEADER_LEN: usize = 112; // also in test-case's host-io.rs & contracts's OneStepProverHostIo.sol

fn main() -> Result<()> {
    let opts = Opts::from_args();

    let mut inbox_contents = HashMap::default();
    let mut inbox_position = opts.inbox_position;
    let mut delayed_position = opts.delayed_inbox_position;
    let inbox_header_len;
    let delayed_header_len;
    if opts.inbox_add_stub_headers {
        inbox_header_len = INBOX_HEADER_LEN;
        delayed_header_len = DELAYED_HEADER_LEN + 1;
    } else {
        inbox_header_len = 0;
        delayed_header_len = 0;
    }

    for path in opts.inbox {
        inbox_contents.insert(
            (InboxIdentifier::Sequencer, inbox_position),
            file_with_stub_header(&path, inbox_header_len)?,
        );
        println!("read file {:?} to seq. inbox {}", &path, inbox_position);
        inbox_position += 1;
    }
    for path in opts.delayed_inbox {
        inbox_contents.insert(
            (InboxIdentifier::Delayed, delayed_position),
            file_with_stub_header(&path, delayed_header_len)?,
        );
        delayed_position += 1;
    }

    let mut preimages: HashMap<PreimageType, HashMap<Bytes32, CBytes>> = HashMap::default();
    if let Some(path) = opts.preimages {
        let mut file = BufReader::new(File::open(path)?);
        loop {
            let mut ty_buf = [0u8; 1];
            match file.read_exact(&mut ty_buf) {
                Ok(()) => {}
                Err(e) if e.kind() == ErrorKind::UnexpectedEof => break,
                Err(e) => return Err(e.into()),
            }
            let preimage_ty: PreimageType = ty_buf[0].try_into()?;

            let mut size_buf = [0u8; 8];
            file.read_exact(&mut size_buf)?;
            let size = u64::from_le_bytes(size_buf) as usize;
            let mut buf = vec![0u8; size];
            file.read_exact(&mut buf)?;

            let hash = preimage_ty.hash(&buf);
            preimages
                .entry(preimage_ty)
                .or_default()
                .insert(hash.into(), buf.as_slice().into());
        }
    }
    let preimage_resolver =
        Arc::new(move |_, ty, hash| preimages.get(&ty).and_then(|m| m.get(&hash)).cloned())
            as PreimageResolver;

    let last_block_hash = decode_hex_arg(&opts.last_block_hash, "--last-block-hash")?;
    let last_send_root = decode_hex_arg(&opts.last_send_root, "--last-send-root")?;

    let global_state = GlobalState {
        u64_vals: [opts.inbox_position, opts.position_within_message],
        bytes32_vals: [last_block_hash, last_send_root],
    };

    let mut mach = Machine::from_paths(
        &opts.libraries,
        &opts.binary,
        true,
        opts.always_merkleize,
        opts.allow_hostapi,
        global_state,
        inbox_contents,
        preimage_resolver,
    )?;
    if let Some(output_path) = opts.generate_binaries {
        let mut module_root_file = File::create(output_path.join("module-root.txt"))?;
        writeln!(module_root_file, "0x{}", mach.get_modules_root())?;
        module_root_file.flush()?;

        mach.serialize_binary(output_path.join("machine.wavm.br"))?;
        while !mach.next_instruction_is_host_io() {
            mach.step_n(1)?;
        }
        mach.serialize_state(output_path.join("until-host-io-state.bin"))?;

        return Ok(());
    }

    println!("Starting machine hash: {}", mach.hash());

    let mut proofs: Vec<ProofInfo> = Vec::new();
    let mut seen_states = HashSet::default();
    let mut opcode_counts: HashMap<Opcode, usize> = HashMap::default();
    let mut opcode_profile: HashMap<Opcode, SimpleProfile> = HashMap::default();
    let mut func_profile: HashMap<(usize, usize), SimpleProfile> = HashMap::default();
    let mut func_stack: Vec<(usize, usize, SimpleProfile)> = Vec::default();
    let mut backtrace_stack: Vec<(usize, usize)> = Vec::default();
    let mut cycles_measured_total: u64 = 0;
    let mut profile_backtrace_counts: HashMap<Vec<(usize, usize)>, u64> = HashMap::default();
    let cycles_bigloop_start: u64;
    let cycles_bigloop_end: u64;
    #[cfg(target_arch = "x86_64")]
    unsafe {
        cycles_bigloop_start = core::arch::x86_64::_rdtsc();
    }
    mach.step_n(opts.proving_start)?;
    if opts.skip_until_host_io && !opts.profile_run {
        while !mach.next_instruction_is_host_io() {
            mach.step_n(1)?;
        }
    }
    let mut skipping_profiling = opts.skip_until_host_io;
    while !mach.is_halted() {
        if let Some(max_steps) = opts.max_steps {
            if mach.get_steps() >= max_steps {
                break;
            }
        }

        let next_inst = mach.get_next_instruction().unwrap();
        let next_opcode = next_inst.opcode;

        if opts.proving_backoff {
            let count_entry = opcode_counts.entry(next_opcode).or_insert(0);
            *count_entry += 1;
            let count = *count_entry;
            // Apply an exponential backoff to how often to prove an instruction;
            let prove =
                count < 5 || (count < 25 && count % 5 == 0) || (count < 125 && count % 25 == 0);
            if !prove {
                mach.step_n(1)?;
                continue;
            }
        }

        if opts.profile_run {
            skipping_profiling = skipping_profiling && !mach.next_instruction_is_host_io();
            let start: u64;
            let end: u64;
            let pc = mach.get_pc().unwrap();
            #[cfg(target_arch = "x86_64")]
            unsafe {
                start = core::arch::x86_64::_rdtsc();
            }
            mach.step_n(1)?;
            #[cfg(target_arch = "x86_64")]
            unsafe {
                end = core::arch::x86_64::_rdtsc();
            }
            #[cfg(not(target_arch = "x86_64"))]
            {
                start = 0;
                end = 1;
            }
            let profile_time = end - start;

            if !skipping_profiling {
                cycles_measured_total += profile_time;
            }

            if opts.profile_sum_opcodes && !skipping_profiling {
                let opprofile = opcode_profile.entry(next_opcode).or_default();
                opprofile.count += 1;
                opprofile.total_cycles += profile_time;
            }

            if pc.inst == 0 {
                func_stack.push((pc.module(), pc.func(), SimpleProfile::default()));
                backtrace_stack.push((pc.module(), pc.func()));
            }
            let this_func_profile = &mut func_stack.last_mut().unwrap().2;
            if !skipping_profiling {
                this_func_profile.count += 1;
                this_func_profile.total_cycles += profile_time;
                this_func_profile.local_cycles += profile_time;
            }
            if next_opcode == Opcode::Return {
                let (module, func, profile) = func_stack.pop().unwrap();

                if opts.profile_sum_funcs && !skipping_profiling {
                    if let Some(parent_func) = &mut func_stack.last_mut() {
                        parent_func.2.count += profile.count;
                        parent_func.2.total_cycles += profile.total_cycles;
                    }
                    let func_profile_entry = func_profile.entry((module, func)).or_default();
                    func_profile_entry.count += profile.count;
                    func_profile_entry.total_cycles += profile.total_cycles;
                    func_profile_entry.local_cycles += profile.local_cycles;
                }

                if opts.profile_output.is_some() && !skipping_profiling {
                    *profile_backtrace_counts
                        .entry(backtrace_stack.clone())
                        .or_default() += profile.local_cycles;
                }
                backtrace_stack.pop();
            }
        } else {
            let values = mach.get_data_stack();
            if !values.is_empty() {
                println!("{} {}", "Machine stack".grey(), format::commas(values));
            }
            print!(
                "Generating proof {} (inst {}) for {}{}",
                proofs.len().blue(),
                mach.get_steps().blue(),
                next_opcode.debug_mint(),
                match next_inst.argument_data {
                    0 => "".into(),
                    v => format!(" with data 0x{v:x}"),
                }
            );
            std::io::stdout().flush().unwrap();
            let before = mach.hash();
            if !seen_states.insert(before) {
                break;
            }
            let proof = mach.serialize_proof();
            mach.step_n(1)?;
            let after = mach.hash();
            println!(" - done");
            proofs.push(ProofInfo {
                before: before.to_string(),
                proof: hex::encode(proof),
                after: after.to_string(),
            });
            mach.step_n(opts.proving_interval.saturating_sub(1))?;
        }
    }
    #[cfg(target_arch = "x86_64")]
    unsafe {
        cycles_bigloop_end = core::arch::x86_64::_rdtsc();
    }
    #[cfg(not(target_arch = "x86_64"))]
    {
        cycles_bigloop_start = 0;
        cycles_bigloop_end = 0;
    }

    let cycles_bigloop = cycles_bigloop_end - cycles_bigloop_start;

    if !proofs.is_empty() && mach.is_halted() {
        let hash = mach.hash();
        proofs.push(ProofInfo {
            before: hash.to_string(),
            proof: hex::encode(mach.serialize_proof()),
            after: hash.to_string(),
        });
    }

    println!("End machine status: {:?}", mach.get_status());
    println!("End machine hash: {}", mach.hash());
    println!("End machine stack: {:?}", mach.get_data_stack());
    println!("End machine backtrace:");
    for (module, func, pc) in mach.get_backtrace() {
        let func = rustc_demangle::demangle(&func);
        println!("  {} {} @ {}", module, func.mint(), pc.blue());
    }

    if let Some(out) = opts.output {
        let out = File::create(out)?;
        serde_json::to_writer_pretty(out, &proofs)?;
    }

    if opts.profile_run {
        let mut sum = SimpleProfile::default();
        while let Some((module, func, profile)) = func_stack.pop() {
            sum.total_cycles += profile.total_cycles;
            sum.count += profile.count;
            let entry = func_profile.entry((module, func)).or_default();
            entry.count += sum.count;
            entry.total_cycles += sum.total_cycles;
            entry.local_cycles += profile.local_cycles;
        }

        println!(
            "Total cycles measured {} out of {} in loop ({}%)",
            cycles_measured_total,
            cycles_bigloop,
            (cycles_measured_total as f64) * 100.0 / (cycles_bigloop as f64)
        );

        if opts.profile_sum_opcodes {
            println!("\n===Operations:");
            let mut ops_vector: Vec<_> = opcode_profile.iter().collect();
            ops_vector.sort_by(|a, b| b.1.total_cycles.cmp(&a.1.total_cycles));
            let mut printed = 0;
            for (opcode, profile) in ops_vector {
                println!(
                    "Opcode {:?}: steps: {} cycles: {} ({}%)",
                    opcode,
                    profile.count,
                    profile.total_cycles,
                    (profile.total_cycles as f64) * 100.0 / (cycles_measured_total as f64),
                );
                printed += 1;
                if printed > 20 {
                    break;
                }
            }
        }

        let opts_binary = opts.binary;
        let opts_libraries = opts.libraries;
        let format_pc = |module_num: usize, func_num: usize| -> (String, String) {
            let names = match mach.get_module_names(module_num) {
                Some(n) => n,
                None => {
                    return (
                        format!("[unknown {}]", module_num),
                        format!("[unknown {}]", func_num),
                    );
                }
            };
            let module_name = if module_num == 0 {
                names.module.clone()
            } else if module_num == &opts_libraries.len() + 1 {
                opts_binary.file_name().unwrap().to_str().unwrap().into()
            } else {
                opts_libraries[module_num - 1]
                    .file_name()
                    .unwrap()
                    .to_str()
                    .unwrap()
                    .into()
            };
            let func_idx = func_num as u32;
            let mut name = names
                .functions
                .get(&func_idx)
                .cloned()
                .unwrap_or_else(|| format!("[unknown {}]", func_idx));
            name = rustc_demangle::demangle(&name).to_string();
            (module_name, name)
        };

        if opts.profile_sum_funcs {
            println!("\n===Functions:");
            let mut func_vector: Vec<_> = func_profile.iter().collect();
            func_vector.sort_by(|a, b| b.1.total_cycles.cmp(&a.1.total_cycles));
            let mut printed = 0;
            for (&(module_num, func), profile) in func_vector {
                let (name, module_name) = format_pc(module_num, func);
                let percent =
                    (profile.total_cycles as f64) * 100.0 / (cycles_measured_total as f64);
                println!(
                    "module {}: function: {} {} steps: {} cycles: {} ({}%)",
                    module_name, func, name, profile.count, profile.total_cycles, percent,
                );
                printed += 1;
                if printed > 20 && percent < 3.0 {
                    break;
                }
            }
        }

        if let Some(out) = opts.profile_output {
            let mut out = BufWriter::new(File::create(out)?);
            for (backtrace, count) in profile_backtrace_counts {
                let mut path = String::new();
                let mut last_module = None;
                for (module, func) in backtrace {
                    let (module_name, func_name) = format_pc(module, func);
                    if last_module != Some(module) {
                        path += "[module] ";
                        path += &module_name;
                        path += ";";
                        last_module = Some(module);
                    }
                    path += &func_name;
                    path += ";";
                }
                path.pop(); // remove trailing ';'
                writeln!(out, "{} {}", path, count)?;
            }
            out.flush()?;
        }
    }

    if opts.require_success && mach.get_status() != MachineStatus::Finished {
        eprintln!("Machine didn't finish: {}", mach.get_status().red());
        std::process::exit(1);
    }

    Ok(())
}

'''
'''--- arbitrator/prover/src/memory.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{
    merkle::{Merkle, MerkleType},
    utils::Bytes32,
    value::{ArbValueType, Value},
};
use digest::Digest;
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use sha3::Keccak256;
use std::{borrow::Cow, convert::TryFrom};

#[derive(PartialEq, Eq, Clone, Debug, Default, Serialize, Deserialize)]
pub struct Memory {
    buffer: Vec<u8>,
    #[serde(skip)]
    pub merkle: Option<Merkle>,
    pub max_size: u64,
}

fn hash_leaf(bytes: [u8; Memory::LEAF_SIZE]) -> Bytes32 {
    let mut h = Keccak256::new();
    h.update("Memory leaf:");
    h.update(bytes);
    h.finalize().into()
}

fn round_up_to_power_of_two(mut input: usize) -> usize {
    if input == 0 {
        return 1;
    }
    input -= 1;
    1usize
        .checked_shl(usize::BITS - input.leading_zeros())
        .expect("Can't round buffer up to power of two and fit in memory")
}

/// Overflow safe divide and round up
fn div_round_up(num: usize, denom: usize) -> usize {
    let mut res = num / denom;
    if num % denom > 0 {
        res += 1;
    }
    res
}

impl Memory {
    pub const LEAF_SIZE: usize = 32;
    /// Only used when initializing a memory to determine its size
    pub const PAGE_SIZE: u64 = 65536;
    /// The number of layers in the memory merkle tree
    /// 1 + log2(2^32 / LEAF_SIZE) = 1 + log2(2^(32 - log2(LEAF_SIZE))) = 1 + 32 - 5
    const MEMORY_LAYERS: usize = 1 + 32 - 5;

    pub fn new(size: usize, max_size: u64) -> Memory {
        Memory {
            buffer: vec![0u8; size],
            merkle: None,
            max_size,
        }
    }

    pub fn size(&self) -> u64 {
        self.buffer.len() as u64
    }

    pub fn merkelize(&self) -> Cow<'_, Merkle> {
        if let Some(m) = &self.merkle {
            return Cow::Borrowed(m);
        }
        // Round the size up to 8 byte long leaves, then round up to the next power of two number of leaves
        let leaves = round_up_to_power_of_two(div_round_up(self.buffer.len(), Self::LEAF_SIZE));
        let mut leaf_hashes: Vec<Bytes32> = self
            .buffer
            .par_chunks(Self::LEAF_SIZE)
            .map(|leaf| {
                let mut full_leaf = [0u8; 32];
                full_leaf[..leaf.len()].copy_from_slice(leaf);
                hash_leaf(full_leaf)
            })
            .collect();
        if leaf_hashes.len() < leaves {
            let empty_hash = hash_leaf([0u8; 32]);
            leaf_hashes.resize(leaves, empty_hash);
        }
        Cow::Owned(Merkle::new_advanced(
            MerkleType::Memory,
            leaf_hashes,
            hash_leaf([0u8; 32]),
            Self::MEMORY_LAYERS,
        ))
    }

    pub fn get_leaf_data(&self, leaf_idx: usize) -> [u8; Self::LEAF_SIZE] {
        let mut buf = [0u8; Self::LEAF_SIZE];
        let idx = match leaf_idx.checked_mul(Self::LEAF_SIZE) {
            Some(x) if x < self.buffer.len() => x,
            _ => return buf,
        };
        let size = std::cmp::min(Self::LEAF_SIZE, self.buffer.len() - idx);
        buf[..size].copy_from_slice(&self.buffer[idx..(idx + size)]);
        buf
    }

    pub fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update("Memory:");
        h.update((self.buffer.len() as u64).to_be_bytes());
        h.update(self.max_size.to_be_bytes());
        h.update(self.merkelize().root());
        h.finalize().into()
    }

    pub fn get_u8(&self, idx: u64) -> Option<u8> {
        if idx >= self.buffer.len() as u64 {
            None
        } else {
            Some(self.buffer[idx as usize])
        }
    }

    pub fn get_u16(&self, idx: u64) -> Option<u16> {
        // The index after the last index containing the u16
        let end_idx = idx.checked_add(2)?;
        if end_idx > self.buffer.len() as u64 {
            None
        } else {
            let mut buf = [0u8; 2];
            buf.copy_from_slice(&self.buffer[(idx as usize)..(end_idx as usize)]);
            Some(u16::from_le_bytes(buf))
        }
    }

    pub fn get_u32(&self, idx: u64) -> Option<u32> {
        let end_idx = idx.checked_add(4)?;
        if end_idx > self.buffer.len() as u64 {
            None
        } else {
            let mut buf = [0u8; 4];
            buf.copy_from_slice(&self.buffer[(idx as usize)..(end_idx as usize)]);
            Some(u32::from_le_bytes(buf))
        }
    }

    pub fn get_u64(&self, idx: u64) -> Option<u64> {
        let end_idx = idx.checked_add(8)?;
        if end_idx > self.buffer.len() as u64 {
            None
        } else {
            let mut buf = [0u8; 8];
            buf.copy_from_slice(&self.buffer[(idx as usize)..(end_idx as usize)]);
            Some(u64::from_le_bytes(buf))
        }
    }

    pub fn get_value(&self, idx: u64, ty: ArbValueType, bytes: u8, signed: bool) -> Option<Value> {
        let contents = match (bytes, signed) {
            (1, false) => i64::from(self.get_u8(idx)?),
            (2, false) => i64::from(self.get_u16(idx)?),
            (4, false) => i64::from(self.get_u32(idx)?),
            (8, false) => self.get_u64(idx)? as i64,
            (1, true) => i64::from(self.get_u8(idx)? as i8),
            (2, true) => i64::from(self.get_u16(idx)? as i16),
            (4, true) => i64::from(self.get_u32(idx)? as i32),
            _ => panic!(
                "Attempted to load from memory with {} bytes and signed {}",
                bytes, signed,
            ),
        };
        Some(match ty {
            ArbValueType::I32 => Value::I32(contents as u32),
            ArbValueType::I64 => Value::I64(contents as u64),
            ArbValueType::F32 => {
                assert!(bytes == 4 && !signed, "Invalid source for f32");
                Value::F32(f32::from_bits(contents as u32))
            }
            ArbValueType::F64 => {
                assert!(bytes == 8 && !signed, "Invalid source for f64");
                Value::F64(f64::from_bits(contents as u64))
            }
            _ => panic!("Invalid memory load output type {:?}", ty),
        })
    }

    #[must_use]
    pub fn store_value(&mut self, idx: u64, value: u64, bytes: u8) -> bool {
        let end_idx = match idx.checked_add(bytes.into()) {
            Some(x) => x,
            None => return false,
        };
        if end_idx > self.buffer.len() as u64 {
            return false;
        }
        let idx = idx as usize;
        let end_idx = end_idx as usize;
        let buf = value.to_le_bytes();
        self.buffer[idx..end_idx].copy_from_slice(&buf[..bytes.into()]);

        if let Some(mut merkle) = self.merkle.take() {
            let start_leaf = idx / Self::LEAF_SIZE;
            merkle.set(start_leaf, hash_leaf(self.get_leaf_data(start_leaf)));
            let end_leaf = (end_idx - 1) / Self::LEAF_SIZE;
            if end_leaf != start_leaf {
                merkle.set(end_leaf, hash_leaf(self.get_leaf_data(end_leaf)));
            }
            self.merkle = Some(merkle);
        }

        true
    }

    #[must_use]
    pub fn store_slice_aligned(&mut self, idx: u64, value: &[u8]) -> bool {
        if idx % Self::LEAF_SIZE as u64 != 0 {
            return false;
        }
        let end_idx = match idx.checked_add(value.len() as u64) {
            Some(x) => x,
            None => return false,
        };
        if end_idx > self.buffer.len() as u64 {
            return false;
        }
        let idx = idx as usize;
        let end_idx = end_idx as usize;
        self.buffer[idx..end_idx].copy_from_slice(value);

        if let Some(mut merkle) = self.merkle.take() {
            let start_leaf = idx / Self::LEAF_SIZE;
            merkle.set(start_leaf, hash_leaf(self.get_leaf_data(start_leaf)));
            // No need for second merkle
            assert!(value.len() <= Self::LEAF_SIZE);
        }

        true
    }

    #[must_use]
    pub fn load_32_byte_aligned(&self, idx: u64) -> Option<Bytes32> {
        if idx % Self::LEAF_SIZE as u64 != 0 {
            return None;
        }
        let idx = match usize::try_from(idx) {
            Ok(x) => x,
            Err(_) => return None,
        };

        let slice = self.get_range(idx, 32)?;
        let mut bytes = Bytes32::default();
        bytes.copy_from_slice(slice);
        Some(bytes)
    }

    pub fn get_range(&self, offset: usize, len: usize) -> Option<&[u8]> {
        let end = offset.checked_add(len)?;
        if end > self.buffer.len() {
            return None;
        }
        Some(&self.buffer[offset..end])
    }

    pub fn set_range(&mut self, offset: usize, data: &[u8]) {
        self.merkle = None;
        let end = offset
            .checked_add(data.len())
            .expect("Overflow in offset+data.len() in Memory::set_range");
        self.buffer[offset..end].copy_from_slice(data);
    }

    pub fn cache_merkle_tree(&mut self) {
        self.merkle = Some(self.merkelize().into_owned());
    }

    pub fn resize(&mut self, new_size: usize) {
        let had_merkle_tree = self.merkle.is_some();
        self.merkle = None;
        self.buffer.resize(new_size, 0);
        if had_merkle_tree {
            self.cache_merkle_tree();
        }
    }
}

#[cfg(test)]
mod test {
    use crate::memory::round_up_to_power_of_two;

    #[test]
    pub fn test_round_up_power_of_two() {
        assert_eq!(round_up_to_power_of_two(0), 1);
        assert_eq!(round_up_to_power_of_two(1), 1);
        assert_eq!(round_up_to_power_of_two(2), 2);
        assert_eq!(round_up_to_power_of_two(3), 4);
        assert_eq!(round_up_to_power_of_two(4), 4);
        assert_eq!(round_up_to_power_of_two(5), 8);
        assert_eq!(round_up_to_power_of_two(6), 8);
        assert_eq!(round_up_to_power_of_two(7), 8);
        assert_eq!(round_up_to_power_of_two(8), 8);
    }
}

'''
'''--- arbitrator/prover/src/merkle.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::utils::Bytes32;
use digest::Digest;
use rayon::prelude::*;
use sha3::Keccak256;
use std::convert::TryFrom;

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum MerkleType {
    Empty,
    Value,
    Function,
    Instruction,
    Memory,
    Table,
    TableElement,
    Module,
}

impl Default for MerkleType {
    fn default() -> Self {
        Self::Empty
    }
}

impl MerkleType {
    pub fn get_prefix(self) -> &'static str {
        match self {
            MerkleType::Empty => panic!("Attempted to get prefix of empty merkle type"),
            MerkleType::Value => "Value merkle tree:",
            MerkleType::Function => "Function merkle tree:",
            MerkleType::Instruction => "Instruction merkle tree:",
            MerkleType::Memory => "Memory merkle tree:",
            MerkleType::Table => "Table merkle tree:",
            MerkleType::TableElement => "Table element merkle tree:",
            MerkleType::Module => "Module merkle tree:",
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Default)]
pub struct Merkle {
    ty: MerkleType,
    layers: Vec<Vec<Bytes32>>,
    empty_layers: Vec<Bytes32>,
}

fn hash_node(ty: MerkleType, a: Bytes32, b: Bytes32) -> Bytes32 {
    let mut h = Keccak256::new();
    h.update(ty.get_prefix());
    h.update(a);
    h.update(b);
    h.finalize().into()
}

impl Merkle {
    pub fn new(ty: MerkleType, hashes: Vec<Bytes32>) -> Merkle {
        Self::new_advanced(ty, hashes, Bytes32::default(), 0)
    }

    pub fn new_advanced(
        ty: MerkleType,
        hashes: Vec<Bytes32>,
        empty_hash: Bytes32,
        min_depth: usize,
    ) -> Merkle {
        if hashes.is_empty() {
            return Merkle::default();
        }
        let mut layers = vec![hashes];
        let mut empty_layers = vec![empty_hash];
        while layers.last().unwrap().len() > 1 || layers.len() < min_depth {
            let empty_layer = *empty_layers.last().unwrap();
            let new_layer = layers
                .last()
                .unwrap()
                .par_chunks(2)
                .map(|window| {
                    hash_node(ty, window[0], window.get(1).cloned().unwrap_or(empty_layer))
                })
                .collect();
            empty_layers.push(hash_node(ty, empty_layer, empty_layer));
            layers.push(new_layer);
        }
        Merkle {
            ty,
            layers,
            empty_layers,
        }
    }

    pub fn root(&self) -> Bytes32 {
        if let Some(layer) = self.layers.last() {
            assert_eq!(layer.len(), 1);
            layer[0]
        } else {
            Bytes32::default()
        }
    }

    pub fn leaves(&self) -> &[Bytes32] {
        if self.layers.is_empty() {
            &[]
        } else {
            &self.layers[0]
        }
    }

    #[must_use]
    pub fn prove(&self, mut idx: usize) -> Option<Vec<u8>> {
        if idx >= self.leaves().len() {
            return None;
        }
        let mut proof = vec![u8::try_from(self.layers.len() - 1).unwrap()];
        for (layer_i, layer) in self.layers.iter().enumerate() {
            if layer_i == self.layers.len() - 1 {
                break;
            }
            let counterpart = idx ^ 1;
            proof.extend(
                layer
                    .get(counterpart)
                    .cloned()
                    .unwrap_or_else(|| self.empty_layers[layer_i]),
            );
            idx >>= 1;
        }
        Some(proof)
    }

    pub fn set(&mut self, mut idx: usize, hash: Bytes32) {
        if self.layers[0][idx] == hash {
            return;
        }
        let mut next_hash = hash;
        let empty_layers = &self.empty_layers;
        let layers_len = self.layers.len();
        for (layer_i, layer) in self.layers.iter_mut().enumerate() {
            layer[idx] = next_hash;
            if layer_i == layers_len - 1 {
                // next_hash isn't needed
                break;
            }
            let counterpart = layer
                .get(idx ^ 1)
                .cloned()
                .unwrap_or_else(|| empty_layers[layer_i]);
            if idx % 2 == 0 {
                next_hash = hash_node(self.ty, next_hash, counterpart);
            } else {
                next_hash = hash_node(self.ty, counterpart, next_hash);
            }
            idx >>= 1;
        }
    }
}

'''
'''--- arbitrator/prover/src/reinterpret.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use num::Zero;
use std::{
    num::Wrapping,
    ops::{Add, BitAnd, BitOr, BitXor, Div, Mul, Rem, Shl, Shr, Sub},
};

pub trait ReinterpretAsSigned:
    Sized
    + Add<Output = Self>
    + Sub<Output = Self>
    + Mul<Output = Self>
    + Div<Output = Self>
    + Shl<usize, Output = Self>
    + Shr<usize, Output = Self>
    + Rem<Output = Self>
    + BitAnd<Output = Self>
    + BitOr<Output = Self>
    + BitXor<Output = Self>
    + Zero
{
    type Signed: ReinterpretAsUnsigned<Unsigned = Self>;
    fn cast_signed(self) -> Self::Signed;
    fn cast_usize(self) -> usize;

    fn rotl(self, n: usize) -> Self;
    fn rotr(self, n: usize) -> Self;
}

pub trait ReinterpretAsUnsigned:
    Sized
    + Add<Output = Self>
    + Sub<Output = Self>
    + Mul<Output = Self>
    + Div<Output = Self>
    + Shl<usize, Output = Self>
    + Shr<usize, Output = Self>
    + Rem<Output = Self>
    + BitAnd<Output = Self>
    + BitOr<Output = Self>
    + BitXor<Output = Self>
    + Zero
{
    type Unsigned: ReinterpretAsSigned<Signed = Self>;
    fn cast_unsigned(self) -> Self::Unsigned;
}

impl ReinterpretAsSigned for Wrapping<u32> {
    type Signed = Wrapping<i32>;

    fn cast_signed(self) -> Wrapping<i32> {
        Wrapping(self.0 as i32)
    }

    fn cast_usize(self) -> usize {
        self.0 as usize
    }

    fn rotl(self, n: usize) -> Self {
        Wrapping(self.0.rotate_left(n as u32))
    }

    fn rotr(self, n: usize) -> Self {
        Wrapping(self.0.rotate_right(n as u32))
    }
}

impl ReinterpretAsUnsigned for Wrapping<i32> {
    type Unsigned = Wrapping<u32>;

    fn cast_unsigned(self) -> Wrapping<u32> {
        Wrapping(self.0 as u32)
    }
}

impl ReinterpretAsSigned for Wrapping<u64> {
    type Signed = Wrapping<i64>;

    fn cast_signed(self) -> Wrapping<i64> {
        Wrapping(self.0 as i64)
    }

    fn cast_usize(self) -> usize {
        self.0 as usize
    }

    fn rotl(self, n: usize) -> Self {
        Wrapping(self.0.rotate_left(n as u32))
    }

    fn rotr(self, n: usize) -> Self {
        Wrapping(self.0.rotate_right(n as u32))
    }
}

impl ReinterpretAsUnsigned for Wrapping<i64> {
    type Unsigned = Wrapping<u64>;

    fn cast_unsigned(self) -> Wrapping<u64> {
        Wrapping(self.0 as u64)
    }
}

'''
'''--- arbitrator/prover/src/utils.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use eyre::{eyre, Result};
use serde::{Deserialize, Serialize};
use std::{
    borrow::Borrow,
    convert::TryInto,
    fmt,
    fs::File,
    io::Read,
    ops::{Deref, DerefMut},
    path::Path,
};
use wasmparser::{TableType, Type};

/// cbindgen:field-names=[bytes]
#[derive(Default, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[repr(C)]
pub struct Bytes32(pub [u8; 32]);

impl Deref for Bytes32 {
    type Target = [u8; 32];

    fn deref(&self) -> &Self::Target {
        &self.0
    }
}

impl DerefMut for Bytes32 {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.0
    }
}

impl AsRef<[u8]> for Bytes32 {
    fn as_ref(&self) -> &[u8] {
        &self.0
    }
}

impl Borrow<[u8]> for Bytes32 {
    fn borrow(&self) -> &[u8] {
        &self.0
    }
}

impl From<[u8; 32]> for Bytes32 {
    fn from(x: [u8; 32]) -> Self {
        Self(x)
    }
}

impl From<u32> for Bytes32 {
    fn from(x: u32) -> Self {
        let mut b = [0u8; 32];
        b[(32 - 4)..].copy_from_slice(&x.to_be_bytes());
        Self(b)
    }
}

impl From<u64> for Bytes32 {
    fn from(x: u64) -> Self {
        let mut b = [0u8; 32];
        b[(32 - 8)..].copy_from_slice(&x.to_be_bytes());
        Self(b)
    }
}

impl From<usize> for Bytes32 {
    fn from(x: usize) -> Self {
        let mut b = [0u8; 32];
        b[(32 - (usize::BITS as usize / 8))..].copy_from_slice(&x.to_be_bytes());
        Self(b)
    }
}

impl IntoIterator for Bytes32 {
    type Item = u8;
    type IntoIter = std::array::IntoIter<u8, 32>;

    fn into_iter(self) -> Self::IntoIter {
        IntoIterator::into_iter(self.0)
    }
}

type GenericBytes32 = digest::generic_array::GenericArray<u8, digest::generic_array::typenum::U32>;

impl From<GenericBytes32> for Bytes32 {
    fn from(x: GenericBytes32) -> Self {
        <[u8; 32]>::from(x).into()
    }
}

impl fmt::Display for Bytes32 {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}", hex::encode(self))
    }
}

impl fmt::Debug for Bytes32 {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}", hex::encode(self))
    }
}

/// A Vec<u8> allocated with libc::malloc
pub struct CBytes {
    ptr: *mut u8,
    len: usize,
}

impl CBytes {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn as_slice(&self) -> &[u8] {
        unsafe { std::slice::from_raw_parts(self.ptr, self.len) }
    }

    pub unsafe fn from_raw_parts(ptr: *mut u8, len: usize) -> Self {
        Self { ptr, len }
    }
}

impl Default for CBytes {
    fn default() -> Self {
        Self {
            ptr: std::ptr::null_mut(),
            len: 0,
        }
    }
}

impl fmt::Debug for CBytes {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{:?}", self.as_slice())
    }
}

impl From<&[u8]> for CBytes {
    fn from(slice: &[u8]) -> Self {
        if slice.is_empty() {
            return Self::default();
        }
        unsafe {
            let ptr = libc::malloc(slice.len()) as *mut u8;
            if ptr.is_null() {
                panic!("Failed to allocate memory instantiating CBytes");
            }
            std::ptr::copy_nonoverlapping(slice.as_ptr(), ptr, slice.len());
            Self {
                ptr,
                len: slice.len(),
            }
        }
    }
}

// There's no thread safety concerns for CBytes.
// This type is basically a Box<[u8]> (which is Send + Sync) with libc as an allocator.
// Any data races between threads are prevented by Rust borrowing rules,
// and the data isn't thread-local so there's no concern moving it between threads.
unsafe impl Send for CBytes {}
unsafe impl Sync for CBytes {}

#[derive(Serialize, Deserialize)]
#[serde(remote = "Type")]
enum RemoteType {
    I32,
    I64,
    F32,
    F64,
    V128,
    FuncRef,
    ExternRef,
}

#[derive(Serialize, Deserialize)]
#[serde(remote = "TableType")]
pub struct RemoteTableType {
    #[serde(with = "RemoteType")]
    pub element_type: Type,
    pub initial: u32,
    pub maximum: Option<u32>,
}

impl Drop for CBytes {
    fn drop(&mut self) {
        unsafe { libc::free(self.ptr as _) }
    }
}

impl Clone for CBytes {
    fn clone(&self) -> Self {
        self.as_slice().into()
    }
}

impl Deref for CBytes {
    type Target = [u8];

    fn deref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl AsRef<[u8]> for CBytes {
    fn as_ref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl Borrow<[u8]> for CBytes {
    fn borrow(&self) -> &[u8] {
        self.as_slice()
    }
}

#[derive(Clone)]
pub struct CBytesIntoIter(CBytes, usize);

impl Iterator for CBytesIntoIter {
    type Item = u8;

    fn next(&mut self) -> Option<u8> {
        if self.1 >= self.0.len {
            return None;
        }
        let byte = self.0[self.1];
        self.1 += 1;
        Some(byte)
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        let len = self.0.len - self.1;
        (len, Some(len))
    }
}

impl IntoIterator for CBytes {
    type Item = u8;
    type IntoIter = CBytesIntoIter;

    fn into_iter(self) -> CBytesIntoIter {
        CBytesIntoIter(self, 0)
    }
}

pub fn file_bytes(path: &Path) -> Result<Vec<u8>> {
    let mut f = File::open(path)?;
    let mut buf = Vec::new();
    f.read_to_end(&mut buf)?;
    Ok(buf)
}

pub fn split_import(qualified: &str) -> Result<(&str, &str)> {
    let parts: Vec<_> = qualified.split("__").collect();
    let parts = parts.try_into().map_err(|_| eyre!("bad import"))?;
    let [module, name]: [&str; 2] = parts;
    Ok((module, name))
}

'''
'''--- arbitrator/prover/src/value.rs ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use crate::{binary::FloatType, utils::Bytes32};
use arbutil::Color;
use digest::Digest;
use eyre::{bail, Result};
use serde::{Deserialize, Serialize};
use serde_with::{serde_as, TryFromInto};
use sha3::Keccak256;
use std::{convert::TryFrom, fmt::Display};
use wasmparser::{FuncType, Type};

#[derive(Clone, Copy, PartialEq, Eq, Debug, Hash, Serialize, Deserialize)]
#[repr(u8)]
pub enum ArbValueType {
    I32,
    I64,
    F32,
    F64,
    RefNull,
    FuncRef,
    InternalRef,
}

impl ArbValueType {
    pub fn serialize(self) -> u8 {
        self as u8
    }
}

impl TryFrom<Type> for ArbValueType {
    type Error = eyre::Error;

    fn try_from(ty: Type) -> Result<ArbValueType> {
        use Type::*;
        Ok(match ty {
            I32 => Self::I32,
            I64 => Self::I64,
            F32 => Self::F32,
            F64 => Self::F64,
            FuncRef => Self::FuncRef,
            ExternRef => Self::FuncRef,
            V128 => bail!("128-bit types are not supported"),
        })
    }
}

impl From<FloatType> for ArbValueType {
    fn from(ty: FloatType) -> ArbValueType {
        match ty {
            FloatType::F32 => ArbValueType::F32,
            FloatType::F64 => ArbValueType::F64,
        }
    }
}

#[derive(Clone, Copy, PartialEq, Eq, Debug, Hash, Serialize, Deserialize)]
pub enum IntegerValType {
    I32,
    I64,
}

impl From<IntegerValType> for ArbValueType {
    fn from(ty: IntegerValType) -> ArbValueType {
        match ty {
            IntegerValType::I32 => ArbValueType::I32,
            IntegerValType::I64 => ArbValueType::I64,
        }
    }
}

#[serde_as]
#[derive(Clone, Copy, Debug, Default, PartialEq, Eq, Serialize, Deserialize)]
pub struct ProgramCounter {
    #[serde_as(as = "TryFromInto<usize>")]
    pub module: u32,
    #[serde_as(as = "TryFromInto<usize>")]
    pub func: u32,
    #[serde_as(as = "TryFromInto<usize>")]
    pub inst: u32,
}

#[cfg(not(any(
    target_pointer_width = "32",
    target_pointer_width = "64",
    target_pointer_width = "128"
)))]
compile_error!("Architectures with less than a 32 bit pointer width are not supported");

impl ProgramCounter {
    pub fn serialize(self) -> Bytes32 {
        let mut b = [0u8; 32];
        b[28..].copy_from_slice(&self.inst.to_be_bytes());
        b[24..28].copy_from_slice(&self.func.to_be_bytes());
        b[20..24].copy_from_slice(&self.module.to_be_bytes());
        Bytes32(b)
    }

    // These casts are safe because we checked above that a usize is at least as big as a u32

    pub fn module(self) -> usize {
        self.module as usize
    }

    pub fn func(self) -> usize {
        self.func as usize
    }

    pub fn inst(self) -> usize {
        self.inst as usize
    }
}

impl Display for ProgramCounter {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "{} {} {} {}{}{}",
            "inst".grey(),
            self.inst.pink(),
            "in".grey(),
            self.module.pink(),
            ":".grey(),
            self.func.pink()
        )
    }
}

#[derive(Clone, Copy, Debug, Serialize, Deserialize)]
pub enum Value {
    I32(u32),
    I64(u64),
    F32(f32),
    F64(f64),
    RefNull,
    FuncRef(u32),
    InternalRef(ProgramCounter),
}

impl Value {
    pub fn ty(self) -> ArbValueType {
        match self {
            Value::I32(_) => ArbValueType::I32,
            Value::I64(_) => ArbValueType::I64,
            Value::F32(_) => ArbValueType::F32,
            Value::F64(_) => ArbValueType::F64,
            Value::RefNull => ArbValueType::RefNull,
            Value::FuncRef(_) => ArbValueType::FuncRef,
            Value::InternalRef(_) => ArbValueType::InternalRef,
        }
    }

    pub fn contents_for_proof(self) -> Bytes32 {
        match self {
            Value::I32(x) => x.into(),
            Value::I64(x) => x.into(),
            Value::F32(x) => x.to_bits().into(),
            Value::F64(x) => x.to_bits().into(),
            Value::RefNull => Bytes32::default(),
            Value::FuncRef(x) => x.into(),
            Value::InternalRef(pc) => pc.serialize(),
        }
    }

    pub fn serialize_for_proof(self) -> [u8; 33] {
        let mut ret = [0u8; 33];
        ret[0] = self.ty().serialize();
        ret[1..].copy_from_slice(&*self.contents_for_proof());
        ret
    }

    pub fn is_i32_zero(self) -> bool {
        match self {
            Value::I32(0) => true,
            Value::I32(_) => false,
            _ => panic!(
                "WASM validation failed: i32.eqz equivalent called on {:?}",
                self,
            ),
        }
    }

    pub fn is_i64_zero(self) -> bool {
        match self {
            Value::I64(0) => true,
            Value::I64(_) => false,
            _ => panic!(
                "WASM validation failed: i64.eqz equivalent called on {:?}",
                self,
            ),
        }
    }

    pub fn assume_u32(self) -> u32 {
        match self {
            Value::I32(x) => x,
            _ => panic!("WASM validation failed: assume_u32 called on {:?}", self),
        }
    }

    pub fn assume_u64(self) -> u64 {
        match self {
            Value::I64(x) => x,
            _ => panic!("WASM validation failed: assume_u64 called on {:?}", self),
        }
    }

    pub fn hash(self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update(b"Value:");
        h.update([self.ty() as u8]);
        h.update(self.contents_for_proof());
        h.finalize().into()
    }

    pub fn default_of_type(ty: ArbValueType) -> Value {
        match ty {
            ArbValueType::I32 => Value::I32(0),
            ArbValueType::I64 => Value::I64(0),
            ArbValueType::F32 => Value::F32(0.),
            ArbValueType::F64 => Value::F64(0.),
            ArbValueType::RefNull | ArbValueType::FuncRef | ArbValueType::InternalRef => {
                Value::RefNull
            }
        }
    }
}

impl Display for Value {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let lparem = "(".grey();
        let rparem = ")".grey();

        macro_rules! single {
            ($ty:expr, $value:expr) => {{
                write!(f, "{}{}{}{}", $ty.grey(), lparem, $value, rparem)
            }};
        }
        macro_rules! pair {
            ($ty:expr, $left:expr, $right:expr) => {{
                let eq = "=".grey();
                write!(
                    f,
                    "{}{}{} {} {}{}",
                    $ty.grey(),
                    lparem,
                    $left,
                    eq,
                    $right,
                    rparem
                )
            }};
        }
        match self {
            Value::I32(value) => {
                if (*value as i32) < 0 {
                    pair!("i32", *value as i32, value)
                } else {
                    single!("i32", *value)
                }
            }
            Value::I64(value) => {
                if (*value as i64) < 0 {
                    pair!("i64", *value as i64, value)
                } else {
                    single!("i64", *value)
                }
            }
            Value::F32(value) => single!("f32", *value),
            Value::F64(value) => single!("f64", *value),
            Value::RefNull => write!(f, "null"),
            Value::FuncRef(func) => write!(f, "func {func}"),
            Value::InternalRef(pc) => write!(f, "{pc}"),
        }
    }
}

impl PartialEq for Value {
    fn eq(&self, other: &Self) -> bool {
        self.ty() == other.ty() && self.contents_for_proof() == other.contents_for_proof()
    }
}

impl Eq for Value {}

#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize)]
pub struct FunctionType {
    pub inputs: Vec<ArbValueType>,
    pub outputs: Vec<ArbValueType>,
}

impl FunctionType {
    pub fn new(inputs: Vec<ArbValueType>, outputs: Vec<ArbValueType>) -> FunctionType {
        FunctionType { inputs, outputs }
    }

    pub fn hash(&self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update(b"Function type:");
        h.update(Bytes32::from(self.inputs.len()));
        for input in &self.inputs {
            h.update([*input as u8]);
        }
        h.update(Bytes32::from(self.outputs.len()));
        for output in &self.outputs {
            h.update([*output as u8]);
        }
        h.finalize().into()
    }
}

impl TryFrom<FuncType> for FunctionType {
    type Error = eyre::Error;

    fn try_from(func: FuncType) -> Result<Self> {
        let mut inputs = vec![];
        let mut outputs = vec![];

        for input in func.params.iter() {
            inputs.push(ArbValueType::try_from(*input)?)
        }
        for output in func.returns.iter() {
            outputs.push(ArbValueType::try_from(*output)?)
        }

        Ok(Self { inputs, outputs })
    }
}

'''
'''--- arbitrator/prover/src/wavm.rs ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

use crate::{
    binary::FloatInstruction,
    host::InternalFunc,
    utils::Bytes32,
    value::{ArbValueType, FunctionType, IntegerValType},
};
use digest::Digest;
use eyre::{bail, ensure, Result};
use fnv::FnvHashMap as HashMap;
use serde::{Deserialize, Serialize};
use sha3::Keccak256;
use std::ops::{Add, AddAssign, Sub, SubAssign};
use wasmparser::{BlockType, Operator};

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum IRelOpType {
    Eq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
}

fn irelop_type(t: IRelOpType, signed: bool) -> u16 {
    match (t, signed) {
        (IRelOpType::Eq, _) => 0,
        (IRelOpType::Ne, _) => 1,
        (IRelOpType::Lt, true) => 2,
        (IRelOpType::Lt, false) => 3,
        (IRelOpType::Gt, true) => 4,
        (IRelOpType::Gt, false) => 5,
        (IRelOpType::Le, true) => 6,
        (IRelOpType::Le, false) => 7,
        (IRelOpType::Ge, true) => 8,
        (IRelOpType::Ge, false) => 9,
    }
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[repr(u8)]
pub enum IUnOpType {
    Clz = 0,
    Ctz,
    Popcnt,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[repr(u8)]
pub enum IBinOpType {
    Add = 0,
    Sub,
    Mul,
    DivS,
    DivU,
    RemS,
    RemU,
    And,
    Or,
    Xor,
    Shl,
    ShrS,
    ShrU,
    Rotl,
    Rotr,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum Opcode {
    Unreachable,
    Nop,

    Return,
    Call,
    CallIndirect,

    Drop,
    Select,

    LocalGet,
    LocalSet,
    GlobalGet,
    GlobalSet,

    MemoryLoad {
        /// The type we are loading into.
        ty: ArbValueType,
        /// How many bytes in memory we are loading from.
        bytes: u8,
        /// When bytes matches the type's size, this is irrelevant and should be false.
        signed: bool,
    },
    MemoryStore {
        /// The type we are storing from.
        ty: ArbValueType,
        /// How many bytes in memory we are storing into.
        bytes: u8,
    },

    MemorySize,
    MemoryGrow,

    I32Const,
    I64Const,
    F32Const,
    F64Const,

    I32Eqz,
    I64Eqz,
    IRelOp(IntegerValType, IRelOpType, bool),

    I32WrapI64,
    I64ExtendI32(bool),

    /// Parameterized by destination type, then source type
    Reinterpret(ArbValueType, ArbValueType),

    /// Parameterized by the number of source bits
    I32ExtendS(u8),
    /// Parameterized by the number of source bits
    I64ExtendS(u8),

    IUnOp(IntegerValType, IUnOpType),
    IBinOp(IntegerValType, IBinOpType),

    // Custom opcodes not in WASM. Documented more in "Custom opcodes.md".
    /// Custom opcode not in wasm.
    InitFrame,
    /// Unconditional jump to an arbitrary point in code.
    ArbitraryJump,
    /// Conditional jump to an arbitrary point in code.
    ArbitraryJumpIf,
    /// Pop a value from the value stack and push it to the internal stack
    MoveFromStackToInternal,
    /// Pop a value from the internal stack and push it to the value stack
    MoveFromInternalToStack,
    /// Duplicate the top value on the stack
    Dup,
    /// Call a function in a different module
    CrossModuleCall,
    /// Call a caller module's internal method with a given function offset
    CallerModuleInternalCall,
    /// Gets bytes32 from global state
    GetGlobalStateBytes32,
    /// Sets bytes32 in global state
    SetGlobalStateBytes32,
    /// Gets u64 from global state
    GetGlobalStateU64,
    /// Sets u64 in global state
    SetGlobalStateU64,
    /// Reads the preimage of a hash in-place into the pointer on the stack at an offset
    ReadPreImage,
    /// Reads the current inbox message into the pointer on the stack at an offset
    ReadInboxMessage,
    /// Stop exexcuting the machine and move to the finished status
    HaltAndSetFinished,
}

impl Opcode {
    pub fn repr(self) -> u16 {
        match self {
            Opcode::Unreachable => 0x00,
            Opcode::Nop => 0x01,
            Opcode::Return => 0x0F,
            Opcode::Call => 0x10,
            Opcode::CallIndirect => 0x11,
            Opcode::Drop => 0x1A,
            Opcode::Select => 0x1B,
            Opcode::LocalGet => 0x20,
            Opcode::LocalSet => 0x21,
            Opcode::GlobalGet => 0x23,
            Opcode::GlobalSet => 0x24,
            Opcode::MemoryLoad { ty, bytes, signed } => match (ty, bytes, signed) {
                (ArbValueType::I32, 4, false) => 0x28,
                (ArbValueType::I64, 8, false) => 0x29,
                (ArbValueType::F32, 4, false) => 0x2A,
                (ArbValueType::F64, 8, false) => 0x2B,
                (ArbValueType::I32, 1, true) => 0x2C,
                (ArbValueType::I32, 1, false) => 0x2D,
                (ArbValueType::I32, 2, true) => 0x2E,
                (ArbValueType::I32, 2, false) => 0x2F,
                (ArbValueType::I64, 1, true) => 0x30,
                (ArbValueType::I64, 1, false) => 0x31,
                (ArbValueType::I64, 2, true) => 0x32,
                (ArbValueType::I64, 2, false) => 0x33,
                (ArbValueType::I64, 4, true) => 0x34,
                (ArbValueType::I64, 4, false) => 0x35,
                _ => panic!(
                    "Unsupported memory load of type {:?} from {} bytes with signed {}",
                    ty, bytes, signed,
                ),
            },
            Opcode::MemoryStore { ty, bytes } => match (ty, bytes) {
                (ArbValueType::I32, 4) => 0x36,
                (ArbValueType::I64, 8) => 0x37,
                (ArbValueType::F32, 4) => 0x38,
                (ArbValueType::F64, 8) => 0x39,
                (ArbValueType::I32, 1) => 0x3A,
                (ArbValueType::I32, 2) => 0x3B,
                (ArbValueType::I64, 1) => 0x3C,
                (ArbValueType::I64, 2) => 0x3D,
                (ArbValueType::I64, 4) => 0x3E,
                _ => panic!(
                    "Unsupported memory store of type {:?} to {} bytes",
                    ty, bytes,
                ),
            },
            Opcode::MemorySize => 0x3F,
            Opcode::MemoryGrow => 0x40,
            Opcode::I32Const => 0x41,
            Opcode::I64Const => 0x42,
            Opcode::F32Const => 0x43,
            Opcode::F64Const => 0x44,
            Opcode::I32Eqz => 0x45,
            Opcode::I64Eqz => 0x50,
            Opcode::IRelOp(w, op, signed) => match w {
                IntegerValType::I32 => 0x46 + irelop_type(op, signed),
                IntegerValType::I64 => 0x51 + irelop_type(op, signed),
            },
            Opcode::IUnOp(w, op) => match w {
                IntegerValType::I32 => 0x67 + (op as u16),
                IntegerValType::I64 => 0x79 + (op as u16),
            },
            Opcode::IBinOp(w, op) => match w {
                IntegerValType::I32 => 0x6a + (op as u16),
                IntegerValType::I64 => 0x7c + (op as u16),
            },
            Opcode::I32WrapI64 => 0xA7,
            Opcode::I64ExtendI32(signed) => match signed {
                true => 0xac,
                false => 0xad,
            },
            Opcode::Reinterpret(dest, source) => match (dest, source) {
                (ArbValueType::I32, ArbValueType::F32) => 0xBC,
                (ArbValueType::I64, ArbValueType::F64) => 0xBD,
                (ArbValueType::F32, ArbValueType::I32) => 0xBE,
                (ArbValueType::F64, ArbValueType::I64) => 0xBF,
                _ => panic!("Unsupported reinterpret to {:?} from {:?}", dest, source),
            },
            Opcode::I32ExtendS(x) => match x {
                8 => 0xC0,
                16 => 0xC1,
                _ => panic!("Unsupported {:?}", self),
            },
            Opcode::I64ExtendS(x) => match x {
                8 => 0xC2,
                16 => 0xC3,
                32 => 0xC4,
                _ => panic!("Unsupported {:?}", self),
            },
            // Internal instructions:
            Opcode::InitFrame => 0x8002,
            Opcode::ArbitraryJump => 0x8003,
            Opcode::ArbitraryJumpIf => 0x8004,
            Opcode::MoveFromStackToInternal => 0x8005,
            Opcode::MoveFromInternalToStack => 0x8006,
            Opcode::Dup => 0x8008,
            Opcode::CrossModuleCall => 0x8009,
            Opcode::CallerModuleInternalCall => 0x800A,
            Opcode::GetGlobalStateBytes32 => 0x8010,
            Opcode::SetGlobalStateBytes32 => 0x8011,
            Opcode::GetGlobalStateU64 => 0x8012,
            Opcode::SetGlobalStateU64 => 0x8013,
            Opcode::ReadPreImage => 0x8020,
            Opcode::ReadInboxMessage => 0x8021,
            Opcode::HaltAndSetFinished => 0x8022,
        }
    }

    pub fn is_host_io(self) -> bool {
        matches!(
            self,
            Opcode::GetGlobalStateBytes32
                | Opcode::SetGlobalStateBytes32
                | Opcode::GetGlobalStateU64
                | Opcode::SetGlobalStateU64
                | Opcode::ReadPreImage
                | Opcode::ReadInboxMessage
        )
    }
}

pub type FloatingPointImpls = HashMap<FloatInstruction, (u32, u32)>;

#[derive(Clone, Copy, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub struct Instruction {
    pub opcode: Opcode,
    pub argument_data: u64,
    pub proving_argument_data: Option<Bytes32>,
}

fn pack_call_indirect(table: u32, ty: u32) -> u64 {
    u64::from(table) | (u64::from(ty) << 32)
}

pub fn unpack_call_indirect(data: u64) -> (u32, u32) {
    (data as u32, (data >> 32) as u32)
}

pub fn pack_cross_module_call(module: u32, func: u32) -> u64 {
    u64::from(func) | (u64::from(module) << 32)
}

pub fn unpack_cross_module_call(data: u64) -> (u32, u32) {
    ((data >> 32) as u32, data as u32)
}

impl Instruction {
    #[must_use]
    pub fn simple(opcode: Opcode) -> Instruction {
        Instruction {
            opcode,
            argument_data: 0,
            proving_argument_data: None,
        }
    }

    #[must_use]
    pub fn with_data(opcode: Opcode, argument_data: u64) -> Instruction {
        Instruction {
            opcode,
            argument_data,
            proving_argument_data: None,
        }
    }

    pub fn get_proving_argument_data(self) -> Bytes32 {
        if let Some(data) = self.proving_argument_data {
            data
        } else {
            let mut b = [0u8; 32];
            b[24..].copy_from_slice(&self.argument_data.to_be_bytes());
            Bytes32(b)
        }
    }

    pub fn serialize_for_proof(self) -> [u8; 34] {
        let mut ret = [0u8; 34];
        ret[..2].copy_from_slice(&self.opcode.repr().to_be_bytes());
        ret[2..].copy_from_slice(&*self.get_proving_argument_data());
        ret
    }

    pub fn hash(self) -> Bytes32 {
        let mut h = Keccak256::new();
        h.update(b"Instruction:");
        h.update(self.opcode.repr().to_be_bytes());
        h.update(self.get_proving_argument_data());
        h.finalize().into()
    }
}

/// Note: An Unreachable stack state is equal to any other stack state.
/// That's because an unreachable code path merging with another code path
/// will not have a mismatch in the stack contents.
#[derive(Clone, Copy, Debug)]
enum StackState {
    Reachable(usize),
    Unreachable,
}

impl Add<isize> for StackState {
    type Output = Self;

    fn add(self, rhs: isize) -> Self {
        match self {
            Self::Reachable(x) => {
                if rhs > 0 {
                    Self::Reachable(x.checked_add(rhs as usize).unwrap())
                } else {
                    Self::Reachable(
                        x.checked_sub(rhs.unsigned_abs())
                            .expect("Stack state underflow"),
                    )
                }
            }
            Self::Unreachable => self,
        }
    }
}

impl Sub<isize> for StackState {
    type Output = Self;

    #[allow(clippy::suspicious_arithmetic_impl)]
    fn sub(self, rhs: isize) -> Self {
        self + rhs.checked_neg().unwrap()
    }
}

impl AddAssign<isize> for StackState {
    fn add_assign(&mut self, rhs: isize) {
        *self = *self + rhs;
    }
}

impl SubAssign<isize> for StackState {
    fn sub_assign(&mut self, rhs: isize) {
        *self = *self - rhs;
    }
}

impl PartialEq for StackState {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (Self::Reachable(x), Self::Reachable(y)) => x == y,
            _ => true,
        }
    }
}

impl Sub for StackState {
    type Output = isize;

    fn sub(self, rhs: Self) -> Self::Output {
        let s = match self {
            Self::Reachable(s) => s,
            Self::Unreachable => return 0,
        };
        let rhs = match rhs {
            Self::Reachable(rhs) => rhs,
            Self::Unreachable => return 0,
        };
        s as isize - rhs as isize
    }
}

pub fn wasm_to_wavm(
    code: &[Operator<'_>],
    out: &mut Vec<Instruction>,
    fp_impls: &FloatingPointImpls,
    func_types: &[FunctionType],
    all_types: &[FunctionType],
    all_types_func_idx: u32,
    internals_offset: u32,
) -> Result<()> {
    use Operator::*;

    let mut stack = StackState::Reachable(0);
    let func_ty = &all_types[all_types_func_idx as usize];

    macro_rules! op {
        ($first:ident $(,$opcode:ident)*) => {
            $first $(| $opcode)*
        };
    }
    macro_rules! dot {
        ($first:ident $(,$opcode:ident)*) => {
            $first { .. } $(| $opcode { .. })*
        };
    }
    macro_rules! opcode {
        ($opcode:ident ($($inside:expr),*)) => {{
            out.push(Instruction::simple(Opcode::$opcode($($inside,)*)));
        }};
        ($opcode:ident ($($inside:expr),*), @push $delta:expr) => {{
            out.push(Instruction::simple(Opcode::$opcode($($inside,)*)));
            stack += $delta;
        }};
        ($opcode:ident ($($inside:expr),*), @pop $delta:expr) => {{
            out.push(Instruction::simple(Opcode::$opcode($($inside,)*)));
            stack -= $delta;
        }};
        ($opcode:ident) => {{
            out.push(Instruction::simple(Opcode::$opcode));
        }};
        ($opcode:ident, @push $delta:expr) => {{
            out.push(Instruction::simple(Opcode::$opcode));
            stack += $delta;
        }};
        ($opcode:ident, @pop $delta:expr) => {{
            out.push(Instruction::simple(Opcode::$opcode));
            stack -= $delta;
        }};
        ($opcode:ident, $value:expr) => {{
            out.push(Instruction::with_data(Opcode::$opcode, $value));
        }};
        ($opcode:ident, $value:expr, @push $delta:expr) => {{
            out.push(Instruction::with_data(Opcode::$opcode, $value));
            stack += $delta;
        }};
        ($opcode:ident, $value:expr, @pop $delta:expr) => {{
            out.push(Instruction::with_data(Opcode::$opcode, $value));
            stack -= $delta;
        }};
        (@cross, $module:expr, $func:expr) => {
            out.push(Instruction::with_data(
                Opcode::CrossModuleCall,
                pack_cross_module_call($module, $func),
            ));
        };
    }
    macro_rules! load {
        ($type:ident, $memory:expr, $bytes:expr, $signed:ident) => {{
            ensure!($memory.memory == 0, "multi-memory proposal not supported");
            let op = Opcode::MemoryLoad {
                ty: ArbValueType::$type,
                bytes: $bytes,
                signed: $signed,
            };
            out.push(Instruction::with_data(op, $memory.offset));
        }};
    }
    macro_rules! store {
        ($type:ident, $memory:expr, $bytes:expr) => {{
            ensure!($memory.memory == 0, "multi-memory proposal not supported");
            let op = Opcode::MemoryStore {
                ty: ArbValueType::$type,
                bytes: $bytes,
            };
            out.push(Instruction::with_data(op, $memory.offset));
            stack -= 2;
        }};
    }
    macro_rules! compare {
        ($type:ident, $rel:ident, $signed:expr) => {{
            let op = Opcode::IRelOp(IntegerValType::$type, IRelOpType::$rel, $signed);
            out.push(Instruction::simple(op));
            stack -= 1;
        }};
    }
    macro_rules! unary {
        ($type:ident, $op:ident) => {{
            let op = Opcode::IUnOp(IntegerValType::$type, IUnOpType::$op);
            out.push(Instruction::simple(op));
        }};
    }
    macro_rules! binary {
        ($type:ident, $op:ident) => {{
            let op = Opcode::IBinOp(IntegerValType::$type, IBinOpType::$op);
            out.push(Instruction::simple(op));
            stack -= 1;
        }};
    }
    macro_rules! reinterpret {
        ($dest:ident, $source:ident) => {{
            let op = Opcode::Reinterpret(ArbValueType::$dest, ArbValueType::$source);
            out.push(Instruction::simple(op));
        }};
    }
    macro_rules! call {
        ($func:expr) => {{
            let ty = &func_types[($func) as usize];
            let delta = ty.outputs.len() as isize - ty.inputs.len() as isize;
            opcode!(Call, ($func).into(), @push delta)
        }}
    }
    macro_rules! float {
        ($func:ident) => {
            float!(@impl $func)
        };
        ($func:ident $(,$data:ident)+) => {
            float!(@impl $func($($data),+))
        };
        (@impl $func:expr) => {{
            #[allow(unused_imports)]
            use crate::{
                binary::{FloatInstruction::*, FloatType::*, FloatUnOp::*, FloatBinOp::*, FloatRelOp::*},
                value::IntegerValType::*,
            };

            let func = $func;
            let sig = func.signature();
            let (module, func) = match fp_impls.get(&func) {
                Some((module, func)) => (module, func),
                None => bail!("No implementation for floating point operation {:?}", &func),
            };

            // Reinterpret float args into ints
            for &arg in sig.inputs.iter().rev() {
                match arg {
                    ArbValueType::I32 | ArbValueType::I64 => {}
                    ArbValueType::F32 => reinterpret!(I32, F32),
                    ArbValueType::F64 => reinterpret!(I64, F64),
                    _ => bail!("Floating point operation {:?} has bad args", &func),
                }
                opcode!(MoveFromStackToInternal)
            }
            for _ in &sig.inputs {
                opcode!(MoveFromInternalToStack)
            }
            opcode!(@cross, *module, *func);

            // Reinterpret returned ints that should be floats into floats
            let outputs = sig.outputs;
            match outputs.as_slice() {
                &[ArbValueType::I32] => {}
                &[ArbValueType::I64] => {}
                &[ArbValueType::F32] => reinterpret!(F32, I32),
                &[ArbValueType::F64] => reinterpret!(F64, I64),
                _ => panic!("Floating point op {:?} should have 1 output but has {}", func, outputs.len()),
            }

            stack += outputs.len() as isize - sig.inputs.len() as isize;
        }};
    }

    /// represents a wasm scope
    #[derive(Debug)]
    enum Scope {
        /// block type, jumps, and height afterward
        Simple(BlockType, Vec<usize>, StackState),
        /// block type, start, height before, and height afterward
        Loop(BlockType, usize, StackState, StackState),
        /// block type, jumps, start, height before, and height afterward
        IfElse(BlockType, Vec<usize>, Option<usize>, StackState, StackState),
    }
    let mut scopes = vec![Scope::Simple(
        BlockType::FuncType(all_types_func_idx),
        vec![],
        StackState::Reachable(func_ty.outputs.len()),
    )]; // start with the func's scope

    let block_type_params = |ty: BlockType| -> usize {
        match ty {
            BlockType::Empty => 0,
            BlockType::Type(_) => 0,
            BlockType::FuncType(idx) => all_types[idx as usize].inputs.len(),
        }
    };

    let block_type_results = |ty: BlockType| -> usize {
        match ty {
            BlockType::Empty => 0,
            BlockType::Type(_) => 1,
            BlockType::FuncType(idx) => all_types[idx as usize].outputs.len(),
        }
    };

    macro_rules! branch {
        ($kind:ident, $depth:expr) => {{
            use Scope::*;
            let mut dest = 0;
            let scope = scopes.len() - $depth as usize - 1;
            let (branch_params, height) = match &scopes[scope] {
                Simple(ty, _, height) | IfElse(ty, .., height) => {
                    (block_type_results(*ty), *height)
                }
                Loop(ty, _, height, _) => {
                    (block_type_params(*ty), *height)
                }
            };
            let mut jump_op = Opcode::$kind;
            if jump_op == Opcode::ArbitraryJumpIf {
                stack -= 1;
            } else {
                assert_eq!(jump_op, Opcode::ArbitraryJump, "unrecognized jump op");
            }
            let stack_if_not_taken = stack;
            let mut jump_to_after = None;
            if stack != height {
                if jump_op == Opcode::ArbitraryJumpIf {
                    opcode!(I32Eqz);
                    jump_to_after = Some(out.len());
                    opcode!(ArbitraryJumpIf);
                }
                let diff = stack - height;
                assert!(diff > 0, "stack doesn't have needed elements for branch");
                for _ in 0..branch_params {
                    opcode!(MoveFromStackToInternal, @pop 1);
                }
                for _ in 0..diff {
                    opcode!(Drop, @pop 1);
                }
                for _ in 0..branch_params {
                    opcode!(MoveFromInternalToStack, @push 1);
                }
                assert_eq!(stack, height);
                jump_op = Opcode::ArbitraryJump;
            }
            match &mut scopes[scope] {
                Simple(_, jumps, ..) | IfElse(_, jumps, ..) => {
                    jumps.push(out.len()); // dest not yet known
                }
                Loop(_, start, ..) => {
                    dest = *start;
                }
            }
            out.push(Instruction::with_data(jump_op, dest as u64));
            if let Some(jump_to_after) = jump_to_after {
                out[jump_to_after].argument_data = out.len() as u64;
                stack = stack_if_not_taken;
            } else if jump_op == Opcode::ArbitraryJump {
                stack = StackState::Unreachable;
            }
        }};
    }
    macro_rules! height_after_block {
        ($ty:expr) => {{
            let ty = $ty;
            stack + block_type_results(*ty) as isize - block_type_params(*ty) as isize
        }};
    }

    for op in code {
        #[rustfmt::skip]
        match op {
            Unreachable => {
                opcode!(Unreachable);
                stack = StackState::Unreachable;
            },
            Nop => opcode!(Nop),
            Block { ty } => {
                scopes.push(Scope::Simple(*ty, vec![], height_after_block!(ty)));
            }
            Loop { ty } => {
                scopes.push(Scope::Loop(*ty, out.len(), stack, height_after_block!(ty)));
            }
            If { ty } => {
                opcode!(I32Eqz);
                stack -= 1; // the else block shouldn't have the conditional that gets popped next instruction
                scopes.push(Scope::IfElse(*ty, vec![], Some(out.len()), stack, height_after_block!(ty)));
                opcode!(ArbitraryJumpIf);
            }
            Else => {
                branch!(ArbitraryJump, 0);
                let _ = stack; // silence warning from above (we overwrite stack below so the assignment is unused)

                match scopes.last_mut() {
                    Some(Scope::IfElse(_, _, cond, if_height, _)) if cond.is_some() => {
                        out[cond.unwrap()].argument_data = out.len() as u64;
                        *cond = None;
                        stack = *if_height;
                    }
                    x => bail!("malformed if-else scope {:?}", x),
                }
            }

            unsupported @ dot!(Try, Catch, Throw, Rethrow) => {
                bail!("exception-handling extension not supported {:?}", unsupported)
            },

            End => {
                let (jumps, dest, height) = match scopes.pop().unwrap() {
                    Scope::Simple(_, jumps, height) => (jumps, out.len(), height),
                    Scope::Loop(_, dest, _, height) => (vec![], dest, height),
                    Scope::IfElse(_, mut jumps, cond, _, height) => {
                        jumps.extend(cond);
                        (jumps, out.len(), height)
                    },
                };
                for jump in jumps {
                    out[jump].argument_data = dest as u64;
                }
                assert_eq!(stack, height, "unexpected stack height at end of block");
                stack = height;
            }
            Br { relative_depth } => branch!(ArbitraryJump, *relative_depth),
            BrIf { relative_depth } => branch!(ArbitraryJumpIf, *relative_depth),
            BrTable { table } => {
                let start_stack = stack;
                // evaluate each branch
                let mut subjumps = vec![];
                for (index, target) in table.targets().enumerate() {
                    opcode!(Dup, @push 1);
                    opcode!(I32Const, index as u64, @push 1);
                    compare!(I32, Eq, false);
                    subjumps.push((out.len(), target?));
                    opcode!(ArbitraryJumpIf, @pop 1);
                }

                // nothing matched: drop the index and jump to the default.
                opcode!(Drop, @pop 1);
                branch!(ArbitraryJump, table.default());

                // simulate a jump table of branches
                for (jump, branch) in subjumps {
                    out[jump].argument_data = out.len() as u64;
                    stack = start_stack;
                    opcode!(Drop, @pop 1);
                    branch!(ArbitraryJump, branch);
                }
            }
            Return => branch!(ArbitraryJump, scopes.len() - 1),
            Call { function_index } => call!(*function_index),

            CallIndirect { index, table_index, .. } => {
                let ty = &all_types[*index as usize];
                let delta = ty.outputs.len() as isize - ty.inputs.len() as isize;
                opcode!(CallIndirect, pack_call_indirect(*table_index, *index), @push delta - 1);
            }

            unsupported @ dot!(ReturnCall, ReturnCallIndirect) => {
                bail!("tail-call extension not supported {:?}", unsupported)
            }

            unsupported @ (dot!(Delegate) | op!(CatchAll)) => {
                bail!("exception-handling extension not supported {:?}", unsupported)
            },

            Drop => opcode!(Drop, @pop 1),
            Select => opcode!(Select, @pop 2),

            unsupported @ dot!(TypedSelect) => {
                bail!("reference-types extension not supported {:?}", unsupported)
            },

            LocalGet { local_index } => opcode!(LocalGet, *local_index as u64, @push 1),
            LocalSet { local_index } => opcode!(LocalSet, *local_index as u64, @pop 1),
            LocalTee { local_index } => {
                opcode!(Dup);
                opcode!(LocalSet, *local_index as u64);
            },
            GlobalGet { global_index } => opcode!(GlobalGet, *global_index as u64, @push 1),
            GlobalSet { global_index } => opcode!(GlobalSet, *global_index as u64, @pop 1),
            I32Load { memarg } => load!(I32, memarg, 4, false),
            I64Load { memarg } => load!(I64, memarg, 8, false),
            F32Load { memarg } => load!(F32, memarg, 4, false),
            F64Load { memarg } => load!(F64, memarg, 8, false),
            I32Load8S { memarg } => load!(I32, memarg, 1, true),
            I32Load8U { memarg } => load!(I32, memarg, 1, false),
            I32Load16S { memarg } => load!(I32, memarg, 2, true),
            I32Load16U { memarg } => load!(I32, memarg, 2, false),
            I64Load8S { memarg } => load!(I64, memarg, 1, true),
            I64Load8U { memarg } => load!(I64, memarg, 1, false),
            I64Load16S { memarg } => load!(I64, memarg, 2, true),
            I64Load16U { memarg } => load!(I64, memarg, 2, false),
            I64Load32S { memarg } => load!(I64, memarg, 4, true),
            I64Load32U { memarg } => load!(I64, memarg, 4, false),
            I32Store { memarg } => store!(I32, memarg, 4),
            I64Store { memarg } => store!(I64, memarg, 8),
            F32Store { memarg } => store!(F32, memarg, 4),
            F64Store { memarg } => store!(F64, memarg, 8),
            I32Store8 { memarg } => store!(I32, memarg, 1),
            I32Store16 { memarg } => store!(I32, memarg, 2),
            I64Store8 { memarg } => store!(I64, memarg, 1),
            I64Store16 { memarg } => store!(I64, memarg, 2),
            I64Store32 { memarg } => store!(I64, memarg, 4),
            MemorySize { mem, mem_byte } => {
                ensure!(*mem == 0 && *mem_byte == 0, "MemorySize args must be 0");
                opcode!(MemorySize, @push 1)
            }
            MemoryGrow { mem, mem_byte } => {
                ensure!(*mem == 0 && *mem_byte == 0, "MemoryGrow args must be 0");
                opcode!(MemoryGrow)
            }
            I32Const { value } => opcode!(I32Const, *value as u32 as u64, @push 1),
            I64Const { value } => opcode!(I64Const, *value as u64,        @push 1),
            F32Const { value } => opcode!(F32Const, value.bits() as u64,  @push 1),
            F64Const { value } => opcode!(F64Const, value.bits(),         @push 1),

            unsupported @ (dot!(RefNull) | op!(RefIsNull) | dot!(RefFunc)) => {
                bail!("reference-types extension not supported {:?}", unsupported)
            },

            I32Eqz => opcode!(I32Eqz),
            I32Eq => compare!(I32, Eq, false),
            I32Ne => compare!(I32, Ne, false),
            I32LtS => compare!(I32, Lt, true),
            I32LtU => compare!(I32, Lt, false),
            I32GtS => compare!(I32, Gt, true),
            I32GtU => compare!(I32, Gt, false),
            I32LeS => compare!(I32, Le, true),
            I32LeU => compare!(I32, Le, false),
            I32GeS => compare!(I32, Ge, true),
            I32GeU => compare!(I32, Ge, false),
            I64Eqz => opcode!(I64Eqz),
            I64Eq => compare!(I64, Eq, false),
            I64Ne => compare!(I64, Ne, false),
            I64LtS => compare!(I64, Lt, true),
            I64LtU => compare!(I64, Lt, false),
            I64GtS => compare!(I64, Gt, true),
            I64GtU => compare!(I64, Gt, false),
            I64LeS => compare!(I64, Le, true),
            I64LeU => compare!(I64, Le, false),
            I64GeS => compare!(I64, Ge, true),
            I64GeU => compare!(I64, Ge, false),
            F32Eq => float!(RelOp, F32, Eq),
            F32Ne => float!(RelOp, F32, Ne),
            F32Lt => float!(RelOp, F32, Lt),
            F32Gt => float!(RelOp, F32, Gt),
            F32Le => float!(RelOp, F32, Le),
            F32Ge => float!(RelOp, F32, Ge),
            F64Eq => float!(RelOp, F64, Eq),
            F64Ne => float!(RelOp, F64, Ne),
            F64Lt => float!(RelOp, F64, Lt),
            F64Gt => float!(RelOp, F64, Gt),
            F64Le => float!(RelOp, F64, Le),
            F64Ge => float!(RelOp, F64, Ge),
            I32Clz => unary!(I32, Clz),
            I32Ctz => unary!(I32, Ctz),
            I32Popcnt => unary!(I32, Popcnt),
            I32Add => binary!(I32, Add),
            I32Sub => binary!(I32, Sub),
            I32Mul => binary!(I32, Mul),
            I32DivS => binary!(I32, DivS),
            I32DivU => binary!(I32, DivU),
            I32RemS => binary!(I32, RemS),
            I32RemU => binary!(I32, RemU),
            I32And => binary!(I32, And),
            I32Or => binary!(I32, Or),
            I32Xor => binary!(I32, Xor),
            I32Shl => binary!(I32, Shl),
            I32ShrS => binary!(I32, ShrS),
            I32ShrU => binary!(I32, ShrU),
            I32Rotl => binary!(I32, Rotl),
            I32Rotr => binary!(I32, Rotr),
            I64Clz => unary!(I64, Clz),
            I64Ctz => unary!(I64, Ctz),
            I64Popcnt => unary!(I64, Popcnt),
            I64Add => binary!(I64, Add),
            I64Sub => binary!(I64, Sub),
            I64Mul => binary!(I64, Mul),
            I64DivS => binary!(I64, DivS),
            I64DivU => binary!(I64, DivU),
            I64RemS => binary!(I64, RemS),
            I64RemU => binary!(I64, RemU),
            I64And => binary!(I64, And),
            I64Or => binary!(I64, Or),
            I64Xor => binary!(I64, Xor),
            I64Shl => binary!(I64, Shl),
            I64ShrS => binary!(I64, ShrS),
            I64ShrU => binary!(I64, ShrU),
            I64Rotl => binary!(I64, Rotl),
            I64Rotr => binary!(I64, Rotr),
            F32Abs => float!(UnOp, F32, Abs),
            F32Neg => float!(UnOp, F32, Neg),
            F32Ceil => float!(UnOp, F32, Ceil),
            F32Floor => float!(UnOp, F32, Floor),
            F32Trunc => float!(UnOp, F32, Trunc),
            F32Nearest => float!(UnOp, F32, Nearest),
            F32Sqrt => float!(UnOp, F32, Sqrt),
            F32Add => float!(BinOp, F32, Add),
            F32Sub => float!(BinOp, F32, Sub),
            F32Mul => float!(BinOp, F32, Mul),
            F32Div => float!(BinOp, F32, Div),
            F32Min => float!(BinOp, F32, Min),
            F32Max => float!(BinOp, F32, Max),
            F32Copysign => float!(BinOp, F32, CopySign),
            F64Abs => float!(UnOp, F64, Abs),
            F64Neg => float!(UnOp, F64, Neg),
            F64Ceil => float!(UnOp, F64, Ceil),
            F64Floor => float!(UnOp, F64, Floor),
            F64Trunc => float!(UnOp, F64, Trunc),
            F64Nearest => float!(UnOp, F64, Nearest),
            F64Sqrt => float!(UnOp, F64, Sqrt),
            F64Add => float!(BinOp, F64, Add),
            F64Sub => float!(BinOp, F64, Sub),
            F64Mul => float!(BinOp, F64, Mul),
            F64Div => float!(BinOp, F64, Div),
            F64Min => float!(BinOp, F64, Min),
            F64Max => float!(BinOp, F64, Max),
            F64Copysign => float!(BinOp, F64, CopySign),
            I32WrapI64 => opcode!(I32WrapI64),
            I32TruncF32S => float!(TruncIntOp, I32, F32, false, true),
            I32TruncF32U => float!(TruncIntOp, I32, F32, false, false),
            I32TruncF64S => float!(TruncIntOp, I32, F64, false, true),
            I32TruncF64U => float!(TruncIntOp, I32, F64, false, false),
            I64ExtendI32S => opcode!(I64ExtendI32(true)),
            I64ExtendI32U => opcode!(I64ExtendI32(false)),
            I64TruncF32S => float!(TruncIntOp, I64, F32, false, true),
            I64TruncF32U => float!(TruncIntOp, I64, F32, false, false),
            I64TruncF64S => float!(TruncIntOp, I64, F64, false, true),
            I64TruncF64U => float!(TruncIntOp, I64, F64, false, false),
            F32ConvertI32S => float!(ConvertIntOp, F32, I32, true),
            F32ConvertI32U => float!(ConvertIntOp, F32, I32, false),
            F32ConvertI64S => float!(ConvertIntOp, F32, I64, true),
            F32ConvertI64U => float!(ConvertIntOp, F32, I64, false),
            F32DemoteF64 => float!(F32DemoteF64),
            F64ConvertI32S => float!(ConvertIntOp, F64, I32, true),
            F64ConvertI32U => float!(ConvertIntOp, F64, I32, false),
            F64ConvertI64S => float!(ConvertIntOp, F64, I64, true),
            F64ConvertI64U => float!(ConvertIntOp, F64, I64, false),
            F64PromoteF32 => float!(F64PromoteF32),
            I32ReinterpretF32 => reinterpret!(I32, F32),
            I64ReinterpretF64 => reinterpret!(I64, F64),
            F32ReinterpretI32 => reinterpret!(F32, I32),
            F64ReinterpretI64 => reinterpret!(F64, I64),
            I32Extend8S => opcode!(I32ExtendS(8)),
            I32Extend16S => opcode!(I32ExtendS(16)),
            I64Extend8S => opcode!(I64ExtendS(8)),
            I64Extend16S => opcode!(I64ExtendS(16)),
            I64Extend32S => opcode!(I64ExtendS(32)),
            I32TruncSatF32S => float!(TruncIntOp, I32, F32, true, true),
            I32TruncSatF32U => float!(TruncIntOp, I32, F32, true, false),
            I32TruncSatF64S => float!(TruncIntOp, I32, F64, true, true),
            I32TruncSatF64U => float!(TruncIntOp, I32, F64, true, false),
            I64TruncSatF32S => float!(TruncIntOp, I64, F32, true, true),
            I64TruncSatF32U => float!(TruncIntOp, I64, F32, true, false),
            I64TruncSatF64S => float!(TruncIntOp, I64, F64, true, true),
            I64TruncSatF64U => float!(TruncIntOp, I64, F64, true, false),

            MemoryFill { mem } => {
                ensure!(*mem == 0, "multi-memory proposal not supported");
                call!(internals_offset + InternalFunc::MemoryFill as u32)
            },
            MemoryCopy { src, dst } => {
                ensure!(*src == 0 && *dst == 0, "multi-memory proposal not supported");
                call!(internals_offset + InternalFunc::MemoryCopy as u32)
            },

            unsupported @ (
                dot!(
                    MemoryInit, DataDrop, TableInit, ElemDrop,
                    TableCopy, TableFill, TableGet, TableSet, TableGrow, TableSize
                )
            ) => bail!("bulk-memory-operations extension not fully supported {:?}", unsupported),

            unsupported @ (
                dot!(
                    MemoryAtomicNotify, MemoryAtomicWait32, MemoryAtomicWait64, AtomicFence, I32AtomicLoad,
                    I64AtomicLoad, I32AtomicLoad8U, I32AtomicLoad16U, I64AtomicLoad8U, I64AtomicLoad16U,
                    I64AtomicLoad32U, I32AtomicStore, I64AtomicStore, I32AtomicStore8, I32AtomicStore16,
                    I64AtomicStore8, I64AtomicStore16, I64AtomicStore32, I32AtomicRmwAdd, I64AtomicRmwAdd,
                    I32AtomicRmw8AddU, I32AtomicRmw16AddU, I64AtomicRmw8AddU, I64AtomicRmw16AddU, I64AtomicRmw32AddU,
                    I32AtomicRmwSub, I64AtomicRmwSub, I32AtomicRmw8SubU, I32AtomicRmw16SubU, I64AtomicRmw8SubU,
                    I64AtomicRmw16SubU, I64AtomicRmw32SubU, I32AtomicRmwAnd, I64AtomicRmwAnd, I32AtomicRmw8AndU,
                    I32AtomicRmw16AndU, I64AtomicRmw8AndU, I64AtomicRmw16AndU, I64AtomicRmw32AndU, I32AtomicRmwOr,
                    I64AtomicRmwOr, I32AtomicRmw8OrU, I32AtomicRmw16OrU, I64AtomicRmw8OrU, I64AtomicRmw16OrU,
                    I64AtomicRmw32OrU, I32AtomicRmwXor, I64AtomicRmwXor, I32AtomicRmw8XorU, I32AtomicRmw16XorU,
                    I64AtomicRmw8XorU, I64AtomicRmw16XorU, I64AtomicRmw32XorU, I32AtomicRmwXchg, I64AtomicRmwXchg,
                    I32AtomicRmw8XchgU, I32AtomicRmw16XchgU, I64AtomicRmw8XchgU, I64AtomicRmw16XchgU,
                    I64AtomicRmw32XchgU, I32AtomicRmwCmpxchg, I64AtomicRmwCmpxchg, I32AtomicRmw8CmpxchgU,
                    I32AtomicRmw16CmpxchgU, I64AtomicRmw8CmpxchgU, I64AtomicRmw16CmpxchgU, I64AtomicRmw32CmpxchgU
                )
            ) => bail!("threads extension not supported {:?}", unsupported),

            unsupported @ (
                dot!(
                    V128Load, V128Load8x8S, V128Load8x8U, V128Load16x4S, V128Load16x4U, V128Load32x2S, V128Load32x2U,
                    V128Load8Splat, V128Load16Splat, V128Load32Splat, V128Load64Splat, V128Load32Zero, V128Load64Zero,
                    V128Store, V128Load8Lane, V128Load16Lane, V128Load32Lane, V128Load64Lane, V128Store8Lane,
                    V128Store16Lane, V128Store32Lane, V128Store64Lane, V128Const,
                    I8x16Shuffle, I8x16ExtractLaneS, I8x16ExtractLaneU, I8x16ReplaceLane, I16x8ExtractLaneS,
                    I16x8ExtractLaneU, I16x8ReplaceLane, I32x4ExtractLane, I32x4ReplaceLane, I64x2ExtractLane,
                    I64x2ReplaceLane, F32x4ExtractLane, F32x4ReplaceLane, F64x2ExtractLane, F64x2ReplaceLane,
                    I8x16Swizzle, I8x16Splat, I16x8Splat, I32x4Splat, I64x2Splat, F32x4Splat, F64x2Splat, I8x16Eq,
                    I8x16Ne, I8x16LtS, I8x16LtU, I8x16GtS, I8x16GtU, I8x16LeS, I8x16LeU, I8x16GeS, I8x16GeU, I16x8Eq,
                    I16x8Ne, I16x8LtS, I16x8LtU, I16x8GtS, I16x8GtU, I16x8LeS, I16x8LeU, I16x8GeS, I16x8GeU, I32x4Eq,
                    I32x4Ne, I32x4LtS, I32x4LtU, I32x4GtS, I32x4GtU, I32x4LeS, I32x4LeU, I32x4GeS, I32x4GeU, I64x2Eq,
                    I64x2Ne, I64x2LtS, I64x2GtS, I64x2LeS, I64x2GeS,
                    F32x4Eq, F32x4Ne, F32x4Lt, F32x4Gt, F32x4Le, F32x4Ge,
                    F64x2Eq, F64x2Ne, F64x2Lt, F64x2Gt, F64x2Le, F64x2Ge,
                    V128Not, V128And, V128AndNot, V128Or, V128Xor, V128Bitselect, V128AnyTrue,
                    I8x16Abs, I8x16Neg, I8x16Popcnt, I8x16AllTrue, I8x16Bitmask, I8x16NarrowI16x8S, I8x16NarrowI16x8U,
                    I8x16Shl, I8x16ShrS, I8x16ShrU, I8x16Add, I8x16AddSatS, I8x16AddSatU, I8x16Sub, I8x16SubSatS,
                    I8x16SubSatU, I8x16MinS, I8x16MinU, I8x16MaxS, I8x16MaxU, I8x16RoundingAverageU,
                    I16x8ExtAddPairwiseI8x16S, I16x8ExtAddPairwiseI8x16U, I16x8Abs, I16x8Neg, I16x8Q15MulrSatS,
                    I16x8AllTrue, I16x8Bitmask, I16x8NarrowI32x4S, I16x8NarrowI32x4U, I16x8ExtendLowI8x16S,
                    I16x8ExtendHighI8x16S, I16x8ExtendLowI8x16U, I16x8ExtendHighI8x16U, I16x8Shl, I16x8ShrS, I16x8ShrU,
                    I16x8Add, I16x8AddSatS, I16x8AddSatU, I16x8Sub, I16x8SubSatS, I16x8SubSatU, I16x8Mul, I16x8MinS,
                    I16x8MinU, I16x8MaxS, I16x8MaxU, I16x8RoundingAverageU, I16x8ExtMulLowI8x16S,
                    I16x8ExtMulHighI8x16S, I16x8ExtMulLowI8x16U, I16x8ExtMulHighI8x16U, I32x4ExtAddPairwiseI16x8S,
                    I32x4ExtAddPairwiseI16x8U, I32x4Abs, I32x4Neg, I32x4AllTrue, I32x4Bitmask, I32x4ExtendLowI16x8S,
                    I32x4ExtendHighI16x8S, I32x4ExtendLowI16x8U, I32x4ExtendHighI16x8U, I32x4Shl, I32x4ShrS, I32x4ShrU,
                    I32x4Add, I32x4Sub, I32x4Mul, I32x4MinS, I32x4MinU, I32x4MaxS, I32x4MaxU, I32x4DotI16x8S,
                    I32x4ExtMulLowI16x8S, I32x4ExtMulHighI16x8S, I32x4ExtMulLowI16x8U, I32x4ExtMulHighI16x8U, I64x2Abs,
                    I64x2Neg, I64x2AllTrue, I64x2Bitmask, I64x2ExtendLowI32x4S, I64x2ExtendHighI32x4S,
                    I64x2ExtendLowI32x4U, I64x2ExtendHighI32x4U, I64x2Shl, I64x2ShrS, I64x2ShrU, I64x2Add, I64x2Sub,
                    I64x2Mul, I64x2ExtMulLowI32x4S, I64x2ExtMulHighI32x4S, I64x2ExtMulLowI32x4U, I64x2ExtMulHighI32x4U,
                    F32x4Ceil, F32x4Floor, F32x4Trunc, F32x4Nearest, F32x4Abs, F32x4Neg, F32x4Sqrt, F32x4Add, F32x4Sub,
                    F32x4Mul, F32x4Div, F32x4Min, F32x4Max, F32x4PMin, F32x4PMax, F64x2Ceil, F64x2Floor, F64x2Trunc,
                    F64x2Nearest, F64x2Abs, F64x2Neg, F64x2Sqrt, F64x2Add, F64x2Sub, F64x2Mul, F64x2Div, F64x2Min,
                    F64x2Max, F64x2PMin, F64x2PMax, I32x4TruncSatF32x4S, I32x4TruncSatF32x4U, F32x4ConvertI32x4S,
                    F32x4ConvertI32x4U, I32x4TruncSatF64x2SZero, I32x4TruncSatF64x2UZero, F64x2ConvertLowI32x4S,
                    F64x2ConvertLowI32x4U, F32x4DemoteF64x2Zero, F64x2PromoteLowF32x4, I8x16RelaxedSwizzle,
                    I32x4RelaxedTruncSatF32x4S, I32x4RelaxedTruncSatF32x4U, I32x4RelaxedTruncSatF64x2SZero,
                    I32x4RelaxedTruncSatF64x2UZero, F32x4Fma, F32x4Fms, F64x2Fma, F64x2Fms, I8x16LaneSelect,
                    I16x8LaneSelect, I32x4LaneSelect, I64x2LaneSelect, F32x4RelaxedMin, F32x4RelaxedMax,
                    F64x2RelaxedMin, F64x2RelaxedMax
                )
            ) => bail!("SIMD extension not supported {:?}", unsupported)
        };
    }

    Ok(())
}

'''
'''--- arbitrator/prover/test-cases/go/main.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"bytes"
	"encoding/hex"
	"fmt"
	"os"
	"runtime"
	"time"

	"github.com/ethereum/go-ethereum/common"
	merkletree "github.com/wealdtech/go-merkletree"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/wavmio"
)

// MerkleSample is an example using the Merkle tree to generate and verify proofs.
func MerkleSample(data [][]byte, toproove int) (bool, error) {
	// Create the tree
	tree, err := merkletree.New(data)
	if err != nil {
		return false, err
	}

	// Fetch the root hash of the tree
	root := tree.Root()

	baz := []byte("yoo")

	if toproove >= 0 {
		baz = data[toproove]
	}

	// Generate a proof for 'Baz'
	proof, err := tree.GenerateProof(baz)
	if err != nil {
		return false, err
	}
	return merkletree.VerifyProof(baz, proof, root)
	// Verify the proof for 'Baz'
}

func testCompression(data []byte) {
	compressed, err := arbcompress.CompressLevel(data, 0)
	if err != nil {
		panic(err)
	}
	decompressed, err := arbcompress.Decompress(compressed, len(data)*2+0x100)
	if err != nil {
		panic(err)
	}
	if !bytes.Equal(decompressed, data) {
		panic("data differs after compression / decompression")
	}
}

func main() {
	fmt.Printf("starting executable with %v arg(s): %v\n", len(os.Args), os.Args)
	runtime.GC()
	time.Sleep(time.Second)

	// Data for the tree
	data := [][]byte{
		[]byte("Foo"),
		[]byte("Bar"),
		[]byte("Baz"),
	}

	verified, err := MerkleSample(data, 0)
	if err != nil {
		panic(err)
	}
	if !verified {
		panic("failed to verify proof for Baz")
	}
	verified, err = MerkleSample(data, 1)
	if err != nil {
		panic(err)
	}
	if !verified {
		panic("failed to verify proof for Baz")
	}

	verified, err = MerkleSample(data, -1)
	if err != nil {
		if verified {
			panic("succeeded to verify proof invalid")
		}
	}

	println("verified both proofs!\n")

	testCompression([]byte{})
	testCompression([]byte("This is a test string la la la la la la la la la la"))

	println("test compression passed!\n")

	checkPreimage := func(ty arbutil.PreimageType, hash common.Hash) {
		preimage, err := wavmio.ResolveTypedPreimage(ty, hash)
		if err != nil {
			panic(fmt.Sprintf("failed to resolve preimage of type %v: %v", ty, err))
		}
		if !bytes.Equal(preimage, []byte("hello world")) {
			panic(fmt.Sprintf("got wrong preimage of type %v: %v", ty, hex.EncodeToString(preimage)))
		}
	}

	checkPreimage(arbutil.Keccak256PreimageType, common.HexToHash("47173285a8d7341e5e972fc677286384f802f8ef42a5ec5f03bbfa254cb01fad"))
	checkPreimage(arbutil.Sha2_256PreimageType, common.HexToHash("b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9"))

	println("verified preimage resolution!\n")
}

'''
'''--- arbitrator/prover/test-cases/rust/Cargo.toml ---
[package]
name = "test-cases"
version = "0.1.0"
edition = "2018"
publish = false

[dependencies]
digest = { version = "0.9.0", default-features = false }
hex-literal = "0.3.4"
sha3 = { version = "0.9.1", default-features = false }

[workspace]

'''
'''--- arbitrator/prover/test-cases/rust/src/bin/basics.rs ---
fn main() {
	let mut x: i32 = 100;
	if std::env::vars().count() == 0 {
		x = x.wrapping_add(1);
	}
	std::process::exit(x ^ 101)
}

'''
'''--- arbitrator/prover/test-cases/rust/src/bin/globalstate.rs ---
extern "C" {
    pub fn wavm_get_globalstate_bytes32(idx: u32, ptr: *mut u8);
    pub fn wavm_set_globalstate_bytes32(idx: u32, ptr: *const u8);
    pub fn wavm_get_globalstate_u64(idx: u32) -> u64;
    pub fn wavm_set_globalstate_u64(idx: u32, val: u64);
}

const BYTES32_VALS_NUM: usize = 2;
const U64_VALS_NUM: usize = 2;

#[repr(C, align(32))]
struct Bytes32([u8; 32]);

fn main() {
    println!("hello!");
    unsafe {
        for i in 0..U64_VALS_NUM {
            println!("set_u64 nr. {}", i);
            wavm_set_globalstate_u64(i as u32, (i as u64) * 0x100 + 1);
        }
        for i in 0..BYTES32_VALS_NUM {
            let mut bytebuffer = Bytes32([0x0; 32]);
            for j in 0..32 {
                bytebuffer.0[j] = (j + i*0x10) as u8;
            }
            println!("set_bytes32 nr. {}", i);
            wavm_set_globalstate_bytes32(i as u32, bytebuffer.0.as_ptr());
        }
        for i in 0..U64_VALS_NUM {
            println!("get_u64 nr. {}", i);
            let val: u64;
            val = wavm_get_globalstate_u64(i as u32);
            let exp = (i as u64) * 0x100 + 1;
            if val != exp {
                panic!("globalstate u64 val {} expected {} got {}", i, exp, val)
            }
        }
        for i in 0..BYTES32_VALS_NUM {
            let mut bytebuffer = Bytes32([0xff; 32]);
            println!("get_bytes32 nr. {}", i);
            wavm_get_globalstate_bytes32(i as u32, bytebuffer.0.as_mut_ptr());
            let localarray: [u8; 32] = bytebuffer.0;
            for j in 0..32 {
                let foundval = localarray[j];
                println!("byte {} found {}", j, foundval);
                if foundval != (j + i*0x10) as u8 {
                    panic!("globalstate bytes32 idx {} byte {} expected {} got{}",
                            i, j, j + i*0x10, foundval)
                }
            }
        }
    }
    println!("Done!");
}
'''
'''--- arbitrator/prover/test-cases/rust/src/bin/host-io.rs ---
use hex_literal::hex;

extern "C" {
    pub fn wavm_read_keccak_256_preimage(ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_sha2_256_preimage(ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_inbox_message(msg_num: u64, ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_delayed_inbox_message(seq_num: u64, ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_halt_and_set_finished();
}

#[repr(C, align(32))]
struct Bytes32([u8; 32]);

const INBOX_HEADER_LEN: usize = 40; // also in src/main.rs & contracts's OneStepProverHostIo.sol
const DELAYED_HEADER_LEN: usize = 112; // also in src/main.rs & contracts's OneStepProverHostIo.sol

fn main() {
    println!("hello!");
    unsafe {
        let mut bytebuffer = Bytes32([0x0; 32]);
        // in delayed inbox - we're skipping the "kind" byte
        println!("delayed inbox message 0");
        let len = wavm_read_delayed_inbox_message(0, bytebuffer.0.as_mut_ptr(), DELAYED_HEADER_LEN);
        assert_eq!(len, 2);
        assert_eq!(bytebuffer.0[1], 0xaa);
        println!("delayed inbox message 1");
        let len = wavm_read_delayed_inbox_message(1, bytebuffer.0.as_mut_ptr(), DELAYED_HEADER_LEN);
        assert_eq!(len, 32);
        for j in 1..31 {
            assert_eq!(bytebuffer.0[j], (j as u8));
        }
        println!("inbox message 0");
        let len = wavm_read_inbox_message(0, bytebuffer.0.as_mut_ptr(), INBOX_HEADER_LEN);
        assert_eq!(len, 1);
        assert_eq!(bytebuffer.0[0], 0xaa);
        println!("inbox message 1");
        let len = wavm_read_inbox_message(1, bytebuffer.0.as_mut_ptr(), INBOX_HEADER_LEN);
        assert_eq!(len, 32);
        for j in 0..32 {
            assert_eq!(bytebuffer.0[j], (j as u8) + 1);
        }
        let keccak_hash = hex!("47173285a8d7341e5e972fc677286384f802f8ef42a5ec5f03bbfa254cb01fad");
        bytebuffer = Bytes32(keccak_hash);
        println!("keccak preimage");
        let expected_preimage = b"hello world";
        let len = wavm_read_keccak_256_preimage(bytebuffer.0.as_mut_ptr(), 0);
        assert_eq!(len, expected_preimage.len());
        assert_eq!(&bytebuffer.0[..len], expected_preimage);
        println!("sha2 preimage");
        let sha2_hash = hex!("b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9");
        bytebuffer = Bytes32(sha2_hash);
        let len = wavm_read_sha2_256_preimage(bytebuffer.0.as_mut_ptr(), 0);
        assert_eq!(len, expected_preimage.len());
        assert_eq!(&bytebuffer.0[..len], expected_preimage);
    }
    println!("Done!");
}

'''
'''--- arbitrator/prover/test-cases/rust/src/bin/keccak256.rs ---
use sha3::Keccak256;
use digest::Digest;

fn main() {
	let mut hasher = Keccak256::new();
	for i in 0..5 {
		hasher.update(&[i]);
	}
	let output: [u8; 32] = hasher.finalize().into();
	std::process::exit(i32::from(output[0]) ^ 183);
}

'''
'''--- arbitrator/prover/test-cases/rust/src/bin/pi.rs ---
fn main() {
    // Compute Ï€ with Ramanujan's approximation

    macro_rules! compute {
		($t:ty) => {
			let mut estimate = 0.;
			let multiplier = 2. * (2 as $t).sqrt() / 9801.;
			let mut fact = 1.;
			let mut quad_fact = 1.;
			let mut k = 0. as $t;
			while k <= 10. {
				let quad = k * 4.;
				if k > 0. {
					fact *= k;
					quad_fact *= quad - 3.;
					quad_fact *= quad - 2.;
					quad_fact *= quad - 1.;
					quad_fact *= quad;
				}
				let fact_square = fact * fact;
				estimate += multiplier * quad_fact * (1103. + 26390. * k) / (fact_square * fact_square * (396 as $t).powf(quad));
				let pi = 1./estimate;
				println!("Estimation with {} after iteration {}: {}", stringify!($t), k, pi);
				assert!(pi.is_nan() || (pi > 3.13 && pi < 3.15));
				k += 1.;
			}
		}
	}

    compute!(f32);
    compute!(f64);
}

'''
'''--- arbitrator/prover/test-cases/rust/src/bin/stdlib.rs ---
fn main() {
	let mut x = Vec::new();
	for i in 1..10 {
		x.push(i);
	}
	let sum: usize = x.iter().cloned().sum();
	let product = x.into_iter().fold(1, |p, x| p * x);
	println!("Sum: {}", sum);
	eprintln!("Product: {}", product);
	assert_eq!(sum, 45);
	assert_eq!(product, 362880);
}

'''
'''--- arbitrator/prover/test-cases/rust/src/lib.rs ---

'''
'''--- arbitrator/wasm-libraries/Cargo.toml ---
[workspace]
members = [
	"brotli",
	"wasi-stub",
	"go-stub",
	"go-abi",
	"host-io",
]

'''
'''--- arbitrator/wasm-libraries/brotli/Cargo.toml ---
[package]
name = "brotli"
version = "0.1.0"
edition = "2021"
publish = false

[lib]
crate-type = ["cdylib"]

[dependencies]
go-abi = { path = "../go-abi" }

'''
'''--- arbitrator/wasm-libraries/brotli/build.rs ---
fn main() {
    // Tell Cargo that if the given file changes, to rerun this build script.
    println!("cargo:rustc-link-search=../../target/lib-wasm/");
    println!("cargo:rustc-link-lib=static=brotlienc-static");
    println!("cargo:rustc-link-lib=static=brotlidec-static");
    println!("cargo:rustc-link-lib=static=brotlicommon-static");
}

'''
'''--- arbitrator/wasm-libraries/brotli/src/lib.rs ---
use go_abi::*;

extern "C" {
    pub fn BrotliDecoderDecompress(
        encoded_size: usize,
        encoded_buffer: *const u8,
        decoded_size: *mut usize,
        decoded_buffer: *mut u8,
    ) -> u32;

    pub fn BrotliEncoderCompress(
        quality: u32,
        lgwin: u32,
        mode: u32,
        input_size: usize,
        input_buffer: *const u8,
        encoded_size: *mut usize,
        encoded_buffer: *mut u8,
    ) -> u32;
}

const BROTLI_MODE_GENERIC: u32 = 0;
const BROTLI_RES_SUCCESS: u32 = 1;

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_arbcompress_brotliDecompress(
    sp: GoStack,
) {
    //(inBuf []byte, outBuf []byte) int
    let in_buf_ptr = sp.read_u64(0);
    let in_buf_len = sp.read_u64(1);
    let out_buf_ptr = sp.read_u64(3);
    let out_buf_len = sp.read_u64(4);
    const OUTPUT_ARG: usize = 6;

    let in_slice = read_slice(in_buf_ptr, in_buf_len);
    let mut output = vec![0u8; out_buf_len as usize];
    let mut output_len = out_buf_len as usize;
    let res = BrotliDecoderDecompress(
        in_buf_len as usize,
        in_slice.as_ptr(),
        &mut output_len,
        output.as_mut_ptr(),
    );
    if (res != BROTLI_RES_SUCCESS) || (output_len as u64 > out_buf_len) {
        sp.write_u64(OUTPUT_ARG, u64::MAX);
        return;
    }
    write_slice(&output[..output_len], out_buf_ptr);
    sp.write_u64(OUTPUT_ARG, output_len as u64);
    return;
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_arbcompress_brotliCompress(sp: GoStack) {
    //(inBuf []byte, outBuf []byte, level int, windowSize int) int
    let in_buf_ptr = sp.read_u64(0);
    let in_buf_len = sp.read_u64(1);
    let out_buf_ptr = sp.read_u64(3);
    let out_buf_len = sp.read_u64(4);
    let level = sp.read_u64(6) as u32;
    let windowsize = sp.read_u64(7) as u32;
    const OUTPUT_ARG: usize = 8;

    let in_slice = read_slice(in_buf_ptr, in_buf_len);
    let mut output = vec![0u8; out_buf_len as usize];
    let mut output_len = out_buf_len as usize;
    let res = BrotliEncoderCompress(
        level,
        windowsize,
        BROTLI_MODE_GENERIC,
        in_buf_len as usize,
        in_slice.as_ptr(),
        &mut output_len,
        output.as_mut_ptr(),
    );
    if (res != BROTLI_RES_SUCCESS) || (output_len as u64 > out_buf_len) {
        sp.write_u64(OUTPUT_ARG, u64::MAX);
        return;
    }
    write_slice(&output[..output_len], out_buf_ptr);
    sp.write_u64(OUTPUT_ARG, output_len as u64);
    return;
}

'''
'''--- arbitrator/wasm-libraries/go-abi/Cargo.toml ---
[package]
name = "go-abi"
version = "0.1.0"
edition = "2018"
publish = false

[dependencies]

'''
'''--- arbitrator/wasm-libraries/go-abi/src/lib.rs ---
use std::convert::TryFrom;

extern "C" {
    pub fn wavm_caller_load8(ptr: usize) -> u8;
    pub fn wavm_caller_load32(ptr: usize) -> u32;
    pub fn wavm_caller_store8(ptr: usize, val: u8);
    pub fn wavm_caller_store32(ptr: usize, val: u32);

    pub fn wavm_guest_call__getsp() -> usize;
    pub fn wavm_guest_call__resume();
}

pub unsafe fn wavm_caller_load64(ptr: usize) -> u64 {
    let lower = wavm_caller_load32(ptr);
    let upper = wavm_caller_load32(ptr + 4);
    lower as u64 | ((upper as u64) << 32)
}

pub unsafe fn wavm_caller_store64(ptr: usize, val: u64) {
    wavm_caller_store32(ptr, val as u32);
    wavm_caller_store32(ptr + 4, (val >> 32) as u32);
}

#[derive(Clone, Copy)]
#[repr(transparent)]
pub struct GoStack(pub usize);

impl GoStack {
    fn offset(&self, arg: usize) -> usize {
        self.0 + (arg + 1) * 8
    }

    pub unsafe fn read_u8(self, arg: usize) -> u8 {
        wavm_caller_load8(self.offset(arg))
    }

    pub unsafe fn read_u32(self, arg: usize) -> u32 {
        wavm_caller_load32(self.offset(arg))
    }

    pub unsafe fn read_u64(self, arg: usize) -> u64 {
        wavm_caller_load64(self.offset(arg))
    }

    pub unsafe fn write_u8(self, arg: usize, x: u8) {
        wavm_caller_store8(self.offset(arg), x);
    }

    pub unsafe fn write_u32(self, arg: usize, x: u32) {
        wavm_caller_store32(self.offset(arg), x);
    }

    pub unsafe fn write_u64(self, arg: usize, x: u64) {
        wavm_caller_store64(self.offset(arg), x);
    }
}

pub unsafe fn read_slice(ptr: u64, mut len: u64) -> Vec<u8> {
    let mut data = Vec::with_capacity(len as usize);
    if len == 0 {
        return data;
    }
    let mut ptr = usize::try_from(ptr).expect("Go pointer didn't fit in usize");
    while len >= 4 {
        data.extend(wavm_caller_load32(ptr).to_le_bytes());
        ptr += 4;
        len -= 4;
    }
    for _ in 0..len {
        data.push(wavm_caller_load8(ptr));
        ptr += 1;
    }
    data
}

pub unsafe fn write_slice(mut src: &[u8], ptr: u64) {
    if src.len() == 0 {
        return;
    }
    let mut ptr = usize::try_from(ptr).expect("Go pointer didn't fit in usize");
    while src.len() >= 4 {
        let mut arr = [0u8; 4];
        arr.copy_from_slice(&src[..4]);
        wavm_caller_store32(ptr, u32::from_le_bytes(arr));
        ptr += 4;
        src = &src[4..];
    }
    for &byte in src {
        wavm_caller_store8(ptr, byte);
        ptr += 1;
    }
}

'''
'''--- arbitrator/wasm-libraries/go-stub/Cargo.toml ---
[package]
name = "go-stub"
version = "0.1.0"
edition = "2018"
publish = false

[lib]
crate-type = ["cdylib"]

[dependencies]
fnv = "1.0.7"
rand = { version = "0.8.4", default-features = false }
rand_pcg = { version = "0.3.1", default-features = false }
go-abi = { path = "../go-abi" }

[features]

'''
'''--- arbitrator/wasm-libraries/go-stub/src/lib.rs ---
mod value;

use crate::value::*;
use fnv::FnvHashSet as HashSet;
use go_abi::*;
use rand::RngCore;
use rand_pcg::Pcg32;
use std::{collections::BinaryHeap, convert::TryFrom, io::Write};

fn interpret_value(repr: u64) -> InterpValue {
    if repr == 0 {
        return InterpValue::Undefined;
    }
    let float = f64::from_bits(repr);
    if float.is_nan() && repr != f64::NAN.to_bits() {
        let id = repr as u32;
        if id == ZERO_ID {
            return InterpValue::Number(0.);
        }
        return InterpValue::Ref(id);
    }
    InterpValue::Number(float)
}

unsafe fn read_value_slice(mut ptr: u64, len: u64) -> Vec<InterpValue> {
    let mut values = Vec::new();
    for _ in 0..len {
        let p = usize::try_from(ptr).expect("Go pointer didn't fit in usize");
        values.push(interpret_value(wavm_caller_load64(p)));
        ptr += 8;
    }
    values
}

#[no_mangle]
pub unsafe extern "C" fn go__debug(x: usize) {
    println!("go debug: {}", x);
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_resetMemoryDataView(_: GoStack) {}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_wasmExit(sp: GoStack) {
    std::process::exit(sp.read_u32(0) as i32);
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_wasmWrite(sp: GoStack) {
    let fd = sp.read_u64(0);
    let ptr = sp.read_u64(1);
    let len = sp.read_u32(2);
    let buf = read_slice(ptr, len.into());
    if fd == 2 {
        let stderr = std::io::stderr();
        let mut stderr = stderr.lock();
        stderr.write_all(&buf).unwrap();
    } else {
        let stdout = std::io::stdout();
        let mut stdout = stdout.lock();
        stdout.write_all(&buf).unwrap();
    }
}

// An increasing clock used when Go asks for time, measured in nanoseconds.
static mut TIME: u64 = 0;
// The amount of TIME advanced each check. Currently 10 milliseconds.
static mut TIME_INTERVAL: u64 = 10_000_000;

#[no_mangle]
pub unsafe extern "C" fn go__runtime_nanotime1(sp: GoStack) {
    TIME += TIME_INTERVAL;
    sp.write_u64(0, TIME);
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_walltime(sp: GoStack) {
    TIME += TIME_INTERVAL;
    sp.write_u64(0, TIME / 1_000_000_000);
    sp.write_u32(1, (TIME % 1_000_000_000) as u32);
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_walltime1(sp: GoStack) {
    TIME += TIME_INTERVAL;
    sp.write_u64(0, TIME / 1_000_000_000);
    sp.write_u64(1, TIME % 1_000_000_000);
}

static mut RNG: Option<Pcg32> = None;

unsafe fn get_rng<'a>() -> &'a mut Pcg32 {
    RNG.get_or_insert_with(|| Pcg32::new(0xcafef00dd15ea5e5, 0xa02bdbf7bb3c0a7))
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_getRandomData(sp: GoStack) {
    let rng = get_rng();
    let mut ptr =
        usize::try_from(sp.read_u64(0)).expect("Go getRandomData pointer didn't fit in usize");
    let mut len = sp.read_u64(1);
    while len >= 4 {
        wavm_caller_store32(ptr, rng.next_u32());
        ptr += 4;
        len -= 4;
    }
    if len > 0 {
        let mut rem = rng.next_u32();
        for _ in 0..len {
            wavm_caller_store8(ptr, rem as u8);
            ptr += 1;
            rem >>= 8;
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
struct TimeoutInfo {
    time: u64,
    id: u32,
}

impl Ord for TimeoutInfo {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        other
            .time
            .cmp(&self.time)
            .then_with(|| other.id.cmp(&self.id))
    }
}

impl PartialOrd for TimeoutInfo {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(&other))
    }
}

#[derive(Default, Debug)]
struct TimeoutState {
    /// Contains tuples of (time, id)
    times: BinaryHeap<TimeoutInfo>,
    pending_ids: HashSet<u32>,
    next_id: u32,
}

static mut TIMEOUT_STATE: Option<TimeoutState> = None;

#[no_mangle]
pub unsafe extern "C" fn go__runtime_scheduleTimeoutEvent(sp: GoStack) {
    let mut time = sp.read_u64(0);
    time = time.saturating_mul(1_000_000); // milliseconds to nanoseconds
    time = time.saturating_add(TIME); // add the current time to the delay

    let state = TIMEOUT_STATE.get_or_insert_with(Default::default);
    let id = state.next_id;
    state.next_id += 1;
    state.times.push(TimeoutInfo { time, id });
    state.pending_ids.insert(id);

    sp.write_u32(1, id);
}

#[no_mangle]
pub unsafe extern "C" fn go__runtime_clearTimeoutEvent(sp: GoStack) {
    let id = sp.read_u32(0);

    let state = TIMEOUT_STATE.get_or_insert_with(Default::default);
    if !state.pending_ids.remove(&id) {
        eprintln!("Go attempting to clear not pending timeout event {}", id);
    }
}

macro_rules! unimpl_js {
    ($($f:ident),* $(,)?) => {
        $(
            #[no_mangle]
            pub unsafe extern "C" fn $f(_: GoStack) {
                unimplemented!("Go JS interface {} not supported", stringify!($f));
            }
        )*
    }
}

unimpl_js!(
    go__syscall_js_stringVal,
    go__syscall_js_valueSetIndex,
    go__syscall_js_valuePrepareString,
    go__syscall_js_valueLoadString,
    go__syscall_js_valueDelete,
    go__syscall_js_valueInvoke,
    go__syscall_js_valueInstanceOf,
);

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueGet(sp: GoStack) {
    let source = interpret_value(sp.read_u64(0));
    let field_ptr = sp.read_u64(1);
    let field_len = sp.read_u64(2);
    let field = read_slice(field_ptr, field_len);
    let value = match source {
        InterpValue::Ref(id) => get_field(id, &field),
        val => {
            eprintln!(
                "Go attempting to read field {:?} . {}",
                val,
                String::from_utf8_lossy(&field),
            );
            GoValue::Null
        }
    };
    sp.write_u64(3, value.encode());
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueNew(sp: GoStack) {
    let class = sp.read_u32(0);
    let args_ptr = sp.read_u64(1);
    let args_len = sp.read_u64(2);
    let args = read_value_slice(args_ptr, args_len);
    if class == UINT8_ARRAY_ID {
        if let Some(InterpValue::Number(size)) = args.get(0) {
            let id = DynamicObjectPool::singleton()
                .insert(DynamicObject::Uint8Array(vec![0; *size as usize]));
            sp.write_u64(4, GoValue::Object(id).encode());
            sp.write_u8(5, 1);
            return;
        } else {
            eprintln!(
                "Go attempted to construct Uint8Array with bad args: {:?}",
                args,
            );
        }
    } else if class == DATE_ID {
        let id = DynamicObjectPool::singleton().insert(DynamicObject::Date);
        sp.write_u64(4, GoValue::Object(id).encode());
        sp.write_u8(5, 1);
        return;
    } else {
        eprintln!(
            "Go attempting to construct unimplemented JS value {}",
            class,
        );
    }
    sp.write_u64(4, GoValue::Null.encode());
    sp.write_u8(5, 0);
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_copyBytesToJS(sp: GoStack) {
    let dest_val = interpret_value(sp.read_u64(0));
    if let InterpValue::Ref(dest_id) = dest_val {
        let src_ptr = sp.read_u64(1);
        let src_len = sp.read_u64(2);
        let dest = DynamicObjectPool::singleton().get_mut(dest_id);
        if let Some(DynamicObject::Uint8Array(buf)) = dest {
            if buf.len() as u64 != src_len {
                eprintln!(
                    "Go copying bytes from Go source length {} to JS dest length {}",
                    src_len,
                    buf.len(),
                );
            }
            let len = std::cmp::min(src_len, buf.len() as u64) as usize;
            // Slightly inefficient as this allocates a new temporary buffer
            buf[..len].copy_from_slice(&read_slice(src_ptr, len as u64));
            sp.write_u64(4, GoValue::Number(len as f64).encode());
            sp.write_u8(5, 1);
            return;
        } else {
            eprintln!(
                "Go attempting to copy bytes into unsupported target {:?}",
                dest,
            );
        }
    } else {
        eprintln!("Go attempting to copy bytes into {:?}", dest_val);
    }
    sp.write_u64(4, GoValue::Null.encode());
    sp.write_u8(5, 0);
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_copyBytesToGo(sp: GoStack) {
    let dest_ptr = sp.read_u64(0);
    let dest_len = sp.read_u64(1);
    let src_val = interpret_value(sp.read_u64(3));
    if let InterpValue::Ref(src_id) = src_val {
        let source = DynamicObjectPool::singleton().get_mut(src_id);
        if let Some(DynamicObject::Uint8Array(buf)) = source {
            if buf.len() as u64 != dest_len {
                eprintln!(
                    "Go copying bytes from JS source length {} to Go dest length {}",
                    buf.len(),
                    dest_len,
                );
            }
            let len = std::cmp::min(buf.len() as u64, dest_len) as usize;
            write_slice(&buf[..len], dest_ptr);

            sp.write_u64(4, GoValue::Number(len as f64).encode());
            sp.write_u8(5, 1);
            return;
        } else {
            eprintln!(
                "Go attempting to copy bytes from unsupported source {:?}",
                source,
            );
        }
    } else {
        eprintln!("Go attempting to copy bytes from {:?}", src_val);
    }
    sp.write_u8(5, 0);
}

unsafe fn value_call_impl(sp: &mut GoStack) -> Result<GoValue, String> {
    let object = interpret_value(sp.read_u64(0));
    let method_name_ptr = sp.read_u64(1);
    let method_name_len = sp.read_u64(2);
    let method_name = read_slice(method_name_ptr, method_name_len);
    let args_ptr = sp.read_u64(3);
    let args_len = sp.read_u64(4);
    let args = read_value_slice(args_ptr, args_len);
    if object == InterpValue::Ref(GO_ID) && &method_name == b"_makeFuncWrapper" {
        let id = args.get(0).ok_or_else(|| {
            format!(
                "Go attempting to call Go._makeFuncWrapper with bad args {:?}",
                args,
            )
        })?;
        let ref_id =
            DynamicObjectPool::singleton().insert(DynamicObject::FunctionWrapper(*id, object));
        Ok(GoValue::Function(ref_id))
    } else if object == InterpValue::Ref(FS_ID) && &method_name == b"write" {
        let args_len = std::cmp::min(6, args.len());
        if let &[InterpValue::Number(fd), InterpValue::Ref(buf_id), InterpValue::Number(offset), InterpValue::Number(length), InterpValue::Ref(NULL_ID), InterpValue::Ref(callback_id)] =
            &args.as_slice()[..args_len]
        {
            let object_pool = DynamicObjectPool::singleton();
            let buf = match object_pool.get(buf_id) {
                Some(DynamicObject::Uint8Array(x)) => x,
                x => {
                    return Err(format!(
                        "Go attempting to call fs.write with bad buffer {:?}",
                        x,
                    ))
                }
            };
            let (func_id, this) = match object_pool.get(callback_id) {
                Some(DynamicObject::FunctionWrapper(f, t)) => (f, t),
                x => {
                    return Err(format!(
                        "Go attempting to call fs.write with bad buffer {:?}",
                        x,
                    ))
                }
            };
            let mut offset = offset as usize;
            let mut length = length as usize;
            if offset > buf.len() {
                eprintln!(
                    "Go attempting to call fs.write with offset {} >= buf.len() {}",
                    offset,
                    buf.len(),
                );
                offset = buf.len();
            }
            if offset + length > buf.len() {
                eprintln!(
                    "Go attempting to call fs.write with offset {} + length {} >= buf.len() {}",
                    offset,
                    length,
                    buf.len(),
                );
                length = buf.len() - offset;
            }

            if fd == 1. {
                let stdout = std::io::stdout();
                let mut stdout = stdout.lock();
                stdout.write_all(&buf[offset..(offset + length)]).unwrap();
            } else if fd == 2. {
                let stderr = std::io::stderr();
                let mut stderr = stderr.lock();
                stderr.write_all(&buf[offset..(offset + length)]).unwrap();
            } else {
                eprintln!("Go attempting to write to unknown FD {}", fd);
            }

            PENDING_EVENT = Some(PendingEvent {
                id: *func_id,
                this: *this,
                args: vec![
                    GoValue::Null,                  // no error
                    GoValue::Number(length as f64), // amount written
                ],
            });
            wavm_guest_call__resume();

            *sp = GoStack(wavm_guest_call__getsp());
            Ok(GoValue::Null)
        } else {
            Err(format!(
                "Go attempting to call fs.write with bad args {:?}",
                args
            ))
        }
    } else if object == InterpValue::Ref(CRYPTO_ID) && &method_name == b"getRandomValues" {
        let id = match args.get(0) {
            Some(InterpValue::Ref(x)) => *x,
            _ => {
                return Err(format!(
                    "Go attempting to call crypto.getRandomValues with bad args {:?}",
                    args,
                ));
            }
        };
        match DynamicObjectPool::singleton().get_mut(id) {
            Some(DynamicObject::Uint8Array(buf)) => {
                get_rng().fill_bytes(buf.as_mut_slice());
            }
            Some(x) => {
                return Err(format!(
                    "Go attempting to call crypto.getRandomValues on bad object {:?}",
                    x,
                ));
            }
            None => {
                return Err(format!(
                    "Go attempting to call crypto.getRandomValues on unknown reference {}",
                    id,
                ));
            }
        }
        Ok(GoValue::Undefined)
    } else if let InterpValue::Ref(obj_id) = object {
        let val = DynamicObjectPool::singleton().get(obj_id);
        if let Some(DynamicObject::Date) = val {
            if &method_name == b"getTimezoneOffset" {
                return Ok(GoValue::Number(0.0));
            } else {
                return Err(format!(
                    "Go attempting to call unknown method {} for date object",
                    String::from_utf8_lossy(&method_name),
                ));
            }
        } else {
            return Err(format!(
                "Go attempting to call method {} for unknown object - id {}",
                String::from_utf8_lossy(&method_name),
                obj_id,
            ));
        }
    } else {
        Err(format!(
            "Go attempting to call unknown method {:?} . {}",
            object,
            String::from_utf8_lossy(&method_name),
        ))
    }
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueCall(mut sp: GoStack) {
    match value_call_impl(&mut sp) {
        Ok(val) => {
            sp.write_u64(6, val.encode());
            sp.write_u8(7, 1);
        }
        Err(err) => {
            eprintln!("{}", err);
            sp.write_u64(6, GoValue::Null.encode());
            sp.write_u8(7, 0);
        }
    }
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueSet(sp: GoStack) {
    let source = interpret_value(sp.read_u64(0));
    let field_ptr = sp.read_u64(1);
    let field_len = sp.read_u64(2);
    let new_value = interpret_value(sp.read_u64(3));
    let field = read_slice(field_ptr, field_len);
    if source == InterpValue::Ref(GO_ID)
        && &field == b"_pendingEvent"
        && new_value == InterpValue::Ref(NULL_ID)
    {
        PENDING_EVENT = None;
        return;
    }
    let pool = DynamicObjectPool::singleton();
    if let InterpValue::Ref(id) = source {
        let source = pool.get(id);
        if let Some(DynamicObject::PendingEvent(_)) = source {
            if field == b"result" {
                return;
            }
        }
    }
    eprintln!(
        "Go attempted to set unsupported value {:?} field {} to {:?}",
        source,
        String::from_utf8_lossy(&field),
        new_value,
    );
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueLength(sp: GoStack) {
    let source = interpret_value(sp.read_u64(0));
    let pool = DynamicObjectPool::singleton();
    let source = match source {
        InterpValue::Ref(x) => pool.get(x),
        _ => None,
    };
    let len = match source {
        Some(DynamicObject::Uint8Array(x)) => Some(x.len()),
        Some(DynamicObject::ValueArray(x)) => Some(x.len()),
        _ => None,
    };
    if let Some(len) = len {
        sp.write_u64(1, len as u64);
    } else {
        eprintln!(
            "Go attempted to get length of unsupported value {:?}",
            source,
        );
        sp.write_u64(1, 0);
    }
}

unsafe fn value_index_impl(sp: GoStack) -> Result<GoValue, String> {
    let pool = DynamicObjectPool::singleton();
    let source = match interpret_value(sp.read_u64(0)) {
        InterpValue::Ref(x) => pool.get(x),
        val => return Err(format!("Go attempted to index into {:?}", val)),
    };
    let index = usize::try_from(sp.read_u64(1)).map_err(|e| format!("{:?}", e))?;
    let val = match source {
        Some(DynamicObject::Uint8Array(x)) => {
            Some(x.get(index).map(|x| GoValue::Number(*x as f64)))
        }
        Some(DynamicObject::ValueArray(x)) => Some(x.get(index).cloned()),
        _ => None,
    };
    match val {
        Some(Some(val)) => Ok(val),
        Some(None) => Err(format!(
            "Go attempted to index out of bounds into value {:?} index {}",
            source, index,
        )),
        None => Err(format!(
            "Go attempted to index into unsupported value {:?}",
            source
        )),
    }
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_valueIndex(sp: GoStack) {
    match value_index_impl(sp) {
        Ok(v) => sp.write_u64(2, v.encode()),
        Err(e) => {
            eprintln!("{}", e);
            sp.write_u64(2, GoValue::Null.encode());
        }
    }
}

#[no_mangle]
pub unsafe extern "C" fn go__syscall_js_finalizeRef(sp: GoStack) {
    let val = interpret_value(sp.read_u64(0));
    match val {
        InterpValue::Ref(x) if x < DYNAMIC_OBJECT_ID_BASE => {}
        InterpValue::Ref(x) => {
            if DynamicObjectPool::singleton().remove(x).is_none() {
                eprintln!("Go attempting to finalize unknown ref {}", x);
            }
        }
        val => eprintln!("Go attempting to finalize {:?}", val),
    }
}

#[no_mangle]
pub unsafe extern "C" fn wavm__go_after_run() {
    let mut state = TIMEOUT_STATE.get_or_insert_with(Default::default);
    while let Some(info) = state.times.pop() {
        while state.pending_ids.contains(&info.id) {
            TIME = std::cmp::max(TIME, info.time);
            // Important: the current reference to state shouldn't be used after this resume call,
            // as it might during the resume call the reference might be invalidated.
            // That's why immediately after this resume call, we replace the reference
            // with a new reference to TIMEOUT_STATE.
            wavm_guest_call__resume();
            state = TIMEOUT_STATE.get_or_insert_with(Default::default);
        }
    }
}

'''
'''--- arbitrator/wasm-libraries/go-stub/src/value.rs ---
use fnv::FnvHashMap as HashMap;

pub const ZERO_ID: u32 = 1;
pub const NULL_ID: u32 = 2;
pub const GLOBAL_ID: u32 = 5;
pub const GO_ID: u32 = 6;

pub const OBJECT_ID: u32 = 100;
pub const ARRAY_ID: u32 = 101;
pub const PROCESS_ID: u32 = 102;
pub const FS_ID: u32 = 103;
pub const UINT8_ARRAY_ID: u32 = 104;
pub const CRYPTO_ID: u32 = 105;
pub const DATE_ID: u32 = 106;

pub const FS_CONSTANTS_ID: u32 = 200;

pub const DYNAMIC_OBJECT_ID_BASE: u32 = 10000;

#[derive(Clone, Copy, Debug, PartialEq)]
pub enum InterpValue {
    Undefined,
    Number(f64),
    Ref(u32),
}

impl InterpValue {
    pub fn assume_num_or_object(self) -> GoValue {
        match self {
            InterpValue::Undefined => GoValue::Undefined,
            InterpValue::Number(x) => GoValue::Number(x),
            InterpValue::Ref(x) => GoValue::Object(x),
        }
    }
}

#[derive(Clone, Copy, Debug)]
#[allow(dead_code)]
pub enum GoValue {
    Undefined,
    Number(f64),
    Null,
    Object(u32),
    String(u32),
    Symbol(u32),
    Function(u32),
}

impl GoValue {
    pub fn encode(self) -> u64 {
        let (ty, id): (u32, u32) = match self {
            GoValue::Undefined => return 0,
            GoValue::Number(mut f) => {
                // Canonicalize NaNs so they don't collide with other value types
                if f.is_nan() {
                    f = f64::NAN;
                }
                if f == 0. {
                    // Zeroes are encoded differently for some reason
                    (0, ZERO_ID)
                } else {
                    return f.to_bits();
                }
            }
            GoValue::Null => (0, NULL_ID),
            GoValue::Object(x) => (1, x),
            GoValue::String(x) => (2, x),
            GoValue::Symbol(x) => (3, x),
            GoValue::Function(x) => (4, x),
        };
        // Must not be all zeroes, otherwise it'd collide with a real NaN
        assert!(ty != 0 || id != 0, "GoValue must not be empty");
        f64::NAN.to_bits() | (u64::from(ty) << 32) | u64::from(id)
    }
}

#[derive(Clone, Debug)]
pub struct PendingEvent {
    pub id: InterpValue,
    pub this: InterpValue,
    pub args: Vec<GoValue>,
}

#[derive(Debug, Clone)]
pub enum DynamicObject {
    Uint8Array(Vec<u8>),
    FunctionWrapper(InterpValue, InterpValue),
    PendingEvent(PendingEvent),
    ValueArray(Vec<GoValue>),
    Date,
}

#[derive(Default, Debug)]
pub struct DynamicObjectPool {
    objects: HashMap<u32, DynamicObject>,
    free_ids: Vec<u32>,
}

static mut DYNAMIC_OBJECT_POOL: Option<DynamicObjectPool> = None;

impl DynamicObjectPool {
    pub unsafe fn singleton<'a>() -> &'a mut Self {
        DYNAMIC_OBJECT_POOL.get_or_insert_with(Default::default)
    }

    pub fn insert(&mut self, object: DynamicObject) -> u32 {
        let id = self
            .free_ids
            .pop()
            .unwrap_or_else(|| DYNAMIC_OBJECT_ID_BASE + self.objects.len() as u32);
        self.objects.insert(id, object);
        id
    }

    pub fn get(&self, id: u32) -> Option<&DynamicObject> {
        self.objects.get(&id)
    }

    pub fn get_mut(&mut self, id: u32) -> Option<&mut DynamicObject> {
        self.objects.get_mut(&id)
    }

    pub fn remove(&mut self, id: u32) -> Option<DynamicObject> {
        let res = self.objects.remove(&id);
        if res.is_some() {
            self.free_ids.push(id);
        }
        res
    }
}

pub static mut PENDING_EVENT: Option<PendingEvent> = None;

pub unsafe fn get_field(source: u32, field: &[u8]) -> GoValue {
    if source == GLOBAL_ID {
        if field == b"Object" {
            return GoValue::Function(OBJECT_ID);
        } else if field == b"Array" {
            return GoValue::Function(ARRAY_ID);
        } else if field == b"process" {
            return GoValue::Object(PROCESS_ID);
        } else if field == b"fs" {
            return GoValue::Object(FS_ID);
        } else if field == b"Uint8Array" {
            return GoValue::Function(UINT8_ARRAY_ID);
        } else if field == b"crypto" {
            return GoValue::Object(CRYPTO_ID);
        } else if field == b"Date" {
            return GoValue::Object(DATE_ID);
        } else if field == b"fetch" {
            // Triggers a code path in Go for a fake network implementation
            return GoValue::Undefined;
        }
    } else if source == FS_ID {
        if field == b"constants" {
            return GoValue::Object(FS_CONSTANTS_ID);
        }
    } else if source == FS_CONSTANTS_ID {
        if matches!(
            field,
            b"O_WRONLY" | b"O_RDWR" | b"O_CREAT" | b"O_TRUNC" | b"O_APPEND" | b"O_EXCL"
        ) {
            return GoValue::Number(-1.);
        }
    } else if source == GO_ID {
        if field == b"_pendingEvent" {
            if let Some(event) = &PENDING_EVENT {
                let id = DynamicObjectPool::singleton()
                    .insert(DynamicObject::PendingEvent(event.clone()));
                return GoValue::Object(id);
            } else {
                return GoValue::Null;
            }
        }
    }

    if let Some(source) = DynamicObjectPool::singleton().get(source).cloned() {
        if let DynamicObject::PendingEvent(event) = &source {
            if field == b"id" {
                return event.id.assume_num_or_object();
            } else if field == b"this" {
                return event.this.assume_num_or_object();
            } else if field == b"args" {
                let id = DynamicObjectPool::singleton()
                    .insert(DynamicObject::ValueArray(event.args.clone()));
                return GoValue::Object(id);
            }
        }

        eprintln!(
            "Go attempting to access unimplemented unknown JS value {:?} field {}",
            source,
            String::from_utf8_lossy(field),
        );
        GoValue::Undefined
    } else {
        eprintln!(
            "Go attempting to access unimplemented unknown JS value {} field {}",
            source,
            String::from_utf8_lossy(field),
        );
        GoValue::Undefined
    }
}

'''
'''--- arbitrator/wasm-libraries/host-io/Cargo.toml ---
[package]
name = "host-io"
version = "0.1.0"
edition = "2018"
publish = false

[lib]
crate-type = ["cdylib"]

[dependencies]
go-abi = { path = "../go-abi" }
arbutil = { path = "../../arbutil" }

'''
'''--- arbitrator/wasm-libraries/host-io/src/lib.rs ---
use arbutil::PreimageType;
use go_abi::*;
use std::convert::TryInto;

extern "C" {
    pub fn wavm_get_globalstate_bytes32(idx: u32, ptr: *mut u8);
    pub fn wavm_set_globalstate_bytes32(idx: u32, ptr: *const u8);
    pub fn wavm_get_globalstate_u64(idx: u32) -> u64;
    pub fn wavm_set_globalstate_u64(idx: u32, val: u64);
    pub fn wavm_read_keccak_256_preimage(ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_sha2_256_preimage(ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_inbox_message(msg_num: u64, ptr: *mut u8, offset: usize) -> usize;
    pub fn wavm_read_delayed_inbox_message(seq_num: u64, ptr: *mut u8, offset: usize) -> usize;
}

#[repr(C, align(256))]
struct MemoryLeaf([u8; 32]);

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_getGlobalStateBytes32(
    sp: GoStack,
) {
    let idx = sp.read_u64(0) as u32;
    let out_ptr = sp.read_u64(1);
    let mut out_len = sp.read_u64(2);
    if out_len < 32 {
        eprintln!(
            "Go attempting to read block hash into {} bytes long buffer",
            out_len,
        );
    } else {
        out_len = 32;
    }
    let mut our_buf = MemoryLeaf([0u8; 32]);
    let our_ptr = our_buf.0.as_mut_ptr();
    assert_eq!(our_ptr as usize % 32, 0);
    wavm_get_globalstate_bytes32(idx, our_ptr);
    write_slice(&our_buf.0[..(out_len as usize)], out_ptr);
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_setGlobalStateBytes32(
    sp: GoStack,
) {
    let idx = sp.read_u64(0) as u32;
    let src_ptr = sp.read_u64(1);
    let src_len = sp.read_u64(2);
    if src_len != 32 {
        eprintln!(
            "Go attempting to set block hash from {} bytes long buffer",
            src_len,
        );
        return;
    }
    let mut our_buf = MemoryLeaf([0u8; 32]);
    our_buf.0.copy_from_slice(&read_slice(src_ptr, src_len));
    let our_ptr = our_buf.0.as_ptr();
    assert_eq!(our_ptr as usize % 32, 0);
    wavm_set_globalstate_bytes32(idx, our_ptr);
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_getGlobalStateU64(sp: GoStack) {
    let idx = sp.read_u64(0) as u32;
    sp.write_u64(1, wavm_get_globalstate_u64(idx));
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_setGlobalStateU64(sp: GoStack) {
    let idx = sp.read_u64(0) as u32;
    wavm_set_globalstate_u64(idx, sp.read_u64(1));
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_readInboxMessage(sp: GoStack) {
    let msg_num = sp.read_u64(0);
    let offset = sp.read_u64(1);
    let out_ptr = sp.read_u64(2);
    let out_len = sp.read_u64(3);
    if out_len != 32 {
        eprintln!(
            "Go attempting to read inbox message with out len {}",
            out_len,
        );
        sp.write_u64(5, 0);
        return;
    }
    let mut our_buf = MemoryLeaf([0u8; 32]);
    let our_ptr = our_buf.0.as_mut_ptr();
    assert_eq!(our_ptr as usize % 32, 0);
    let read = wavm_read_inbox_message(msg_num, our_ptr, offset as usize);
    assert!(read <= 32);
    write_slice(&our_buf.0[..read], out_ptr);
    sp.write_u64(5, read as u64);
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_readDelayedInboxMessage(
    sp: GoStack,
) {
    let seq_num = sp.read_u64(0);
    let offset = sp.read_u64(1);
    let out_ptr = sp.read_u64(2);
    let out_len = sp.read_u64(3);
    if out_len != 32 {
        eprintln!(
            "Go attempting to read inbox message with out len {}",
            out_len,
        );
        sp.write_u64(4, 0);
        return;
    }
    let mut our_buf = MemoryLeaf([0u8; 32]);
    let our_ptr = our_buf.0.as_mut_ptr();
    assert_eq!(our_ptr as usize % 32, 0);
    let read = wavm_read_delayed_inbox_message(seq_num, our_ptr, offset as usize);
    assert!(read <= 32);
    write_slice(&our_buf.0[..read], out_ptr);
    sp.write_u64(5, read as u64);
}

#[no_mangle]
pub unsafe extern "C" fn go__github_com_offchainlabs_nitro_wavmio_resolveTypedPreimage(sp: GoStack) {
    let preimage_type = sp.read_u8(0);
    let hash_ptr = sp.read_u64(1);
    let hash_len = sp.read_u64(2);
    let offset = sp.read_u64(4);
    let out_ptr = sp.read_u64(5);
    let out_len = sp.read_u64(6);
    if hash_len != 32 || out_len != 32 {
        eprintln!(
            "Go attempting to resolve preimage with hash len {} and out len {}",
            hash_len, out_len,
        );
        sp.write_u64(8, 0);
        return;
    }
    let Ok(preimage_type) = preimage_type.try_into() else {
        eprintln!(
            "Go trying to resolve preimage with unknown type {}",
            preimage_type
        );
        sp.write_u64(8, 0);
        return;
    };
    let mut our_buf = MemoryLeaf([0u8; 32]);
    our_buf.0.copy_from_slice(&read_slice(hash_ptr, hash_len));
    let our_ptr = our_buf.0.as_mut_ptr();
    assert_eq!(our_ptr as usize % 32, 0);
    let preimage_reader = match preimage_type {
        PreimageType::Keccak256 => wavm_read_keccak_256_preimage,
        PreimageType::Sha2_256 => wavm_read_sha2_256_preimage,
    };
    let read = preimage_reader(our_ptr, offset as usize);
    assert!(read <= 32);
    write_slice(&our_buf.0[..read], out_ptr);
    sp.write_u64(8, read as u64);
}

'''
'''--- arbitrator/wasm-libraries/soft-float/bindings32.c ---
#include "softfloat.h"

bool f32_isReal(float32_t f) {
	uint32_t exponentMask = (1u << 31) - (1u << 23);
	return (f.v & exponentMask) != exponentMask;
}

bool f32_isNaN(float32_t f) {
	if (f32_isReal(f)) return false;
	uint32_t fraction = f.v & ((1 << 23) - 1);
	return fraction != 0;
}

bool f32_isInfinity(float32_t f) {
	if (f32_isReal(f)) return false;
	uint32_t fraction = f.v & ((1 << 23) - 1);
	return fraction == 0;
}

const uint32_t F32_SIGN_BIT = 1u << 31;

bool f32_isNegative(float32_t f) {
	return f.v & F32_SIGN_BIT;
}

bool f32_isZero(float32_t f) {
	return (f.v & ~F32_SIGN_BIT) == 0;
}

float32_t f32_positiveZero() {
	float32_t f = {0};
	return f;
}

float32_t f32_negativeZero() {
	float32_t f = {F32_SIGN_BIT};
	return f;
}

uint32_t wavm__f32_abs(uint32_t v) {
	v &= ~F32_SIGN_BIT;
	return v;
}

uint32_t wavm__f32_neg(uint32_t v) {
	v ^= F32_SIGN_BIT;
	return v;
}

uint32_t wavm__f32_ceil(uint32_t v) {
	float32_t f = {v};
	f = f32_roundToInt(f, softfloat_round_max, true);
	return f.v;
}

uint32_t wavm__f32_floor(uint32_t v) {
	float32_t f = {v};
	f = f32_roundToInt(f, softfloat_round_min, true);
	return f.v;
}

uint32_t wavm__f32_trunc(uint32_t v) {
	float32_t f = {v};
	f = f32_roundToInt(f, softfloat_round_minMag, true);
	return f.v;
}

uint32_t wavm__f32_nearest(uint32_t v) {
	float32_t f = {v};
	f = f32_roundToInt(f, softfloat_round_near_even, true);
	return f.v;
}

uint32_t wavm__f32_sqrt(uint32_t v) {
	float32_t f = {v};
	f = f32_sqrt(f);
	return f.v;
}

uint32_t wavm__f32_add(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	float32_t f = f32_add(a, b);
	return f.v;
}

uint32_t wavm__f32_sub(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	float32_t f = f32_sub(a, b);
	return f.v;
}

uint32_t wavm__f32_mul(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	float32_t f = f32_mul(a, b);
	return f.v;
}

uint32_t wavm__f32_div(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	float32_t f = f32_div(a, b);
	return f.v;
}

uint32_t wavm__f32_min(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	if (f32_isNaN(a)) {
		return a.v;
	} else if (f32_isNaN(b)) {
		return b.v;
	} else if (f32_isInfinity(a) && f32_isNegative(a)) {
		return a.v;
	} else if (f32_isInfinity(b) && f32_isNegative(b)) {
		return b.v;
	} else if (f32_isInfinity(a) && !f32_isNegative(a)) {
		return b.v;
	} else if (f32_isInfinity(b) && !f32_isNegative(b)) {
		return a.v;
	} else if (f32_isZero(a) && f32_isZero(b) && (f32_isNegative(a) != f32_isNegative(b))) {
		return f32_negativeZero().v;
	} else {
		if (f32_lt(b, a)) {
			return b.v;
		} else {
			return a.v;
		}
	}
}

uint32_t wavm__f32_max(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	if (f32_isNaN(a)) {
		return a.v;
	} else if (f32_isNaN(b)) {
		return b.v;
	} else if (f32_isInfinity(a) && !f32_isNegative(a)) {
		return a.v;
	} else if (f32_isInfinity(b) && !f32_isNegative(b)) {
		return b.v;
	} else if (f32_isInfinity(a) && f32_isNegative(a)) {
		return b.v;
	} else if (f32_isInfinity(b) && f32_isNegative(b)) {
		return a.v;
	} else if (f32_isZero(a) && f32_isZero(b) && (f32_isNegative(a) != f32_isNegative(b))) {
		return f32_positiveZero().v;
	} else {
		if (f32_lt(a, b)) {
			return b.v;
		} else {
			return a.v;
		}
	}
}

uint32_t wavm__f32_copysign(uint32_t va, uint32_t vb) {
	va &= ~F32_SIGN_BIT;
	va |= (vb & F32_SIGN_BIT);
	return va;
}

uint8_t wavm__f32_eq(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	return f32_eq(a, b);
}

uint8_t wavm__f32_ne(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	return !f32_eq(a, b);
}

uint8_t wavm__f32_lt(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	return f32_lt(a, b);
}

uint8_t wavm__f32_le(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	return f32_le(a, b);
}

uint8_t wavm__f32_gt(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	if (f32_isNaN(a) || f32_isNaN(b)) return false;
	return !f32_le(a, b);
}

uint8_t wavm__f32_ge(uint32_t va, uint32_t vb) {
	float32_t a = {va};
	float32_t b = {vb};
	if (f32_isNaN(a) || f32_isNaN(b)) return false;
	return !f32_lt(a, b);
}

int32_t wavm__i32_trunc_f32_s(uint32_t v) {
	// signed truncation is defined over (i32::min - 1, i32::max + 1)
	float32_t max = {0x4f000000};  // i32::max + 1 = 0x4F000000
	float32_t min = {0xcf000001};  // i32::min - 1 = 0xCF000000 (adjusted due to rounding)
	float32_t val = {v};
	if (f32_isNaN(val) || f32_le(max, val) || f32_le(val, min)) {
		__builtin_trap();
	}
	return f32_to_i32(val, softfloat_round_minMag, true);
}

int32_t wavm__i32_trunc_sat_f32_s(uint32_t v) {
	// signed truncation is defined over (i32::min - 1, i32::max + 1)
	float32_t max = {0x4f000000};  // i32::max + 1 = 0x4F000000
	float32_t min = {0xcf000001};  // i32::min - 1 = 0xCF000000 (adjusted due to rounding)
	float32_t val = {v};
	if (f32_isNaN(val)) {
		return 0;
	}
	if (f32_le(max, val)) {
		return 2147483647;
	}
	if (f32_le(val, min)) {
		return -2147483648;
	}
	return f32_to_i32(val, softfloat_round_minMag, true);
}

uint32_t wavm__i32_trunc_f32_u(uint32_t v) {
	// unsigned truncation is defined over (-1, u32::max + 1)
	float32_t max = {0x4f800000};  // u32::max + 1 = 0x4f800000
	float32_t min = {0xbf800000};  // -1           = 0xbf800000
	float32_t val = {v};
	if (f32_isNaN(val) || f32_le(max, val) || f32_le(val, min)) {
		__builtin_trap();
	}
	if (f32_isNegative(val)) {
		return 0;
	}
	return f32_to_ui32(val, softfloat_round_minMag, true);
}

uint32_t wavm__i32_trunc_sat_f32_u(uint32_t v) {
	// unsigned truncation is defined over (-1, u32::max + 1)
	float32_t max = {0x4f800000};  // u32::max + 1 = 0x4f800000
	float32_t val = {v};
	if (f32_isNaN(val) || f32_isNegative(val)) {
		return 0;
	}
	if (f32_le(max, val)) {
		return ~0u;
	}
	return f32_to_ui32(val, softfloat_round_minMag, true);
}

int64_t wavm__i64_trunc_f32_s(uint32_t v) {
	// unsigned truncation is defined over (i64::min - 1, i64::max + 1)
	float32_t max = {0x5f000000};  // i64::max + 1 = 0x5f000000
	float32_t min = {0xdf000001};  // i64::min - 1 = 0xdf000000 (adjusted due to rounding)
	float32_t val = {v};
	if (f32_isNaN(val) || f32_le(max, val) || f32_le(val, min)) {
		__builtin_trap();
	}
	return f32_to_i64(val, softfloat_round_minMag, true);
}

int64_t wavm__i64_trunc_sat_f32_s(uint32_t v) {
	// unsigned truncation is defined over (i64::min - 1, i64::max + 1)
	float32_t max = {0x5f000000};  // i64::max + 1 = 0x5f000000
	float32_t min = {0xdf000001};  // i64::min - 1 = 0xdf000000 (adjusted due to rounding)
	float32_t val = {v};
	if (f32_isNaN(val)) {
		return 0;
	}
	if (f32_le(max, val)) {
		return 9223372036854775807ll;
	}
	if (f32_le(val, min)) {
		return -(((int64_t) 1) << 63);
	}
	return f32_to_i64(val, softfloat_round_minMag, true);
}

uint64_t wavm__i64_trunc_f32_u(uint32_t v) {
	// unsigned truncation is defined over (-1, i64::max + 1)
	float32_t max = {0x5f800000};  // i64::max + 1 = 0x5f800000
	float32_t min = {0xbf800000};  // -1           = 0xbf800000
	float32_t val = {v};
	if (f32_isNaN(val) || f32_le(max, val) || f32_le(val, min)) {
		__builtin_trap();
	}
	if (f32_isNegative(val)) {
		return 0;
	}
	return f32_to_ui64(val, softfloat_round_minMag, true);
}

uint64_t wavm__i64_trunc_sat_f32_u(uint32_t v) {
	// unsigned truncation is defined over (-1, i64::max + 1)
	float32_t max = {0x5f800000};  // i64::max + 1 = 0x5f800000
	float32_t val = {v};
	if (f32_isNaN(val) || f32_isNegative(val)) {
		return 0;
	}
	if (f32_le(max, val)) {
		return ~0ull;
	}
	return f32_to_ui64(val, softfloat_round_minMag, true);
}

uint32_t wavm__f32_convert_i32_s(int32_t x) {
	return i32_to_f32(x).v;
}

uint32_t wavm__f32_convert_i32_u(uint32_t x) {
	return ui32_to_f32(x).v;
}

uint32_t wavm__f32_convert_i64_s(int64_t x) {
	return i64_to_f32(x).v;
}

uint32_t wavm__f32_convert_i64_u(uint64_t x) {
	return ui64_to_f32(x).v;
}

'''
'''--- arbitrator/wasm-libraries/soft-float/bindings64.c ---
#include "softfloat.h"

bool f64_isReal(float64_t f) {
	uint64_t exponentMask = (1ull << 63) - (1ull << 52);
	return (f.v & exponentMask) != exponentMask;
}

bool f64_isNaN(float64_t f) {
	if (f64_isReal(f)) return false;
	uint64_t fraction = f.v & ((1ull << 52) - 1);
	return fraction != 0;
}

bool f64_isInfinity(float64_t f) {
	if (f64_isReal(f)) return false;
	uint64_t fraction = f.v & ((1ull << 52) - 1);
	return fraction == 0;
}

const uint64_t F64_SIGN_BIT = 1ull << 63;

bool f64_isNegative(float64_t f) {
	return f.v & F64_SIGN_BIT;
}

bool f64_isZero(float64_t f) {
	return (f.v & ~F64_SIGN_BIT) == 0;
}

float64_t f64_positiveZero() {
	float64_t f = {0};
	return f;
}

float64_t f64_negativeZero() {
	float64_t f = {F64_SIGN_BIT};
	return f;
}

uint64_t wavm__f64_abs(uint64_t v) {
	v &= ~F64_SIGN_BIT;
	return v;
}

uint64_t wavm__f64_neg(uint64_t v) {
	v ^= F64_SIGN_BIT;
	return v;
}

uint64_t wavm__f64_ceil(uint64_t v) {
	float64_t f = {v};
	f = f64_roundToInt(f, softfloat_round_max, true);
	return f.v;
}

uint64_t wavm__f64_floor(uint64_t v) {
	float64_t f = {v};
	f = f64_roundToInt(f, softfloat_round_min, true);
	return f.v;
}

uint64_t wavm__f64_trunc(uint64_t v) {
	float64_t f = {v};
	f = f64_roundToInt(f, softfloat_round_minMag, true);
	return f.v;
}

uint64_t wavm__f64_nearest(uint64_t v) {
	float64_t f = {v};
	f = f64_roundToInt(f, softfloat_round_near_even, true);
	return f.v;
}

uint64_t wavm__f64_sqrt(uint64_t v) {
	float64_t f = {v};
	f = f64_sqrt(f);
	return f.v;
}

uint64_t wavm__f64_add(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	float64_t f = f64_add(a, b);
	return f.v;
}

uint64_t wavm__f64_sub(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	float64_t f = f64_sub(a, b);
	return f.v;
}

uint64_t wavm__f64_mul(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	float64_t f = f64_mul(a, b);
	return f.v;
}

uint64_t wavm__f64_div(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	float64_t f = f64_div(a, b);
	return f.v;
}

uint64_t wavm__f64_min(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	if (f64_isNaN(a)) {
		return a.v;
	} else if (f64_isNaN(b)) {
		return b.v;
	} else if (f64_isInfinity(a) && f64_isNegative(a)) {
		return a.v;
	} else if (f64_isInfinity(b) && f64_isNegative(b)) {
		return b.v;
	} else if (f64_isInfinity(a) && !f64_isNegative(a)) {
		return b.v;
	} else if (f64_isInfinity(b) && !f64_isNegative(b)) {
		return a.v;
	} else if (f64_isZero(a) && f64_isZero(b) && (f64_isNegative(a) != f64_isNegative(b))) {
		return f64_negativeZero().v;
	} else {
		if (f64_lt(b, a)) {
			return b.v;
		} else {
			return a.v;
		}
	}
}

uint64_t wavm__f64_max(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	if (f64_isNaN(a)) {
		return a.v;
	} else if (f64_isNaN(b)) {
		return b.v;
	} else if (f64_isInfinity(b) && !f64_isNegative(b)) {
		return b.v;
	} else if (f64_isInfinity(a) && f64_isNegative(a)) {
		return b.v;
	} else if (f64_isInfinity(b) && f64_isNegative(b)) {
		return a.v;
	} else if (f64_isZero(a) && f64_isZero(b) && (f64_isNegative(a) != f64_isNegative(b))) {
		return f64_positiveZero().v;
	} else {
		if (f64_lt(a, b)) {
			return b.v;
		} else {
			return a.v;
		}
	}
}

uint64_t wavm__f64_copysign(uint64_t va, uint64_t vb) {
	va &= ~F64_SIGN_BIT;
	va |= (vb & F64_SIGN_BIT);
	return va;
}

uint8_t wavm__f64_eq(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	return f64_eq(a, b);
}

uint8_t wavm__f64_ne(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	return !f64_eq(a, b);
}

uint8_t wavm__f64_lt(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	return f64_lt(a, b);
}

uint8_t wavm__f64_le(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	return f64_le(a, b);
}

uint8_t wavm__f64_gt(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	if (f64_isNaN(a) || f64_isNaN(b)) return false;
	return !f64_le(a, b);
}

uint8_t wavm__f64_ge(uint64_t va, uint64_t vb) {
	float64_t a = {va};
	float64_t b = {vb};
	if (f64_isNaN(a) || f64_isNaN(b)) return false;
	return !f64_lt(a, b);
}

int32_t wavm__i32_trunc_f64_s(uint64_t v) {
	// signed truncation is defined over (i32::min - 1, i32::max + 1)
	float64_t max = {0x41e0000000000000};  // i32::max + 1 = 0x41e0000000000000
	float64_t min = {0xc1e0000000200000};  // i32::min - 1 = 0xc1e0000000200000
	float64_t val = {v};
	if (f64_isNaN(val) || f64_le(max, val) || f64_le(val, min)) {
		__builtin_trap();
	}
	return f64_to_i32(val, softfloat_round_minMag, true);
}

int32_t wavm__i32_trunc_sat_f64_s(uint64_t v) {
	// signed truncation is defined over (i32::min - 1, i32::max + 1)
	float64_t max = {0x41e0000000000000};  // i32::max + 1 = 0x41e0000000000000
	float64_t min = {0xc1e0000000200000};  // i32::min - 1 = 0xc1e0000000200000
	float64_t val = {v};
	if (f64_isNaN(val)) {
		return 0;
	}
	if (f64_le(max, val)) {
		return 2147483647;
	}
	if (f64_le(val, min)) {
		return -2147483648;
	}
	return f64_to_i32(val, softfloat_round_minMag, true);
}

uint32_t wavm__i32_trunc_f64_u(uint64_t v) {
	// unsigned truncation is defined over (-1, u32::max + 1)
	float64_t max = {0x41f0000000000000};  // u32::max + 1 = 0x41f0000000000000
	float64_t min = {0xbff0000000000000};  // -1           = 0xbff0000000000000
	float64_t val = {v};
	if (f64_isNaN(val) || f64_le(max, val) || f64_le(val, min)) {
		__builtin_trap();
	}
	if (f64_isNegative(val)) {
		return 0;
	}
	return f64_to_ui32(val, softfloat_round_minMag, true);
}

uint32_t wavm__i32_trunc_sat_f64_u(uint64_t v) {
	// unsigned truncation is defined over (-1, u32::max + 1)
	float64_t max = {0x41f0000000000000};  // u32::max + 1 = 0x41f0000000000000
	float64_t val = {v};
	if (f64_isNaN(val) || f64_isNegative(val)) {
		return 0;
	}
	if (f64_le(max, val)) {
		return ~0u;
	}
	return f64_to_ui32(val, softfloat_round_minMag, true);
}

int64_t wavm__i64_trunc_f64_s(uint64_t v) {
	// signed truncation is defined over (i64::min - 1, u64::max + 1)
	float64_t max = {0x43e0000000000000};  // i64::max + 1 = 0x43e0000000000000
	float64_t min = {0xc3e0000000000001};  // i64::min - 1 = 0xc3e0000000000000 (adjusted due to rounding)
	float64_t val = {v};
	if (f64_isNaN(val) || f64_le(max, val) || f64_le(val, min)) {
		__builtin_trap();
	}
	return f64_to_i64(val, softfloat_round_minMag, true);
}

int64_t wavm__i64_trunc_sat_f64_s(uint64_t v) {
	// signed truncation is defined over (i64::min - 1, u64::max + 1)
	float64_t max = {0x43e0000000000000};  // i64::max + 1 = 0x43e0000000000000
	float64_t min = {0xc3e0000000000001};  // i64::min - 1 = 0xc3e0000000000000 (adjusted due to rounding)
	float64_t val = {v};
	if (f64_isNaN(val)) {
		return 0;
	}
	if (f64_le(max, val)) {
		return 9223372036854775807ll;
	}
	if (f64_le(val, min)) {
		return -(((int64_t) 1) << 63);
	}
	return f64_to_i64(val, softfloat_round_minMag, true);
}

uint64_t wavm__i64_trunc_f64_u(uint64_t v) {
	// unsigned truncation is defined over (-1, u64::max + 1)
	float64_t max = {0x43f0000000000000};  // u64::max + 1 = 0x43f0000000000000
	float64_t min = {0xbff0000000000000};  // -1           = 0xbff0000000000000
	float64_t f = {v};
	if (f64_isNaN(f) || f64_le(max, f) || f64_le(f, min)) {
		__builtin_trap();
	}
	if (f64_isNegative(f)) {
		return 0;
	}
	return f64_to_ui64(f, softfloat_round_minMag, true);
}

uint64_t wavm__i64_trunc_sat_f64_u(uint64_t v) {
	// unsigned truncation is defined over (-1, u64::max + 1)
	float64_t max = {0x43f0000000000000};  // u64::max + 1 = 0x43f0000000000000
	float64_t val = {v};
	if (f64_isNaN(val) || f64_isNegative(val)) {
		return 0;
	}
	if (f64_le(max, val)) {
		return 18446744073709551615ull;
	}
	return f64_to_ui64(val, softfloat_round_minMag, true);
}

uint64_t wavm__f64_convert_i32_s(int32_t x) {
	return i32_to_f64(x).v;
}

uint64_t wavm__f64_convert_i32_u(uint32_t x) {
	return ui32_to_f64(x).v;
}

uint64_t wavm__f64_convert_i64_s(int64_t x) {
	return i64_to_f64(x).v;
}

uint64_t wavm__f64_convert_i64_u(uint64_t x) {
	return ui64_to_f64(x).v;
}

uint32_t wavm__f32_demote_f64(uint64_t x) {
	float64_t f = {x};
	return f64_to_f32(f).v;
}

uint64_t wavm__f64_promote_f32(uint32_t x) {
	float32_t f = {x};
	return f32_to_f64(f).v;
}

'''
'''--- arbitrator/wasm-libraries/wasi-stub/Cargo.toml ---
[package]
name = "wasi-stub"
version = "0.1.0"
edition = "2018"
publish = false

[lib]
crate-type = ["cdylib"]

[dependencies]

'''
'''--- arbitrator/wasm-libraries/wasi-stub/src/lib.rs ---
#![no_std]

const ERRNO_BADF: u16 = 8;
const ERRNO_INTVAL: u16 = 28;

#[allow(dead_code)]
extern "C" {
    fn wavm_caller_load8(ptr: usize) -> u8;
    fn wavm_caller_load32(ptr: usize) -> u32;
    fn wavm_caller_store8(ptr: usize, val: u8);
    fn wavm_caller_store32(ptr: usize, val: u32);
    fn wavm_halt_and_set_finished() -> !;
}

#[panic_handler]
unsafe fn panic(_: &core::panic::PanicInfo) -> ! {
    core::arch::wasm32::unreachable()
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__proc_exit(code: u32) -> ! {
    if code == 0 {
        wavm_halt_and_set_finished()
    } else {
        core::arch::wasm32::unreachable()
    }
}

#[no_mangle]
pub unsafe extern "C" fn env__exit(code: u32) {
    if code == 0 {
        wavm_halt_and_set_finished()
    } else {
        core::arch::wasm32::unreachable()
    }
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__environ_sizes_get(
    length_ptr: usize,
    data_size_ptr: usize,
) -> u16 {
    wavm_caller_store32(length_ptr, 0);
    wavm_caller_store32(data_size_ptr, 0);
    0
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__fd_write(
    fd: usize,
    iovecs_ptr: usize,
    iovecs_len: usize,
    ret_ptr: usize,
) -> u16 {
    if fd != 1 && fd != 2 {
        return ERRNO_BADF;
    }
    let mut size = 0;
    for i in 0..iovecs_len {
        let ptr = iovecs_ptr + i * 8;
        size += wavm_caller_load32(ptr + 4);
    }
    wavm_caller_store32(ret_ptr, size);
    0
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__environ_get(_: usize, _: usize) -> u16 {
    ERRNO_INTVAL
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__fd_close(_: usize) -> u16 {
    ERRNO_BADF
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__fd_read(
    _: usize,
    _: usize,
    _: usize,
    _: usize,
) -> u16 {
    ERRNO_BADF
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__path_open(
    _: usize,
    _: usize,
    _: usize,
    _: usize,
    _: usize,
    _: u64,
    _: u64,
    _: usize,
    _: usize,
) -> u16 {
    ERRNO_BADF
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__fd_prestat_get(_: usize, _: usize) -> u16 {
    ERRNO_BADF
}

#[no_mangle]
pub unsafe extern "C" fn wasi_snapshot_preview1__fd_prestat_dir_name(
    _: usize,
    _: usize,
    _: usize,
) -> u16 {
    ERRNO_BADF
}

'''
'''--- arbitrator/wasm-testsuite/Cargo.toml ---
[package]
name = "wasm-testsuite"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
prover = { path = "../prover" }
structopt = "0.3.23"
serde = { version = "1.0.130", features = ["derive", "rc"] }
serde_json = "1.0.67"
eyre = "0.6.5"
hex = "0.4.3"

[workspace]
members = []

'''
'''--- arbitrator/wasm-testsuite/check.sh ---
#!/usr/bin/bash

# Copyright 2022, Offchain Labs, Inc.
# For license information, see https://github.com/nitro/blob/master/LICENSE

rm -rf tests ../../contracts/test/prover/spec-proofs
mkdir -p tests/
mkdir -p ../../contracts/test/prover/spec-proofs/

for file in testsuite/*wast; do
    wast="${file##testsuite/}"
    json="tests/${wast%.wast}.json"
    wast2json $file -o $json 2>/dev/null
done

cargo build --release

for file in tests/*.json; do
    base="${file#tests/}"
    name="${base%.wasm}"
    target/release/wasm-testsuite $name &
done

wait

'''
'''--- arbitrator/wasm-testsuite/src/main.rs ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

use eyre::bail;
use prover::{
    console::Color,
    machine,
    machine::{GlobalState, Machine, MachineStatus, ProofInfo},
    value::Value,
};
use serde::{Deserialize, Serialize};
use std::{
    collections::{HashMap, HashSet},
    fs::File,
    io::BufReader,
    path::PathBuf,
    time::Instant,
};
use structopt::StructOpt;

#[derive(StructOpt)]
#[structopt(name = "wasm-testsuite")]
struct Opts {
    json: PathBuf,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
struct Case {
    source_filename: String,
    commands: Vec<Command>,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
enum Command {
    Module {
        filename: String,
    },
    AssertReturn {
        action: Action,
        expected: Vec<TextValue>,
    },
    AssertExhaustion {
        action: Action,
    },
    AssertTrap {
        action: Action,
    },
    Action {
        action: Action,
    },
    AssertMalformed {
        filename: String,
    },
    AssertInvalid {},
    AssertUninstantiable {},
}

#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
enum Action {
    Invoke { field: String, args: Vec<TextValue> },
    Get { field: String },
}

#[derive(Clone, Debug, Serialize, Deserialize)]
struct TextValue {
    #[serde(rename = "type")]
    ty: TextValueType,
    value: String,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
enum TextValueType {
    I32,
    I64,
    F32,
    F64,
}

impl Into<Value> for TextValue {
    fn into(self) -> Value {
        match self.ty {
            TextValueType::I32 => {
                let value = self.value.parse().expect("not an i32");
                Value::I32(value)
            }
            TextValueType::I64 => {
                let value = self.value.parse().expect("not an i64");
                Value::I64(value)
            }
            TextValueType::F32 => {
                if self.value.contains("nan") {
                    return Value::F32(f32::NAN);
                }
                let message = format!("{} not the bit representation of an f32", self.value);
                let bits: u32 = self.value.parse().expect(&message);
                Value::F32(f32::from_bits(bits))
            }
            TextValueType::F64 => {
                if self.value.contains("nan") {
                    return Value::F64(f64::NAN);
                }
                let message = format!("{} not the bit representation of an f64", self.value);
                let bits: u64 = self.value.parse().expect(&message);
                Value::F64(f64::from_bits(bits))
            }
        }
    }
}

impl PartialEq<Value> for TextValue {
    fn eq(&self, other: &Value) -> bool {
        if &Into::<Value>::into(self.clone()) == other {
            return true;
        }

        match self.ty {
            TextValueType::F32 => match other {
                Value::F32(value) => value.is_nan() && self.value.contains("nan"),
                _ => false,
            },
            TextValueType::F64 => match other {
                Value::F64(value) => value.is_nan() && self.value.contains("nan"),
                _ => false,
            },
            _ => false,
        }
    }
}

fn pretty_print_values(prefix: &str, values: Vec<Value>) {
    let mut result = format!("  {}  ", prefix);
    for value in values {
        result += &format!("{}, ", value.pretty_print());
    }
    if result.len() > 2 {
        result.pop();
        result.pop();
    }
    println!("{}", result)
}

fn main() -> eyre::Result<()> {
    let opts = Opts::from_args();
    println!("test {:?}", opts.json);

    let mut path = PathBuf::from("tests/");
    path.push(&opts.json);

    let reader = BufReader::new(File::open(path)?);
    let case: Case = serde_json::from_reader(reader)?;
    let start_time = Instant::now();

    let soft_float = PathBuf::from("../../target/machines/latest/soft-float.wasm");

    // The modules listed below will be tested for compliance with the spec, but won't produce proofs for the OSP test.
    // We list the soft-float modules because, while compliance is necessary, the funcs are comprised of opcodes
    // better tested elsewhere and aren't worth 10x the test time.
    let mut do_not_prove = HashSet::new();
    do_not_prove.insert(PathBuf::from("f32.json"));
    do_not_prove.insert(PathBuf::from("f64.json"));
    do_not_prove.insert(PathBuf::from("f32_cmp.json"));
    do_not_prove.insert(PathBuf::from("f64_cmp.json"));
    do_not_prove.insert(PathBuf::from("float_exprs.json"));
    let export_proofs = !do_not_prove.contains(&opts.json);
    if !export_proofs {
        println!("{}", Color::grey("skipping OSP proof generation"));
    }

    let mut wasmfile = String::new();
    let mut machine = None;
    let mut subtest = 0;
    let mut skip = false;

    macro_rules! run {
        ($machine:expr, $bound:expr, $path:expr, $prove:expr) => {{
            let mut proofs = vec![];
            let mut count = 0;
            let mut leap = 1;
            let prove = $prove && export_proofs;

            if !prove {
                $machine.step_n($bound)?;
            }

            while count + leap < $bound && prove {
                count += 1;

                let prior = $machine.hash().to_string();
                let proof = hex::encode($machine.serialize_proof());
                $machine.step_n(1)?;
                let after = $machine.hash().to_string();
                proofs.push(ProofInfo::new(prior, proof, after));
                $machine.step_n(leap - 1)?;

                if count % 100 == 0 {
                    leap *= leap + 1;
                    if leap > 6 {
                        let message = format!("backing off {} {} {}", leap, count, $bound);
                        println!("{}", Color::grey(message));
                        $machine.stop_merkle_caching();
                    }
                }
                if $machine.is_halted() {
                    break;
                }
            }
            if prove {
                let out = File::create($path)?;
                serde_json::to_writer_pretty(out, &proofs)?;
            }
        }};
    }
    macro_rules! action {
        ($action:expr) => {
            match $action {
                Action::Invoke { field, args } => (field, args),
                Action::Get { .. } => {
                    // get() is only used in the export test, which we don't support
                    println!("skipping unsupported action {}", Color::red("get"));
                    continue;
                }
            }
        };
    }
    macro_rules! outname {
        () => {
            format!(
                "../../contracts/test/prover/spec-proofs/{}-{:04}.json",
                wasmfile, subtest
            )
        };
    }

    for (index, command) in case.commands.into_iter().enumerate() {
        macro_rules! test_success {
            ($func:expr, $args:expr, $expected:expr) => {
                let args: Vec<_> = $args.into_iter().map(Into::into).collect();
                if skip {
                    println!("skipping {}", Color::red($func));
                    subtest += 1;
                    continue;
                }

                let machine = machine.as_mut().expect("no machine");
                machine.jump_into_function(&$func, args.clone());
                machine.start_merkle_caching();
                run!(machine, 10_000_000, outname!(), true);

                let output = match machine.get_final_result() {
                    Ok(output) => output,
                    Err(error) => {
                        let expected: Vec<Value> = $expected.into_iter().map(Into::into).collect();
                        println!(
                            "Divergence in func {} of test {}",
                            Color::red($func),
                            Color::red(index),
                        );
                        pretty_print_values("Args    ", args);
                        pretty_print_values("Expected", expected);
                        println!();
                        bail!("{}", error)
                    }
                };

                if $expected != output {
                    let expected: Vec<Value> = $expected.into_iter().map(Into::into).collect();
                    println!(
                        "Divergence in func {} of test {}",
                        Color::red($func),
                        Color::red(index),
                    );
                    pretty_print_values("Args    ", args);
                    pretty_print_values("Expected", expected);
                    pretty_print_values("Observed", output);
                    println!();
                    bail!(
                        "Failure in test {}",
                        Color::red(format!("{} #{}", wasmfile, subtest))
                    )
                }
                subtest += 1;
            };
        }

        match command {
            Command::Module { filename } => {
                wasmfile = filename;
                machine = None;
                subtest = 1;

                let mech = Machine::from_paths(
                    &[soft_float.clone()],
                    &PathBuf::from("tests").join(&wasmfile),
                    false,
                    false,
                    false,
                    GlobalState::default(),
                    HashMap::default(),
                    machine::get_empty_preimage_resolver(),
                );

                if let Err(error) = &mech {
                    let error = error.root_cause().to_string();
                    skip = true;

                    if error.contains("Module has no code") {
                        // We don't support metadata-only modules that have no code
                        continue;
                    }
                    if error.contains("Unsupported import") {
                        // We don't support the import test's functions
                        continue;
                    }
                    if error.contains("multiple tables") {
                        // We don't support the reference-type extension
                        continue;
                    }
                    if error.contains("bulk memory") {
                        // We don't support the bulk-memory extension
                        continue;
                    }
                    bail!("Unexpected error parsing module {}: {}", wasmfile, error)
                }

                machine = mech.ok();
                skip = false;

                if let Some(machine) = &mut machine {
                    machine.step_n(1000)?; // run init
                    machine.start_merkle_caching();
                }
            }
            Command::AssertReturn { action, expected } => {
                let (func, args) = action!(action);
                test_success!(func, args, expected);
            }
            Command::Action { action } => {
                let (func, args) = action!(action);
                let expected: Vec<TextValue> = vec![];
                test_success!(func, args, expected);
            }
            Command::AssertTrap { action } => {
                let (func, args) = action!(action);
                let args: Vec<_> = args.into_iter().map(Into::into).collect();
                let test = Color::red(format!("{} #{}", wasmfile, subtest));

                let machine = machine.as_mut().unwrap();
                machine.jump_into_function(&func, args.clone());
                run!(machine, 1000, outname!(), true);

                if machine.get_status() == MachineStatus::Running {
                    bail!("machine failed to trap in test {}", test)
                }
                if let Ok(output) = machine.get_final_result() {
                    println!(
                        "Divergence in func {} of test {}",
                        Color::red(func),
                        Color::red(index),
                    );
                    pretty_print_values("Args  ", args);
                    pretty_print_values("Output", output);
                    println!();
                    bail!("Unexpected success in test {}", test)
                }
                subtest += 1;
            }
            Command::AssertExhaustion { action } => {
                let (func, args) = action!(action);
                let args: Vec<_> = args.into_iter().map(Into::into).collect();
                let test = Color::red(format!("{} #{}", wasmfile, subtest));

                let machine = machine.as_mut().unwrap();
                machine.jump_into_function(&func, args.clone());
                run!(machine, 100_000, outname!(), false); // this is proportional to the amount of RAM

                if machine.get_status() != MachineStatus::Running {
                    bail!("machine should spin {}", test)
                }
                subtest += 1;
            }
            Command::AssertMalformed { filename } => {
                let wasmpath = PathBuf::from("tests").join(&filename);

                let _ = Machine::from_paths(
                    &[soft_float.clone()],
                    &wasmpath,
                    false,
                    false,
                    false,
                    GlobalState::default(),
                    HashMap::default(),
                    machine::get_empty_preimage_resolver(),
                )
                .expect_err(&format!("failed to reject invalid module {}", filename));
            }
            _ => {}
        }
    }

    println!(
        "{} {}",
        Color::grey("done in"),
        Color::pink(format!("{}ms", start_time.elapsed().as_millis()))
    );
    Ok(())
}

'''
'''--- arbnode/api.go ---
package arbnode

import (
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/validator"
)

type BlockValidatorAPI struct {
	val *staker.BlockValidator
}

func (a *BlockValidatorAPI) LatestValidated(ctx context.Context) (*staker.GlobalStateValidatedInfo, error) {
	return a.val.ReadLastValidatedInfo()
}

type BlockValidatorDebugAPI struct {
	val *staker.StatelessBlockValidator
}

type ValidateBlockResult struct {
	Valid       bool                    `json:"valid"`
	Latency     string                  `json:"latency"`
	GlobalState validator.GoGlobalState `json:"globalstate"`
}

func (a *BlockValidatorDebugAPI) ValidateMessageNumber(
	ctx context.Context, msgNum hexutil.Uint64, full bool, moduleRootOptional *common.Hash,
) (ValidateBlockResult, error) {
	result := ValidateBlockResult{}

	var moduleRoot common.Hash
	if moduleRootOptional != nil {
		moduleRoot = *moduleRootOptional
	} else {
		moduleRoots := a.val.GetModuleRootsToValidate()
		if len(moduleRoots) == 0 {
			return result, errors.New("no current WasmModuleRoot configured, must provide parameter")
		}
		moduleRoot = moduleRoots[0]
	}
	start_time := time.Now()
	valid, gs, err := a.val.ValidateResult(ctx, arbutil.MessageIndex(msgNum), full, moduleRoot)
	result.Latency = fmt.Sprintf("%vms", time.Since(start_time).Milliseconds())
	if gs != nil {
		result.GlobalState = *gs
	}
	result.Valid = valid
	return result, err
}

'''
'''--- arbnode/batch_poster.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"bytes"
	"context"
	"encoding/hex"
	"errors"
	"fmt"
	"math"
	"math/big"
	"strings"
	"sync/atomic"
	"time"

	"github.com/andybalholm/brotli"
	"github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbnode/redislock"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

var (
	batchPosterWalletBalance      = metrics.NewRegisteredGaugeFloat64("arb/batchposter/wallet/balanceether", nil)
	batchPosterGasRefunderBalance = metrics.NewRegisteredGaugeFloat64("arb/batchposter/gasrefunder/balanceether", nil)
	batchPosterSimpleRedisLockKey = "node.batch-poster.redis-lock.simple-lock-key"
)

type batchPosterPosition struct {
	MessageCount        arbutil.MessageIndex
	DelayedMessageCount uint64
	NextSeqNum          uint64
}

type BatchPoster struct {
	stopwaiter.StopWaiter
	l1Reader            *headerreader.HeaderReader
	inbox               *InboxTracker
	streamer            *TransactionStreamer
	config              BatchPosterConfigFetcher
	seqInbox            *bridgegen.SequencerInbox
	bridge              *bridgegen.Bridge
	syncMonitor         *SyncMonitor
	seqInboxABI         *abi.ABI
	seqInboxAddr        common.Address
	bridgeAddr          common.Address
	gasRefunderAddr     common.Address
	building            *buildingBatch
	daWriter            das.DataAvailabilityServiceWriter
	dataPoster          *dataposter.DataPoster
	redisLock           *redislock.Simple
	firstEphemeralError time.Time // first time a continuous error suspected to be ephemeral occurred
	// An estimate of the number of batches we want to post but haven't yet.
	// This doesn't include batches which we don't want to post yet due to the L1 bounds.
	backlog         uint64
	lastHitL1Bounds time.Time // The last time we wanted to post a message but hit the L1 bounds

	batchReverted        atomic.Bool // indicates whether data poster batch was reverted
	nextRevertCheckBlock int64       // the last parent block scanned for reverting batches

	accessList func(SequencerInboxAccs, AfterDelayedMessagesRead int) types.AccessList
}

type l1BlockBound int

// This enum starts at 1 to avoid the empty initialization of 0 being valid
const (
	// Default is Safe if the L1 reader has finality data enabled, otherwise Latest
	l1BlockBoundDefault l1BlockBound = iota + 1
	l1BlockBoundSafe
	l1BlockBoundFinalized
	l1BlockBoundLatest
	l1BlockBoundIgnore
)

type BatchPosterConfig struct {
	Enable                             bool `koanf:"enable"`
	DisableDasFallbackStoreDataOnChain bool `koanf:"disable-das-fallback-store-data-on-chain" reload:"hot"`
	// Max batch size.
	MaxSize int `koanf:"max-size" reload:"hot"`
	// Max batch post delay.
	MaxDelay time.Duration `koanf:"max-delay" reload:"hot"`
	// Wait for max BatchPost delay.
	WaitForMaxDelay bool `koanf:"wait-for-max-delay" reload:"hot"`
	// Batch post polling interval.
	PollInterval time.Duration `koanf:"poll-interval" reload:"hot"`
	// Batch posting error delay.
	ErrorDelay         time.Duration               `koanf:"error-delay" reload:"hot"`
	CompressionLevel   int                         `koanf:"compression-level" reload:"hot"`
	DASRetentionPeriod time.Duration               `koanf:"das-retention-period" reload:"hot"`
	GasRefunderAddress string                      `koanf:"gas-refunder-address" reload:"hot"`
	DataPoster         dataposter.DataPosterConfig `koanf:"data-poster" reload:"hot"`
	RedisUrl           string                      `koanf:"redis-url"`
	RedisLock          redislock.SimpleCfg         `koanf:"redis-lock" reload:"hot"`
	ExtraBatchGas      uint64                      `koanf:"extra-batch-gas" reload:"hot"`
	ParentChainWallet  genericconf.WalletConfig    `koanf:"parent-chain-wallet"`
	L1BlockBound       string                      `koanf:"l1-block-bound" reload:"hot"`
	L1BlockBoundBypass time.Duration               `koanf:"l1-block-bound-bypass" reload:"hot"`

	gasRefunder  common.Address
	l1BlockBound l1BlockBound
}

func (c *BatchPosterConfig) Validate() error {
	if len(c.GasRefunderAddress) > 0 && !common.IsHexAddress(c.GasRefunderAddress) {
		return fmt.Errorf("invalid gas refunder address \"%v\"", c.GasRefunderAddress)
	}
	c.gasRefunder = common.HexToAddress(c.GasRefunderAddress)
	if c.MaxSize <= 40 {
		return errors.New("MaxBatchSize too small")
	}
	if c.L1BlockBound == "" {
		c.l1BlockBound = l1BlockBoundDefault
	} else if c.L1BlockBound == "safe" {
		c.l1BlockBound = l1BlockBoundSafe
	} else if c.L1BlockBound == "finalized" {
		c.l1BlockBound = l1BlockBoundFinalized
	} else if c.L1BlockBound == "latest" {
		c.l1BlockBound = l1BlockBoundLatest
	} else if c.L1BlockBound == "ignore" {
		c.l1BlockBound = l1BlockBoundIgnore
	} else {
		return fmt.Errorf("invalid L1 block bound tag \"%v\" (see --help for options)", c.L1BlockBound)
	}
	return nil
}

type BatchPosterConfigFetcher func() *BatchPosterConfig

func BatchPosterConfigAddOptions(prefix string, f *pflag.FlagSet) {
	f.Bool(prefix+".enable", DefaultBatchPosterConfig.Enable, "enable posting batches to l1")
	f.Bool(prefix+".disable-das-fallback-store-data-on-chain", DefaultBatchPosterConfig.DisableDasFallbackStoreDataOnChain, "If unable to batch to DAS, disable fallback storing data on chain")
	f.Int(prefix+".max-size", DefaultBatchPosterConfig.MaxSize, "maximum batch size")
	f.Duration(prefix+".max-delay", DefaultBatchPosterConfig.MaxDelay, "maximum batch posting delay")
	f.Bool(prefix+".wait-for-max-delay", DefaultBatchPosterConfig.WaitForMaxDelay, "wait for the max batch delay, even if the batch is full")
	f.Duration(prefix+".poll-interval", DefaultBatchPosterConfig.PollInterval, "how long to wait after no batches are ready to be posted before checking again")
	f.Duration(prefix+".error-delay", DefaultBatchPosterConfig.ErrorDelay, "how long to delay after error posting batch")
	f.Int(prefix+".compression-level", DefaultBatchPosterConfig.CompressionLevel, "batch compression level")
	f.Duration(prefix+".das-retention-period", DefaultBatchPosterConfig.DASRetentionPeriod, "In AnyTrust mode, the period which DASes are requested to retain the stored batches.")
	f.String(prefix+".gas-refunder-address", DefaultBatchPosterConfig.GasRefunderAddress, "The gas refunder contract address (optional)")
	f.Uint64(prefix+".extra-batch-gas", DefaultBatchPosterConfig.ExtraBatchGas, "use this much more gas than estimation says is necessary to post batches")
	f.String(prefix+".redis-url", DefaultBatchPosterConfig.RedisUrl, "if non-empty, the Redis URL to store queued transactions in")
	f.String(prefix+".l1-block-bound", DefaultBatchPosterConfig.L1BlockBound, "only post messages to batches when they're within the max future block/timestamp as of this L1 block tag (\"safe\", \"finalized\", \"latest\", or \"ignore\" to ignore this check)")
	f.Duration(prefix+".l1-block-bound-bypass", DefaultBatchPosterConfig.L1BlockBoundBypass, "post batches even if not within the layer 1 future bounds if we're within this margin of the max delay")
	redislock.AddConfigOptions(prefix+".redis-lock", f)
	dataposter.DataPosterConfigAddOptions(prefix+".data-poster", f, dataposter.DefaultDataPosterConfig)
	genericconf.WalletConfigAddOptions(prefix+".parent-chain-wallet", f, DefaultBatchPosterConfig.ParentChainWallet.Pathname)
}

var DefaultBatchPosterConfig = BatchPosterConfig{
	Enable:                             false,
	DisableDasFallbackStoreDataOnChain: false,
	// This default is overridden for L3 chains in applyChainParameters in cmd/nitro/nitro.go
	MaxSize:            100000,
	PollInterval:       time.Second * 10,
	ErrorDelay:         time.Second * 10,
	MaxDelay:           time.Hour,
	WaitForMaxDelay:    false,
	CompressionLevel:   brotli.BestCompression,
	DASRetentionPeriod: time.Hour * 24 * 15,
	GasRefunderAddress: "",
	ExtraBatchGas:      50_000,
	DataPoster:         dataposter.DefaultDataPosterConfig,
	ParentChainWallet:  DefaultBatchPosterL1WalletConfig,
	L1BlockBound:       "",
	L1BlockBoundBypass: time.Hour,
	RedisLock:          redislock.DefaultCfg,
}

var DefaultBatchPosterL1WalletConfig = genericconf.WalletConfig{
	Pathname:      "batch-poster-wallet",
	Password:      genericconf.WalletConfigDefault.Password,
	PrivateKey:    genericconf.WalletConfigDefault.PrivateKey,
	Account:       genericconf.WalletConfigDefault.Account,
	OnlyCreateKey: genericconf.WalletConfigDefault.OnlyCreateKey,
}

var TestBatchPosterConfig = BatchPosterConfig{
	Enable:             true,
	MaxSize:            100000,
	PollInterval:       time.Millisecond * 10,
	ErrorDelay:         time.Millisecond * 10,
	MaxDelay:           0,
	WaitForMaxDelay:    false,
	CompressionLevel:   2,
	DASRetentionPeriod: time.Hour * 24 * 15,
	GasRefunderAddress: "",
	ExtraBatchGas:      10_000,
	DataPoster:         dataposter.TestDataPosterConfig,
	ParentChainWallet:  DefaultBatchPosterL1WalletConfig,
	L1BlockBound:       "",
	L1BlockBoundBypass: time.Hour,
}

type BatchPosterOpts struct {
	DataPosterDB ethdb.Database
	L1Reader     *headerreader.HeaderReader
	Inbox        *InboxTracker
	Streamer     *TransactionStreamer
	SyncMonitor  *SyncMonitor
	Config       BatchPosterConfigFetcher
	DeployInfo   *chaininfo.RollupAddresses
	TransactOpts *bind.TransactOpts
	DAWriter     das.DataAvailabilityServiceWriter
}

func NewBatchPoster(ctx context.Context, opts *BatchPosterOpts) (*BatchPoster, error) {
	seqInbox, err := bridgegen.NewSequencerInbox(opts.DeployInfo.SequencerInbox, opts.L1Reader.Client())
	if err != nil {
		return nil, err
	}
	bridge, err := bridgegen.NewBridge(opts.DeployInfo.Bridge, opts.L1Reader.Client())
	if err != nil {
		return nil, err
	}
	if err = opts.Config().Validate(); err != nil {
		return nil, err
	}
	seqInboxABI, err := bridgegen.SequencerInboxMetaData.GetAbi()
	if err != nil {
		return nil, err
	}
	redisClient, err := redisutil.RedisClientFromURL(opts.Config().RedisUrl)
	if err != nil {
		return nil, err
	}
	redisLockConfigFetcher := func() *redislock.SimpleCfg {
		simpleRedisLockConfig := opts.Config().RedisLock
		simpleRedisLockConfig.Key = batchPosterSimpleRedisLockKey
		return &simpleRedisLockConfig
	}
	redisLock, err := redislock.NewSimple(redisClient, redisLockConfigFetcher, func() bool { return opts.SyncMonitor.Synced() })
	if err != nil {
		return nil, err
	}
	b := &BatchPoster{
		l1Reader:        opts.L1Reader,
		inbox:           opts.Inbox,
		streamer:        opts.Streamer,
		syncMonitor:     opts.SyncMonitor,
		config:          opts.Config,
		bridge:          bridge,
		seqInbox:        seqInbox,
		seqInboxABI:     seqInboxABI,
		seqInboxAddr:    opts.DeployInfo.SequencerInbox,
		gasRefunderAddr: opts.Config().gasRefunder,
		bridgeAddr:      opts.DeployInfo.Bridge,
		daWriter:        opts.DAWriter,
		redisLock:       redisLock,
		accessList: func(SequencerInboxAccs, AfterDelayedMessagesRead int) types.AccessList {
			return AccessList(&AccessListOpts{
				SequencerInboxAddr:       opts.DeployInfo.SequencerInbox,
				DataPosterAddr:           opts.TransactOpts.From,
				BridgeAddr:               opts.DeployInfo.Bridge,
				GasRefunderAddr:          opts.Config().gasRefunder,
				SequencerInboxAccs:       SequencerInboxAccs,
				AfterDelayedMessagesRead: AfterDelayedMessagesRead,
			})
		},
	}
	dataPosterConfigFetcher := func() *dataposter.DataPosterConfig {
		return &(opts.Config().DataPoster)
	}
	b.dataPoster, err = dataposter.NewDataPoster(ctx,
		&dataposter.DataPosterOpts{
			Database:          opts.DataPosterDB,
			HeaderReader:      opts.L1Reader,
			Auth:              opts.TransactOpts,
			RedisClient:       redisClient,
			RedisLock:         redisLock,
			Config:            dataPosterConfigFetcher,
			MetadataRetriever: b.getBatchPosterPosition,
			RedisKey:          "data-poster.queue",
		})
	if err != nil {
		return nil, err
	}
	return b, nil
}

type AccessListOpts struct {
	SequencerInboxAddr       common.Address
	BridgeAddr               common.Address
	DataPosterAddr           common.Address
	GasRefunderAddr          common.Address
	SequencerInboxAccs       int
	AfterDelayedMessagesRead int
}

// AccessList returns access list (contracts, storage slots) for batchposter.
func AccessList(opts *AccessListOpts) types.AccessList {
	l := types.AccessList{
		types.AccessTuple{
			Address: opts.SequencerInboxAddr,
			StorageKeys: []common.Hash{
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000000"), // totalDelayedMessagesRead
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000001"), // bridge
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000004"), // maxTimeVariation.delayBlocks
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000005"), // maxTimeVariation.futureBlocks
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000006"), // maxTimeVariation.delaySeconds
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000007"), // maxTimeVariation.futureSeconds
				// ADMIN_SLOT from OpenZeppelin, keccak-256 hash of
				// "eip1967.proxy.admin" subtracted by 1.
				common.HexToHash("0xb53127684a568b3173ae13b9f8a6016e243e63b6e8ee1178d6a717850b5d6103"),
				// IMPLEMENTATION_SLOT from OpenZeppelin,  keccak-256 hash
				// of "eip1967.proxy.implementation" subtracted by 1.
				common.HexToHash("0x360894a13ba1a3210667c828492db98dca3e2076cc3735a920a3ca505d382bbc"),
				// isBatchPoster[batchPosterAddr]; for mainnnet it's: "0xa10aa54071443520884ed767b0684edf43acec528b7da83ab38ce60126562660".
				common.Hash(arbutil.PaddedKeccak256(opts.DataPosterAddr.Bytes(), []byte{3})),
			},
		},
		types.AccessTuple{
			Address: opts.BridgeAddr,
			StorageKeys: []common.Hash{
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000006"), // delayedInboxAccs.length
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000007"), // sequencerInboxAccs.length
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000009"), // sequencerInbox
				common.HexToHash("0x000000000000000000000000000000000000000000000000000000000000000a"), // sequencerReportedSubMessageCount
				// ADMIN_SLOT from OpenZeppelin, keccak-256 hash of
				// "eip1967.proxy.admin" subtracted by 1.
				common.HexToHash("0xb53127684a568b3173ae13b9f8a6016e243e63b6e8ee1178d6a717850b5d6103"),
				// IMPLEMENTATION_SLOT from OpenZeppelin,  keccak-256 hash
				// of "eip1967.proxy.implementation" subtracted by 1.
				common.HexToHash("0x360894a13ba1a3210667c828492db98dca3e2076cc3735a920a3ca505d382bbc"),
				// These below may change when transaction is actually executed:
				// - delayedInboxAccs[delayedInboxAccs.length - 1]
				// - delayedInboxAccs.push(...);
			},
		},
	}

	for _, v := range []struct{ slotIdx, val int }{
		{7, opts.SequencerInboxAccs - 1},       // - sequencerInboxAccs[sequencerInboxAccs.length - 1]; (keccak256(7, sequencerInboxAccs.length - 1))
		{7, opts.SequencerInboxAccs},           // - sequencerInboxAccs.push(...); (keccak256(7, sequencerInboxAccs.length))
		{6, opts.AfterDelayedMessagesRead - 1}, // - delayedInboxAccs[afterDelayedMessagesRead - 1]; (keccak256(6, afterDelayedMessagesRead - 1))
	} {
		sb := arbutil.SumBytes(arbutil.PaddedKeccak256([]byte{byte(v.slotIdx)}), big.NewInt(int64(v.val)).Bytes())
		l[1].StorageKeys = append(l[1].StorageKeys, common.Hash(sb))
	}

	if (opts.GasRefunderAddr != common.Address{}) {
		l = append(l, types.AccessTuple{
			Address: opts.GasRefunderAddr,
			StorageKeys: []common.Hash{
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000004"), // CommonParameters.{maxRefundeeBalance, extraGasMargin, calldataCost, maxGasTip}
				common.HexToHash("0x0000000000000000000000000000000000000000000000000000000000000005"), // CommonParameters.{maxGasCost, maxSingleGasUsage}
				// allowedContracts[msg.sender]; for mainnet it's: "0x7686888b19bb7b75e46bb1aa328b65150743f4899443d722f0adf8e252ccda41".
				common.Hash(arbutil.PaddedKeccak256(opts.SequencerInboxAddr.Bytes(), []byte{1})),
				// allowedRefundees[refundee]; for mainnet it's: "0xe85fd79f89ff278fc57d40aecb7947873df9f0beac531c8f71a98f630e1eab62".
				common.Hash(arbutil.PaddedKeccak256(opts.DataPosterAddr.Bytes(), []byte{2})),
			},
		})
	}
	return l
}

// checkRevert checks blocks with number in range [from, to] whether they
// contain reverted batch_poster transaction.
// It returns true if it finds batch posting needs to halt, which is true if a batch reverts
// unless the data poster is configured with noop storage which can tolerate reverts.
func (b *BatchPoster) checkReverts(ctx context.Context, to int64) (bool, error) {
	if b.nextRevertCheckBlock > to {
		return false, fmt.Errorf("wrong range, from: %d > to: %d", b.nextRevertCheckBlock, to)
	}
	for ; b.nextRevertCheckBlock <= to; b.nextRevertCheckBlock++ {
		number := big.NewInt(b.nextRevertCheckBlock)
		block, err := b.l1Reader.Client().BlockByNumber(ctx, number)
		if err != nil {
			return false, fmt.Errorf("getting block: %v by number: %w", number, err)
		}
		for idx, tx := range block.Transactions() {
			from, err := b.l1Reader.Client().TransactionSender(ctx, tx, block.Hash(), uint(idx))
			if err != nil {
				return false, fmt.Errorf("getting sender of transaction tx: %v, %w", tx.Hash(), err)
			}
			if from == b.dataPoster.Sender() {
				r, err := b.l1Reader.Client().TransactionReceipt(ctx, tx.Hash())
				if err != nil {
					return false, fmt.Errorf("getting a receipt for transaction: %v, %w", tx.Hash(), err)
				}
				if r.Status == types.ReceiptStatusFailed {
					shouldHalt := !b.config().DataPoster.UseNoOpStorage
					logLevel := log.Warn
					if shouldHalt {
						logLevel = log.Error
					}
					logLevel("Transaction from batch poster reverted", "nonce", tx.Nonce(), "txHash", tx.Hash(), "blockNumber", r.BlockNumber, "blockHash", r.BlockHash)
					return shouldHalt, nil
				}
			}
		}
	}
	return false, nil
}

// pollForReverts runs a gouroutine that listens to l1 block headers, checks
// if any transaction made by batch poster was reverted.
func (b *BatchPoster) pollForReverts(ctx context.Context) {
	headerCh, unsubscribe := b.l1Reader.Subscribe(false)
	defer unsubscribe()

	for {
		// Poll until:
		// - L1 headers reader channel is closed, or
		// - polling is through context, or
		// - we see a transaction in the block from dataposter that was reverted.
		select {
		case h, ok := <-headerCh:
			if !ok {
				log.Info("L1 headers channel checking for batch poster reverts has been closed")
				return
			}
			blockNum := h.Number.Int64()
			// If this is the first block header, set last seen as number-1.
			// We may see same block number again if there is L1 reorg, in that
			// case we check the block again.
			if b.nextRevertCheckBlock == 0 || b.nextRevertCheckBlock > blockNum {
				b.nextRevertCheckBlock = blockNum
			}
			if blockNum-b.nextRevertCheckBlock > 100 {
				log.Warn("Large gap between last seen and current block number, skipping check for reverts", "last", b.nextRevertCheckBlock, "current", blockNum)
				b.nextRevertCheckBlock = blockNum
				continue
			}

			reverted, err := b.checkReverts(ctx, blockNum)
			if err != nil {
				logLevel := log.Warn
				if strings.Contains(err.Error(), "not found") {
					// Just parent chain node inconsistency
					// One node sent us a block, but another didn't have it
					// We'll try to check this block again next loop
					logLevel = log.Debug
				}
				logLevel("Error checking batch reverts", "err", err)
				continue
			}
			if reverted {
				b.batchReverted.Store(true)
				return
			}
		case <-ctx.Done():
			return
		}
	}
}

func (b *BatchPoster) getBatchPosterPosition(ctx context.Context, blockNum *big.Int) ([]byte, error) {
	bigInboxBatchCount, err := b.seqInbox.BatchCount(&bind.CallOpts{Context: ctx, BlockNumber: blockNum})
	if err != nil {
		return nil, fmt.Errorf("error getting latest batch count: %w", err)
	}
	inboxBatchCount := bigInboxBatchCount.Uint64()
	var prevBatchMeta BatchMetadata
	if inboxBatchCount > 0 {
		var err error
		prevBatchMeta, err = b.inbox.GetBatchMetadata(inboxBatchCount - 1)
		if err != nil {
			return nil, fmt.Errorf("error getting latest batch metadata: %w", err)
		}
	}
	return rlp.EncodeToBytes(batchPosterPosition{
		MessageCount:        prevBatchMeta.MessageCount,
		DelayedMessageCount: prevBatchMeta.DelayedMessageCount,
		NextSeqNum:          inboxBatchCount,
	})
}

var errBatchAlreadyClosed = errors.New("batch segments already closed")

type batchSegments struct {
	compressedBuffer      *bytes.Buffer
	compressedWriter      *brotli.Writer
	rawSegments           [][]byte
	timestamp             uint64
	blockNum              uint64
	delayedMsg            uint64
	sizeLimit             int
	recompressionLevel    int
	newUncompressedSize   int
	totalUncompressedSize int
	lastCompressedSize    int
	trailingHeaders       int // how many trailing segments are headers
	isDone                bool
}

type buildingBatch struct {
	segments          *batchSegments
	startMsgCount     arbutil.MessageIndex
	msgCount          arbutil.MessageIndex
	haveUsefulMessage bool
}

func newBatchSegments(firstDelayed uint64, config *BatchPosterConfig, backlog uint64) *batchSegments {
	compressedBuffer := bytes.NewBuffer(make([]byte, 0, config.MaxSize*2))
	if config.MaxSize <= 40 {
		panic("MaxBatchSize too small")
	}
	compressionLevel := config.CompressionLevel
	recompressionLevel := config.CompressionLevel
	if backlog > 20 {
		compressionLevel = arbmath.MinInt(compressionLevel, brotli.DefaultCompression)
	}
	if backlog > 40 {
		recompressionLevel = arbmath.MinInt(recompressionLevel, brotli.DefaultCompression)
	}
	if backlog > 60 {
		compressionLevel = arbmath.MinInt(compressionLevel, 4)
	}
	if recompressionLevel < compressionLevel {
		// This should never be possible
		log.Warn(
			"somehow the recompression level was lower than the compression level",
			"recompressionLevel", recompressionLevel,
			"compressionLevel", compressionLevel,
		)
		recompressionLevel = compressionLevel
	}
	return &batchSegments{
		compressedBuffer:   compressedBuffer,
		compressedWriter:   brotli.NewWriterLevel(compressedBuffer, compressionLevel),
		sizeLimit:          config.MaxSize - 40, // TODO
		recompressionLevel: recompressionLevel,
		rawSegments:        make([][]byte, 0, 128),
		delayedMsg:         firstDelayed,
	}
}

func (s *batchSegments) recompressAll() error {
	s.compressedBuffer = bytes.NewBuffer(make([]byte, 0, s.sizeLimit*2))
	s.compressedWriter = brotli.NewWriterLevel(s.compressedBuffer, s.recompressionLevel)
	s.newUncompressedSize = 0
	s.totalUncompressedSize = 0
	for _, segment := range s.rawSegments {
		err := s.addSegmentToCompressed(segment)
		if err != nil {
			return err
		}
	}
	if s.totalUncompressedSize > arbstate.MaxDecompressedLen {
		return fmt.Errorf("batch size %v exceeds maximum decompressed length %v", s.totalUncompressedSize, arbstate.MaxDecompressedLen)
	}
	if len(s.rawSegments) >= arbstate.MaxSegmentsPerSequencerMessage {
		return fmt.Errorf("number of raw segments %v excees maximum number %v", len(s.rawSegments), arbstate.MaxSegmentsPerSequencerMessage)
	}
	return nil
}

func (s *batchSegments) testForOverflow(isHeader bool) (bool, error) {
	// we've reached the max decompressed size
	if s.totalUncompressedSize > arbstate.MaxDecompressedLen {
		return true, nil
	}
	// we've reached the max number of segments
	if len(s.rawSegments) >= arbstate.MaxSegmentsPerSequencerMessage {
		return true, nil
	}
	// there is room, no need to flush
	if (s.lastCompressedSize + s.newUncompressedSize) < s.sizeLimit {
		return false, nil
	}
	// don't want to flush for headers or the first message
	if isHeader || len(s.rawSegments) == s.trailingHeaders {
		return false, nil
	}
	err := s.compressedWriter.Flush()
	if err != nil {
		return true, err
	}
	s.lastCompressedSize = s.compressedBuffer.Len()
	s.newUncompressedSize = 0
	if s.lastCompressedSize >= s.sizeLimit {
		return true, nil
	}
	return false, nil
}

func (s *batchSegments) close() error {
	s.rawSegments = s.rawSegments[:len(s.rawSegments)-s.trailingHeaders]
	s.trailingHeaders = 0
	err := s.recompressAll()
	if err != nil {
		return err
	}
	s.isDone = true
	return nil
}

func (s *batchSegments) addSegmentToCompressed(segment []byte) error {
	encoded, err := rlp.EncodeToBytes(segment)
	if err != nil {
		return err
	}
	lenWritten, err := s.compressedWriter.Write(encoded)
	s.newUncompressedSize += lenWritten
	s.totalUncompressedSize += lenWritten
	return err
}

// returns false if segment was too large, error in case of real error
func (s *batchSegments) addSegment(segment []byte, isHeader bool) (bool, error) {
	if s.isDone {
		return false, errBatchAlreadyClosed
	}
	err := s.addSegmentToCompressed(segment)
	if err != nil {
		return false, err
	}
	// Force include headers because we don't want to re-compress and we can just trim them later if necessary
	overflow, err := s.testForOverflow(isHeader)
	if err != nil {
		return false, err
	}
	if overflow {
		return false, s.close()
	}
	s.rawSegments = append(s.rawSegments, segment)
	if isHeader {
		s.trailingHeaders++
	} else {
		s.trailingHeaders = 0
	}
	return true, nil
}

func (s *batchSegments) addL2Msg(l2msg []byte) (bool, error) {
	segment := make([]byte, 1, len(l2msg)+1)
	segment[0] = arbstate.BatchSegmentKindL2Message
	segment = append(segment, l2msg...)
	return s.addSegment(segment, false)
}

func (s *batchSegments) prepareIntSegment(val uint64, segmentHeader byte) ([]byte, error) {
	segment := make([]byte, 1, 16)
	segment[0] = segmentHeader
	enc, err := rlp.EncodeToBytes(val)
	if err != nil {
		return nil, err
	}
	return append(segment, enc...), nil
}

func (s *batchSegments) maybeAddDiffSegment(base *uint64, newVal uint64, segmentHeader byte) (bool, error) {
	if newVal == *base {
		return true, nil
	}
	diff := newVal - *base
	seg, err := s.prepareIntSegment(diff, segmentHeader)
	if err != nil {
		return false, err
	}
	success, err := s.addSegment(seg, true)
	if success {
		*base = newVal
	}
	return success, err
}

func (s *batchSegments) addDelayedMessage() (bool, error) {
	segment := []byte{arbstate.BatchSegmentKindDelayedMessages}
	success, err := s.addSegment(segment, false)
	if (err == nil) && success {
		s.delayedMsg += 1
	}
	return success, err
}

func (s *batchSegments) AddMessage(msg *arbostypes.MessageWithMetadata) (bool, error) {
	if s.isDone {
		return false, errBatchAlreadyClosed
	}
	if msg.DelayedMessagesRead > s.delayedMsg {
		if msg.DelayedMessagesRead != s.delayedMsg+1 {
			return false, fmt.Errorf("attempted to add delayed msg %d after %d", msg.DelayedMessagesRead, s.delayedMsg)
		}
		return s.addDelayedMessage()
	}
	success, err := s.maybeAddDiffSegment(&s.timestamp, msg.Message.Header.Timestamp, arbstate.BatchSegmentKindAdvanceTimestamp)
	if !success {
		return false, err
	}
	success, err = s.maybeAddDiffSegment(&s.blockNum, msg.Message.Header.BlockNumber, arbstate.BatchSegmentKindAdvanceL1BlockNumber)
	if !success {
		return false, err
	}
	return s.addL2Msg(msg.Message.L2msg)
}

func (s *batchSegments) IsDone() bool {
	return s.isDone
}

// Returns nil (as opposed to []byte{}) if there's no segments to put in the batch
func (s *batchSegments) CloseAndGetBytes() ([]byte, error) {
	if !s.isDone {
		err := s.close()
		if err != nil {
			return nil, err
		}
	}
	if len(s.rawSegments) == 0 {
		return nil, nil
	}
	err := s.compressedWriter.Close()
	if err != nil {
		return nil, err
	}
	compressedBytes := s.compressedBuffer.Bytes()
	fullMsg := make([]byte, 1, len(compressedBytes)+1)
	fullMsg[0] = arbstate.BrotliMessageHeaderByte
	fullMsg = append(fullMsg, compressedBytes...)
	return fullMsg, nil
}

func (b *BatchPoster) encodeAddBatch(seqNum *big.Int, prevMsgNum arbutil.MessageIndex, newMsgNum arbutil.MessageIndex, message []byte, delayedMsg uint64) ([]byte, error) {
	method, ok := b.seqInboxABI.Methods["addSequencerL2BatchFromOrigin0"]
	if !ok {
		return nil, errors.New("failed to find add batch method")
	}
	inputData, err := method.Inputs.Pack(
		seqNum,
		message,
		new(big.Int).SetUint64(delayedMsg),
		b.config().gasRefunder,
		new(big.Int).SetUint64(uint64(prevMsgNum)),
		new(big.Int).SetUint64(uint64(newMsgNum)),
	)
	if err != nil {
		return nil, err
	}
	fullData := append([]byte{}, method.ID...)
	fullData = append(fullData, inputData...)
	return fullData, nil
}

func (b *BatchPoster) estimateGas(ctx context.Context, sequencerMessage []byte, delayedMessages uint64) (uint64, error) {
	config := b.config()
	callOpts := &bind.CallOpts{
		Context: ctx,
	}
	if config.DataPoster.WaitForL1Finality {
		callOpts.BlockNumber = big.NewInt(int64(rpc.SafeBlockNumber))
	}
	safeDelayedMessagesBig, err := b.bridge.DelayedMessageCount(callOpts)
	if err != nil {
		return 0, fmt.Errorf("failed to get the confirmed delayed message count: %w", err)
	}
	if !safeDelayedMessagesBig.IsUint64() {
		return 0, fmt.Errorf("calling delayedMessageCount() on the bridge returned a non-uint64 result %v", safeDelayedMessagesBig)
	}
	safeDelayedMessages := safeDelayedMessagesBig.Uint64()
	if safeDelayedMessages > delayedMessages {
		// On restart, we may be trying to estimate gas for a batch whose successor has
		// already made it into pending state, if not latest state.
		// In that case, we might get a revert with `DelayedBackwards()`.
		// To avoid that, we artificially increase the delayed messages to `safeDelayedMessages`.
		// In theory, this might reduce gas usage, but only by a factor that's already
		// accounted for in `config.ExtraBatchGas`, as that same factor can appear if a user
		// posts a new delayed message that we didn't see while gas estimating.
		delayedMessages = safeDelayedMessages
	}
	// Here we set seqNum to MaxUint256, and prevMsgNum to 0, because it disables the smart contracts' consistency checks.
	// However, we set nextMsgNum to 1 because it is necessary for a correct estimation for the final to be non-zero.
	// Because we're likely estimating against older state, this might not be the actual next message,
	// but the gas used should be the same.
	data, err := b.encodeAddBatch(abi.MaxUint256, 0, 1, sequencerMessage, delayedMessages)
	if err != nil {
		return 0, err
	}
	gas, err := b.l1Reader.Client().EstimateGas(ctx, ethereum.CallMsg{
		From: b.dataPoster.Sender(),
		To:   &b.seqInboxAddr,
		Data: data,
	})
	if err != nil {
		sequencerMessageHeader := sequencerMessage
		if len(sequencerMessageHeader) > 33 {
			sequencerMessageHeader = sequencerMessageHeader[:33]
		}
		log.Warn(
			"error estimating gas for batch",
			"err", err,
			"delayedMessages", delayedMessages,
			"safeDelayedMessages", safeDelayedMessages,
			"sequencerMessageHeader", hex.EncodeToString(sequencerMessageHeader),
			"sequencerMessageLen", len(sequencerMessage),
		)
		return 0, fmt.Errorf("error estimating gas for batch: %w", err)
	}
	return gas + config.ExtraBatchGas, nil
}

const ethPosBlockTime = 12 * time.Second

func (b *BatchPoster) maybePostSequencerBatch(ctx context.Context) (bool, error) {
	if b.batchReverted.Load() {
		return false, fmt.Errorf("batch was reverted, not posting any more batches")
	}
	nonce, batchPositionBytes, err := b.dataPoster.GetNextNonceAndMeta(ctx)
	if err != nil {
		return false, err
	}
	var batchPosition batchPosterPosition
	if err := rlp.DecodeBytes(batchPositionBytes, &batchPosition); err != nil {
		return false, fmt.Errorf("decoding batch position: %w", err)
	}

	dbBatchCount, err := b.inbox.GetBatchCount()
	if err != nil {
		return false, err
	}
	if dbBatchCount > batchPosition.NextSeqNum {
		return false, fmt.Errorf("attempting to post batch %v, but the local inbox tracker database already has %v batches", batchPosition.NextSeqNum, dbBatchCount)
	}

	if b.building == nil || b.building.startMsgCount != batchPosition.MessageCount {
		b.building = &buildingBatch{
			segments:      newBatchSegments(batchPosition.DelayedMessageCount, b.config(), b.backlog),
			msgCount:      batchPosition.MessageCount,
			startMsgCount: batchPosition.MessageCount,
		}
	}
	msgCount, err := b.streamer.GetMessageCount()
	if err != nil {
		return false, err
	}
	if msgCount <= batchPosition.MessageCount {
		// There's nothing after the newest batch, therefore batch posting was not required
		return false, nil
	}
	firstMsg, err := b.streamer.GetMessage(batchPosition.MessageCount)
	if err != nil {
		return false, err
	}
	firstMsgTime := time.Unix(int64(firstMsg.Message.Header.Timestamp), 0)

	config := b.config()
	forcePostBatch := time.Since(firstMsgTime) >= config.MaxDelay

	var l1BoundMaxBlockNumber uint64 = math.MaxUint64
	var l1BoundMaxTimestamp uint64 = math.MaxUint64
	var l1BoundMinBlockNumber uint64
	var l1BoundMinTimestamp uint64
	hasL1Bound := config.l1BlockBound != l1BlockBoundIgnore
	if hasL1Bound {
		var l1Bound *types.Header
		var err error
		if config.l1BlockBound == l1BlockBoundLatest {
			l1Bound, err = b.l1Reader.LastHeader(ctx)
		} else if config.l1BlockBound == l1BlockBoundSafe || config.l1BlockBound == l1BlockBoundDefault {
			l1Bound, err = b.l1Reader.LatestSafeBlockHeader(ctx)
			if errors.Is(err, headerreader.ErrBlockNumberNotSupported) && config.l1BlockBound == l1BlockBoundDefault {
				// If getting the latest safe block is unsupported, and the L1BlockBound configuration is the default,
				// fall back to using the latest block instead of the safe block.
				l1Bound, err = b.l1Reader.LastHeader(ctx)
			}
		} else {
			if config.l1BlockBound != l1BlockBoundFinalized {
				log.Error(
					"unknown L1 block bound config value; falling back on using finalized",
					"l1BlockBoundString", config.L1BlockBound,
					"l1BlockBoundEnum", config.l1BlockBound,
				)
			}
			l1Bound, err = b.l1Reader.LatestFinalizedBlockHeader(ctx)
		}
		if err != nil {
			return false, fmt.Errorf("error getting L1 bound block: %w", err)
		}

		maxTimeVariation, err := b.seqInbox.MaxTimeVariation(&bind.CallOpts{
			Context:     ctx,
			BlockNumber: l1Bound.Number,
		})
		if err != nil {
			// This might happen if the latest finalized block is old enough that our L1 node no longer has its state
			log.Warn("error getting max time variation on L1 bound block; falling back on latest block", "err", err)
			maxTimeVariation, err = b.seqInbox.MaxTimeVariation(&bind.CallOpts{Context: ctx})
			if err != nil {
				return false, fmt.Errorf("error getting max time variation: %w", err)
			}
		}

		l1BoundBlockNumber := arbutil.ParentHeaderToL1BlockNumber(l1Bound)
		l1BoundMaxBlockNumber = arbmath.SaturatingUAdd(l1BoundBlockNumber, arbmath.BigToUintSaturating(maxTimeVariation.FutureBlocks))
		l1BoundMaxTimestamp = arbmath.SaturatingUAdd(l1Bound.Time, arbmath.BigToUintSaturating(maxTimeVariation.FutureSeconds))

		if config.L1BlockBoundBypass > 0 {
			latestHeader, err := b.l1Reader.LastHeader(ctx)
			if err != nil {
				return false, err
			}
			latestBlockNumber := arbutil.ParentHeaderToL1BlockNumber(latestHeader)
			blockNumberWithPadding := arbmath.SaturatingUAdd(latestBlockNumber, uint64(config.L1BlockBoundBypass/ethPosBlockTime))
			timestampWithPadding := arbmath.SaturatingUAdd(latestHeader.Time, uint64(config.L1BlockBoundBypass/time.Second))

			l1BoundMinBlockNumber = arbmath.SaturatingUSub(blockNumberWithPadding, arbmath.BigToUintSaturating(maxTimeVariation.DelayBlocks))
			l1BoundMinTimestamp = arbmath.SaturatingUSub(timestampWithPadding, arbmath.BigToUintSaturating(maxTimeVariation.DelaySeconds))
		}
	}

	for b.building.msgCount < msgCount {
		msg, err := b.streamer.GetMessage(b.building.msgCount)
		if err != nil {
			log.Error("error getting message from streamer", "error", err)
			break
		}
		if msg.Message.Header.BlockNumber < l1BoundMinBlockNumber || msg.Message.Header.Timestamp < l1BoundMinTimestamp {
			log.Error(
				"disabling L1 bound as batch posting message is close to the maximum delay",
				"blockNumber", msg.Message.Header.BlockNumber,
				"l1BoundMinBlockNumber", l1BoundMinBlockNumber,
				"timestamp", msg.Message.Header.Timestamp,
				"l1BoundMinTimestamp", l1BoundMinTimestamp,
			)
			l1BoundMaxBlockNumber = math.MaxUint64
			l1BoundMaxTimestamp = math.MaxUint64
		}
		if msg.Message.Header.BlockNumber > l1BoundMaxBlockNumber || msg.Message.Header.Timestamp > l1BoundMaxTimestamp {
			b.lastHitL1Bounds = time.Now()
			log.Info(
				"not posting more messages because block number or timestamp exceed L1 bounds",
				"blockNumber", msg.Message.Header.BlockNumber,
				"l1BoundMaxBlockNumber", l1BoundMaxBlockNumber,
				"timestamp", msg.Message.Header.Timestamp,
				"l1BoundMaxTimestamp", l1BoundMaxTimestamp,
			)
			break
		}
		success, err := b.building.segments.AddMessage(msg)
		if err != nil {
			// Clear our cache
			b.building = nil
			return false, fmt.Errorf("error adding message to batch: %w", err)
		}
		if !success {
			// this batch is full
			if !config.WaitForMaxDelay {
				forcePostBatch = true
			}
			b.building.haveUsefulMessage = true
			break
		}
		if msg.Message.Header.Kind != arbostypes.L1MessageType_BatchPostingReport {
			b.building.haveUsefulMessage = true
		}
		b.building.msgCount++
	}

	if !forcePostBatch || !b.building.haveUsefulMessage {
		// the batch isn't full yet and we've posted a batch recently
		// don't post anything for now
		return false, nil
	}
	sequencerMsg, err := b.building.segments.CloseAndGetBytes()
	if err != nil {
		return false, err
	}
	if sequencerMsg == nil {
		log.Debug("BatchPoster: batch nil", "sequence nr.", batchPosition.NextSeqNum, "from", batchPosition.MessageCount, "prev delayed", batchPosition.DelayedMessageCount)
		b.building = nil // a closed batchSegments can't be reused
		return false, nil
	}

	if b.daWriter != nil {
		cert, err := b.daWriter.Store(ctx, sequencerMsg, uint64(time.Now().Add(config.DASRetentionPeriod).Unix()), []byte{}) // b.daWriter will append signature if enabled
		if errors.Is(err, das.BatchToDasFailed) {
			if config.DisableDasFallbackStoreDataOnChain {
				return false, errors.New("unable to batch to DAS and fallback storing data on chain is disabled")
			}
			log.Warn("Falling back to storing data on chain", "err", err)
		} else if err != nil {
			return false, err
		} else {
			sequencerMsg = das.Serialize(cert)
		}
	}

	gasLimit, err := b.estimateGas(ctx, sequencerMsg, b.building.segments.delayedMsg)
	if err != nil {
		return false, err
	}
	data, err := b.encodeAddBatch(new(big.Int).SetUint64(batchPosition.NextSeqNum), batchPosition.MessageCount, b.building.msgCount, sequencerMsg, b.building.segments.delayedMsg)
	if err != nil {
		return false, err
	}
	newMeta, err := rlp.EncodeToBytes(batchPosterPosition{
		MessageCount:        b.building.msgCount,
		DelayedMessageCount: b.building.segments.delayedMsg,
		NextSeqNum:          batchPosition.NextSeqNum + 1,
	})
	if err != nil {
		return false, err
	}
	tx, err := b.dataPoster.PostTransaction(ctx,
		firstMsgTime,
		nonce,
		newMeta,
		b.seqInboxAddr,
		data,
		gasLimit,
		new(big.Int),
		b.accessList(
			int(batchPosition.NextSeqNum),
			int(b.building.segments.delayedMsg)),
	)
	if err != nil {
		return false, err
	}
	log.Info(
		"BatchPoster: batch sent",
		"sequence nr.", batchPosition.NextSeqNum,
		"from", batchPosition.MessageCount,
		"to", b.building.msgCount,
		"prev delayed", batchPosition.DelayedMessageCount,
		"current delayed", b.building.segments.delayedMsg,
		"total segments", len(b.building.segments.rawSegments),
	)
	recentlyHitL1Bounds := time.Since(b.lastHitL1Bounds) < config.PollInterval*3
	postedMessages := b.building.msgCount - batchPosition.MessageCount
	unpostedMessages := msgCount - b.building.msgCount
	b.backlog = uint64(unpostedMessages) / uint64(postedMessages)
	if b.backlog > 10 {
		logLevel := log.Warn
		if recentlyHitL1Bounds {
			logLevel = log.Info
		} else if b.backlog > 30 {
			logLevel = log.Error
		}
		logLevel(
			"a large batch posting backlog exists",
			"recentlyHitL1Bounds", recentlyHitL1Bounds,
			"currentPosition", b.building.msgCount,
			"messageCount", msgCount,
			"lastPostedMessages", postedMessages,
			"unpostedMessages", unpostedMessages,
			"batchBacklogEstimate", b.backlog,
		)
	}
	if recentlyHitL1Bounds {
		// This backlog isn't "real" in that we don't want to post any more messages.
		// Setting the backlog to 0 here ensures that we don't lower compression as a result.
		b.backlog = 0
	}
	b.building = nil

	// If we aren't queueing up transactions, wait for the receipt before moving on to the next batch.
	if config.DataPoster.UseNoOpStorage {
		receipt, err := b.l1Reader.WaitForTxApproval(ctx, tx)
		if err != nil {
			return false, fmt.Errorf("error waiting for tx receipt: %w", err)
		}
		log.Info("Got successful receipt from batch poster transaction", "txHash", tx.Hash(), "blockNumber", receipt.BlockNumber, "blockHash", receipt.BlockHash)
	}

	return true, nil
}

func (b *BatchPoster) Start(ctxIn context.Context) {
	b.dataPoster.Start(ctxIn)
	b.redisLock.Start(ctxIn)
	b.StopWaiter.Start(ctxIn, b)
	b.LaunchThread(b.pollForReverts)
	b.CallIteratively(func(ctx context.Context) time.Duration {
		var err error
		if common.HexToAddress(b.config().GasRefunderAddress) != (common.Address{}) {
			gasRefunderBalance, err := b.l1Reader.Client().BalanceAt(ctx, common.HexToAddress(b.config().GasRefunderAddress), nil)
			if err != nil {
				log.Warn("error fetching batch poster gas refunder balance", "err", err)
			} else {
				batchPosterGasRefunderBalance.Update(arbmath.BalancePerEther(gasRefunderBalance))
			}
		}
		if b.dataPoster.Sender() != (common.Address{}) {
			walletBalance, err := b.l1Reader.Client().BalanceAt(ctx, b.dataPoster.Sender(), nil)
			if err != nil {
				log.Warn("error fetching batch poster wallet balance", "err", err)
			} else {
				batchPosterWalletBalance.Update(arbmath.BalancePerEther(walletBalance))
			}
		}
		if !b.redisLock.AttemptLock(ctx) {
			b.building = nil
			return b.config().PollInterval
		}
		posted, err := b.maybePostSequencerBatch(ctx)
		if err == nil {
			b.firstEphemeralError = time.Time{}
		}
		if err != nil {
			b.building = nil
			logLevel := log.Error
			// Likely the inbox tracker just isn't caught up.
			// Let's see if this error disappears naturally.
			if b.firstEphemeralError == (time.Time{}) {
				b.firstEphemeralError = time.Now()
				logLevel = log.Warn
			} else if time.Since(b.firstEphemeralError) < time.Minute {
				logLevel = log.Warn
			} else if time.Since(b.firstEphemeralError) < time.Minute*5 && strings.Contains(err.Error(), "will exceed max mempool size") {
				logLevel = log.Warn
			}
			logLevel("error posting batch", "err", err)
			return b.config().ErrorDelay
		} else if posted {
			return 0
		} else {
			return b.config().PollInterval
		}
	})
}

func (b *BatchPoster) StopAndWait() {
	b.StopWaiter.StopAndWait()
	b.dataPoster.StopAndWait()
	b.redisLock.StopAndWait()
}

'''
'''--- arbnode/classicMessage.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"
	"math/bits"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/ethdb"
)

type ClassicOutboxRetriever struct {
	db ethdb.Database
}

func NewClassicOutboxRetriever(db ethdb.Database) *ClassicOutboxRetriever {
	return &ClassicOutboxRetriever{
		db: db,
	}
}

type ClassicOutboxMsg struct {
	ProofNodes [][32]byte
	PathInt    *big.Int
	Data       []byte
}

func msgBatchKey(batchNum *big.Int) []byte {
	return crypto.Keccak256(append([]byte("msgBatch"), batchNum.Bytes()...))
}

func (m *ClassicOutboxRetriever) GetMsg(batchNum *big.Int, index uint64) (*ClassicOutboxMsg, error) {
	batchHeader, err := m.db.Get(msgBatchKey(batchNum))
	if err != nil {
		return nil, fmt.Errorf("%w: batch %d not found", err, batchNum)
	}
	if len(batchHeader) != 40 {
		return nil, fmt.Errorf("unexpected batch header: %v", batchHeader)
	}
	merkleSize := binary.BigEndian.Uint64(batchHeader[0:8])
	lowest := uint64(0)
	var root common.Hash
	copy(root[:], batchHeader[8:40])
	if merkleSize < index {
		return nil, fmt.Errorf("batch %d only has %d indexes", batchNum, merkleSize)
	}
	proofNodes := [][32]byte{}
	pathInt := big.NewInt(0)
	for merkleSize > 1 {
		merkleNode, err := m.db.Get(root[:])
		if err != nil {
			return nil, err
		}
		if len(merkleNode) != 64 {
			return nil, errors.New("unexpected merkle node")
		}
		// left side is always full
		var merkleLeftSize uint64
		if bits.OnesCount64(merkleSize) == 1 {
			merkleLeftSize = merkleSize / 2
		} else {
			merkleLeftSize = uint64(1) << (bits.Len64(merkleSize) - 1)
		}
		var leftHash, rightHash [32]byte
		copy(leftHash[:], merkleNode[0:32])
		copy(rightHash[:], merkleNode[32:64])
		pathInt.Mul(pathInt, common.Big2)
		if index < lowest+merkleLeftSize {
			// take a left turn
			copy(root[:], leftHash[:])
			proofNodes = append([][32]byte{rightHash}, proofNodes...)
			// lowest doesn't change
			merkleSize = merkleLeftSize
			pathInt.Add(pathInt, common.Big1)
		} else {
			// take a right turn
			copy(root[:], rightHash[:])
			proofNodes = append([][32]byte{leftHash}, proofNodes...)
			lowest = lowest + merkleLeftSize
			merkleSize = merkleSize - merkleLeftSize
			// equivalent bit in pathInt is zero
		}
	}
	if index != lowest {
		return nil, errors.New("unexpected error moving through merkle tree")
	}
	data, err := m.db.Get(root[:])
	if err != nil {
		return nil, err
	}
	return &ClassicOutboxMsg{
		ProofNodes: proofNodes,
		PathInt:    pathInt,
		Data:       data,
	}, nil
}

'''
'''--- arbnode/dataposter/data_poster.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// Package dataposter implements generic functionality to post transactions.
package dataposter

import (
	"context"
	"crypto/tls"
	"crypto/x509"
	"errors"
	"fmt"
	"math/big"
	"net/http"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/go-redis/redis/v8"
	"github.com/offchainlabs/nitro/arbnode/dataposter/dbstorage"
	"github.com/offchainlabs/nitro/arbnode/dataposter/noop"
	"github.com/offchainlabs/nitro/arbnode/dataposter/slice"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/spf13/pflag"

	redisstorage "github.com/offchainlabs/nitro/arbnode/dataposter/redis"
)

// Dataposter implements functionality to post transactions on the chain. It
// is initialized with specified sender/signer and keeps nonce of that address
// as it posts transactions.
// Transactions are also saved in the queue when it's being sent, and when
// persistent storage is used for the queue, after restarting the node
// dataposter will pick up where it left.
// DataPoster must be RLP serializable and deserializable
type DataPoster struct {
	stopwaiter.StopWaiter
	headerReader      *headerreader.HeaderReader
	client            arbutil.L1Interface
	sender            common.Address
	signer            signerFn
	redisLock         AttemptLocker
	config            ConfigFetcher
	replacementTimes  []time.Duration
	metadataRetriever func(ctx context.Context, blockNum *big.Int) ([]byte, error)

	// These fields are protected by the mutex.
	// TODO: factor out these fields into separate structure, since now one
	// needs to make sure call sites of methods that change these values hold
	// the lock (currently ensured by having comments like:
	// "the mutex must be held by the caller" above the function).
	mutex      sync.Mutex
	lastBlock  *big.Int
	balance    *big.Int
	nonce      uint64
	queue      QueueStorage
	errorCount map[uint64]int // number of consecutive intermittent errors rbf-ing or sending, per nonce
}

// signerFn is a signer function callback when a contract requires a method to
// sign the transaction before submission.
// This can be local or external, hence the context parameter.
type signerFn func(context.Context, common.Address, *types.Transaction) (*types.Transaction, error)

type AttemptLocker interface {
	AttemptLock(context.Context) bool
}

func parseReplacementTimes(val string) ([]time.Duration, error) {
	var res []time.Duration
	var lastReplacementTime time.Duration
	for _, s := range strings.Split(val, ",") {
		t, err := time.ParseDuration(s)
		if err != nil {
			return nil, fmt.Errorf("parsing durations: %w", err)
		}
		if t <= lastReplacementTime {
			return nil, errors.New("replacement times must be increasing")
		}
		res = append(res, t)
		lastReplacementTime = t
	}
	if len(res) == 0 {
		log.Warn("Disabling replace-by-fee for data poster")
	}
	// To avoid special casing "don't replace again", replace in 10 years.
	return append(res, time.Hour*24*365*10), nil
}

type DataPosterOpts struct {
	Database          ethdb.Database
	HeaderReader      *headerreader.HeaderReader
	Auth              *bind.TransactOpts
	RedisClient       redis.UniversalClient
	RedisLock         AttemptLocker
	Config            ConfigFetcher
	MetadataRetriever func(ctx context.Context, blockNum *big.Int) ([]byte, error)
	RedisKey          string // Redis storage key
}

func NewDataPoster(ctx context.Context, opts *DataPosterOpts) (*DataPoster, error) {
	cfg := opts.Config()
	replacementTimes, err := parseReplacementTimes(cfg.ReplacementTimes)
	if err != nil {
		return nil, err
	}
	if opts.HeaderReader.IsParentChainArbitrum() && !cfg.UseNoOpStorage {
		cfg.UseNoOpStorage = true
		log.Info("Disabling data poster storage, as parent chain appears to be an Arbitrum chain without a mempool")
	}
	encF := func() storage.EncoderDecoderInterface {
		if opts.Config().LegacyStorageEncoding {
			return &storage.LegacyEncoderDecoder{}
		}
		return &storage.EncoderDecoder{}
	}
	var queue QueueStorage
	switch {
	case cfg.UseNoOpStorage:
		queue = &noop.Storage{}
	case opts.RedisClient != nil:
		var err error
		queue, err = redisstorage.NewStorage(opts.RedisClient, opts.RedisKey, &cfg.RedisSigner, encF)
		if err != nil {
			return nil, err
		}
	case cfg.UseDBStorage:
		storage := dbstorage.New(opts.Database, func() storage.EncoderDecoderInterface { return &storage.EncoderDecoder{} })
		if cfg.Dangerous.ClearDBStorage {
			if err := storage.PruneAll(ctx); err != nil {
				return nil, err
			}
		}
		queue = storage
	default:
		queue = slice.NewStorage(func() storage.EncoderDecoderInterface { return &storage.EncoderDecoder{} })
	}
	dp := &DataPoster{
		headerReader: opts.HeaderReader,
		client:       opts.HeaderReader.Client(),
		sender:       opts.Auth.From,
		signer: func(_ context.Context, addr common.Address, tx *types.Transaction) (*types.Transaction, error) {
			return opts.Auth.Signer(addr, tx)
		},
		config:            opts.Config,
		replacementTimes:  replacementTimes,
		metadataRetriever: opts.MetadataRetriever,
		queue:             queue,
		redisLock:         opts.RedisLock,
		errorCount:        make(map[uint64]int),
	}
	if cfg.ExternalSigner.URL != "" {
		signer, sender, err := externalSigner(ctx, &cfg.ExternalSigner)
		if err != nil {
			return nil, err
		}
		dp.signer, dp.sender = signer, sender
	}
	return dp, nil
}

func rpcClient(ctx context.Context, opts *ExternalSignerCfg) (*rpc.Client, error) {
	tlsCfg := &tls.Config{
		MinVersion: tls.VersionTLS12,
	}

	if opts.ClientCert != "" && opts.ClientPrivateKey != "" {
		log.Info("Client certificate for external signer is enabled")
		clientCert, err := tls.LoadX509KeyPair(opts.ClientCert, opts.ClientPrivateKey)
		if err != nil {
			return nil, fmt.Errorf("error loading client certificate and private key: %w", err)
		}
		tlsCfg.Certificates = []tls.Certificate{clientCert}
	}

	if opts.RootCA != "" {
		rootCrt, err := os.ReadFile(opts.RootCA)
		if err != nil {
			return nil, fmt.Errorf("error reading external signer root CA: %w", err)
		}
		rootCertPool := x509.NewCertPool()
		rootCertPool.AppendCertsFromPEM(rootCrt)
		tlsCfg.RootCAs = rootCertPool
	}

	return rpc.DialOptions(
		ctx,
		opts.URL,
		rpc.WithHTTPClient(
			&http.Client{
				Transport: &http.Transport{
					TLSClientConfig: tlsCfg,
				},
			},
		),
	)
}

// externalSigner returns signer function and ethereum address of the signer.
// Returns an error if address isn't specified or if it can't connect to the
// signer RPC server.
func externalSigner(ctx context.Context, opts *ExternalSignerCfg) (signerFn, common.Address, error) {
	if opts.Address == "" {
		return nil, common.Address{}, errors.New("external signer (From) address specified")
	}

	client, err := rpcClient(ctx, opts)
	if err != nil {
		return nil, common.Address{}, fmt.Errorf("error connecting external signer: %w", err)
	}
	sender := common.HexToAddress(opts.Address)

	var hasher types.Signer
	return func(ctx context.Context, addr common.Address, tx *types.Transaction) (*types.Transaction, error) {
		// According to the "eth_signTransaction" API definition, this should be
		// RLP encoded transaction object.
		// https://ethereum.org/en/developers/docs/apis/json-rpc/#eth_signtransaction
		var data hexutil.Bytes
		if err := client.CallContext(ctx, &data, opts.Method, tx); err != nil {
			return nil, fmt.Errorf("signing transaction: %w", err)
		}
		var signedTx types.Transaction
		if err := rlp.DecodeBytes(data, &signedTx); err != nil {
			return nil, fmt.Errorf("error decoding signed transaction: %w", err)
		}
		if hasher == nil {
			hasher = types.LatestSignerForChainID(tx.ChainId())
		}
		if hasher.Hash(tx) != hasher.Hash(&signedTx) {
			return nil, fmt.Errorf("transaction: %x from external signer differs from request: %x", hasher.Hash(&signedTx), hasher.Hash(tx))
		}
		return &signedTx, nil
	}, sender, nil
}

func (p *DataPoster) Sender() common.Address {
	return p.sender
}

// Does basic check whether posting transaction with specified nonce would
// result in exceeding maximum queue length or maximum transactions in mempool.
func (p *DataPoster) canPostWithNonce(ctx context.Context, nextNonce uint64) error {
	cfg := p.config()
	// If the queue has reached configured max size, don't post a transaction.
	if cfg.MaxQueuedTransactions > 0 {
		queueLen, err := p.queue.Length(ctx)
		if err != nil {
			return fmt.Errorf("getting queue length: %w", err)
		}
		if queueLen >= cfg.MaxQueuedTransactions {
			return fmt.Errorf("posting a transaction with nonce: %d will exceed max allowed dataposter queued transactions: %d, current nonce: %d", nextNonce, cfg.MaxQueuedTransactions, p.nonce)
		}
	}
	// Check that posting a new transaction won't exceed maximum pending
	// transactions in mempool.
	if cfg.MaxMempoolTransactions > 0 {
		unconfirmedNonce, err := p.client.NonceAt(ctx, p.sender, nil)
		if err != nil {
			return fmt.Errorf("getting nonce of a dataposter sender: %w", err)
		}
		if nextNonce >= cfg.MaxMempoolTransactions+unconfirmedNonce {
			return fmt.Errorf("posting a transaction with nonce: %d will exceed max mempool size: %d, unconfirmed nonce: %d", nextNonce, cfg.MaxMempoolTransactions, unconfirmedNonce)
		}
	}
	return nil
}

func (p *DataPoster) waitForL1Finality() bool {
	return p.config().WaitForL1Finality && !p.headerReader.IsParentChainArbitrum()
}

// Requires the caller hold the mutex.
// Returns the next nonce, its metadata if stored, a bool indicating if the metadata is present, and an error.
// Unlike GetNextNonceAndMeta, this does not call the metadataRetriever if the metadata is not stored in the queue.
func (p *DataPoster) getNextNonceAndMaybeMeta(ctx context.Context) (uint64, []byte, bool, error) {
	// Ensure latest finalized block state is available.
	blockNum, err := p.client.BlockNumber(ctx)
	if err != nil {
		return 0, nil, false, err
	}
	lastQueueItem, err := p.queue.FetchLast(ctx)
	if err != nil {
		return 0, nil, false, fmt.Errorf("fetching last element from queue: %w", err)
	}
	if lastQueueItem != nil {
		nextNonce := lastQueueItem.Data.Nonce + 1
		if err := p.canPostWithNonce(ctx, nextNonce); err != nil {
			return 0, nil, false, err
		}
		return nextNonce, lastQueueItem.Meta, true, nil
	}

	if err := p.updateNonce(ctx); err != nil {
		if !p.queue.IsPersistent() && p.waitForL1Finality() {
			return 0, nil, false, fmt.Errorf("error getting latest finalized nonce (and queue is not persistent): %w", err)
		}
		// Fall back to using a recent block to get the nonce. This is safe because there's nothing in the queue.
		nonceQueryBlock := arbmath.UintToBig(arbmath.SaturatingUSub(blockNum, 1))
		log.Warn("failed to update nonce with queue empty; falling back to using a recent block", "recentBlock", nonceQueryBlock, "err", err)
		nonce, err := p.client.NonceAt(ctx, p.sender, nonceQueryBlock)
		if err != nil {
			return 0, nil, false, fmt.Errorf("failed to get nonce at block %v: %w", nonceQueryBlock, err)
		}
		p.lastBlock = nonceQueryBlock
		p.nonce = nonce
	}
	return p.nonce, nil, false, nil
}

// GetNextNonceAndMeta retrieves generates next nonce, validates that a
// transaction can be posted with that nonce, and fetches "Meta" either last
// queued iterm (if queue isn't empty) or retrieves with last block.
func (p *DataPoster) GetNextNonceAndMeta(ctx context.Context) (uint64, []byte, error) {
	p.mutex.Lock()
	defer p.mutex.Unlock()
	nonce, meta, hasMeta, err := p.getNextNonceAndMaybeMeta(ctx)
	if err != nil {
		return 0, nil, err
	}
	if !hasMeta {
		meta, err = p.metadataRetriever(ctx, p.lastBlock)
	}
	return nonce, meta, err
}

const minRbfIncrease = arbmath.OneInBips * 11 / 10

func (p *DataPoster) feeAndTipCaps(ctx context.Context, nonce uint64, gasLimit uint64, lastFeeCap *big.Int, lastTipCap *big.Int, dataCreatedAt time.Time, backlogOfBatches uint64) (*big.Int, *big.Int, error) {
	config := p.config()
	latestHeader, err := p.headerReader.LastHeader(ctx)
	if err != nil {
		return nil, nil, err
	}
	if latestHeader.BaseFee == nil {
		return nil, nil, fmt.Errorf("latest parent chain block %v missing BaseFee (either the parent chain does not have EIP-1559 or the parent chain node is not synced)", latestHeader.Number)
	}
	softConfBlock := arbmath.BigSubByUint(latestHeader.Number, config.NonceRbfSoftConfs)
	softConfNonce, err := p.client.NonceAt(ctx, p.sender, softConfBlock)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to get latest nonce %v blocks ago (block %v): %w", config.NonceRbfSoftConfs, softConfBlock, err)
	}
	newFeeCap := new(big.Int).Mul(latestHeader.BaseFee, big.NewInt(2))
	newFeeCap = arbmath.BigMax(newFeeCap, arbmath.FloatToBig(config.MinFeeCapGwei*params.GWei))

	newTipCap, err := p.client.SuggestGasTipCap(ctx)
	if err != nil {
		return nil, nil, err
	}
	newTipCap = arbmath.BigMax(newTipCap, arbmath.FloatToBig(config.MinTipCapGwei*params.GWei))
	newTipCap = arbmath.BigMin(newTipCap, arbmath.FloatToBig(config.MaxTipCapGwei*params.GWei))

	hugeTipIncrease := false
	if lastTipCap != nil {
		newTipCap = arbmath.BigMax(newTipCap, arbmath.BigMulByBips(lastTipCap, minRbfIncrease))
		// hugeTipIncrease is true if the new tip cap is at least 10x the last tip cap
		hugeTipIncrease = lastTipCap.Sign() == 0 || arbmath.BigDiv(newTipCap, lastTipCap).Cmp(big.NewInt(10)) >= 0
	}

	newFeeCap.Add(newFeeCap, newTipCap)
	if lastFeeCap != nil && hugeTipIncrease {
		log.Warn("data poster recommending huge tip increase", "lastTipCap", lastTipCap, "newTipCap", newTipCap)
		// If we're trying to drastically increase the tip, make sure we increase the fee cap by minRbfIncrease.
		newFeeCap = arbmath.BigMax(newFeeCap, arbmath.BigMulByBips(lastFeeCap, minRbfIncrease))
	}

	elapsed := time.Since(dataCreatedAt)
	// MaxFeeCap = (BacklogOfBatches^2 * UrgencyGWei^2 + TargetPriceGWei) * GWei
	maxFeeCap :=
		arbmath.FloatToBig(
			(float64(arbmath.SquareUint(backlogOfBatches))*
				arbmath.SquareFloat(config.UrgencyGwei) +
				config.TargetPriceGwei) *
				params.GWei)
	if arbmath.BigGreaterThan(newFeeCap, maxFeeCap) {
		log.Warn(
			"reducing proposed fee cap to current maximum",
			"proposedFeeCap", newFeeCap,
			"maxFeeCap", maxFeeCap,
			"elapsed", elapsed,
		)
		newFeeCap = maxFeeCap
	}

	latestBalance := p.balance
	balanceForTx := new(big.Int).Set(latestBalance)
	if config.AllocateMempoolBalance && !config.UseNoOpStorage {
		// We reserve half the balance for the first transaction, and then split the remaining balance for all after that.
		// With noop storage, we don't try to replace-by-fee, so we don't need to worry about this.
		balanceForTx.Div(balanceForTx, common.Big2)
		if nonce != softConfNonce && config.MaxMempoolTransactions > 1 {
			// balanceForTx /= config.MaxMempoolTransactions-1
			balanceForTx.Div(balanceForTx, arbmath.UintToBig(config.MaxMempoolTransactions-1))
		}
	}
	balanceFeeCap := arbmath.BigDivByUint(balanceForTx, gasLimit)
	if arbmath.BigGreaterThan(newFeeCap, balanceFeeCap) {
		log.Error(
			"lack of L1 balance prevents posting transaction with desired fee cap",
			"balance", latestBalance,
			"maxTransactions", config.MaxMempoolTransactions,
			"balanceForTransaction", balanceForTx,
			"gasLimit", gasLimit,
			"desiredFeeCap", newFeeCap,
			"balanceFeeCap", balanceFeeCap,
			"nonce", nonce,
			"softConfNonce", softConfNonce,
		)
		newFeeCap = balanceFeeCap
	}

	if arbmath.BigGreaterThan(newTipCap, newFeeCap) {
		log.Warn(
			"reducing new tip cap to new fee cap",
			"proposedTipCap", newTipCap,
			"newFeeCap", newFeeCap,
		)
		newTipCap = new(big.Int).Set(newFeeCap)
	}

	return newFeeCap, newTipCap, nil
}

func (p *DataPoster) PostTransaction(ctx context.Context, dataCreatedAt time.Time, nonce uint64, meta []byte, to common.Address, calldata []byte, gasLimit uint64, value *big.Int, accessList types.AccessList) (*types.Transaction, error) {
	p.mutex.Lock()
	defer p.mutex.Unlock()

	expectedNonce, _, _, err := p.getNextNonceAndMaybeMeta(ctx)
	if err != nil {
		return nil, err
	}
	if nonce != expectedNonce {
		return nil, fmt.Errorf("data poster expected next transaction to have nonce %v but was requested to post transaction with nonce %v", expectedNonce, nonce)
	}

	err = p.updateBalance(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to update data poster balance: %w", err)
	}

	feeCap, tipCap, err := p.feeAndTipCaps(ctx, nonce, gasLimit, nil, nil, dataCreatedAt, 0)
	if err != nil {
		return nil, err
	}
	inner := types.DynamicFeeTx{
		Nonce:      nonce,
		GasTipCap:  tipCap,
		GasFeeCap:  feeCap,
		Gas:        gasLimit,
		To:         &to,
		Value:      value,
		Data:       calldata,
		AccessList: accessList,
	}
	fullTx, err := p.signer(ctx, p.sender, types.NewTx(&inner))
	if err != nil {
		return nil, fmt.Errorf("signing transaction: %w", err)
	}
	queuedTx := storage.QueuedTransaction{
		Data:            inner,
		FullTx:          fullTx,
		Meta:            meta,
		Sent:            false,
		Created:         dataCreatedAt,
		NextReplacement: time.Now().Add(p.replacementTimes[0]),
	}
	return fullTx, p.sendTx(ctx, nil, &queuedTx)
}

// the mutex must be held by the caller
func (p *DataPoster) saveTx(ctx context.Context, prevTx, newTx *storage.QueuedTransaction) error {
	if prevTx != nil && prevTx.Data.Nonce != newTx.Data.Nonce {
		return fmt.Errorf("prevTx nonce %v doesn't match newTx nonce %v", prevTx.Data.Nonce, newTx.Data.Nonce)
	}
	if err := p.queue.Put(ctx, newTx.Data.Nonce, prevTx, newTx); err != nil {
		return fmt.Errorf("putting new tx in the queue: %w", err)
	}
	return nil
}

func (p *DataPoster) sendTx(ctx context.Context, prevTx *storage.QueuedTransaction, newTx *storage.QueuedTransaction) error {
	if prevTx == nil || (newTx.FullTx.Hash() != prevTx.FullTx.Hash()) {
		if err := p.saveTx(ctx, prevTx, newTx); err != nil {
			return err
		}
	}
	if err := p.client.SendTransaction(ctx, newTx.FullTx); err != nil {
		if !strings.Contains(err.Error(), "already known") && !strings.Contains(err.Error(), "nonce too low") {
			log.Warn("DataPoster failed to send transaction", "err", err, "nonce", newTx.FullTx.Nonce(), "feeCap", newTx.FullTx.GasFeeCap(), "tipCap", newTx.FullTx.GasTipCap())
			return err
		}
		log.Info("DataPoster transaction already known", "err", err, "nonce", newTx.FullTx.Nonce(), "hash", newTx.FullTx.Hash())
	} else {
		log.Info("DataPoster sent transaction", "nonce", newTx.FullTx.Nonce(), "hash", newTx.FullTx.Hash(), "feeCap", newTx.FullTx.GasFeeCap())
	}
	newerTx := *newTx
	newerTx.Sent = true
	return p.saveTx(ctx, newTx, &newerTx)
}

// The mutex must be held by the caller.
func (p *DataPoster) replaceTx(ctx context.Context, prevTx *storage.QueuedTransaction, backlogOfBatches uint64) error {
	newFeeCap, newTipCap, err := p.feeAndTipCaps(ctx, prevTx.Data.Nonce, prevTx.Data.Gas, prevTx.Data.GasFeeCap, prevTx.Data.GasTipCap, prevTx.Created, backlogOfBatches)
	if err != nil {
		return err
	}

	minNewFeeCap := arbmath.BigMulByBips(prevTx.Data.GasFeeCap, minRbfIncrease)
	newTx := *prevTx
	if newFeeCap.Cmp(minNewFeeCap) < 0 {
		log.Debug(
			"no need to replace by fee transaction",
			"nonce", prevTx.Data.Nonce,
			"lastFeeCap", prevTx.Data.GasFeeCap,
			"recommendedFeeCap", newFeeCap,
			"lastTipCap", prevTx.Data.GasTipCap,
			"recommendedTipCap", newTipCap,
		)
		newTx.NextReplacement = time.Now().Add(time.Minute)
		return p.sendTx(ctx, prevTx, &newTx)
	}

	elapsed := time.Since(prevTx.Created)
	for _, replacement := range p.replacementTimes {
		if elapsed >= replacement {
			continue
		}
		newTx.NextReplacement = prevTx.Created.Add(replacement)
		break
	}
	newTx.Sent = false
	newTx.Data.GasFeeCap = newFeeCap
	newTx.Data.GasTipCap = newTipCap
	newTx.FullTx, err = p.signer(ctx, p.sender, types.NewTx(&newTx.Data))
	if err != nil {
		return err
	}

	return p.sendTx(ctx, prevTx, &newTx)
}

// Gets latest known or finalized block header (depending on config flag),
// gets the nonce of the dataposter sender and stores it if it has increased.
// The mutex must be held by the caller.
func (p *DataPoster) updateNonce(ctx context.Context) error {
	var blockNumQuery *big.Int
	if p.waitForL1Finality() {
		blockNumQuery = big.NewInt(int64(rpc.FinalizedBlockNumber))
	}
	header, err := p.client.HeaderByNumber(ctx, blockNumQuery)
	if err != nil {
		return fmt.Errorf("failed to get the latest or finalized L1 header: %w", err)
	}
	if p.lastBlock != nil && arbmath.BigEquals(p.lastBlock, header.Number) {
		return nil
	}
	nonce, err := p.client.NonceAt(ctx, p.sender, header.Number)
	if err != nil {
		if p.lastBlock != nil {
			log.Warn("Failed to get current nonce", "lastBlock", p.lastBlock, "newBlock", header.Number, "err", err)
			return nil
		}
		return err
	}
	// Ignore if nonce hasn't increased.
	if nonce <= p.nonce {
		// Still update last block number.
		if nonce == p.nonce {
			p.lastBlock = header.Number
		}
		return nil
	}
	log.Info("Data poster transactions confirmed", "previousNonce", p.nonce, "newNonce", nonce, "previousL1Block", p.lastBlock, "newL1Block", header.Number)
	if len(p.errorCount) > 0 {
		for x := p.nonce; x < nonce; x++ {
			delete(p.errorCount, x)
		}
	}
	// We don't prune the most recent transaction in order to ensure that the data poster
	// always has a reference point in its queue of the latest transaction nonce and metadata.
	// nonce > 0 is implied by nonce > p.nonce, so this won't underflow.
	if err := p.queue.Prune(ctx, nonce-1); err != nil {
		return err
	}
	// We update these two variables together because they should remain in sync even if there's an error.
	p.lastBlock = header.Number
	p.nonce = nonce
	return nil
}

// Updates dataposter balance to balance at pending block.
func (p *DataPoster) updateBalance(ctx context.Context) error {
	// Use the pending (representated as -1) balance because we're looking at batches we'd post,
	// so we want to see how much gas we could afford with our pending state.
	balance, err := p.client.BalanceAt(ctx, p.sender, big.NewInt(-1))
	if err != nil {
		return err
	}
	p.balance = balance
	return nil
}

const maxConsecutiveIntermittentErrors = 10

func (p *DataPoster) maybeLogError(err error, tx *storage.QueuedTransaction, msg string) {
	nonce := tx.Data.Nonce
	if err == nil {
		delete(p.errorCount, nonce)
		return
	}
	logLevel := log.Error
	if errors.Is(err, storage.ErrStorageRace) {
		p.errorCount[nonce]++
		if p.errorCount[nonce] <= maxConsecutiveIntermittentErrors {
			logLevel = log.Debug
		}
	} else {
		delete(p.errorCount, nonce)
	}
	logLevel(msg, "err", err, "nonce", nonce, "feeCap", tx.Data.GasFeeCap, "tipCap", tx.Data.GasTipCap, "gas", tx.Data.Gas)
}

const minWait = time.Second * 10

// Tries to acquire redis lock, updates balance and nonce,
func (p *DataPoster) Start(ctxIn context.Context) {
	p.StopWaiter.Start(ctxIn, p)
	p.CallIteratively(func(ctx context.Context) time.Duration {
		p.mutex.Lock()
		defer p.mutex.Unlock()
		if !p.redisLock.AttemptLock(ctx) {
			return minWait
		}
		err := p.updateBalance(ctx)
		if err != nil {
			log.Warn("failed to update tx poster balance", "err", err)
			return minWait
		}
		err = p.updateNonce(ctx)
		if err != nil {
			// This is non-fatal because it's only needed for clearing out old queue items.
			log.Warn("failed to update tx poster nonce", "err", err)
		}
		now := time.Now()
		nextCheck := now.Add(p.replacementTimes[0])
		maxTxsToRbf := p.config().MaxMempoolTransactions
		if maxTxsToRbf == 0 {
			maxTxsToRbf = 512
		}
		unconfirmedNonce, err := p.client.NonceAt(ctx, p.sender, nil)
		if err != nil {
			log.Warn("Failed to get latest nonce", "err", err)
			return minWait
		}
		// We use unconfirmedNonce here to replace-by-fee transactions that aren't in a block,
		// excluding those that are in an unconfirmed block. If a reorg occurs, we'll continue
		// replacing them by fee.
		queueContents, err := p.queue.FetchContents(ctx, unconfirmedNonce, maxTxsToRbf)
		if err != nil {
			log.Error("Failed to fetch tx queue contents", "err", err)
			return minWait
		}
		for index, tx := range queueContents {
			backlogOfBatches := len(queueContents) - index - 1
			replacing := false
			if now.After(tx.NextReplacement) {
				replacing = true
				err := p.replaceTx(ctx, tx, uint64(backlogOfBatches))
				p.maybeLogError(err, tx, "failed to replace-by-fee transaction")
			}
			if nextCheck.After(tx.NextReplacement) {
				nextCheck = tx.NextReplacement
			}
			if !replacing && !tx.Sent {
				err := p.sendTx(ctx, tx, tx)
				p.maybeLogError(err, tx, "failed to re-send transaction")
				if err != nil {
					nextSend := time.Now().Add(time.Minute)
					if nextCheck.After(nextSend) {
						nextCheck = nextSend
					}
				}
			}
		}
		wait := time.Until(nextCheck)
		if wait < minWait {
			wait = minWait
		}
		return wait
	})
}

// Implements queue-alike storage that can
// - Insert item at specified index
// - Update item with the condition that existing value equals assumed value
// - Delete all the items up to specified index (prune)
// - Calculate length
// Note: one of the implementation of this interface (Redis storage) does not
// support duplicate values.
type QueueStorage interface {
	// Returns at most maxResults items starting from specified index.
	FetchContents(ctx context.Context, startingIndex uint64, maxResults uint64) ([]*storage.QueuedTransaction, error)
	// Returns item with the biggest index.
	FetchLast(ctx context.Context) (*storage.QueuedTransaction, error)
	// Prunes items up to (excluding) specified index.
	Prune(ctx context.Context, until uint64) error
	// Inserts new item at specified index if previous value matches specified value.
	Put(ctx context.Context, index uint64, prevItem, newItem *storage.QueuedTransaction) error
	// Returns the size of a queue.
	Length(ctx context.Context) (int, error)
	// Indicates whether queue stored at disk.
	IsPersistent() bool
}

type DataPosterConfig struct {
	RedisSigner      signature.SimpleHmacConfig `koanf:"redis-signer"`
	ReplacementTimes string                     `koanf:"replacement-times"`
	// This is forcibly disabled if the parent chain is an Arbitrum chain,
	// so you should probably use DataPoster's waitForL1Finality method instead of reading this field directly.
	WaitForL1Finality      bool              `koanf:"wait-for-l1-finality" reload:"hot"`
	MaxMempoolTransactions uint64            `koanf:"max-mempool-transactions" reload:"hot"`
	MaxQueuedTransactions  int               `koanf:"max-queued-transactions" reload:"hot"`
	TargetPriceGwei        float64           `koanf:"target-price-gwei" reload:"hot"`
	UrgencyGwei            float64           `koanf:"urgency-gwei" reload:"hot"`
	MinFeeCapGwei          float64           `koanf:"min-fee-cap-gwei" reload:"hot"`
	MinTipCapGwei          float64           `koanf:"min-tip-cap-gwei" reload:"hot"`
	MaxTipCapGwei          float64           `koanf:"max-tip-cap-gwei" reload:"hot"`
	NonceRbfSoftConfs      uint64            `koanf:"nonce-rbf-soft-confs" reload:"hot"`
	AllocateMempoolBalance bool              `koanf:"allocate-mempool-balance" reload:"hot"`
	UseDBStorage           bool              `koanf:"use-db-storage"`
	UseNoOpStorage         bool              `koanf:"use-noop-storage"`
	LegacyStorageEncoding  bool              `koanf:"legacy-storage-encoding" reload:"hot"`
	Dangerous              DangerousConfig   `koanf:"dangerous"`
	ExternalSigner         ExternalSignerCfg `koanf:"external-signer"`
}

type ExternalSignerCfg struct {
	// URL of the external signer rpc server, if set this overrides transaction
	// options and uses external signer
	// for signing transactions.
	URL string `koanf:"url"`
	// Hex encoded ethereum address of the external signer.
	Address string `koanf:"address"`
	// API method name (e.g. eth_signTransaction).
	Method string `koanf:"method"`
	// (Optional) Path to the external signer root CA certificate.
	// This allows us to use self-signed certificats on the external signer.
	RootCA string `koanf:"root-ca"`
	// (Optional) Client certificate for mtls.
	ClientCert string `koanf:"client-cert"`
	// (Optional) Client certificate key for mtls.
	// This is required when client-cert is set.
	ClientPrivateKey string `koanf:"client-private-key"`
}

type DangerousConfig struct {
	// This should be used with caution, only when dataposter somehow gets in a
	// bad state and we require clearing it.
	ClearDBStorage bool `koanf:"clear-dbstorage"`
}

// ConfigFetcher function type is used instead of directly passing config so
// that flags can be reloaded dynamically.
type ConfigFetcher func() *DataPosterConfig

func DataPosterConfigAddOptions(prefix string, f *pflag.FlagSet, defaultDataPosterConfig DataPosterConfig) {
	f.String(prefix+".replacement-times", defaultDataPosterConfig.ReplacementTimes, "comma-separated list of durations since first posting to attempt a replace-by-fee")
	f.Bool(prefix+".wait-for-l1-finality", defaultDataPosterConfig.WaitForL1Finality, "only treat a transaction as confirmed after L1 finality has been achieved (recommended)")
	f.Uint64(prefix+".max-mempool-transactions", defaultDataPosterConfig.MaxMempoolTransactions, "the maximum number of transactions to have queued in the mempool at once (0 = unlimited)")
	f.Int(prefix+".max-queued-transactions", defaultDataPosterConfig.MaxQueuedTransactions, "the maximum number of unconfirmed transactions to track at once (0 = unlimited)")
	f.Float64(prefix+".target-price-gwei", defaultDataPosterConfig.TargetPriceGwei, "the target price to use for maximum fee cap calculation")
	f.Float64(prefix+".urgency-gwei", defaultDataPosterConfig.UrgencyGwei, "the urgency to use for maximum fee cap calculation")
	f.Float64(prefix+".min-fee-cap-gwei", defaultDataPosterConfig.MinFeeCapGwei, "the minimum fee cap to post transactions at")
	f.Float64(prefix+".min-tip-cap-gwei", defaultDataPosterConfig.MinTipCapGwei, "the minimum tip cap to post transactions at")
	f.Float64(prefix+".max-tip-cap-gwei", defaultDataPosterConfig.MaxTipCapGwei, "the maximum tip cap to post transactions at")
	f.Uint64(prefix+".nonce-rbf-soft-confs", defaultDataPosterConfig.NonceRbfSoftConfs, "the maximum probable reorg depth, used to determine when a transaction will no longer likely need replaced-by-fee")
	f.Bool(prefix+".allocate-mempool-balance", defaultDataPosterConfig.AllocateMempoolBalance, "if true, don't put transactions in the mempool that spend a total greater than the batch poster's balance")
	f.Bool(prefix+".use-db-storage", defaultDataPosterConfig.UseDBStorage, "uses database storage when enabled")
	f.Bool(prefix+".use-noop-storage", defaultDataPosterConfig.UseNoOpStorage, "uses noop storage, it doesn't store anything")
	f.Bool(prefix+".legacy-storage-encoding", defaultDataPosterConfig.LegacyStorageEncoding, "encodes items in a legacy way (as it was before dropping generics)")

	signature.SimpleHmacConfigAddOptions(prefix+".redis-signer", f)
	addDangerousOptions(prefix+".dangerous", f)
	addExternalSignerOptions(prefix+".external-signer", f)
}

func addDangerousOptions(prefix string, f *pflag.FlagSet) {
	f.Bool(prefix+".clear-dbstorage", DefaultDataPosterConfig.Dangerous.ClearDBStorage, "clear database storage")
}

func addExternalSignerOptions(prefix string, f *pflag.FlagSet) {
	f.String(prefix+".url", DefaultDataPosterConfig.ExternalSigner.URL, "external signer url")
	f.String(prefix+".address", DefaultDataPosterConfig.ExternalSigner.Address, "external signer address")
	f.String(prefix+".method", DefaultDataPosterConfig.ExternalSigner.Method, "external signer method")
	f.String(prefix+".root-ca", DefaultDataPosterConfig.ExternalSigner.RootCA, "external signer root CA")
	f.String(prefix+".client-cert", DefaultDataPosterConfig.ExternalSigner.ClientCert, "rpc client cert")
	f.String(prefix+".client-private-key", DefaultDataPosterConfig.ExternalSigner.ClientPrivateKey, "rpc client private key")
}

var DefaultDataPosterConfig = DataPosterConfig{
	ReplacementTimes:       "5m,10m,20m,30m,1h,2h,4h,6h,8h,12h,16h,18h,20h,22h",
	WaitForL1Finality:      true,
	TargetPriceGwei:        60.,
	UrgencyGwei:            2.,
	MaxMempoolTransactions: 10,
	MinTipCapGwei:          0.05,
	MaxTipCapGwei:          5,
	NonceRbfSoftConfs:      1,
	AllocateMempoolBalance: true,
	UseDBStorage:           true,
	UseNoOpStorage:         false,
	LegacyStorageEncoding:  false,
	Dangerous:              DangerousConfig{ClearDBStorage: false},
	ExternalSigner:         ExternalSignerCfg{Method: "eth_signTransaction"},
}

var DefaultDataPosterConfigForValidator = func() DataPosterConfig {
	config := DefaultDataPosterConfig
	config.MaxMempoolTransactions = 1 // the validator cannot queue transactions
	return config
}()

var TestDataPosterConfig = DataPosterConfig{
	ReplacementTimes:       "1s,2s,5s,10s,20s,30s,1m,5m",
	RedisSigner:            signature.TestSimpleHmacConfig,
	WaitForL1Finality:      false,
	TargetPriceGwei:        60.,
	UrgencyGwei:            2.,
	MaxMempoolTransactions: 10,
	MinTipCapGwei:          0.05,
	MaxTipCapGwei:          5,
	NonceRbfSoftConfs:      1,
	AllocateMempoolBalance: true,
	UseDBStorage:           false,
	UseNoOpStorage:         false,
	LegacyStorageEncoding:  false,
	ExternalSigner:         ExternalSignerCfg{Method: "eth_signTransaction"},
}

var TestDataPosterConfigForValidator = func() DataPosterConfig {
	config := TestDataPosterConfig
	config.MaxMempoolTransactions = 1 // the validator cannot queue transactions
	return config
}()

'''
'''--- arbnode/dataposter/dataposter_test.go ---
package dataposter

import (
	"context"
	"crypto/tls"
	"crypto/x509"
	"encoding/json"
	"fmt"
	"io"
	"math/big"
	"net/http"
	"os"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/keystore"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/ethereum/go-ethereum/signer/core/apitypes"
	"github.com/google/go-cmp/cmp"
)

func TestParseReplacementTimes(t *testing.T) {
	for _, tc := range []struct {
		desc, replacementTimes string
		want                   []time.Duration
		wantErr                bool
	}{
		{
			desc:             "valid case",
			replacementTimes: "1s,2s,1m,5m",
			want: []time.Duration{
				time.Duration(time.Second),
				time.Duration(2 * time.Second),
				time.Duration(time.Minute),
				time.Duration(5 * time.Minute),
				time.Duration(time.Hour * 24 * 365 * 10),
			},
		},
		{
			desc:             "non-increasing replacement times",
			replacementTimes: "1s,2s,1m,5m,1s",
			wantErr:          true,
		},
	} {
		t.Run(tc.desc, func(t *testing.T) {
			got, err := parseReplacementTimes(tc.replacementTimes)
			if gotErr := (err != nil); gotErr != tc.wantErr {
				t.Fatalf("Got error: %t, want: %t", gotErr, tc.wantErr)
			}
			if diff := cmp.Diff(tc.want, got); diff != "" {
				t.Errorf("parseReplacementTimes(%s) unexpected diff:\n%s", tc.replacementTimes, diff)
			}
		})
	}
}

func TestExternalSigner(t *testing.T) {
	ctx := context.Background()
	httpSrv, srv := newServer(ctx, t)
	t.Cleanup(func() {
		if err := httpSrv.Shutdown(ctx); err != nil {
			t.Fatalf("Error shutting down http server: %v", err)
		}
	})
	cert, key := "./testdata/localhost.crt", "./testdata/localhost.key"
	go func() {
		fmt.Println("Server is listening on port 1234...")
		if err := httpSrv.ListenAndServeTLS(cert, key); err != nil && err != http.ErrServerClosed {
			t.Errorf("ListenAndServeTLS() unexpected error:  %v", err)
			return
		}
	}()
	signer, addr, err := externalSigner(ctx,
		&ExternalSignerCfg{
			Address:          srv.address.Hex(),
			URL:              "https://localhost:1234",
			Method:           "test_signTransaction",
			RootCA:           cert,
			ClientCert:       "./testdata/client.crt",
			ClientPrivateKey: "./testdata/client.key",
		})
	if err != nil {
		t.Fatalf("Error getting external signer: %v", err)
	}
	tx := types.NewTransaction(13, common.HexToAddress("0x01"), big.NewInt(1), 2, big.NewInt(3), []byte{0x01, 0x02, 0x03})
	got, err := signer(ctx, addr, tx)
	if err != nil {
		t.Fatalf("Error signing transaction with external signer: %v", err)
	}
	want, err := srv.signerFn(addr, tx)
	if err != nil {
		t.Fatalf("Error signing transaction: %v", err)
	}
	if diff := cmp.Diff(want.Hash(), got.Hash()); diff != "" {
		t.Errorf("Signing transaction: unexpected diff: %v\n", diff)
	}
}

type server struct {
	handlers map[string]func(*json.RawMessage) (string, error)
	signerFn bind.SignerFn
	address  common.Address
}

type request struct {
	ID     *json.RawMessage `json:"id"`
	Method string           `json:"method"`
	Params *json.RawMessage `json:"params"`
}

type response struct {
	ID     *json.RawMessage `json:"id"`
	Result string           `json:"result,omitempty"`
}

// newServer returns http server and server struct that implements RPC methods.
// It sets up an account in temporary directory and cleans up after test is
// done.
func newServer(ctx context.Context, t *testing.T) (*http.Server, *server) {
	t.Helper()
	signer, address, err := setupAccount("/tmp/keystore")
	if err != nil {
		t.Fatalf("Error setting up account: %v", err)
	}
	t.Cleanup(func() { os.RemoveAll("/tmp/keystore") })

	s := &server{signerFn: signer, address: address}
	s.handlers = map[string]func(*json.RawMessage) (string, error){
		"test_signTransaction": s.signTransaction,
	}
	m := http.NewServeMux()

	clientCert, err := os.ReadFile("./testdata/client.crt")
	if err != nil {
		t.Fatalf("Error reading client certificate: %v", err)
	}
	pool := x509.NewCertPool()
	pool.AppendCertsFromPEM(clientCert)

	httpSrv := &http.Server{
		Addr:        ":1234",
		Handler:     m,
		ReadTimeout: 5 * time.Second,
		TLSConfig: &tls.Config{
			MinVersion: tls.VersionTLS12,
			ClientAuth: tls.RequireAndVerifyClientCert,
			ClientCAs:  pool,
		},
	}
	m.HandleFunc("/", s.mux)
	return httpSrv, s
}

// setupAccount creates a new account in a given directory, unlocks it, creates
// signer with that account and returns it along with account address.
func setupAccount(dir string) (bind.SignerFn, common.Address, error) {
	ks := keystore.NewKeyStore(
		dir,
		keystore.StandardScryptN,
		keystore.StandardScryptP,
	)
	a, err := ks.NewAccount("password")
	if err != nil {
		return nil, common.Address{}, fmt.Errorf("creating account account: %w", err)
	}
	if err := ks.Unlock(a, "password"); err != nil {
		return nil, common.Address{}, fmt.Errorf("unlocking account: %w", err)
	}
	txOpts, err := bind.NewKeyStoreTransactorWithChainID(ks, a, big.NewInt(1))
	if err != nil {
		return nil, common.Address{}, fmt.Errorf("creating transactor: %w", err)
	}
	return txOpts.Signer, a.Address, nil
}

// UnmarshallFirst unmarshalls slice of params and returns the first one.
// Parameters in Go ethereum RPC calls are marashalled as slices. E.g.
// eth_sendRawTransaction or eth_signTransaction, marshall transaction as a
// slice of transactions in a message:
// https://github.com/ethereum/go-ethereum/blob/0004c6b229b787281760b14fb9460ffd9c2496f1/rpc/client.go#L548
func unmarshallFirst(params []byte) (*types.Transaction, error) {
	var arr []apitypes.SendTxArgs
	if err := json.Unmarshal(params, &arr); err != nil {
		return nil, fmt.Errorf("unmarshaling first param: %w", err)
	}
	if len(arr) != 1 {
		return nil, fmt.Errorf("argument should be a single transaction, but got: %d", len(arr))
	}
	return arr[0].ToTransaction(), nil
}

func (s *server) signTransaction(params *json.RawMessage) (string, error) {
	tx, err := unmarshallFirst(*params)
	if err != nil {
		return "", err
	}
	signedTx, err := s.signerFn(s.address, tx)
	if err != nil {
		return "", fmt.Errorf("signing transaction: %w", err)
	}
	data, err := rlp.EncodeToBytes(signedTx)
	if err != nil {
		return "", fmt.Errorf("rlp encoding transaction: %w", err)
	}
	return hexutil.Encode(data), nil
}

func (s *server) mux(w http.ResponseWriter, r *http.Request) {
	body, err := io.ReadAll(r.Body)
	if err != nil {
		http.Error(w, "can't read body", http.StatusBadRequest)
		return
	}
	var req request
	if err := json.Unmarshal(body, &req); err != nil {
		http.Error(w, "can't unmarshal JSON request", http.StatusBadRequest)
		return
	}
	method, ok := s.handlers[req.Method]
	if !ok {
		http.Error(w, "method not found", http.StatusNotFound)
		return
	}
	result, err := method(req.Params)
	if err != nil {
		fmt.Printf("error calling method: %v\n", err)
		http.Error(w, "error calling method", http.StatusInternalServerError)
		return
	}
	resp := response{ID: req.ID, Result: result}
	respBytes, err := json.Marshal(resp)
	if err != nil {
		http.Error(w, fmt.Sprintf("error encoding response: %v", err), http.StatusInternalServerError)
		return
	}
	w.Header().Set("Content-Type", "application/json")
	if _, err := w.Write(respBytes); err != nil {
		fmt.Printf("error writing response: %v\n", err)
	}
}

'''
'''--- arbnode/dataposter/dbstorage/storage.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package dbstorage

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"strconv"

	"github.com/cockroachdb/pebble"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/ethdb/memorydb"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/syndtr/goleveldb/leveldb"
)

// Storage implements db based storage for batch poster.
type Storage struct {
	db     ethdb.Database
	encDec storage.EncoderDecoderF
}

var (
	// Value at this index holds the *index* of last item.
	// Keys that we never want to be accidentally deleted by "Prune()" should be
	// lexicographically less than minimum index (that is "0"), hence the prefix
	// ".".
	lastItemIdxKey = []byte(".last_item_idx_key")
	countKey       = []byte(".count_key")
)

func New(db ethdb.Database, enc storage.EncoderDecoderF) *Storage {
	return &Storage{db: db, encDec: enc}
}

func idxToKey(idx uint64) []byte {
	return []byte(fmt.Sprintf("%020d", idx))
}

func (s *Storage) FetchContents(_ context.Context, startingIndex uint64, maxResults uint64) ([]*storage.QueuedTransaction, error) {
	var res []*storage.QueuedTransaction
	it := s.db.NewIterator([]byte(""), idxToKey(startingIndex))
	defer it.Release()
	for i := 0; i < int(maxResults); i++ {
		if !it.Next() {
			break
		}
		item, err := s.encDec().Decode(it.Value())
		if err != nil {
			return nil, err
		}
		res = append(res, item)
	}
	return res, it.Error()
}

func (s *Storage) lastItemIdx(context.Context) ([]byte, error) {
	return s.db.Get(lastItemIdxKey)
}

func (s *Storage) FetchLast(ctx context.Context) (*storage.QueuedTransaction, error) {
	size, err := s.Length(ctx)
	if err != nil {
		return nil, err
	}
	if size == 0 {
		return nil, nil
	}
	lastItemIdx, err := s.lastItemIdx(ctx)
	if err != nil {
		return nil, fmt.Errorf("getting last item index: %w", err)
	}
	val, err := s.db.Get(lastItemIdx)
	if err != nil {
		return nil, err
	}
	return s.encDec().Decode(val)
}

func (s *Storage) PruneAll(ctx context.Context) error {
	idx, err := s.lastItemIdx(ctx)
	if err != nil {
		return fmt.Errorf("pruning all keys: %w", err)
	}
	until, err := strconv.Atoi(string(idx))
	if err != nil {
		return fmt.Errorf("converting last item index bytes to integer: %w", err)
	}
	return s.Prune(ctx, uint64(until+1))
}

func (s *Storage) Prune(ctx context.Context, until uint64) error {
	cnt, err := s.Length(ctx)
	if err != nil {
		return err
	}
	end := idxToKey(until)
	it := s.db.NewIterator([]byte{}, idxToKey(0))
	defer it.Release()
	b := s.db.NewBatch()
	for it.Next() {
		if bytes.Compare(it.Key(), end) >= 0 {
			break
		}
		if err := b.Delete(it.Key()); err != nil {
			return fmt.Errorf("deleting key: %w", err)
		}
		cnt--
	}
	if err := b.Put(countKey, []byte(strconv.Itoa(cnt))); err != nil {
		return fmt.Errorf("updating length counter: %w", err)
	}
	return b.Write()
}

// valueAt gets returns the value at key. If it doesn't exist then it returns
// encoded bytes of nil.
func (s *Storage) valueAt(_ context.Context, key []byte) ([]byte, error) {
	val, err := s.db.Get(key)
	if err != nil {
		if isErrNotFound(err) {
			return s.encDec().Encode((*storage.QueuedTransaction)(nil))
		}
		return nil, err
	}
	return val, nil
}

func (s *Storage) Put(ctx context.Context, index uint64, prev, new *storage.QueuedTransaction) error {
	key := idxToKey(index)
	stored, err := s.valueAt(ctx, key)
	if err != nil {
		return err
	}
	prevEnc, err := s.encDec().Encode(prev)
	if err != nil {
		return fmt.Errorf("encoding previous item: %w", err)
	}
	if !bytes.Equal(stored, prevEnc) {
		return fmt.Errorf("replacing different item than expected at index: %v, stored: %v, prevEnc: %v", index, stored, prevEnc)
	}
	newEnc, err := s.encDec().Encode(new)
	if err != nil {
		return fmt.Errorf("encoding new item: %w", err)
	}
	b := s.db.NewBatch()
	cnt, err := s.Length(ctx)
	if err != nil {
		return err
	}
	if err := b.Put(key, newEnc); err != nil {
		return fmt.Errorf("updating value at: %v: %w", key, err)
	}
	lastItemIdx, err := s.lastItemIdx(ctx)
	if err != nil && !isErrNotFound(err) {
		return err
	}
	if isErrNotFound(err) {
		lastItemIdx = []byte{}
	}
	if cnt == 0 || bytes.Compare(key, lastItemIdx) > 0 {
		if err := b.Put(lastItemIdxKey, key); err != nil {
			return fmt.Errorf("updating last item: %w", err)
		}
		if err := b.Put(countKey, []byte(strconv.Itoa(cnt+1))); err != nil {
			return fmt.Errorf("updating length counter: %w", err)
		}
	}
	return b.Write()
}

func (s *Storage) Length(context.Context) (int, error) {
	val, err := s.db.Get(countKey)
	if err != nil {
		if isErrNotFound(err) {
			return 0, nil
		}
		return 0, err
	}
	return strconv.Atoi(string(val))
}

func (s *Storage) IsPersistent() bool {
	return true
}

func isErrNotFound(err error) bool {
	return errors.Is(err, leveldb.ErrNotFound) || errors.Is(err, pebble.ErrNotFound) || errors.Is(err, memorydb.ErrMemorydbNotFound)
}

'''
'''--- arbnode/dataposter/noop/storage.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE
package noop

import (
	"context"

	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
)

// Storage implements noop storage for dataposter. This is for clients that want
// to have option to directly post to geth without keeping state.
type Storage struct{}

func (s *Storage) FetchContents(_ context.Context, _, _ uint64) ([]*storage.QueuedTransaction, error) {
	return nil, nil
}

func (s *Storage) FetchLast(ctx context.Context) (*storage.QueuedTransaction, error) {
	return nil, nil
}

func (s *Storage) Prune(_ context.Context, _ uint64) error {
	return nil
}

func (s *Storage) Put(_ context.Context, _ uint64, _, _ *storage.QueuedTransaction) error {
	return nil
}

func (s *Storage) Length(context.Context) (int, error) {
	return 0, nil
}

func (s *Storage) IsPersistent() bool {
	return false
}

'''
'''--- arbnode/dataposter/redis/redisstorage.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package redis

import (
	"bytes"
	"context"
	"errors"
	"fmt"

	"github.com/go-redis/redis/v8"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/util/signature"
)

// Storage implements redis sorted set backed storage. It does not support
// duplicate keys or values. That is, putting the same element on different
// indexes will not yield expected behavior.
// More  at: https://redis.io/commands/zadd/.
type Storage struct {
	client redis.UniversalClient
	signer *signature.SimpleHmac
	key    string
	encDec storage.EncoderDecoderF
}

func NewStorage(client redis.UniversalClient, key string, signerConf *signature.SimpleHmacConfig, enc storage.EncoderDecoderF) (*Storage, error) {
	signer, err := signature.NewSimpleHmac(signerConf)
	if err != nil {
		return nil, err
	}
	return &Storage{client, signer, key, enc}, nil
}

func joinHmacMsg(msg []byte, sig []byte) ([]byte, error) {
	if len(sig) != 32 {
		return nil, errors.New("signature is wrong length")
	}
	return append(sig, msg...), nil
}

func (s *Storage) peelVerifySignature(data []byte) ([]byte, error) {
	if len(data) < 32 {
		return nil, errors.New("data is too short to contain message signature")
	}

	err := s.signer.VerifySignature(data[:32], data[32:])
	if err != nil {
		return nil, err
	}
	return data[32:], nil
}

func (s *Storage) FetchContents(ctx context.Context, startingIndex uint64, maxResults uint64) ([]*storage.QueuedTransaction, error) {
	query := redis.ZRangeArgs{
		Key:     s.key,
		ByScore: true,
		Start:   startingIndex,
		Stop:    startingIndex + maxResults - 1,
	}
	itemStrings, err := s.client.ZRangeArgs(ctx, query).Result()
	if err != nil {
		return nil, err
	}
	var items []*storage.QueuedTransaction
	for _, itemString := range itemStrings {
		data, err := s.peelVerifySignature([]byte(itemString))
		if err != nil {
			return nil, err
		}
		item, err := s.encDec().Decode(data)
		if err != nil {
			return nil, err
		}
		items = append(items, item)
	}
	return items, nil
}

func (s *Storage) FetchLast(ctx context.Context) (*storage.QueuedTransaction, error) {
	query := redis.ZRangeArgs{
		Key:   s.key,
		Start: 0,
		Stop:  0,
		Rev:   true,
	}
	itemStrings, err := s.client.ZRangeArgs(ctx, query).Result()
	if err != nil {
		return nil, err
	}
	if len(itemStrings) > 1 {
		return nil, fmt.Errorf("expected only one return value for GetLast but got %v", len(itemStrings))
	}
	var ret *storage.QueuedTransaction
	if len(itemStrings) > 0 {
		data, err := s.peelVerifySignature([]byte(itemStrings[0]))
		if err != nil {
			return nil, err
		}
		item, err := s.encDec().Decode(data)
		if err != nil {
			return nil, err
		}
		ret = item
	}
	return ret, nil
}

func (s *Storage) Prune(ctx context.Context, until uint64) error {
	if until > 0 {
		return s.client.ZRemRangeByScore(ctx, s.key, "-inf", fmt.Sprintf("%v", until-1)).Err()
	}
	return nil
}

// normalizeDecoding decodes data (regardless of what encoding it used), and
// encodes it according to current encoding for storage.
// As a result, encoded data is transformed to currently used encoding.
func (s *Storage) normalizeDecoding(data []byte) ([]byte, error) {
	item, err := s.encDec().Decode(data)
	if err != nil {
		return nil, err
	}
	return s.encDec().Encode(item)
}

func (s *Storage) Put(ctx context.Context, index uint64, prev, new *storage.QueuedTransaction) error {
	if new == nil {
		return fmt.Errorf("tried to insert nil item at index %v", index)
	}
	action := func(tx *redis.Tx) error {
		query := redis.ZRangeArgs{
			Key:     s.key,
			ByScore: true,
			Start:   index,
			Stop:    index,
		}
		haveItems, err := s.client.ZRangeArgs(ctx, query).Result()
		if err != nil {
			return err
		}
		pipe := tx.TxPipeline()
		if len(haveItems) == 0 {
			if prev != nil {
				return fmt.Errorf("%w: tried to replace item at index %v but no item exists there", storage.ErrStorageRace, index)
			}
		} else if len(haveItems) == 1 {
			if prev == nil {
				return fmt.Errorf("%w: tried to insert new item at index %v but an item exists there", storage.ErrStorageRace, index)
			}
			verifiedItem, err := s.peelVerifySignature([]byte(haveItems[0]))
			if err != nil {
				return fmt.Errorf("failed to validate item already in redis at index%v: %w", index, err)
			}
			verifiedItem, err = s.normalizeDecoding(verifiedItem)
			if err != nil {
				return fmt.Errorf("error normalizing encoding for verified item: %w", err)
			}
			prevItemEncoded, err := s.encDec().Encode(prev)
			if err != nil {
				return err
			}
			if !bytes.Equal(verifiedItem, prevItemEncoded) {
				return fmt.Errorf("%w: replacing different item than expected at index %v", storage.ErrStorageRace, index)
			}
			if err := pipe.ZRem(ctx, s.key, haveItems[0]).Err(); err != nil {
				return err
			}
		} else {
			return fmt.Errorf("expected only one return value for Put but got %v", len(haveItems))
		}
		newItemEncoded, err := s.encDec().Encode(new)
		if err != nil {
			return err
		}
		sig, err := s.signer.SignMessage(newItemEncoded)
		if err != nil {
			return err
		}
		signedItem, err := joinHmacMsg(newItemEncoded, sig)
		if err != nil {
			return err
		}
		if err := pipe.ZAdd(ctx, s.key, &redis.Z{
			Score:  float64(index),
			Member: string(signedItem),
		}).Err(); err != nil {
			return err
		}
		_, err = pipe.Exec(ctx)
		if errors.Is(err, redis.TxFailedErr) {
			// Unfortunately, we can't wrap two errors.
			//nolint:errorlint
			err = fmt.Errorf("%w: %v", storage.ErrStorageRace, err.Error())
		}
		return err
	}
	// WATCH works with sorted sets: https://redis.io/docs/manual/transactions/#using-watch-to-implement-zpop
	return s.client.Watch(ctx, action, s.key)
}

func (s *Storage) Length(ctx context.Context) (int, error) {
	count, err := s.client.ZCount(ctx, s.key, "-inf", "+inf").Result()
	if err != nil {
		return 0, err
	}
	return int(count), nil
}

func (s *Storage) IsPersistent() bool {
	return true
}

'''
'''--- arbnode/dataposter/slice/slicestorage.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package slice

import (
	"bytes"
	"context"
	"errors"
	"fmt"

	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
)

type Storage struct {
	firstNonce uint64
	queue      [][]byte
	encDec     func() storage.EncoderDecoderInterface
}

func NewStorage(encDec func() storage.EncoderDecoderInterface) *Storage {
	return &Storage{encDec: encDec}
}

func (s *Storage) FetchContents(_ context.Context, startingIndex uint64, maxResults uint64) ([]*storage.QueuedTransaction, error) {
	txs := s.queue
	if startingIndex >= s.firstNonce+uint64(len(s.queue)) || maxResults == 0 {
		return nil, nil
	}
	if startingIndex > s.firstNonce {
		txs = txs[startingIndex-s.firstNonce:]
	}
	if uint64(len(txs)) > maxResults {
		txs = txs[:maxResults]
	}
	var res []*storage.QueuedTransaction
	for _, r := range txs {
		item, err := s.encDec().Decode(r)
		if err != nil {
			return nil, err
		}
		res = append(res, item)
	}
	return res, nil
}

func (s *Storage) FetchLast(context.Context) (*storage.QueuedTransaction, error) {
	if len(s.queue) == 0 {
		return nil, nil
	}
	return s.encDec().Decode(s.queue[len(s.queue)-1])
}

func (s *Storage) Prune(_ context.Context, until uint64) error {
	if until >= s.firstNonce+uint64(len(s.queue)) {
		s.queue = nil
	} else if until >= s.firstNonce {
		s.queue = s.queue[until-s.firstNonce:]
		s.firstNonce = until
	}
	return nil
}

func (s *Storage) Put(_ context.Context, index uint64, prev, new *storage.QueuedTransaction) error {
	if new == nil {
		return fmt.Errorf("tried to insert nil item at index %v", index)
	}
	newEnc, err := s.encDec().Encode(new)
	if err != nil {
		return fmt.Errorf("encoding new item: %w", err)
	}
	if len(s.queue) == 0 {
		if prev != nil {
			return errors.New("prevItem isn't nil but queue is empty")
		}
		s.queue = append(s.queue, newEnc)
		s.firstNonce = index
	} else if index == s.firstNonce+uint64(len(s.queue)) {
		if prev != nil {
			return errors.New("prevItem isn't nil but item is just after end of queue")
		}
		s.queue = append(s.queue, newEnc)
	} else if index >= s.firstNonce {
		queueIdx := int(index - s.firstNonce)
		if queueIdx > len(s.queue) {
			return fmt.Errorf("attempted to set out-of-bounds index %v in queue starting at %v of length %v", index, s.firstNonce, len(s.queue))
		}
		prevEnc, err := s.encDec().Encode(prev)
		if err != nil {
			return fmt.Errorf("encoding previous item: %w", err)
		}
		if !bytes.Equal(prevEnc, s.queue[queueIdx]) {
			return fmt.Errorf("replacing different item than expected at index: %v, stored: %v, prevEnc: %v", index, s.queue[queueIdx], prevEnc)
		}
		s.queue[queueIdx] = newEnc
	} else {
		return fmt.Errorf("attempted to set too low index %v in queue starting at %v", index, s.firstNonce)
	}
	return nil
}

func (s *Storage) Length(context.Context) (int, error) {
	return len(s.queue), nil
}

func (s *Storage) IsPersistent() bool {
	return false
}

'''
'''--- arbnode/dataposter/storage/storage.go ---
package storage

import (
	"errors"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/offchainlabs/nitro/arbutil"
)

var (
	ErrStorageRace = errors.New("storage race error")

	BlockValidatorPrefix string = "v" // the prefix for all block validator keys
	StakerPrefix         string = "S" // the prefix for all staker keys
	BatchPosterPrefix    string = "b" // the prefix for all batch poster keys
	// TODO(anodar): move everything else from schema.go file to here once
	// execution split is complete.
)

type QueuedTransaction struct {
	FullTx          *types.Transaction
	Data            types.DynamicFeeTx
	Meta            []byte
	Sent            bool
	Created         time.Time // may be earlier than the tx was given to the tx poster
	NextReplacement time.Time
}

// LegacyQueuedTransaction is used for backwards compatibility.
// Before https://github.com/OffchainLabs/nitro/pull/1773: the queuedTransaction
// looked like this and was rlp encoded directly. After the pr, we are store
// rlp encoding of Meta into queuedTransaction and rlp encoding it once more
// to store it.
type LegacyQueuedTransaction struct {
	FullTx          *types.Transaction
	Data            types.DynamicFeeTx
	Meta            BatchPosterPosition
	Sent            bool
	Created         time.Time // may be earlier than the tx was given to the tx poster
	NextReplacement time.Time
}

// This is also for legacy reason. Since Batchposter is in arbnode package,
// we can't refer to BatchPosterPosition type there even if we export it (that
// would create cyclic dependency).
// We'll drop this struct in a few releases when we drop legacy encoding.
type BatchPosterPosition struct {
	MessageCount        arbutil.MessageIndex
	DelayedMessageCount uint64
	NextSeqNum          uint64
}

func DecodeLegacyQueuedTransaction(data []byte) (*LegacyQueuedTransaction, error) {
	var val LegacyQueuedTransaction
	if err := rlp.DecodeBytes(data, &val); err != nil {
		return nil, fmt.Errorf("decoding legacy queued transaction: %w", err)
	}
	return &val, nil
}

func LegacyToQueuedTransaction(legacyQT *LegacyQueuedTransaction) (*QueuedTransaction, error) {
	meta, err := rlp.EncodeToBytes(legacyQT.Meta)
	if err != nil {
		return nil, fmt.Errorf("converting legacy to queued transaction: %w", err)
	}
	return &QueuedTransaction{
		FullTx:          legacyQT.FullTx,
		Data:            legacyQT.Data,
		Meta:            meta,
		Sent:            legacyQT.Sent,
		Created:         legacyQT.Created,
		NextReplacement: legacyQT.NextReplacement,
	}, nil
}

func QueuedTransactionToLegacy(qt *QueuedTransaction) (*LegacyQueuedTransaction, error) {
	if qt == nil {
		return nil, nil
	}
	var meta BatchPosterPosition
	if qt.Meta != nil {
		if err := rlp.DecodeBytes(qt.Meta, &meta); err != nil {
			return nil, fmt.Errorf("converting queued transaction to legacy: %w", err)
		}
	}
	return &LegacyQueuedTransaction{
		FullTx:          qt.FullTx,
		Data:            qt.Data,
		Meta:            meta,
		Sent:            qt.Sent,
		Created:         qt.Created,
		NextReplacement: qt.NextReplacement,
	}, nil
}

// Decode tries to decode QueuedTransaction, if that fails it tries to decode
// into legacy queued transaction and converts to queued
func decode(data []byte) (*QueuedTransaction, error) {
	var item QueuedTransaction
	if err := rlp.DecodeBytes(data, &item); err != nil {
		log.Debug("Failed to decode QueuedTransaction, attempting to decide legacy queued transaction", "error", err)
		val, err := DecodeLegacyQueuedTransaction(data)
		if err != nil {
			return nil, fmt.Errorf("decoding legacy item: %w", err)
		}
		log.Debug("Succeeded decoding QueuedTransaction with legacy encoder")
		return LegacyToQueuedTransaction(val)
	}
	return &item, nil
}

type EncoderDecoder struct{}

func (e *EncoderDecoder) Encode(qt *QueuedTransaction) ([]byte, error) {
	return rlp.EncodeToBytes(qt)
}

func (e *EncoderDecoder) Decode(data []byte) (*QueuedTransaction, error) {
	return decode(data)
}

type LegacyEncoderDecoder struct{}

func (e *LegacyEncoderDecoder) Encode(qt *QueuedTransaction) ([]byte, error) {
	legacyQt, err := QueuedTransactionToLegacy(qt)
	if err != nil {
		return nil, fmt.Errorf("encoding legacy item: %w", err)
	}
	return rlp.EncodeToBytes(legacyQt)
}

func (le *LegacyEncoderDecoder) Decode(data []byte) (*QueuedTransaction, error) {
	return decode(data)
}

// Typically interfaces belong to where they are being used, not at implementing
// site, but this is used in all storages (besides no-op) and all of them
// require all the functions for this interface.
type EncoderDecoderInterface interface {
	Encode(*QueuedTransaction) ([]byte, error)
	Decode([]byte) (*QueuedTransaction, error)
}

// EncoderDecoderF is a function type that returns encoder/decoder interface.
// This is needed to implement hot-reloading flag to switch encoding/decoding
// strategy on the fly.
type EncoderDecoderF func() EncoderDecoderInterface

'''
'''--- arbnode/dataposter/storage_test.go ---
package dataposter

import (
	"context"
	"math/big"
	"path"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"
	"github.com/offchainlabs/nitro/arbnode/dataposter/dbstorage"
	"github.com/offchainlabs/nitro/arbnode/dataposter/redis"
	"github.com/offchainlabs/nitro/arbnode/dataposter/slice"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/signature"
)

var ignoreData = cmp.Options{
	cmpopts.IgnoreUnexported(
		types.Transaction{},
		types.DynamicFeeTx{},
		big.Int{},
	),
	cmpopts.IgnoreFields(types.Transaction{}, "hash", "size", "from"),
}

func newLevelDBStorage(t *testing.T, encF storage.EncoderDecoderF) *dbstorage.Storage {
	t.Helper()
	db, err := rawdb.NewLevelDBDatabase(path.Join(t.TempDir(), "level.db"), 0, 0, "default", false)
	if err != nil {
		t.Fatalf("NewLevelDBDatabase() unexpected error: %v", err)
	}
	return dbstorage.New(db, encF)
}

func newPebbleDBStorage(t *testing.T, encF storage.EncoderDecoderF) *dbstorage.Storage {
	t.Helper()
	db, err := rawdb.NewPebbleDBDatabase(path.Join(t.TempDir(), "pebble.db"), 0, 0, "default", false)
	if err != nil {
		t.Fatalf("NewPebbleDBDatabase() unexpected error: %v", err)
	}
	return dbstorage.New(db, encF)
}

func newSliceStorage(encF storage.EncoderDecoderF) *slice.Storage {
	return slice.NewStorage(encF)
}

func newRedisStorage(ctx context.Context, t *testing.T, encF storage.EncoderDecoderF) *redis.Storage {
	t.Helper()
	redisUrl := redisutil.CreateTestRedis(ctx, t)
	client, err := redisutil.RedisClientFromURL(redisUrl)
	if err != nil {
		t.Fatalf("RedisClientFromURL(%q) unexpected error: %v", redisUrl, err)
	}
	s, err := redis.NewStorage(client, "", &signature.TestSimpleHmacConfig, encF)
	if err != nil {
		t.Fatalf("redis.NewStorage() unexpected error: %v", err)
	}
	return s
}

func valueOf(t *testing.T, i int) *storage.QueuedTransaction {
	t.Helper()
	meta, err := rlp.EncodeToBytes(storage.BatchPosterPosition{DelayedMessageCount: uint64(i)})
	if err != nil {
		t.Fatalf("Encoding batch poster position, error: %v", err)
	}
	return &storage.QueuedTransaction{
		FullTx: types.NewTransaction(
			uint64(i),
			common.Address{},
			big.NewInt(int64(i)),
			uint64(i),
			big.NewInt(int64(i)),
			[]byte{byte(i)}),
		Meta: meta,
		Data: types.DynamicFeeTx{
			ChainID:    big.NewInt(int64(i)),
			Nonce:      uint64(i),
			GasTipCap:  big.NewInt(int64(i)),
			GasFeeCap:  big.NewInt(int64(i)),
			Gas:        uint64(i),
			Value:      big.NewInt(int64(i)),
			Data:       []byte{byte(i % 8)},
			AccessList: types.AccessList{},
			V:          big.NewInt(int64(i)),
			R:          big.NewInt(int64(i)),
			S:          big.NewInt(int64(i)),
		},
	}
}

func values(t *testing.T, from, to int) []*storage.QueuedTransaction {
	var res []*storage.QueuedTransaction
	for i := from; i <= to; i++ {
		res = append(res, valueOf(t, i))
	}
	return res
}

// Initializes the QueueStorage. Returns the same object (for convenience).
func initStorage(ctx context.Context, t *testing.T, s QueueStorage) QueueStorage {
	t.Helper()
	for i := 0; i < 20; i++ {
		if err := s.Put(ctx, uint64(i), nil, valueOf(t, i)); err != nil {
			t.Fatalf("Error putting a key/value: %v", err)
		}
	}
	return s
}

// Returns a map of all empty storages.
func storages(t *testing.T) map[string]QueueStorage {
	t.Helper()
	f := func(enc storage.EncoderDecoderInterface) storage.EncoderDecoderF {
		return func() storage.EncoderDecoderInterface {
			return enc
		}
	}
	return map[string]QueueStorage{
		"levelDBLegacy": newLevelDBStorage(t, f(&storage.LegacyEncoderDecoder{})),
		"sliceLegacy":   newSliceStorage(f(&storage.LegacyEncoderDecoder{})),
		"redisLegacy":   newRedisStorage(context.Background(), t, f(&storage.LegacyEncoderDecoder{})),
		"levelDB":       newLevelDBStorage(t, f(&storage.EncoderDecoder{})),
		"pebbleDB":      newPebbleDBStorage(t, f(&storage.EncoderDecoder{})),
		"slice":         newSliceStorage(f(&storage.EncoderDecoder{})),
		"redis":         newRedisStorage(context.Background(), t, f(&storage.EncoderDecoder{})),
	}
}

// Returns a map of all initialized storages.
func initStorages(ctx context.Context, t *testing.T) map[string]QueueStorage {
	t.Helper()
	m := map[string]QueueStorage{}
	for k, v := range storages(t) {
		m[k] = initStorage(ctx, t, v)
	}
	return m
}

func TestPruneAll(t *testing.T) {
	s := newLevelDBStorage(t, func() storage.EncoderDecoderInterface { return &storage.EncoderDecoder{} })
	ctx := context.Background()
	for i := 0; i < 20; i++ {
		if err := s.Put(ctx, uint64(i), nil, valueOf(t, i)); err != nil {
			t.Fatalf("Error putting a key/value: %v", err)
		}
	}
	size, err := s.Length(ctx)
	if err != nil {
		t.Fatalf("Length() unexpected error %v", err)
	}
	if size != 20 {
		t.Errorf("Length()=%v want 20", size)
	}
	if err := s.PruneAll(ctx); err != nil {
		t.Fatalf("PruneAll() unexpected error: %v", err)
	}
	size, err = s.Length(ctx)
	if err != nil {
		t.Fatalf("Length() unexpected error %v", err)
	}
	if size != 0 {
		t.Errorf("Length()=%v want 0", size)
	}
}

func TestFetchContents(t *testing.T) {
	ctx := context.Background()
	for name, s := range initStorages(ctx, t) {
		for _, tc := range []struct {
			desc       string
			startIdx   uint64
			maxResults uint64
			want       []*storage.QueuedTransaction
		}{
			{
				desc:       "sequence with single digits",
				startIdx:   5,
				maxResults: 3,
				want:       values(t, 5, 7),
			},
			{
				desc:       "corner case of single element",
				startIdx:   0,
				maxResults: 1,
				want:       values(t, 0, 0),
			},
			{
				desc:       "no elements",
				startIdx:   3,
				maxResults: 0,
			},
			{
				// Making sure it's correctly ordered lexicographically.
				desc:       "sequence with variable number of digits",
				startIdx:   9,
				maxResults: 3,
				want:       values(t, 9, 11),
			},
			{
				desc:       "max results goes over the last element",
				startIdx:   13,
				maxResults: 10,
				want:       values(t, 13, 19),
			},
		} {
			t.Run(name+"_"+tc.desc, func(t *testing.T) {
				values, err := s.FetchContents(ctx, tc.startIdx, tc.maxResults)
				if err != nil {
					t.Fatalf("FetchContents(%d, %d) unexpected error: %v", tc.startIdx, tc.maxResults, err)
				}
				if diff := cmp.Diff(tc.want, values, ignoreData); diff != "" {
					t.Errorf("FetchContents(%d, %d) unexpected diff:\n%s", tc.startIdx, tc.maxResults, diff)
				}
			})
		}
	}
}

func TestLast(t *testing.T) {
	cnt := 100
	for name, s := range storages(t) {
		t.Run(name, func(t *testing.T) {
			ctx := context.Background()
			for i := 0; i < cnt; i++ {
				val := valueOf(t, i)
				if err := s.Put(ctx, uint64(i), nil, val); err != nil {
					t.Fatalf("Error putting a key/value: %v", err)
				}
				got, err := s.FetchLast(ctx)
				if err != nil {
					t.Fatalf("Error getting a last element: %v", err)
				}
				if diff := cmp.Diff(val, got, ignoreData); diff != "" {
					t.Errorf("FetchLast() unexpected diff:\n%s", diff)
				}

			}
		})
		last := valueOf(t, cnt-1)
		t.Run(name+"_update_entries", func(t *testing.T) {
			ctx := context.Background()
			for i := 0; i < cnt-1; i++ {
				prev := valueOf(t, i)
				newVal := valueOf(t, cnt+i)
				if err := s.Put(ctx, uint64(i), prev, newVal); err != nil {
					t.Fatalf("Error putting a key/value: %v, prev: %v, new: %v", err, prev, newVal)
				}
				got, err := s.FetchLast(ctx)
				if err != nil {
					t.Fatalf("Error getting a last element: %v", err)
				}
				if diff := cmp.Diff(last, got, ignoreData); diff != "" {
					t.Errorf("FetchLast() unexpected diff:\n%s", diff)
				}
				gotCnt, err := s.Length(ctx)
				if err != nil {
					t.Fatalf("Length() unexpected error: %v", err)
				}
				if gotCnt != cnt {
					t.Errorf("Length() = %d want %d", gotCnt, cnt)
				}
			}
		})
	}
}

func TestPrune(t *testing.T) {
	ctx := context.Background()
	for _, tc := range []struct {
		desc      string
		pruneFrom uint64
		want      []*storage.QueuedTransaction
	}{
		{
			desc:      "prune all elements",
			pruneFrom: 20,
		},
		{
			desc:      "prune all but one",
			pruneFrom: 19,
			want:      values(t, 19, 19),
		},
		{
			desc:      "pruning first element",
			pruneFrom: 1,
			want:      values(t, 1, 19),
		},
		{
			desc:      "pruning first 11 elements",
			pruneFrom: 11,
			want:      values(t, 11, 19),
		},
		{
			desc:      "pruning from higher than biggest index",
			pruneFrom: 30,
		},
	} {
		// Storages must be re-initialized in each test-case.
		for name, s := range initStorages(ctx, t) {
			t.Run(name+"_"+tc.desc, func(t *testing.T) {
				if err := s.Prune(ctx, tc.pruneFrom); err != nil {
					t.Fatalf("Prune(%d) unexpected error: %v", tc.pruneFrom, err)
				}
				got, err := s.FetchContents(ctx, 0, 20)
				if err != nil {
					t.Fatalf("FetchContents() unexpected error: %v", err)
				}
				if diff := cmp.Diff(tc.want, got, ignoreData); diff != "" {
					t.Errorf("Prune(%d) unexpected diff:\n%s", tc.pruneFrom, diff)
				}
			})
		}
	}
}

func TestLength(t *testing.T) {
	ctx := context.Background()
	for _, tc := range []struct {
		desc      string
		pruneFrom uint64
	}{
		{
			desc: "not prune any elements",
		},
		{
			desc:      "prune all but one",
			pruneFrom: 19,
		},
		{
			desc:      "pruning first element",
			pruneFrom: 1,
		},
		{
			desc:      "pruning first 11 elements",
			pruneFrom: 11,
		},
		{
			desc:      "pruning from higher than biggest index",
			pruneFrom: 30,
		},
	} {
		// Storages must be re-initialized in each test-case.
		for name, s := range initStorages(ctx, t) {
			t.Run(name+"_"+tc.desc, func(t *testing.T) {
				if err := s.Prune(ctx, tc.pruneFrom); err != nil {
					t.Fatalf("Prune(%d) unexpected error: %v", tc.pruneFrom, err)
				}
				got, err := s.Length(ctx)
				if err != nil {
					t.Fatalf("Length() unexpected error: %v", err)
				}
				if want := arbmath.MaxInt(0, 20-int(tc.pruneFrom)); got != want {
					t.Errorf("Length() = %d want %d", got, want)
				}
			})
		}

	}
}

'''
'''--- arbnode/delayed.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"bytes"
	"context"
	"errors"
	"math/big"
	"sort"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

var messageDeliveredID common.Hash
var inboxMessageDeliveredID common.Hash
var inboxMessageFromOriginID common.Hash
var l2MessageFromOriginCallABI abi.Method

func init() {
	parsedIBridgeABI, err := bridgegen.IBridgeMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	messageDeliveredID = parsedIBridgeABI.Events["MessageDelivered"].ID

	parsedIMessageProviderABI, err := bridgegen.IDelayedMessageProviderMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	inboxMessageDeliveredID = parsedIMessageProviderABI.Events["InboxMessageDelivered"].ID
	inboxMessageFromOriginID = parsedIMessageProviderABI.Events["InboxMessageDeliveredFromOrigin"].ID

	parsedIInboxABI, err := bridgegen.IInboxMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	l2MessageFromOriginCallABI = parsedIInboxABI.Methods["sendL2MessageFromOrigin"]
}

type DelayedBridge struct {
	con              *bridgegen.IBridge
	address          common.Address
	fromBlock        uint64
	client           arbutil.L1Interface
	messageProviders map[common.Address]*bridgegen.IDelayedMessageProvider
}

func NewDelayedBridge(client arbutil.L1Interface, addr common.Address, fromBlock uint64) (*DelayedBridge, error) {
	con, err := bridgegen.NewIBridge(addr, client)
	if err != nil {
		return nil, err
	}

	return &DelayedBridge{
		con:              con,
		address:          addr,
		fromBlock:        fromBlock,
		client:           client,
		messageProviders: make(map[common.Address]*bridgegen.IDelayedMessageProvider),
	}, nil
}

func (b *DelayedBridge) FirstBlock() *big.Int {
	return new(big.Int).SetUint64(b.fromBlock)
}

func (b *DelayedBridge) GetMessageCount(ctx context.Context, blockNumber *big.Int) (uint64, error) {
	if (blockNumber != nil) && blockNumber.Cmp(new(big.Int).SetUint64(b.fromBlock)) < 0 {
		return 0, nil
	}
	opts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: blockNumber,
	}
	bigRes, err := b.con.DelayedMessageCount(opts)
	if err != nil {
		return 0, err
	}
	if !bigRes.IsUint64() {
		return 0, errors.New("DelayedBridge MessageCount doesn't make sense!")
	}
	return bigRes.Uint64(), nil
}

func (b *DelayedBridge) GetAccumulator(ctx context.Context, sequenceNumber uint64, blockNumber *big.Int) (common.Hash, error) {
	opts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: blockNumber,
	}
	return b.con.DelayedInboxAccs(opts, new(big.Int).SetUint64(sequenceNumber))
}

type DelayedInboxMessage struct {
	BlockHash              common.Hash
	BeforeInboxAcc         common.Hash
	Message                *arbostypes.L1IncomingMessage
	ParentChainBlockNumber uint64
}

func (m *DelayedInboxMessage) AfterInboxAcc() common.Hash {
	hash := crypto.Keccak256(
		[]byte{m.Message.Header.Kind},
		m.Message.Header.Poster.Bytes(),
		arbmath.UintToBytes(m.Message.Header.BlockNumber),
		arbmath.UintToBytes(m.Message.Header.Timestamp),
		m.Message.Header.RequestId.Bytes(),
		math.U256Bytes(m.Message.Header.L1BaseFee),
		crypto.Keccak256(m.Message.L2msg),
	)
	return crypto.Keccak256Hash(m.BeforeInboxAcc[:], hash)
}

func (b *DelayedBridge) LookupMessagesInRange(ctx context.Context, from, to *big.Int, batchFetcher arbostypes.FallibleBatchFetcher) ([]*DelayedInboxMessage, error) {
	query := ethereum.FilterQuery{
		BlockHash: nil,
		FromBlock: from,
		ToBlock:   to,
		Addresses: []common.Address{b.address},
		Topics:    [][]common.Hash{{messageDeliveredID}},
	}
	logs, err := b.client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	return b.logsToDeliveredMessages(ctx, logs, batchFetcher)
}

type sortableMessageList []*DelayedInboxMessage

func (l sortableMessageList) Len() int {
	return len(l)
}

func (l sortableMessageList) Swap(i, j int) {
	l[i], l[j] = l[j], l[i]
}

func (l sortableMessageList) Less(i, j int) bool {
	return bytes.Compare(l[i].Message.Header.RequestId.Bytes(), l[j].Message.Header.RequestId.Bytes()) < 0
}

func (b *DelayedBridge) logsToDeliveredMessages(ctx context.Context, logs []types.Log, batchFetcher arbostypes.FallibleBatchFetcher) ([]*DelayedInboxMessage, error) {
	if len(logs) == 0 {
		return nil, nil
	}
	parsedLogs := make([]*bridgegen.IBridgeMessageDelivered, 0, len(logs))
	messageIds := make([]common.Hash, 0, len(logs))
	inboxAddresses := make(map[common.Address]struct{})
	minBlockNum := uint64(math.MaxUint64)
	maxBlockNum := uint64(0)
	for _, ethLog := range logs {
		if ethLog.BlockNumber < minBlockNum {
			minBlockNum = ethLog.BlockNumber
		}
		if ethLog.BlockNumber > maxBlockNum {
			maxBlockNum = ethLog.BlockNumber
		}
		parsedLog, err := b.con.ParseMessageDelivered(ethLog)
		if err != nil {
			return nil, err
		}
		messageKey := common.BigToHash(parsedLog.MessageIndex)
		parsedLogs = append(parsedLogs, parsedLog)
		inboxAddresses[parsedLog.Inbox] = struct{}{}
		messageIds = append(messageIds, messageKey)
	}

	messageData := make(map[common.Hash][]byte)
	if err := b.fillMessageData(ctx, inboxAddresses, messageIds, messageData, minBlockNum, maxBlockNum); err != nil {
		return nil, err
	}

	messages := make([]*DelayedInboxMessage, 0, len(logs))
	for _, parsedLog := range parsedLogs {
		msgKey := common.BigToHash(parsedLog.MessageIndex)
		data, ok := messageData[msgKey]
		if !ok {
			return nil, errors.New("message not found")
		}
		if crypto.Keccak256Hash(data) != parsedLog.MessageDataHash {
			return nil, errors.New("found message data with mismatched hash")
		}

		requestId := common.BigToHash(parsedLog.MessageIndex)
		parentChainBlockNumber := parsedLog.Raw.BlockNumber
		l1BlockNumber, err := arbutil.CorrespondingL1BlockNumber(ctx, b.client, parentChainBlockNumber)
		if err != nil {
			return nil, err
		}
		msg := &DelayedInboxMessage{
			BlockHash:      parsedLog.Raw.BlockHash,
			BeforeInboxAcc: parsedLog.BeforeInboxAcc,
			Message: &arbostypes.L1IncomingMessage{
				Header: &arbostypes.L1IncomingMessageHeader{
					Kind:        parsedLog.Kind,
					Poster:      parsedLog.Sender,
					BlockNumber: l1BlockNumber,
					Timestamp:   parsedLog.Timestamp,
					RequestId:   &requestId,
					L1BaseFee:   parsedLog.BaseFeeL1,
				},
				L2msg: data,
			},
			ParentChainBlockNumber: parsedLog.Raw.BlockNumber,
		}
		err = msg.Message.FillInBatchGasCost(batchFetcher)
		if err != nil {
			return nil, err
		}
		messages = append(messages, msg)
	}

	sort.Sort(sortableMessageList(messages))

	return messages, nil
}

func (b *DelayedBridge) fillMessageData(
	ctx context.Context,
	inboxAddressSet map[common.Address]struct{},
	messageIds []common.Hash,
	messageData map[common.Hash][]byte,
	minBlockNum, maxBlockNum uint64,
) error {
	inboxAddressList := make([]common.Address, 0, len(inboxAddressSet))
	for addr := range inboxAddressSet {
		inboxAddressList = append(inboxAddressList, addr)
	}

	query := ethereum.FilterQuery{
		BlockHash: nil,
		FromBlock: new(big.Int).SetUint64(minBlockNum),
		ToBlock:   new(big.Int).SetUint64(maxBlockNum),
		Addresses: inboxAddressList,
		Topics:    [][]common.Hash{{inboxMessageDeliveredID, inboxMessageFromOriginID}, messageIds},
	}
	logs, err := b.client.FilterLogs(ctx, query)
	if err != nil {
		return err
	}
	for _, ethLog := range logs {
		msgNum, msg, err := b.parseMessage(ctx, ethLog)
		if err != nil {
			return err
		}
		messageData[common.BigToHash(msgNum)] = msg
	}
	return nil
}

func (b *DelayedBridge) parseMessage(ctx context.Context, ethLog types.Log) (*big.Int, []byte, error) {
	con, ok := b.messageProviders[ethLog.Address]
	if !ok {
		var err error
		con, err = bridgegen.NewIDelayedMessageProvider(ethLog.Address, b.client)
		if err != nil {
			return nil, nil, err
		}
		b.messageProviders[ethLog.Address] = con
	}
	switch {
	case ethLog.Topics[0] == inboxMessageDeliveredID:
		parsedLog, err := con.ParseInboxMessageDelivered(ethLog)
		if err != nil {
			return nil, nil, err
		}
		return parsedLog.MessageNum, parsedLog.Data, nil
	case ethLog.Topics[0] == inboxMessageFromOriginID:
		parsedLog, err := con.ParseInboxMessageDeliveredFromOrigin(ethLog)
		if err != nil {
			return nil, nil, err
		}
		data, err := arbutil.GetLogEmitterTxData(ctx, b.client, ethLog)
		if err != nil {
			return nil, nil, err
		}
		args := make(map[string]interface{})
		err = l2MessageFromOriginCallABI.Inputs.UnpackIntoMap(args, data[4:])
		if err != nil {
			return nil, nil, err
		}
		return parsedLog.MessageNum, args["messageData"].([]byte), nil
	default:
		return nil, nil, errors.New("unexpected log type")
	}
}

'''
'''--- arbnode/delayed_seq_reorg_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"encoding/binary"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

func TestSequencerReorgFromDelayed(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	exec, streamer, db, _ := NewTransactionStreamerForTest(t, common.Address{})
	tracker, err := NewInboxTracker(db, streamer, nil)
	Require(t, err)

	err = streamer.Start(ctx)
	Require(t, err)
	exec.Start(ctx)
	init, err := streamer.GetMessage(0)
	Require(t, err)

	initMsgDelayed := &DelayedInboxMessage{
		BlockHash:      [32]byte{},
		BeforeInboxAcc: [32]byte{},
		Message:        init.Message,
	}
	delayedRequestId := common.BigToHash(common.Big1)
	userDelayed := &DelayedInboxMessage{
		BlockHash:      [32]byte{},
		BeforeInboxAcc: initMsgDelayed.AfterInboxAcc(),
		Message: &arbostypes.L1IncomingMessage{
			Header: &arbostypes.L1IncomingMessageHeader{
				Kind:        arbostypes.L1MessageType_EndOfBlock,
				Poster:      [20]byte{},
				BlockNumber: 0,
				Timestamp:   0,
				RequestId:   &delayedRequestId,
				L1BaseFee:   common.Big0,
			},
		},
	}
	err = tracker.AddDelayedMessages([]*DelayedInboxMessage{initMsgDelayed, userDelayed}, false)
	Require(t, err)

	serializedInitMsgBatch := make([]byte, 40)
	binary.BigEndian.PutUint64(serializedInitMsgBatch[32:], 1)
	initMsgBatch := &SequencerInboxBatch{
		BlockHash:              [32]byte{},
		ParentChainBlockNumber: 0,
		SequenceNumber:         0,
		BeforeInboxAcc:         [32]byte{},
		AfterInboxAcc:          [32]byte{1},
		AfterDelayedAcc:        initMsgDelayed.AfterInboxAcc(),
		AfterDelayedCount:      1,
		TimeBounds:             bridgegen.ISequencerInboxTimeBounds{},
		rawLog:                 types.Log{},
		dataLocation:           0,
		bridgeAddress:          [20]byte{},
		serialized:             serializedInitMsgBatch,
	}
	serializedUserMsgBatch := make([]byte, 40)
	binary.BigEndian.PutUint64(serializedUserMsgBatch[32:], 2)
	userMsgBatch := &SequencerInboxBatch{
		BlockHash:              [32]byte{},
		ParentChainBlockNumber: 0,
		SequenceNumber:         1,
		BeforeInboxAcc:         [32]byte{1},
		AfterInboxAcc:          [32]byte{2},
		AfterDelayedAcc:        userDelayed.AfterInboxAcc(),
		AfterDelayedCount:      2,
		TimeBounds:             bridgegen.ISequencerInboxTimeBounds{},
		rawLog:                 types.Log{},
		dataLocation:           0,
		bridgeAddress:          [20]byte{},
		serialized:             serializedUserMsgBatch,
	}
	emptyBatch := &SequencerInboxBatch{
		BlockHash:              [32]byte{},
		ParentChainBlockNumber: 0,
		SequenceNumber:         2,
		BeforeInboxAcc:         [32]byte{2},
		AfterInboxAcc:          [32]byte{3},
		AfterDelayedAcc:        userDelayed.AfterInboxAcc(),
		AfterDelayedCount:      2,
		TimeBounds:             bridgegen.ISequencerInboxTimeBounds{},
		rawLog:                 types.Log{},
		dataLocation:           0,
		bridgeAddress:          [20]byte{},
		serialized:             serializedUserMsgBatch,
	}
	err = tracker.AddSequencerBatches(ctx, nil, []*SequencerInboxBatch{initMsgBatch, userMsgBatch, emptyBatch})
	Require(t, err)

	// Reorg out the user delayed message
	err = tracker.ReorgDelayedTo(1, true)
	Require(t, err)

	msgCount, err := streamer.GetMessageCount()
	Require(t, err)
	if msgCount != 1 {
		Fail(t, "Unexpected tx streamer message count", msgCount, "(expected 1)")
	}

	delayedCount, err := tracker.GetDelayedCount()
	Require(t, err)
	if delayedCount != 1 {
		Fail(t, "Unexpected tracker delayed message count", delayedCount, "(expected 1)")
	}

	batchCount, err := tracker.GetBatchCount()
	Require(t, err)
	if batchCount != 1 {
		Fail(t, "Unexpected tracker batch count", batchCount, "(expected 1)")
	}

	emptyBatch = &SequencerInboxBatch{
		BlockHash:              [32]byte{},
		ParentChainBlockNumber: 0,
		SequenceNumber:         1,
		BeforeInboxAcc:         [32]byte{1},
		AfterInboxAcc:          [32]byte{2},
		AfterDelayedAcc:        initMsgDelayed.AfterInboxAcc(),
		AfterDelayedCount:      1,
		TimeBounds:             bridgegen.ISequencerInboxTimeBounds{},
		rawLog:                 types.Log{},
		dataLocation:           0,
		bridgeAddress:          [20]byte{},
		serialized:             serializedInitMsgBatch,
	}
	err = tracker.AddSequencerBatches(ctx, nil, []*SequencerInboxBatch{emptyBatch})
	Require(t, err)

	msgCount, err = streamer.GetMessageCount()
	Require(t, err)
	if msgCount != 2 {
		Fail(t, "Unexpected tx streamer message count", msgCount, "(expected 2)")
	}

	batchCount, err = tracker.GetBatchCount()
	Require(t, err)
	if batchCount != 2 {
		Fail(t, "Unexpected tracker batch count", batchCount, "(expected 2)")
	}
}

'''
'''--- arbnode/delayed_sequencer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"sync"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

type DelayedSequencer struct {
	stopwaiter.StopWaiter
	l1Reader                 *headerreader.HeaderReader
	bridge                   *DelayedBridge
	inbox                    *InboxTracker
	exec                     execution.ExecutionSequencer
	coordinator              *SeqCoordinator
	waitingForFinalizedBlock uint64
	mutex                    sync.Mutex
	config                   DelayedSequencerConfigFetcher
}

type DelayedSequencerConfig struct {
	Enable              bool  `koanf:"enable" reload:"hot"`
	FinalizeDistance    int64 `koanf:"finalize-distance" reload:"hot"`
	RequireFullFinality bool  `koanf:"require-full-finality" reload:"hot"`
	UseMergeFinality    bool  `koanf:"use-merge-finality" reload:"hot"`
}

type DelayedSequencerConfigFetcher func() *DelayedSequencerConfig

func DelayedSequencerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultDelayedSequencerConfig.Enable, "enable delayed sequencer")
	f.Int64(prefix+".finalize-distance", DefaultDelayedSequencerConfig.FinalizeDistance, "how many blocks in the past L1 block is considered final (ignored when using Merge finality)")
	f.Bool(prefix+".require-full-finality", DefaultDelayedSequencerConfig.RequireFullFinality, "whether to wait for full finality before sequencing delayed messages")
	f.Bool(prefix+".use-merge-finality", DefaultDelayedSequencerConfig.UseMergeFinality, "whether to use The Merge's notion of finality before sequencing delayed messages")
}

var DefaultDelayedSequencerConfig = DelayedSequencerConfig{
	Enable:              false,
	FinalizeDistance:    20,
	RequireFullFinality: false,
	UseMergeFinality:    true,
}

var TestDelayedSequencerConfig = DelayedSequencerConfig{
	Enable:              true,
	FinalizeDistance:    20,
	RequireFullFinality: false,
	UseMergeFinality:    false,
}

func NewDelayedSequencer(l1Reader *headerreader.HeaderReader, reader *InboxReader, exec execution.ExecutionSequencer, coordinator *SeqCoordinator, config DelayedSequencerConfigFetcher) (*DelayedSequencer, error) {
	d := &DelayedSequencer{
		l1Reader:    l1Reader,
		bridge:      reader.DelayedBridge(),
		inbox:       reader.Tracker(),
		coordinator: coordinator,
		exec:        exec,
		config:      config,
	}
	if coordinator != nil {
		coordinator.SetDelayedSequencer(d)
	}
	return d, nil
}

func (d *DelayedSequencer) getDelayedMessagesRead() (uint64, error) {
	return d.exec.NextDelayedMessageNumber()
}

func (d *DelayedSequencer) trySequence(ctx context.Context, lastBlockHeader *types.Header) error {
	if d.coordinator != nil && !d.coordinator.CurrentlyChosen() {
		return nil
	}

	return d.sequenceWithoutLockout(ctx, lastBlockHeader)
}

func (d *DelayedSequencer) sequenceWithoutLockout(ctx context.Context, lastBlockHeader *types.Header) error {
	d.mutex.Lock()
	defer d.mutex.Unlock()

	config := d.config()
	if !config.Enable {
		return nil
	}

	var finalized uint64
	if config.UseMergeFinality && lastBlockHeader.Difficulty.Sign() == 0 {
		var err error
		if config.RequireFullFinality {
			finalized, err = d.l1Reader.LatestFinalizedBlockNr(ctx)
		} else {
			finalized, err = d.l1Reader.LatestSafeBlockNr(ctx)
		}
		if err != nil {
			return err
		}
	} else {
		currentNum := lastBlockHeader.Number.Int64()
		if currentNum < config.FinalizeDistance {
			return nil
		}
		finalized = uint64(currentNum - config.FinalizeDistance)
	}

	if d.waitingForFinalizedBlock > finalized {
		return nil
	}

	// Unless we find an unfinalized message (which sets waitingForBlock),
	// we won't find a new finalized message until FinalizeDistance blocks in the future.
	d.waitingForFinalizedBlock = lastBlockHeader.Number.Uint64() + 1

	dbDelayedCount, err := d.inbox.GetDelayedCount()
	if err != nil {
		return err
	}
	startPos, err := d.getDelayedMessagesRead()
	if err != nil {
		return err
	}

	// Retrieve all finalized delayed messages
	pos := startPos
	var lastDelayedAcc common.Hash
	var messages []*arbostypes.L1IncomingMessage
	for pos < dbDelayedCount {
		msg, acc, parentChainBlockNumber, err := d.inbox.GetDelayedMessageAccumulatorAndParentChainBlockNumber(pos)
		if err != nil {
			return err
		}
		if parentChainBlockNumber > finalized {
			// Message isn't finalized yet; stop here
			d.waitingForFinalizedBlock = parentChainBlockNumber
			break
		}
		if lastDelayedAcc != (common.Hash{}) {
			// Ensure that there hasn't been a reorg and this message follows the last
			fullMsg := DelayedInboxMessage{
				BeforeInboxAcc:         lastDelayedAcc,
				Message:                msg,
				ParentChainBlockNumber: parentChainBlockNumber,
			}
			if fullMsg.AfterInboxAcc() != acc {
				return errors.New("delayed message accumulator mismatch while sequencing")
			}
		}
		lastDelayedAcc = acc
		messages = append(messages, msg)
		pos++
	}

	// Sequence the delayed messages, if any
	if len(messages) > 0 {
		delayedBridgeAcc, err := d.bridge.GetAccumulator(ctx, pos-1, new(big.Int).SetUint64(finalized))
		if err != nil {
			return err
		}
		if delayedBridgeAcc != lastDelayedAcc {
			// Probably a reorg that hasn't been picked up by the inbox reader
			return fmt.Errorf("inbox reader at delayed message %v db accumulator %v doesn't match delayed bridge accumulator %v at L1 block %v", pos-1, lastDelayedAcc, delayedBridgeAcc, finalized)
		}
		for i, msg := range messages {
			err = d.exec.SequenceDelayedMessage(msg, startPos+uint64(i))
			if err != nil {
				return err
			}
		}
		log.Info("DelayedSequencer: Sequenced", "msgnum", len(messages), "startpos", startPos)
	}

	return nil
}

// Dangerous: bypasses lockout check!
func (d *DelayedSequencer) ForceSequenceDelayed(ctx context.Context) error {
	lastBlockHeader, err := d.l1Reader.LastHeader(ctx)
	if err != nil {
		return err
	}
	return d.sequenceWithoutLockout(ctx, lastBlockHeader)
}

func (d *DelayedSequencer) run(ctx context.Context) {
	headerChan, cancel := d.l1Reader.Subscribe(false)
	defer cancel()

	for {
		select {
		case nextHeader, ok := <-headerChan:
			if !ok {
				log.Info("delayed sequencer: header channel close")
				return
			}
			if err := d.trySequence(ctx, nextHeader); err != nil {
				log.Error("Delayed sequencer error", "err", err)
			}
		case <-ctx.Done():
			log.Info("delayed sequencer: context done", "err", ctx.Err())
			return
		}
	}
}

func (d *DelayedSequencer) Start(ctxIn context.Context) {
	d.StopWaiter.Start(ctxIn, d)
	d.LaunchThread(d.run)
}

'''
'''--- arbnode/inbox_reader.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"errors"
	"fmt"
	"math"
	"math/big"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/log"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

type InboxReaderConfig struct {
	DelayBlocks         uint64        `koanf:"delay-blocks" reload:"hot"`
	CheckDelay          time.Duration `koanf:"check-delay" reload:"hot"`
	HardReorg           bool          `koanf:"hard-reorg" reload:"hot"`
	MinBlocksToRead     uint64        `koanf:"min-blocks-to-read" reload:"hot"`
	DefaultBlocksToRead uint64        `koanf:"default-blocks-to-read" reload:"hot"`
	TargetMessagesRead  uint64        `koanf:"target-messages-read" reload:"hot"`
	MaxBlocksToRead     uint64        `koanf:"max-blocks-to-read" reload:"hot"`
}

type InboxReaderConfigFetcher func() *InboxReaderConfig

func (c *InboxReaderConfig) Validate() error {
	if c.MaxBlocksToRead == 0 || c.MaxBlocksToRead < c.DefaultBlocksToRead {
		return errors.New("inbox reader max-blocks-to-read cannot be zero or less than default-blocks-to-read")
	}
	return nil
}

func InboxReaderConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".delay-blocks", DefaultInboxReaderConfig.DelayBlocks, "number of latest blocks to ignore to reduce reorgs")
	f.Duration(prefix+".check-delay", DefaultInboxReaderConfig.CheckDelay, "the maximum time to wait between inbox checks (if not enough new blocks are found)")
	f.Bool(prefix+".hard-reorg", DefaultInboxReaderConfig.HardReorg, "erase future transactions in addition to overwriting existing ones on reorg")
	f.Uint64(prefix+".min-blocks-to-read", DefaultInboxReaderConfig.MinBlocksToRead, "the minimum number of blocks to read at once (when caught up lowers load on L1)")
	f.Uint64(prefix+".default-blocks-to-read", DefaultInboxReaderConfig.DefaultBlocksToRead, "the default number of blocks to read at once (will vary based on traffic by default)")
	f.Uint64(prefix+".target-messages-read", DefaultInboxReaderConfig.TargetMessagesRead, "if adjust-blocks-to-read is enabled, the target number of messages to read at once")
	f.Uint64(prefix+".max-blocks-to-read", DefaultInboxReaderConfig.MaxBlocksToRead, "if adjust-blocks-to-read is enabled, the maximum number of blocks to read at once")
}

var DefaultInboxReaderConfig = InboxReaderConfig{
	DelayBlocks:         0,
	CheckDelay:          time.Minute,
	HardReorg:           false,
	MinBlocksToRead:     1,
	DefaultBlocksToRead: 100,
	TargetMessagesRead:  500,
	MaxBlocksToRead:     2000,
}

var TestInboxReaderConfig = InboxReaderConfig{
	DelayBlocks:         0,
	CheckDelay:          time.Millisecond * 10,
	HardReorg:           false,
	MinBlocksToRead:     1,
	DefaultBlocksToRead: 100,
	TargetMessagesRead:  500,
	MaxBlocksToRead:     2000,
}

type InboxReader struct {
	stopwaiter.StopWaiter

	// Only in run thread
	caughtUp          bool
	firstMessageBlock *big.Int
	config            InboxReaderConfigFetcher

	// Thread safe
	tracker        *InboxTracker
	delayedBridge  *DelayedBridge
	sequencerInbox *SequencerInbox
	caughtUpChan   chan struct{}
	client         arbutil.L1Interface
	l1Reader       *headerreader.HeaderReader

	// Atomic
	lastSeenBatchCount uint64

	// Behind the mutex
	lastReadMutex      sync.RWMutex
	lastReadBlock      uint64
	lastReadBatchCount uint64
}

func NewInboxReader(tracker *InboxTracker, client arbutil.L1Interface, l1Reader *headerreader.HeaderReader, firstMessageBlock *big.Int, delayedBridge *DelayedBridge, sequencerInbox *SequencerInbox, config InboxReaderConfigFetcher) (*InboxReader, error) {
	err := config().Validate()
	if err != nil {
		return nil, err
	}
	return &InboxReader{
		tracker:           tracker,
		delayedBridge:     delayedBridge,
		sequencerInbox:    sequencerInbox,
		client:            client,
		l1Reader:          l1Reader,
		firstMessageBlock: firstMessageBlock,
		caughtUpChan:      make(chan struct{}),
		config:            config,
	}, nil
}

func (r *InboxReader) Start(ctxIn context.Context) error {
	r.StopWaiter.Start(ctxIn, r)
	hadError := false
	r.CallIteratively(func(ctx context.Context) time.Duration {
		err := r.run(ctx, hadError)
		if err != nil && !errors.Is(err, context.Canceled) && !strings.Contains(err.Error(), "header not found") {
			log.Warn("error reading inbox", "err", err)
			hadError = true
		} else {
			hadError = false
		}
		return time.Second
	})

	// Ensure we read the init message before other things start up
	for i := 0; ; i++ {
		batchCount, err := r.tracker.GetBatchCount()
		if err != nil {
			return err
		}
		if batchCount > 0 {
			// Validate the init message matches our L2 blockchain
			message, err := r.tracker.GetDelayedMessage(0)
			if err != nil {
				return err
			}
			initMessage, err := message.ParseInitMessage()
			if err != nil {
				return err
			}
			chainConfig := r.tracker.txStreamer.chainConfig
			configChainId := chainConfig.ChainID
			if initMessage.ChainId.Cmp(configChainId) != 0 {
				return fmt.Errorf("expected L2 chain ID %v but read L2 chain ID %v from init message in L1 inbox", configChainId, initMessage.ChainId)
			}
			if initMessage.ChainConfig != nil {
				if err := initMessage.ChainConfig.CheckCompatible(chainConfig, chainConfig.ArbitrumChainParams.GenesisBlockNum, 0); err != nil {
					return fmt.Errorf("incompatible chain config read from init message in L1 inbox: %w", err)
				}
			}
			break
		}
		if i == 30*10 {
			return errors.New("failed to read init message")
		}
		time.Sleep(time.Millisecond * 100)
	}

	return nil
}

// assumes l1block is recent so we could do a simple-search from the end
func (r *InboxReader) recentParentChainBlockToMsg(ctx context.Context, parentChainBlock uint64) (arbutil.MessageIndex, error) {
	batch, err := r.tracker.GetBatchCount()
	if err != nil {
		return 0, err
	}
	for {
		if ctx.Err() != nil {
			return 0, ctx.Err()
		}
		if batch == 0 {
			return 0, nil
		}
		batch -= 1
		meta, err := r.tracker.GetBatchMetadata(batch)
		if err != nil {
			return 0, err
		}
		if meta.ParentChainBlock <= parentChainBlock {
			return meta.MessageCount, nil
		}
	}
}

func (r *InboxReader) GetSafeMsgCount(ctx context.Context) (arbutil.MessageIndex, error) {
	l1block, err := r.l1Reader.LatestSafeBlockNr(ctx)
	if err != nil {
		return 0, err
	}
	return r.recentParentChainBlockToMsg(ctx, l1block)
}

func (r *InboxReader) GetFinalizedMsgCount(ctx context.Context) (arbutil.MessageIndex, error) {
	l1block, err := r.l1Reader.LatestFinalizedBlockNr(ctx)
	if err != nil {
		return 0, err
	}
	return r.recentParentChainBlockToMsg(ctx, l1block)
}

func (r *InboxReader) Tracker() *InboxTracker {
	return r.tracker
}

func (r *InboxReader) DelayedBridge() *DelayedBridge {
	return r.delayedBridge
}

func (r *InboxReader) CaughtUp() chan struct{} {
	return r.caughtUpChan
}

func (r *InboxReader) run(ctx context.Context, hadError bool) error {
	from, err := r.getNextBlockToRead()
	if err != nil {
		return err
	}
	newHeaders, unsubscribe := r.l1Reader.Subscribe(false)
	defer unsubscribe()
	blocksToFetch := r.config().DefaultBlocksToRead
	if hadError {
		blocksToFetch = 1
	}
	seenBatchCount := uint64(0)
	seenBatchCountStored := uint64(math.MaxUint64)
	storeSeenBatchCount := func() {
		if seenBatchCountStored != seenBatchCount {
			atomic.StoreUint64(&r.lastSeenBatchCount, seenBatchCount)
			seenBatchCountStored = seenBatchCount
		}
	}
	defer storeSeenBatchCount() // in case of error
	for {

		latestHeader, err := r.l1Reader.LastHeader(ctx)
		if err != nil {
			return err
		}
		config := r.config()
		currentHeight := latestHeader.Number

		neededBlockAdvance := config.DelayBlocks + arbmath.SaturatingUSub(config.MinBlocksToRead, 1)
		neededBlockHeight := arbmath.BigAddByUint(from, neededBlockAdvance)
		checkDelayTimer := time.NewTimer(config.CheckDelay)
	WaitForHeight:
		for arbmath.BigLessThan(currentHeight, neededBlockHeight) {
			select {
			case latestHeader = <-newHeaders:
				if latestHeader == nil {
					// shutting down
					return nil
				}
				currentHeight = new(big.Int).Set(latestHeader.Number)
			case <-ctx.Done():
				return nil
			case <-checkDelayTimer.C:
				break WaitForHeight
			}
		}
		checkDelayTimer.Stop()

		if config.DelayBlocks > 0 {
			currentHeight = new(big.Int).Sub(currentHeight, new(big.Int).SetUint64(config.DelayBlocks))
			if currentHeight.Cmp(r.firstMessageBlock) < 0 {
				currentHeight = new(big.Int).Set(r.firstMessageBlock)
			}
		}

		reorgingDelayed := false
		reorgingSequencer := false
		missingDelayed := false
		missingSequencer := false

		{
			checkingDelayedCount, err := r.delayedBridge.GetMessageCount(ctx, currentHeight)
			if err != nil {
				return err
			}
			ourLatestDelayedCount, err := r.tracker.GetDelayedCount()
			if err != nil {
				return err
			}
			if ourLatestDelayedCount < checkingDelayedCount {
				checkingDelayedCount = ourLatestDelayedCount
				missingDelayed = true
			} else if ourLatestDelayedCount > checkingDelayedCount {
				log.Info("backwards reorg of delayed messages", "from", ourLatestDelayedCount, "to", checkingDelayedCount)
				err = r.tracker.ReorgDelayedTo(checkingDelayedCount, config.HardReorg)
				if err != nil {
					return err
				}
			}
			if checkingDelayedCount > 0 {
				checkingDelayedSeqNum := checkingDelayedCount - 1
				l1DelayedAcc, err := r.delayedBridge.GetAccumulator(ctx, checkingDelayedSeqNum, currentHeight)
				if err != nil {
					return err
				}
				dbDelayedAcc, err := r.tracker.GetDelayedAcc(checkingDelayedSeqNum)
				if err != nil {
					return err
				}
				if dbDelayedAcc != l1DelayedAcc {
					reorgingDelayed = true
				}
			}
		}

		seenBatchCount, err = r.sequencerInbox.GetBatchCount(ctx, currentHeight)
		if err != nil {
			seenBatchCount = 0
			return err
		}
		checkingBatchCount := seenBatchCount
		{
			ourLatestBatchCount, err := r.tracker.GetBatchCount()
			if err != nil {
				return err
			}
			if ourLatestBatchCount < checkingBatchCount {
				checkingBatchCount = ourLatestBatchCount
				missingSequencer = true
			} else if ourLatestBatchCount > checkingBatchCount && config.HardReorg {
				err = r.tracker.ReorgBatchesTo(checkingBatchCount)
				if err != nil {
					return err
				}
			}
			if checkingBatchCount > 0 {
				checkingBatchSeqNum := checkingBatchCount - 1
				l1BatchAcc, err := r.sequencerInbox.GetAccumulator(ctx, checkingBatchSeqNum, currentHeight)
				if err != nil {
					return err
				}
				dbBatchAcc, err := r.tracker.GetBatchAcc(checkingBatchSeqNum)
				if err != nil {
					return err
				}
				if dbBatchAcc != l1BatchAcc {
					reorgingSequencer = true
				}
			}
		}

		if !missingDelayed && !reorgingDelayed && !missingSequencer && !reorgingSequencer {
			// There's nothing to do
			from = arbmath.BigAddByUint(currentHeight, 1)
			blocksToFetch = config.DefaultBlocksToRead
			r.lastReadMutex.Lock()
			r.lastReadBlock = currentHeight.Uint64()
			r.lastReadBatchCount = checkingBatchCount
			r.lastReadMutex.Unlock()
			storeSeenBatchCount()
			if !r.caughtUp {
				r.caughtUp = true
				close(r.caughtUpChan)
			}
			continue
		}

		readAnyBatches := false
		for {
			if ctx.Err() != nil {
				// the context is done, shut down
				// nolint:nilerr
				return nil
			}
			if from.Cmp(currentHeight) > 0 {
				if missingDelayed {
					reorgingDelayed = true
				}
				if missingSequencer {
					reorgingSequencer = true
				}
				if !reorgingDelayed && !reorgingSequencer {
					break
				} else {
					from = new(big.Int).Set(currentHeight)
				}
			}
			to := new(big.Int).Add(from, new(big.Int).SetUint64(blocksToFetch))
			if to.Cmp(currentHeight) > 0 {
				to.Set(currentHeight)
			}
			sequencerBatches, err := r.sequencerInbox.LookupBatchesInRange(ctx, from, to)
			if err != nil {
				return err
			}
			delayedMessages, err := r.delayedBridge.LookupMessagesInRange(ctx, from, to, func(batchNum uint64) ([]byte, error) {
				if len(sequencerBatches) > 0 && batchNum >= sequencerBatches[0].SequenceNumber {
					idx := int(batchNum - sequencerBatches[0].SequenceNumber)
					if idx < len(sequencerBatches) {
						return sequencerBatches[idx].Serialize(ctx, r.l1Reader.Client())
					}
					log.Warn("missing mentioned batch in L1 message lookup", "batch", batchNum)
				}
				return r.GetSequencerMessageBytes(ctx, batchNum)
			})
			if err != nil {
				return err
			}
			if !r.caughtUp && to.Cmp(currentHeight) == 0 {
				r.caughtUp = true
				close(r.caughtUpChan)
			}
			if len(sequencerBatches) > 0 {
				missingSequencer = false
				reorgingSequencer = false
				firstBatch := sequencerBatches[0]
				if firstBatch.SequenceNumber > 0 {
					haveAcc, err := r.tracker.GetBatchAcc(firstBatch.SequenceNumber - 1)
					if errors.Is(err, AccumulatorNotFoundErr) {
						reorgingSequencer = true
					} else if err != nil {
						return err
					} else if haveAcc != firstBatch.BeforeInboxAcc {
						reorgingSequencer = true
					}
				}
				if !reorgingSequencer {
					// Skip any batches we already have in the database
					for len(sequencerBatches) > 0 {
						batch := sequencerBatches[0]
						haveAcc, err := r.tracker.GetBatchAcc(batch.SequenceNumber)
						if errors.Is(err, AccumulatorNotFoundErr) {
							// This batch is new
							break
						} else if err != nil {
							// Unknown error (database error?)
							return err
						} else if haveAcc == batch.BeforeInboxAcc {
							// Skip this batch, as we already have it in the database
							sequencerBatches = sequencerBatches[1:]
						} else {
							// The first batch BeforeInboxAcc matches, but this batch doesn't,
							// so we'll successfully reorg it when we hit the addMessages
							break
						}
					}
				}
			} else if missingSequencer && to.Cmp(currentHeight) >= 0 {
				// We were missing sequencer batches but didn't find any.
				// This must mean that the sequencer batches are in the past.
				reorgingSequencer = true
			}

			if len(delayedMessages) > 0 {
				missingDelayed = false
				reorgingDelayed = false
				firstMsg := delayedMessages[0]
				beforeAcc := firstMsg.BeforeInboxAcc
				beforeCount, err := firstMsg.Message.Header.SeqNum()
				if err != nil {
					return err
				}
				if beforeCount > 0 {
					haveAcc, err := r.tracker.GetDelayedAcc(beforeCount - 1)
					if errors.Is(err, AccumulatorNotFoundErr) {
						reorgingDelayed = true
					} else if err != nil {
						return err
					} else if haveAcc != beforeAcc {
						reorgingDelayed = true
					}
				}
			} else if missingDelayed && to.Cmp(currentHeight) >= 0 {
				// We were missing delayed messages but didn't find any.
				// This must mean that the delayed messages are in the past.
				reorgingDelayed = true
			}

			log.Trace("looking up messages", "from", from.String(), "to", to.String(), "missingDelayed", missingDelayed, "missingSequencer", missingSequencer, "reorgingDelayed", reorgingDelayed, "reorgingSequencer", reorgingSequencer)
			if !reorgingDelayed && !reorgingSequencer && (len(delayedMessages) != 0 || len(sequencerBatches) != 0) {
				delayedMismatch, err := r.addMessages(ctx, sequencerBatches, delayedMessages)
				if err != nil {
					return err
				}
				if delayedMismatch {
					reorgingDelayed = true
				}
				if len(sequencerBatches) > 0 {
					readAnyBatches = true
					r.lastReadMutex.Lock()
					r.lastReadBlock = to.Uint64()
					r.lastReadBatchCount = sequencerBatches[len(sequencerBatches)-1].SequenceNumber + 1
					r.lastReadMutex.Unlock()
					storeSeenBatchCount()
				}
			}
			if reorgingDelayed || reorgingSequencer {
				from, err = r.getPrevBlockForReorg(from)
				if err != nil {
					return err
				}
			} else {
				from = arbmath.BigAddByUint(to, 1)
			}
			haveMessages := uint64(len(delayedMessages) + len(sequencerBatches))
			if haveMessages <= (config.TargetMessagesRead / 2) {
				blocksToFetch += (blocksToFetch + 4) / 5
			} else if haveMessages >= (config.TargetMessagesRead * 3 / 2) {
				// This cannot overflow, as it'll never try to subtract more than blocksToFetch
				blocksToFetch -= (blocksToFetch + 4) / 5
			}
			if blocksToFetch < 1 {
				blocksToFetch = 1
			} else if blocksToFetch > config.MaxBlocksToRead {
				blocksToFetch = config.MaxBlocksToRead
			}
		}

		if !readAnyBatches {
			r.lastReadMutex.Lock()
			r.lastReadBlock = currentHeight.Uint64()
			r.lastReadBatchCount = checkingBatchCount
			r.lastReadMutex.Unlock()
			storeSeenBatchCount()
		}
	}
}

func (r *InboxReader) addMessages(ctx context.Context, sequencerBatches []*SequencerInboxBatch, delayedMessages []*DelayedInboxMessage) (bool, error) {
	err := r.tracker.AddDelayedMessages(delayedMessages, r.config().HardReorg)
	if err != nil {
		return false, err
	}
	err = r.tracker.AddSequencerBatches(ctx, r.client, sequencerBatches)
	if errors.Is(err, delayedMessagesMismatch) {
		return true, nil
	} else if err != nil {
		return false, err
	}
	return false, nil
}

func (r *InboxReader) getPrevBlockForReorg(from *big.Int) (*big.Int, error) {
	if from.Cmp(r.firstMessageBlock) <= 0 {
		return nil, errors.New("can't get older messages")
	}
	newFrom := arbmath.BigSub(from, big.NewInt(10))
	if newFrom.Cmp(r.firstMessageBlock) < 0 {
		newFrom = new(big.Int).Set(r.firstMessageBlock)
	}
	return newFrom, nil
}

func (r *InboxReader) getNextBlockToRead() (*big.Int, error) {
	delayedCount, err := r.tracker.GetDelayedCount()
	if err != nil {
		return nil, err
	}
	if delayedCount == 0 {
		return new(big.Int).Set(r.firstMessageBlock), nil
	}
	_, _, parentChainBlockNumber, err := r.tracker.GetDelayedMessageAccumulatorAndParentChainBlockNumber(delayedCount - 1)
	if err != nil {
		return nil, err
	}
	msgBlock := new(big.Int).SetUint64(parentChainBlockNumber)
	if arbmath.BigLessThan(msgBlock, r.firstMessageBlock) {
		msgBlock.Set(r.firstMessageBlock)
	}
	return msgBlock, nil
}

func (r *InboxReader) GetSequencerMessageBytes(ctx context.Context, seqNum uint64) ([]byte, error) {
	metadata, err := r.tracker.GetBatchMetadata(seqNum)
	if err != nil {
		return nil, err
	}
	blockNum := arbmath.UintToBig(metadata.ParentChainBlock)
	seqBatches, err := r.sequencerInbox.LookupBatchesInRange(ctx, blockNum, blockNum)
	if err != nil {
		return nil, err
	}
	var seenBatches []uint64
	for _, batch := range seqBatches {
		if batch.SequenceNumber == seqNum {
			return batch.Serialize(ctx, r.client)
		}
		seenBatches = append(seenBatches, batch.SequenceNumber)
	}
	return nil, fmt.Errorf("sequencer batch %v not found in L1 block %v (found batches %v)", seqNum, metadata.ParentChainBlock, seenBatches)
}

func (r *InboxReader) GetLastReadBlockAndBatchCount() (uint64, uint64) {
	r.lastReadMutex.RLock()
	defer r.lastReadMutex.RUnlock()
	return r.lastReadBlock, r.lastReadBatchCount
}

// GetLastSeenBatchCount returns how many sequencer batches the inbox reader has read in from L1.
// Return values:
// >0 - last batchcount seen in run() - only written after lastReadBatchCount updated
// 0 - no batch seen, error
func (r *InboxReader) GetLastSeenBatchCount() uint64 {
	return atomic.LoadUint64(&r.lastSeenBatchCount)
}

func (r *InboxReader) GetDelayBlocks() uint64 {
	return r.config().DelayBlocks
}

'''
'''--- arbnode/inbox_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"encoding/binary"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/statetransfer"

	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/testhelpers"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
)

type execClientWrapper struct {
	*gethexec.ExecutionEngine
	t *testing.T
}

func (w *execClientWrapper) Pause()                     { w.t.Error("not supported") }
func (w *execClientWrapper) Activate()                  { w.t.Error("not supported") }
func (w *execClientWrapper) ForwardTo(url string) error { w.t.Error("not supported"); return nil }

func NewTransactionStreamerForTest(t *testing.T, ownerAddress common.Address) (*gethexec.ExecutionEngine, *TransactionStreamer, ethdb.Database, *core.BlockChain) {
	chainConfig := params.ArbitrumDevTestChainConfig()

	initData := statetransfer.ArbosInitializationInfo{
		Accounts: []statetransfer.AccountInitializationInfo{
			{
				Addr:       ownerAddress,
				EthBalance: big.NewInt(params.Ether),
			},
		},
	}

	chainDb := rawdb.NewMemoryDatabase()
	arbDb := rawdb.NewMemoryDatabase()
	initReader := statetransfer.NewMemoryInitDataReader(&initData)

	bc, err := gethexec.WriteOrTestBlockChain(chainDb, nil, initReader, chainConfig, arbostypes.TestInitMessage, gethexec.ConfigDefaultTest().TxLookupLimit, 0)

	if err != nil {
		Fail(t, err)
	}

	transactionStreamerConfigFetcher := func() *TransactionStreamerConfig { return &DefaultTransactionStreamerConfig }
	execEngine, err := gethexec.NewExecutionEngine(bc)
	if err != nil {
		Fail(t, err)
	}
	execSeq := &execClientWrapper{execEngine, t}
	inbox, err := NewTransactionStreamer(arbDb, bc.Config(), execSeq, nil, make(chan error, 1), transactionStreamerConfigFetcher)
	if err != nil {
		Fail(t, err)
	}

	// Add the init message
	err = inbox.AddFakeInitMessage()
	if err != nil {
		Fail(t, err)
	}

	return execEngine, inbox, arbDb, bc
}

type blockTestState struct {
	balances    map[common.Address]*big.Int
	accounts    []common.Address
	numMessages arbutil.MessageIndex
	blockNumber uint64
}

func TestTransactionStreamer(t *testing.T) {
	ownerAddress := common.HexToAddress("0x1111111111111111111111111111111111111111")

	exec, inbox, _, bc := NewTransactionStreamerForTest(t, ownerAddress)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	err := inbox.Start(ctx)
	Require(t, err)
	exec.Start(ctx)

	maxExpectedGasCost := big.NewInt(l2pricing.InitialBaseFeeWei)
	maxExpectedGasCost.Mul(maxExpectedGasCost, big.NewInt(2100*2))

	minBalance := new(big.Int).Mul(maxExpectedGasCost, big.NewInt(100))

	var blockStates []blockTestState
	blockStates = append(blockStates, blockTestState{
		balances: map[common.Address]*big.Int{
			ownerAddress: new(big.Int).SetUint64(params.Ether),
		},
		accounts:    []common.Address{ownerAddress},
		numMessages: 1,
		blockNumber: 0,
	})
	for i := 1; i < 100; i++ {
		if i%10 == 0 {
			reorgTo := rand.Int() % len(blockStates)
			err := inbox.ReorgTo(blockStates[reorgTo].numMessages)
			if err != nil {
				Fail(t, err)
			}
			blockStates = blockStates[:(reorgTo + 1)]
		} else {
			state := blockStates[len(blockStates)-1]
			newBalances := make(map[common.Address]*big.Int)
			for k, v := range state.balances {
				newBalances[k] = new(big.Int).Set(v)
			}
			state.balances = newBalances

			var messages []arbostypes.MessageWithMetadata
			// TODO replay a random amount of messages too
			numMessages := rand.Int() % 5
			for j := 0; j < numMessages; j++ {
				source := state.accounts[rand.Int()%len(state.accounts)]
				if state.balances[source].Cmp(minBalance) < 0 {
					continue
				}
				value := big.NewInt(int64(rand.Int() % 1000))
				var dest common.Address
				if j == 0 {
					binary.LittleEndian.PutUint64(dest[:], uint64(len(state.accounts)))
					state.accounts = append(state.accounts, dest)
				} else {
					dest = state.accounts[rand.Int()%len(state.accounts)]
				}
				var gas uint64 = 100000
				var l2Message []byte
				l2Message = append(l2Message, arbos.L2MessageKind_ContractTx)
				l2Message = append(l2Message, math.U256Bytes(new(big.Int).SetUint64(gas))...)
				l2Message = append(l2Message, math.U256Bytes(big.NewInt(l2pricing.InitialBaseFeeWei))...)
				l2Message = append(l2Message, dest.Hash().Bytes()...)
				l2Message = append(l2Message, math.U256Bytes(value)...)
				var requestId common.Hash
				binary.BigEndian.PutUint64(requestId.Bytes()[:8], uint64(i))
				messages = append(messages, arbostypes.MessageWithMetadata{
					Message: &arbostypes.L1IncomingMessage{
						Header: &arbostypes.L1IncomingMessageHeader{
							Kind:      arbostypes.L1MessageType_L2Message,
							Poster:    source,
							RequestId: &requestId,
						},
						L2msg: l2Message,
					},
					DelayedMessagesRead: 1,
				})
				state.balances[source].Sub(state.balances[source], value)
				if state.balances[dest] == nil {
					state.balances[dest] = new(big.Int)
				}
				state.balances[dest].Add(state.balances[dest], value)
			}

			Require(t, inbox.AddMessages(state.numMessages, false, messages))

			state.numMessages += arbutil.MessageIndex(len(messages))
			prevBlockNumber := state.blockNumber
			state.blockNumber += uint64(len(messages))
			for i := 0; ; i++ {
				blockNumber := bc.CurrentHeader().Number.Uint64()
				if blockNumber > state.blockNumber {
					Fail(t, "unexpected block number", blockNumber, ">", state.blockNumber)
				} else if blockNumber == state.blockNumber {
					break
				} else if i >= 100 {
					Fail(t, "timed out waiting for new block")
				}
				time.Sleep(10 * time.Millisecond)
			}
			for blockNum := prevBlockNumber + 1; blockNum <= state.blockNumber; blockNum++ {
				block := bc.GetBlockByNumber(blockNum)
				txs := block.Transactions()
				receipts := bc.GetReceiptsByHash(block.Hash())
				if len(txs) != len(receipts) {
					Fail(t, "got", len(txs), "transactions but", len(receipts), "receipts in block", blockNum)
				}
				for i, receipt := range receipts {
					sender, err := types.Sender(types.LatestSigner(bc.Config()), txs[i])
					Require(t, err)
					balance, ok := state.balances[sender]
					if !ok {
						continue
					}
					balance.Sub(balance, arbmath.BigMulByUint(block.BaseFee(), receipt.GasUsed))
				}
			}
			blockStates = append(blockStates, state)
		}

		// Check that state balances are consistent with blockchain's balances
		expectedLastBlockNumber := blockStates[len(blockStates)-1].blockNumber
		for i := 0; ; i++ {
			lastBlockNumber := bc.CurrentHeader().Number.Uint64()
			if lastBlockNumber == expectedLastBlockNumber {
				break
			} else if lastBlockNumber > expectedLastBlockNumber {
				Fail(t, "unexpected block number", lastBlockNumber, "vs", expectedLastBlockNumber)
			} else if i == 10 {
				Fail(t, "timeout waiting for block number", expectedLastBlockNumber, "current", lastBlockNumber)
			}
			time.Sleep(time.Millisecond * 100)
		}

		for _, state := range blockStates {
			block := bc.GetBlockByNumber(state.blockNumber)
			if block == nil {
				Fail(t, "missing state block", state.blockNumber)
			}
			for acct, balance := range state.balances {
				state, err := bc.StateAt(block.Root())
				if err != nil {
					Fail(t, "error getting block state", err)
				}
				haveBalance := state.GetBalance(acct)
				if balance.Cmp(haveBalance) != 0 {
					t.Error("unexpected balance for account", acct, "; expected", balance, "got", haveBalance)
				}
			}
		}
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbnode/inbox_tracker.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"bytes"
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/util/containers"
)

var (
	inboxLatestBatchGauge        = metrics.NewRegisteredGauge("arb/inbox/latest/batch", nil)
	inboxLatestBatchMessageGauge = metrics.NewRegisteredGauge("arb/inbox/latest/batch/message", nil)
)

type InboxTracker struct {
	db         ethdb.Database
	txStreamer *TransactionStreamer
	mutex      sync.Mutex
	validator  *staker.BlockValidator
	das        arbstate.DataAvailabilityReader

	batchMetaMutex sync.Mutex
	batchMeta      *containers.LruCache[uint64, BatchMetadata]
}

func NewInboxTracker(db ethdb.Database, txStreamer *TransactionStreamer, das arbstate.DataAvailabilityReader) (*InboxTracker, error) {
	// We support a nil txStreamer for the pruning code
	if txStreamer != nil && txStreamer.chainConfig.ArbitrumChainParams.DataAvailabilityCommittee && das == nil {
		return nil, errors.New("data availability service required but unconfigured")
	}
	tracker := &InboxTracker{
		db:         db,
		txStreamer: txStreamer,
		das:        das,
		batchMeta:  containers.NewLruCache[uint64, BatchMetadata](1000),
	}
	return tracker, nil
}

func (t *InboxTracker) SetBlockValidator(validator *staker.BlockValidator) {
	t.validator = validator
}

func (t *InboxTracker) Initialize() error {
	batch := t.db.NewBatch()

	hasKey, err := t.db.Has(delayedMessageCountKey)
	if err != nil {
		return err
	}
	if !hasKey {
		value, err := rlp.EncodeToBytes(uint64(0))
		if err != nil {
			return err
		}
		err = batch.Put(delayedMessageCountKey, value)
		if err != nil {
			return err
		}
	}

	hasKey, err = t.db.Has(sequencerBatchCountKey)
	if err != nil {
		return err
	}
	if !hasKey {
		value, err := rlp.EncodeToBytes(uint64(0))
		if err != nil {
			return err
		}
		err = batch.Put(sequencerBatchCountKey, value)
		if err != nil {
			return err
		}
		log.Info("InboxTracker", "SequencerBatchCount", 0)
	}

	err = batch.Write()
	if err != nil {
		return err
	}

	return nil
}

var AccumulatorNotFoundErr = errors.New("accumulator not found")

func (t *InboxTracker) deleteBatchMetadataStartingAt(dbBatch ethdb.Batch, startIndex uint64) error {
	t.batchMetaMutex.Lock()
	defer t.batchMetaMutex.Unlock()
	iter := t.db.NewIterator(sequencerBatchMetaPrefix, uint64ToKey(startIndex))
	defer iter.Release()
	for iter.Next() {
		curKey := iter.Key()
		err := dbBatch.Delete(curKey)
		if err != nil {
			return err
		}
		curIndex := binary.BigEndian.Uint64(bytes.TrimPrefix(curKey, sequencerBatchMetaPrefix))
		t.batchMeta.Remove(curIndex)
	}
	return iter.Error()
}

func (t *InboxTracker) GetDelayedAcc(seqNum uint64) (common.Hash, error) {
	key := dbKey(rlpDelayedMessagePrefix, seqNum)
	hasKey, err := t.db.Has(key)
	if err != nil {
		return common.Hash{}, err
	}
	if !hasKey {
		key = dbKey(legacyDelayedMessagePrefix, seqNum)
		hasKey, err = t.db.Has(key)
		if err != nil {
			return common.Hash{}, err
		}
		if !hasKey {
			return common.Hash{}, fmt.Errorf("%w: not found delayed %d", AccumulatorNotFoundErr, seqNum)
		}
	}
	data, err := t.db.Get(key)
	if err != nil {
		return common.Hash{}, err
	}
	if len(data) < 32 {
		return common.Hash{}, errors.New("delayed message entry missing accumulator")
	}
	var hash common.Hash
	copy(hash[:], data[:32])
	return hash, nil
}

func (t *InboxTracker) GetDelayedCount() (uint64, error) {
	data, err := t.db.Get(delayedMessageCountKey)
	if err != nil {
		return 0, err
	}
	var count uint64
	err = rlp.DecodeBytes(data, &count)
	if err != nil {
		return 0, err
	}
	return count, nil
}

type BatchMetadata struct {
	Accumulator         common.Hash
	MessageCount        arbutil.MessageIndex
	DelayedMessageCount uint64
	ParentChainBlock    uint64
}

func (t *InboxTracker) GetBatchMetadata(seqNum uint64) (BatchMetadata, error) {
	t.batchMetaMutex.Lock()
	defer t.batchMetaMutex.Unlock()
	metadata, exist := t.batchMeta.Get(seqNum)
	if exist {
		return metadata, nil
	}
	key := dbKey(sequencerBatchMetaPrefix, seqNum)
	hasKey, err := t.db.Has(key)
	if err != nil {
		return BatchMetadata{}, err
	}
	if !hasKey {
		return BatchMetadata{}, fmt.Errorf("%w: no metadata for batch %d", AccumulatorNotFoundErr, seqNum)
	}
	data, err := t.db.Get(key)
	if err != nil {
		return BatchMetadata{}, err
	}
	err = rlp.DecodeBytes(data, &metadata)
	if err != nil {
		return BatchMetadata{}, err
	}
	t.batchMeta.Add(seqNum, metadata)
	return metadata, nil
}

func (t *InboxTracker) GetBatchMessageCount(seqNum uint64) (arbutil.MessageIndex, error) {
	metadata, err := t.GetBatchMetadata(seqNum)
	return metadata.MessageCount, err
}

// GetBatchAcc is a convenience function wrapping GetBatchMetadata
func (t *InboxTracker) GetBatchAcc(seqNum uint64) (common.Hash, error) {
	metadata, err := t.GetBatchMetadata(seqNum)
	return metadata.Accumulator, err
}

func (t *InboxTracker) GetBatchCount() (uint64, error) {
	data, err := t.db.Get(sequencerBatchCountKey)
	if err != nil {
		return 0, err
	}
	var count uint64
	err = rlp.DecodeBytes(data, &count)
	if err != nil {
		return 0, err
	}
	return count, nil
}

func (t *InboxTracker) PopulateFeedBacklog(broadcastServer *broadcaster.Broadcaster) error {
	batchCount, err := t.GetBatchCount()
	if err != nil {
		return fmt.Errorf("error getting batch count: %w", err)
	}
	var startMessage arbutil.MessageIndex
	if batchCount >= 2 {
		// As in AddSequencerBatches, we want to keep the most recent batch's messages.
		// This prevents issues if a user's L1 is a bit behind or an L1 reorg occurs.
		// `batchCount - 2` is the index of the batch before the last batch.
		batchIndex := batchCount - 2
		startMessage, err = t.GetBatchMessageCount(batchIndex)
		if err != nil {
			return fmt.Errorf("error getting batch %v message count: %w", batchIndex, err)
		}
	}
	messageCount, err := t.txStreamer.GetMessageCount()
	if err != nil {
		return fmt.Errorf("error getting tx streamer message count: %w", err)
	}
	var feedMessages []*broadcaster.BroadcastFeedMessage
	for seqNum := startMessage; seqNum < messageCount; seqNum++ {
		message, err := t.txStreamer.GetMessage(seqNum)
		if err != nil {
			return fmt.Errorf("error getting message %v: %w", seqNum, err)
		}
		feedMessage, err := broadcastServer.NewBroadcastFeedMessage(*message, seqNum)
		if err != nil {
			return fmt.Errorf("error creating broadcast feed message %v: %w", seqNum, err)
		}
		feedMessages = append(feedMessages, feedMessage)
	}
	broadcastServer.BroadcastFeedMessages(feedMessages)
	return nil
}

func (t *InboxTracker) legacyGetDelayedMessageAndAccumulator(seqNum uint64) (*arbostypes.L1IncomingMessage, common.Hash, error) {
	key := dbKey(legacyDelayedMessagePrefix, seqNum)
	data, err := t.db.Get(key)
	if err != nil {
		return nil, common.Hash{}, err
	}
	if len(data) < 32 {
		return nil, common.Hash{}, errors.New("delayed message legacy entry missing accumulator")
	}
	var acc common.Hash
	copy(acc[:], data[:32])
	msg, err := arbostypes.ParseIncomingL1Message(bytes.NewReader(data[32:]), nil)
	return msg, acc, err
}

func (t *InboxTracker) GetDelayedMessageAccumulatorAndParentChainBlockNumber(seqNum uint64) (*arbostypes.L1IncomingMessage, common.Hash, uint64, error) {
	delayedMessageKey := dbKey(rlpDelayedMessagePrefix, seqNum)
	exists, err := t.db.Has(delayedMessageKey)
	if err != nil {
		return nil, common.Hash{}, 0, err
	}
	if !exists {
		msg, acc, err := t.legacyGetDelayedMessageAndAccumulator(seqNum)
		return msg, acc, 0, err
	}
	data, err := t.db.Get(delayedMessageKey)
	if err != nil {
		return nil, common.Hash{}, 0, err
	}
	if len(data) < 32 {
		return nil, common.Hash{}, 0, errors.New("delayed message new entry missing accumulator")
	}
	var acc common.Hash
	copy(acc[:], data[:32])
	var msg *arbostypes.L1IncomingMessage
	err = rlp.DecodeBytes(data[32:], &msg)
	if err != nil {
		return msg, acc, 0, err
	}

	parentChainBlockNumberKey := dbKey(parentChainBlockNumberPrefix, seqNum)
	exists, err = t.db.Has(parentChainBlockNumberKey)
	if err != nil {
		return msg, acc, 0, err
	}
	if !exists {
		return msg, acc, msg.Header.BlockNumber, nil
	}
	data, err = t.db.Get(parentChainBlockNumberKey)
	if err != nil {
		return msg, acc, 0, err
	}

	return msg, acc, binary.BigEndian.Uint64(data), nil

}

func (t *InboxTracker) GetDelayedMessage(seqNum uint64) (*arbostypes.L1IncomingMessage, error) {
	msg, _, _, err := t.GetDelayedMessageAccumulatorAndParentChainBlockNumber(seqNum)
	return msg, err
}

func (t *InboxTracker) GetDelayedMessageBytes(seqNum uint64) ([]byte, error) {
	msg, err := t.GetDelayedMessage(seqNum)
	if err != nil {
		return nil, err
	}
	return msg.Serialize()
}

func (t *InboxTracker) AddDelayedMessages(messages []*DelayedInboxMessage, hardReorg bool) error {
	if len(messages) == 0 {
		return nil
	}
	t.mutex.Lock()
	defer t.mutex.Unlock()

	pos, err := messages[0].Message.Header.SeqNum()
	if err != nil {
		return err
	}

	if !hardReorg {
		// This math is safe to do as we know len(messages) > 0
		haveLastAcc, err := t.GetDelayedAcc(pos + uint64(len(messages)) - 1)
		if err == nil {
			if haveLastAcc == messages[len(messages)-1].AfterInboxAcc() {
				// We already have these delayed messages
				return nil
			}
		} else if !errors.Is(err, AccumulatorNotFoundErr) {
			return err
		}
	}

	var nextAcc common.Hash
	if pos > 0 {
		var err error
		nextAcc, err = t.GetDelayedAcc(pos - 1)
		if err != nil {
			if errors.Is(err, AccumulatorNotFoundErr) {
				return errors.New("missing previous delayed message")
			}
			return err
		}
	}

	batch := t.db.NewBatch()
	for _, message := range messages {
		seqNum, err := message.Message.Header.SeqNum()
		if err != nil {
			return err
		}

		if seqNum != pos {
			return errors.New("unexpected delayed sequence number")
		}

		if nextAcc != message.BeforeInboxAcc {
			return errors.New("previous delayed accumulator mismatch")
		}
		nextAcc = message.AfterInboxAcc()

		delayedMsgKey := dbKey(rlpDelayedMessagePrefix, seqNum)

		msgData, err := rlp.EncodeToBytes(message.Message)
		if err != nil {
			return err
		}
		data := nextAcc.Bytes()
		data = append(data, msgData...)
		err = batch.Put(delayedMsgKey, data)
		if err != nil {
			return err
		}

		if message.ParentChainBlockNumber != message.Message.Header.BlockNumber {
			parentChainBlockNumberKey := dbKey(parentChainBlockNumberPrefix, seqNum)
			parentChainBlockNumberByte := make([]byte, 8)
			binary.BigEndian.PutUint64(parentChainBlockNumberByte, message.ParentChainBlockNumber)
			err = batch.Put(parentChainBlockNumberKey, parentChainBlockNumberByte)
			if err != nil {
				return err
			}
		}

		pos++
	}

	return t.setDelayedCountReorgAndWriteBatch(batch, pos, true)
}

// All-in-one delayed message count adjuster. Can go forwards or backwards.
// Requires the mutex is held. Sets the delayed count and performs any sequencer batch reorg necessary.
// Also deletes any future delayed messages.
func (t *InboxTracker) setDelayedCountReorgAndWriteBatch(batch ethdb.Batch, newDelayedCount uint64, canReorgBatches bool) error {
	err := deleteStartingAt(t.db, batch, rlpDelayedMessagePrefix, uint64ToKey(newDelayedCount))
	if err != nil {
		return err
	}
	err = deleteStartingAt(t.db, batch, parentChainBlockNumberPrefix, uint64ToKey(newDelayedCount))
	if err != nil {
		return err
	}
	err = deleteStartingAt(t.db, batch, legacyDelayedMessagePrefix, uint64ToKey(newDelayedCount))
	if err != nil {
		return err
	}

	countData, err := rlp.EncodeToBytes(newDelayedCount)
	if err != nil {
		return err
	}
	err = batch.Put(delayedMessageCountKey, countData)
	if err != nil {
		return err
	}

	seqBatchIter := t.db.NewIterator(delayedSequencedPrefix, uint64ToKey(newDelayedCount+1))
	defer seqBatchIter.Release()
	var reorgSeqBatchesToCount *uint64
	for seqBatchIter.Next() {
		var batchSeqNum uint64
		if err := rlp.DecodeBytes(seqBatchIter.Value(), &batchSeqNum); err != nil {
			return err
		}
		if !canReorgBatches {
			return fmt.Errorf("reorging of sequencer batch number %v via delayed messages reorg to count %v disabled in this instance", batchSeqNum, newDelayedCount)
		}
		if err := batch.Delete(seqBatchIter.Key()); err != nil {
			return err
		}
		if reorgSeqBatchesToCount == nil {
			// Set the count to the first deleted sequence number.
			// E.g. if the deleted sequence number is 1, set the count to 1,
			// meaning that the last and only batch is at sequence number 0.
			reorgSeqBatchesToCount = &batchSeqNum
		}
	}
	if err := seqBatchIter.Error(); err != nil {
		return err
	}
	// Release the iterator early.
	// It's fine to call Release multiple times,
	// which we'll do because of the defer.
	seqBatchIter.Release()
	if reorgSeqBatchesToCount == nil {
		return batch.Write()
	}

	count := *reorgSeqBatchesToCount
	if t.validator != nil {
		t.validator.ReorgToBatchCount(count)
	}
	countData, err = rlp.EncodeToBytes(count)
	if err != nil {
		return err
	}
	if err := batch.Put(sequencerBatchCountKey, countData); err != nil {
		return err
	}
	log.Warn("InboxTracker delayed message reorg is causing a sequencer batch reorg", "sequencerBatchCount", count, "delayedCount", newDelayedCount)

	if err := t.deleteBatchMetadataStartingAt(batch, count); err != nil {
		return err
	}
	var prevMesssageCount arbutil.MessageIndex
	if count > 0 {
		prevMesssageCount, err = t.GetBatchMessageCount(count - 1)
		if err != nil {
			return err
		}
	}
	// Writes batch
	return t.txStreamer.ReorgToAndEndBatch(batch, prevMesssageCount)
}

type multiplexerBackend struct {
	batchSeqNum           uint64
	batches               []*SequencerInboxBatch
	positionWithinMessage uint64

	ctx    context.Context
	client arbutil.L1Interface
	inbox  *InboxTracker
}

func (b *multiplexerBackend) PeekSequencerInbox() ([]byte, error) {
	if len(b.batches) == 0 {
		return nil, errors.New("read past end of specified sequencer batches")
	}
	return b.batches[0].Serialize(b.ctx, b.client)
}

func (b *multiplexerBackend) GetSequencerInboxPosition() uint64 {
	return b.batchSeqNum
}

func (b *multiplexerBackend) AdvanceSequencerInbox() {
	b.batchSeqNum++
	if len(b.batches) > 0 {
		b.batches = b.batches[1:]
	}
}

func (b *multiplexerBackend) GetPositionWithinMessage() uint64 {
	return b.positionWithinMessage
}

func (b *multiplexerBackend) SetPositionWithinMessage(pos uint64) {
	b.positionWithinMessage = pos
}

func (b *multiplexerBackend) ReadDelayedInbox(seqNum uint64) (*arbostypes.L1IncomingMessage, error) {
	if len(b.batches) == 0 || seqNum >= b.batches[0].AfterDelayedCount {
		return nil, errors.New("attempted to read past end of sequencer batch delayed messages")
	}
	return b.inbox.GetDelayedMessage(seqNum)
}

var delayedMessagesMismatch = errors.New("sequencer batch delayed messages missing or different")

func (t *InboxTracker) AddSequencerBatches(ctx context.Context, client arbutil.L1Interface, batches []*SequencerInboxBatch) error {
	if len(batches) == 0 {
		return nil
	}
	t.mutex.Lock()
	defer t.mutex.Unlock()

	pos := batches[0].SequenceNumber
	startPos := pos
	var nextAcc common.Hash
	var prevbatchmeta BatchMetadata
	if pos > 0 {
		var err error
		prevbatchmeta, err = t.GetBatchMetadata(pos - 1)
		nextAcc = prevbatchmeta.Accumulator
		if errors.Is(err, AccumulatorNotFoundErr) {
			return errors.New("missing previous sequencer batch")
		} else if err != nil {
			return err
		}
	}

	dbBatch := t.db.NewBatch()
	err := deleteStartingAt(t.db, dbBatch, delayedSequencedPrefix, uint64ToKey(prevbatchmeta.DelayedMessageCount+1))
	if err != nil {
		return err
	}

	for _, batch := range batches {
		if batch.SequenceNumber != pos {
			return errors.New("unexpected batch sequence number")
		}
		if nextAcc != batch.BeforeInboxAcc {
			return errors.New("previous batch accumulator mismatch")
		}

		if batch.AfterDelayedCount > 0 {
			haveDelayedAcc, err := t.GetDelayedAcc(batch.AfterDelayedCount - 1)
			if errors.Is(err, AccumulatorNotFoundErr) {
				// We somehow missed a referenced delayed message; go back and look for it
				return delayedMessagesMismatch
			}
			if err != nil {
				return err
			}
			if haveDelayedAcc != batch.AfterDelayedAcc {
				// We somehow missed a delayed message reorg; go back and look for it
				return delayedMessagesMismatch
			}
		}

		nextAcc = batch.AfterInboxAcc
		pos++
	}

	var messages []arbostypes.MessageWithMetadata
	backend := &multiplexerBackend{
		batchSeqNum: batches[0].SequenceNumber,
		batches:     batches,

		inbox:  t,
		ctx:    ctx,
		client: client,
	}
	multiplexer := arbstate.NewInboxMultiplexer(backend, prevbatchmeta.DelayedMessageCount, t.das, arbstate.KeysetValidate)
	batchMessageCounts := make(map[uint64]arbutil.MessageIndex)
	currentpos := prevbatchmeta.MessageCount + 1
	for {
		if len(backend.batches) == 0 {
			break
		}
		batchSeqNum := backend.batches[0].SequenceNumber
		msg, err := multiplexer.Pop(ctx)
		if err != nil {
			return err
		}
		messages = append(messages, *msg)
		batchMessageCounts[batchSeqNum] = currentpos
		currentpos += 1
	}

	lastBatchMeta := prevbatchmeta
	batchMetas := make(map[uint64]BatchMetadata, len(batches))
	for _, batch := range batches {
		meta := BatchMetadata{
			Accumulator:         batch.AfterInboxAcc,
			DelayedMessageCount: batch.AfterDelayedCount,
			MessageCount:        batchMessageCounts[batch.SequenceNumber],
			ParentChainBlock:    batch.ParentChainBlockNumber,
		}
		batchMetas[batch.SequenceNumber] = meta
		metaBytes, err := rlp.EncodeToBytes(meta)
		if err != nil {
			return err
		}
		err = dbBatch.Put(dbKey(sequencerBatchMetaPrefix, batch.SequenceNumber), metaBytes)
		if err != nil {
			return err
		}

		seqNumData, err := rlp.EncodeToBytes(batch.SequenceNumber)
		if err != nil {
			return err
		}
		if batch.AfterDelayedCount < lastBatchMeta.DelayedMessageCount {
			return errors.New("batch delayed message count went backwards")
		}
		if batch.AfterDelayedCount > lastBatchMeta.DelayedMessageCount {
			err = dbBatch.Put(dbKey(delayedSequencedPrefix, batch.AfterDelayedCount), seqNumData)
			if err != nil {
				return err
			}
		}
		lastBatchMeta = meta
	}

	err = t.deleteBatchMetadataStartingAt(dbBatch, pos)
	if err != nil {
		return err
	}
	countData, err := rlp.EncodeToBytes(pos)
	if err != nil {
		return err
	}
	err = dbBatch.Put(sequencerBatchCountKey, countData)
	if err != nil {
		return err
	}

	newMessageCount := prevbatchmeta.MessageCount + arbutil.MessageIndex(len(messages))
	var latestL1Block uint64
	if len(batches) > 0 {
		latestL1Block = batches[len(batches)-1].ParentChainBlockNumber
	}
	var latestTimestamp uint64
	if len(messages) > 0 {
		latestTimestamp = messages[len(messages)-1].Message.Header.Timestamp
	}
	log.Info(
		"InboxTracker",
		"sequencerBatchCount", pos,
		"messageCount", newMessageCount,
		"l1Block", latestL1Block,
		"l1Timestamp", time.Unix(int64(latestTimestamp), 0),
	)
	inboxLatestBatchGauge.Update(int64(pos))
	inboxLatestBatchMessageGauge.Update(int64(newMessageCount))

	if t.validator != nil {
		t.validator.ReorgToBatchCount(startPos)
	}

	// This also writes the batch
	err = t.txStreamer.AddMessagesAndEndBatch(prevbatchmeta.MessageCount, true, messages, dbBatch)
	if err != nil {
		return err
	}

	// Update the batchMeta cache immediately after writing the batch
	t.batchMetaMutex.Lock()
	for seqNum, meta := range batchMetas {
		t.batchMeta.Add(seqNum, meta)
	}
	t.batchMetaMutex.Unlock()

	if t.txStreamer.broadcastServer != nil && pos > 1 {
		prevprevbatchmeta, err := t.GetBatchMetadata(pos - 2)
		if errors.Is(err, AccumulatorNotFoundErr) {
			return errors.New("missing previous previous sequencer batch")
		}
		if err != nil {
			return err
		}
		if prevprevbatchmeta.MessageCount > 0 {
			// Confirm messages from batch before last batch
			t.txStreamer.broadcastServer.Confirm(prevprevbatchmeta.MessageCount - 1)
		}
	}

	return nil
}

func (t *InboxTracker) ReorgDelayedTo(count uint64, canReorgBatches bool) error {
	t.mutex.Lock()
	defer t.mutex.Unlock()

	currentCount, err := t.GetDelayedCount()
	if err != nil {
		return err
	}
	if currentCount == count {
		return nil
	}
	if currentCount < count {
		return errors.New("attempted to reorg to future delayed count")
	}

	return t.setDelayedCountReorgAndWriteBatch(t.db.NewBatch(), count, canReorgBatches)
}

func (t *InboxTracker) ReorgBatchesTo(count uint64) error {
	t.mutex.Lock()
	defer t.mutex.Unlock()

	var prevBatchMeta BatchMetadata
	if count > 0 {
		var err error
		prevBatchMeta, err = t.GetBatchMetadata(count - 1)
		if errors.Is(err, AccumulatorNotFoundErr) {
			return errors.New("attempted to reorg to future batch count")
		}
		if err != nil {
			return err
		}
	}

	if t.validator != nil {
		t.validator.ReorgToBatchCount(count)
	}

	dbBatch := t.db.NewBatch()

	err := deleteStartingAt(t.db, dbBatch, delayedSequencedPrefix, uint64ToKey(prevBatchMeta.DelayedMessageCount+1))
	if err != nil {
		return err
	}
	err = t.deleteBatchMetadataStartingAt(dbBatch, count)
	if err != nil {
		return err
	}
	countData, err := rlp.EncodeToBytes(count)
	if err != nil {
		return err
	}
	err = dbBatch.Put(sequencerBatchCountKey, countData)
	if err != nil {
		return err
	}
	log.Info("InboxTracker", "SequencerBatchCount", count)
	return t.txStreamer.ReorgToAndEndBatch(dbBatch, prevBatchMeta.MessageCount)
}

'''
'''--- arbnode/inbox_tracker_test.go ---
package arbnode

import (
	"testing"

	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/offchainlabs/nitro/util/containers"
)

func TestDeleteBatchMetadata(t *testing.T) {
	testBytes := []byte("bloop")

	tracker := &InboxTracker{
		db:        rawdb.NewMemoryDatabase(),
		batchMeta: containers.NewLruCache[uint64, BatchMetadata](100),
	}

	for i := uint64(0); i < 30; i += 1 {
		err := tracker.db.Put(dbKey(sequencerBatchMetaPrefix, i), testBytes)
		Require(t, err)
		if i%5 != 0 {
			tracker.batchMeta.Add(i, BatchMetadata{})
		}
	}

	batch := tracker.db.NewBatch()
	err := tracker.deleteBatchMetadataStartingAt(batch, 15)
	if err != nil {
		Fail(t, "deleteBatchMetadataStartingAt returned error: ", err)
	}
	err = batch.Write()
	Require(t, err)

	for i := uint64(0); i < 15; i += 1 {
		has, err := tracker.db.Has(dbKey(sequencerBatchMetaPrefix, i))
		Require(t, err)
		if !has {
			Fail(t, "value removed from db: ", i)
		}
		if i%5 != 0 {
			if !tracker.batchMeta.Contains(i) {
				Fail(t, "value removed from cache: ", i)
			}
		}
	}

	for i := uint64(15); i < 30; i += 1 {
		has, err := tracker.db.Has(dbKey(sequencerBatchMetaPrefix, i))
		Require(t, err)
		if has {
			Fail(t, "value not removed from db: ", i)
		}
		if tracker.batchMeta.Contains(i) {
			Fail(t, "value removed from cache: ", i)
		}
	}

}

'''
'''--- arbnode/maintenance.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbnode/redislock"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

// Regularly runs db compaction if configured
type MaintenanceRunner struct {
	stopwaiter.StopWaiter

	exec            execution.FullExecutionClient
	config          MaintenanceConfigFetcher
	seqCoordinator  *SeqCoordinator
	dbs             []ethdb.Database
	lastMaintenance time.Time

	// lock is used to ensures that at any given time, only single node is on
	// maintenance mode.
	lock *redislock.Simple
}

type MaintenanceConfig struct {
	TimeOfDay string              `koanf:"time-of-day" reload:"hot"`
	Lock      redislock.SimpleCfg `koanf:"lock" reload:"hot"`

	// Generated: the minutes since start of UTC day to compact at
	minutesAfterMidnight int
	enabled              bool
}

// Returns true if successful
func (c *MaintenanceConfig) parseDbCompactionTime() bool {
	if c.TimeOfDay == "" {
		return true
	}
	parts := strings.Split(c.TimeOfDay, ":")
	if len(parts) != 2 {
		return false
	}
	hours, err := strconv.Atoi(parts[0])
	if err != nil || hours >= 24 {
		return false
	}
	minutes, err := strconv.Atoi(parts[1])
	if err != nil || minutes >= 60 {
		return false
	}
	c.enabled = true
	c.minutesAfterMidnight = hours*60 + minutes
	return true
}

func (c *MaintenanceConfig) Validate() error {
	if !c.parseDbCompactionTime() {
		return fmt.Errorf("expected sequencer coordinator db compaction time to be in 24-hour HH:MM format but got \"%v\"", c.TimeOfDay)
	}
	return nil
}

func MaintenanceConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".time-of-day", DefaultMaintenanceConfig.TimeOfDay, "UTC 24-hour time of day to run maintenance (currently only db compaction) at (e.g. 15:00)")
	redislock.AddConfigOptions(prefix+".lock", f)
}

var DefaultMaintenanceConfig = MaintenanceConfig{
	TimeOfDay: "",
	Lock:      redislock.DefaultCfg,

	minutesAfterMidnight: 0,
}

type MaintenanceConfigFetcher func() *MaintenanceConfig

func NewMaintenanceRunner(config MaintenanceConfigFetcher, seqCoordinator *SeqCoordinator, dbs []ethdb.Database, exec execution.FullExecutionClient) (*MaintenanceRunner, error) {
	cfg := config()
	if err := cfg.Validate(); err != nil {
		return nil, fmt.Errorf("validating config: %w", err)
	}
	res := &MaintenanceRunner{
		exec:            exec,
		config:          config,
		seqCoordinator:  seqCoordinator,
		dbs:             dbs,
		lastMaintenance: time.Now().UTC(),
	}

	if seqCoordinator != nil {
		c := func() *redislock.SimpleCfg { return &cfg.Lock }
		r := func() bool { return true } // always ready to lock
		rl, err := redislock.NewSimple(seqCoordinator.Client, c, r)
		if err != nil {
			return nil, fmt.Errorf("creating new simple redis lock: %w", err)
		}
		res.lock = rl
	}
	return res, nil
}

func (mr *MaintenanceRunner) Start(ctxIn context.Context) {
	mr.StopWaiter.Start(ctxIn, mr)
	mr.CallIteratively(mr.maybeRunMaintenance)
}

func wentPastTimeOfDay(before time.Time, after time.Time, timeOfDay int) bool {
	if !after.After(before) {
		return false
	}
	if after.Sub(before) >= time.Hour*24 {
		return true
	}
	prevMinutes := before.Hour()*60 + before.Minute()
	newMinutes := after.Hour()*60 + after.Minute()
	if newMinutes < prevMinutes {
		newMinutes += 60 * 24
	}
	dbCompactionMinutes := timeOfDay
	if dbCompactionMinutes < prevMinutes {
		dbCompactionMinutes += 60 * 24
	}
	return prevMinutes < dbCompactionMinutes && newMinutes >= dbCompactionMinutes
}

func (mr *MaintenanceRunner) maybeRunMaintenance(ctx context.Context) time.Duration {
	config := mr.config()
	if !config.enabled {
		return time.Minute
	}

	now := time.Now().UTC()

	if !wentPastTimeOfDay(mr.lastMaintenance, now, config.minutesAfterMidnight) {
		return time.Minute
	}

	if mr.seqCoordinator == nil {
		mr.lastMaintenance = now
		mr.runMaintenance()
		return time.Minute
	}

	if !mr.lock.AttemptLock(ctx) {
		return time.Minute
	}
	defer mr.lock.Release(ctx)

	log.Info("Attempting avoiding lockout and handing off", "targetTime", config.TimeOfDay)
	// Avoid lockout for the sequencer and try to handoff.
	if mr.seqCoordinator.AvoidLockout(ctx) && mr.seqCoordinator.TryToHandoffChosenOne(ctx) {
		mr.lastMaintenance = now
		mr.runMaintenance()
	}
	defer mr.seqCoordinator.SeekLockout(ctx) // needs called even if c.Zombify returns false

	return time.Minute
}

func (mr *MaintenanceRunner) runMaintenance() {
	log.Info("Compacting databases (this may take a while...)")
	results := make(chan error, len(mr.dbs))
	expected := 0
	for _, db := range mr.dbs {
		expected++
		db := db
		go func() {
			results <- db.Compact(nil, nil)
		}()
	}
	expected++
	go func() {
		results <- mr.exec.Maintenance()
	}()
	for i := 0; i < expected; i++ {
		err := <-results
		if err != nil {
			log.Warn("maintenance error", "err", err)
		}
	}
	log.Info("Done compacting databases")
}

'''
'''--- arbnode/maintenance_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"testing"
	"time"
)

func TestWentPastTimeOfDay(t *testing.T) {
	eleven_pm := time.Date(2000, 1, 1, 23, 0, 0, 0, time.UTC)
	midnight := time.Date(2000, 1, 2, 0, 0, 0, 0, time.UTC)
	one_am := time.Date(2000, 1, 2, 1, 0, 0, 0, time.UTC)

	for _, tc := range []struct {
		before, after time.Time
		timeOfDay     string
		want          bool
	}{
		{before: eleven_pm, after: eleven_pm, timeOfDay: "23:00"},
		{before: midnight, after: midnight, timeOfDay: "00:00"},
		{before: one_am, after: one_am, timeOfDay: "1:00"},
		{before: eleven_pm, after: midnight, timeOfDay: "23:30", want: true},
		{before: eleven_pm, after: midnight, timeOfDay: "00:00", want: true},
		{before: eleven_pm, after: one_am, timeOfDay: "00:00", want: true},
		{before: eleven_pm, after: one_am, timeOfDay: "01:00", want: true},
		{before: eleven_pm, after: one_am, timeOfDay: "02:00"},
		{before: eleven_pm, after: one_am, timeOfDay: "12:00"},
		{before: midnight, after: one_am, timeOfDay: "00:00"},
		{before: midnight, after: one_am, timeOfDay: "00:30", want: true},
		{before: midnight, after: one_am, timeOfDay: "01:00", want: true},
	} {
		config := MaintenanceConfig{TimeOfDay: tc.timeOfDay}
		Require(t, config.Validate(), "Failed to validate sample config")

		if got := wentPastTimeOfDay(tc.before, tc.after, config.minutesAfterMidnight); got != tc.want {
			t.Errorf("wentPastTimeOfDay(%v, %v, %q) = %T want %T", tc.before, tc.after, tc.timeOfDay, got, tc.want)
		}
	}
}

'''
'''--- arbnode/message_pruner.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"bytes"
	"context"
	"encoding/binary"
	"fmt"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"

	flag "github.com/spf13/pflag"
)

type MessagePruner struct {
	stopwaiter.StopWaiter
	transactionStreamer         *TransactionStreamer
	inboxTracker                *InboxTracker
	config                      MessagePrunerConfigFetcher
	pruningLock                 sync.Mutex
	lastPruneDone               time.Time
	cachedPrunedMessages        uint64
	cachedPrunedDelayedMessages uint64
}

type MessagePrunerConfig struct {
	Enable bool `koanf:"enable"`
	// Message pruning interval.
	PruneInterval  time.Duration `koanf:"prune-interval" reload:"hot"`
	MinBatchesLeft uint64        `koanf:"min-batches-left" reload:"hot"`
}

type MessagePrunerConfigFetcher func() *MessagePrunerConfig

var DefaultMessagePrunerConfig = MessagePrunerConfig{
	Enable:         true,
	PruneInterval:  time.Minute,
	MinBatchesLeft: 2,
}

func MessagePrunerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultMessagePrunerConfig.Enable, "enable message pruning")
	f.Duration(prefix+".prune-interval", DefaultMessagePrunerConfig.PruneInterval, "interval for running message pruner")
	f.Uint64(prefix+".min-batches-left", DefaultMessagePrunerConfig.MinBatchesLeft, "min number of batches not pruned")
}

func NewMessagePruner(transactionStreamer *TransactionStreamer, inboxTracker *InboxTracker, config MessagePrunerConfigFetcher) *MessagePruner {
	return &MessagePruner{
		transactionStreamer: transactionStreamer,
		inboxTracker:        inboxTracker,
		config:              config,
	}
}

func (m *MessagePruner) Start(ctxIn context.Context) {
	m.StopWaiter.Start(ctxIn, m)
}

func (m *MessagePruner) UpdateLatestConfirmed(count arbutil.MessageIndex, globalState validator.GoGlobalState) {
	locked := m.pruningLock.TryLock()
	if !locked {
		return
	}

	if m.lastPruneDone.Add(m.config().PruneInterval).After(time.Now()) {
		m.pruningLock.Unlock()
		return
	}
	err := m.LaunchThreadSafe(func(ctx context.Context) {
		defer m.pruningLock.Unlock()
		err := m.prune(ctx, count, globalState)
		if err != nil && ctx.Err() == nil {
			log.Error("error while pruning", "err", err)
		}
	})
	if err != nil {
		log.Info("failed launching prune thread", "err", err)
		m.pruningLock.Unlock()
	}
}

func (m *MessagePruner) prune(ctx context.Context, count arbutil.MessageIndex, globalState validator.GoGlobalState) error {
	trimBatchCount := globalState.Batch
	minBatchesLeft := m.config().MinBatchesLeft
	batchCount, err := m.inboxTracker.GetBatchCount()
	if err != nil {
		return err
	}
	if batchCount < trimBatchCount+minBatchesLeft {
		if batchCount < minBatchesLeft {
			return nil
		}
		trimBatchCount = batchCount - minBatchesLeft
	}
	if trimBatchCount < 1 {
		return nil
	}
	endBatchMetadata, err := m.inboxTracker.GetBatchMetadata(trimBatchCount - 1)
	if err != nil {
		return err
	}
	msgCount := endBatchMetadata.MessageCount
	delayedCount := endBatchMetadata.DelayedMessageCount

	return m.deleteOldMessagesFromDB(ctx, msgCount, delayedCount)
}

func (m *MessagePruner) deleteOldMessagesFromDB(ctx context.Context, messageCount arbutil.MessageIndex, delayedMessageCount uint64) error {
	prunedKeysRange, err := deleteFromLastPrunedUptoEndKey(ctx, m.transactionStreamer.db, messagePrefix, &m.cachedPrunedMessages, uint64(messageCount))
	if err != nil {
		return fmt.Errorf("error deleting last batch messages: %w", err)
	}
	if len(prunedKeysRange) > 0 {
		log.Info("Pruned last batch messages:", "first pruned key", prunedKeysRange[0], "last pruned key", prunedKeysRange[len(prunedKeysRange)-1])
	}

	prunedKeysRange, err = deleteFromLastPrunedUptoEndKey(ctx, m.inboxTracker.db, rlpDelayedMessagePrefix, &m.cachedPrunedDelayedMessages, delayedMessageCount)
	if err != nil {
		return fmt.Errorf("error deleting last batch delayed messages: %w", err)
	}
	if len(prunedKeysRange) > 0 {
		log.Info("Pruned last batch delayed messages:", "first pruned key", prunedKeysRange[0], "last pruned key", prunedKeysRange[len(prunedKeysRange)-1])
	}
	return nil
}

// deleteFromLastPrunedUptoEndKey is similar to deleteFromRange but automatically populates the start key
// cachedStartMinKey must not be nil. It's set to the new start key at the end of this function if successful.
func deleteFromLastPrunedUptoEndKey(ctx context.Context, db ethdb.Database, prefix []byte, cachedStartMinKey *uint64, endMinKey uint64) ([]uint64, error) {
	startMinKey := *cachedStartMinKey
	if startMinKey == 0 {
		startIter := db.NewIterator(prefix, uint64ToKey(1))
		if !startIter.Next() {
			return nil, nil
		}
		startMinKey = binary.BigEndian.Uint64(bytes.TrimPrefix(startIter.Key(), prefix))
		startIter.Release()
	}
	if endMinKey <= startMinKey {
		*cachedStartMinKey = startMinKey
		return nil, nil
	}
	keys, err := deleteFromRange(ctx, db, prefix, startMinKey, endMinKey-1)
	if err == nil {
		*cachedStartMinKey = endMinKey - 1
	}
	return keys, err
}

'''
'''--- arbnode/message_pruner_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"testing"

	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/offchainlabs/nitro/arbutil"
)

func TestMessagePrunerWithPruningEligibleMessagePresent(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	messagesCount := uint64(2 * 100 * 1024)
	inboxTrackerDb, transactionStreamerDb, pruner := setupDatabase(t, 2*100*1024, 2*100*1024)
	err := pruner.deleteOldMessagesFromDB(ctx, arbutil.MessageIndex(messagesCount), messagesCount)
	Require(t, err)

	checkDbKeys(t, messagesCount, transactionStreamerDb, messagePrefix)
	checkDbKeys(t, messagesCount, inboxTrackerDb, rlpDelayedMessagePrefix)

}

func TestMessagePrunerTwoHalves(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	messagesCount := uint64(10)
	_, transactionStreamerDb, pruner := setupDatabase(t, messagesCount, messagesCount)
	// In first iteration message till messagesCount/2 are tried to be deleted.
	err := pruner.deleteOldMessagesFromDB(ctx, arbutil.MessageIndex(messagesCount/2), messagesCount/2)
	Require(t, err)
	// In first iteration all the message till messagesCount/2 are deleted.
	checkDbKeys(t, messagesCount/2, transactionStreamerDb, messagePrefix)
	// In second iteration message till messagesCount are tried to be deleted.
	err = pruner.deleteOldMessagesFromDB(ctx, arbutil.MessageIndex(messagesCount), messagesCount)
	Require(t, err)
	// In second iteration all the message till messagesCount are deleted.
	checkDbKeys(t, messagesCount, transactionStreamerDb, messagePrefix)
}

func TestMessagePrunerPruneTillLessThenEqualTo(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	messagesCount := uint64(10)
	inboxTrackerDb, transactionStreamerDb, pruner := setupDatabase(t, 2*messagesCount, 20)
	err := inboxTrackerDb.Delete(dbKey(messagePrefix, 9))
	Require(t, err)
	err = pruner.deleteOldMessagesFromDB(ctx, arbutil.MessageIndex(messagesCount), messagesCount)
	Require(t, err)
	hasKey, err := transactionStreamerDb.Has(dbKey(messagePrefix, messagesCount))
	Require(t, err)
	if !hasKey {
		Fail(t, "Key", 10, "with prefix", string(messagePrefix), "should be present after pruning")
	}
}

func TestMessagePrunerWithNoPruningEligibleMessagePresent(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	messagesCount := uint64(10)
	inboxTrackerDb, transactionStreamerDb, pruner := setupDatabase(t, messagesCount, messagesCount)
	err := pruner.deleteOldMessagesFromDB(ctx, arbutil.MessageIndex(messagesCount), messagesCount)
	Require(t, err)

	checkDbKeys(t, uint64(messagesCount), transactionStreamerDb, messagePrefix)
	checkDbKeys(t, messagesCount, inboxTrackerDb, rlpDelayedMessagePrefix)

}

func setupDatabase(t *testing.T, messageCount, delayedMessageCount uint64) (ethdb.Database, ethdb.Database, *MessagePruner) {

	transactionStreamerDb := rawdb.NewMemoryDatabase()
	for i := uint64(0); i < uint64(messageCount); i++ {
		err := transactionStreamerDb.Put(dbKey(messagePrefix, i), []byte{})
		Require(t, err)
	}

	inboxTrackerDb := rawdb.NewMemoryDatabase()
	for i := uint64(0); i < delayedMessageCount; i++ {
		err := inboxTrackerDb.Put(dbKey(rlpDelayedMessagePrefix, i), []byte{})
		Require(t, err)
	}

	return inboxTrackerDb, transactionStreamerDb, &MessagePruner{
		transactionStreamer: &TransactionStreamer{db: transactionStreamerDb},
		inboxTracker:        &InboxTracker{db: inboxTrackerDb},
	}
}

func checkDbKeys(t *testing.T, endCount uint64, db ethdb.Database, prefix []byte) {
	t.Helper()
	for i := uint64(0); i < endCount; i++ {
		hasKey, err := db.Has(dbKey(prefix, i))
		Require(t, err)
		if i == 0 || i == endCount-1 {
			if !hasKey {
				Fail(t, "Key", i, "with prefix", string(prefix), "should be present after pruning")
			}
		} else {
			if hasKey {
				Fail(t, "Key", i, "with prefix", string(prefix), "should not be present after pruning")
			}
		}
	}
}

'''
'''--- arbnode/node.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"
	"strings"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/arbnode/redislock"
	"github.com/offchainlabs/nitro/arbnode/resourcemanager"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcastclient"
	"github.com/offchainlabs/nitro/broadcastclients"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/staker/validatorwallet"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

func GenerateRollupConfig(prod bool, wasmModuleRoot common.Hash, rollupOwner common.Address, chainConfig *params.ChainConfig, serializedChainConfig []byte, loserStakeEscrow common.Address) rollupgen.Config {
	var confirmPeriod uint64
	if prod {
		confirmPeriod = 45818
	} else {
		confirmPeriod = 20
	}
	return rollupgen.Config{
		ConfirmPeriodBlocks:      confirmPeriod,
		ExtraChallengeTimeBlocks: 200,
		StakeToken:               common.Address{},
		BaseStake:                big.NewInt(params.Ether),
		WasmModuleRoot:           wasmModuleRoot,
		Owner:                    rollupOwner,
		LoserStakeEscrow:         loserStakeEscrow,
		ChainId:                  chainConfig.ChainID,
		// TODO could the ChainConfig be just []byte?
		ChainConfig: string(serializedChainConfig),
		SequencerInboxMaxTimeVariation: rollupgen.ISequencerInboxMaxTimeVariation{
			DelayBlocks:   big.NewInt(60 * 60 * 24 / 15),
			FutureBlocks:  big.NewInt(12),
			DelaySeconds:  big.NewInt(60 * 60 * 24),
			FutureSeconds: big.NewInt(60 * 60),
		},
	}
}

type Config struct {
	Sequencer           bool                        `koanf:"sequencer"`
	ParentChainReader   headerreader.Config         `koanf:"parent-chain-reader" reload:"hot"`
	InboxReader         InboxReaderConfig           `koanf:"inbox-reader" reload:"hot"`
	DelayedSequencer    DelayedSequencerConfig      `koanf:"delayed-sequencer" reload:"hot"`
	BatchPoster         BatchPosterConfig           `koanf:"batch-poster" reload:"hot"`
	MessagePruner       MessagePrunerConfig         `koanf:"message-pruner" reload:"hot"`
	BlockValidator      staker.BlockValidatorConfig `koanf:"block-validator" reload:"hot"`
	Feed                broadcastclient.FeedConfig  `koanf:"feed" reload:"hot"`
	Staker              staker.L1ValidatorConfig    `koanf:"staker" reload:"hot"`
	SeqCoordinator      SeqCoordinatorConfig        `koanf:"seq-coordinator"`
	DataAvailability    das.DataAvailabilityConfig  `koanf:"data-availability"`
	SyncMonitor         SyncMonitorConfig           `koanf:"sync-monitor"`
	Dangerous           DangerousConfig             `koanf:"dangerous"`
	TransactionStreamer TransactionStreamerConfig   `koanf:"transaction-streamer" reload:"hot"`
	Maintenance         MaintenanceConfig           `koanf:"maintenance" reload:"hot"`
	ResourceMgmt        resourcemanager.Config      `koanf:"resource-mgmt" reload:"hot"`
}

func (c *Config) Validate() error {
	if c.ParentChainReader.Enable && c.Sequencer && !c.DelayedSequencer.Enable {
		log.Warn("delayed sequencer is not enabled, despite sequencer and l1 reader being enabled")
	}
	if c.DelayedSequencer.Enable && !c.Sequencer {
		return errors.New("cannot enable delayed sequencer without enabling sequencer")
	}
	if err := c.BlockValidator.Validate(); err != nil {
		return err
	}
	if err := c.Maintenance.Validate(); err != nil {
		return err
	}
	if err := c.InboxReader.Validate(); err != nil {
		return err
	}
	if err := c.BatchPoster.Validate(); err != nil {
		return err
	}
	if err := c.Feed.Validate(); err != nil {
		return err
	}
	if err := c.Staker.Validate(); err != nil {
		return err
	}
	return nil
}

func (c *Config) ValidatorRequired() bool {
	if c.BlockValidator.Enable {
		return true
	}
	if c.Staker.Enable {
		return c.Staker.ValidatorRequired()
	}
	return false
}

func ConfigAddOptions(prefix string, f *flag.FlagSet, feedInputEnable bool, feedOutputEnable bool) {
	f.Bool(prefix+".sequencer", ConfigDefault.Sequencer, "enable sequencer")
	headerreader.AddOptions(prefix+".parent-chain-reader", f)
	InboxReaderConfigAddOptions(prefix+".inbox-reader", f)
	DelayedSequencerConfigAddOptions(prefix+".delayed-sequencer", f)
	BatchPosterConfigAddOptions(prefix+".batch-poster", f)
	MessagePrunerConfigAddOptions(prefix+".message-pruner", f)
	staker.BlockValidatorConfigAddOptions(prefix+".block-validator", f)
	broadcastclient.FeedConfigAddOptions(prefix+".feed", f, feedInputEnable, feedOutputEnable)
	staker.L1ValidatorConfigAddOptions(prefix+".staker", f)
	SeqCoordinatorConfigAddOptions(prefix+".seq-coordinator", f)
	das.DataAvailabilityConfigAddNodeOptions(prefix+".data-availability", f)
	SyncMonitorConfigAddOptions(prefix+".sync-monitor", f)
	DangerousConfigAddOptions(prefix+".dangerous", f)
	TransactionStreamerConfigAddOptions(prefix+".transaction-streamer", f)
	MaintenanceConfigAddOptions(prefix+".maintenance", f)
}

var ConfigDefault = Config{
	Sequencer:           false,
	ParentChainReader:   headerreader.DefaultConfig,
	InboxReader:         DefaultInboxReaderConfig,
	DelayedSequencer:    DefaultDelayedSequencerConfig,
	BatchPoster:         DefaultBatchPosterConfig,
	MessagePruner:       DefaultMessagePrunerConfig,
	BlockValidator:      staker.DefaultBlockValidatorConfig,
	Feed:                broadcastclient.FeedConfigDefault,
	Staker:              staker.DefaultL1ValidatorConfig,
	SeqCoordinator:      DefaultSeqCoordinatorConfig,
	DataAvailability:    das.DefaultDataAvailabilityConfig,
	SyncMonitor:         DefaultSyncMonitorConfig,
	Dangerous:           DefaultDangerousConfig,
	TransactionStreamer: DefaultTransactionStreamerConfig,
	ResourceMgmt:        resourcemanager.DefaultConfig,
	Maintenance:         DefaultMaintenanceConfig,
}

func ConfigDefaultL1Test() *Config {
	config := ConfigDefaultL1NonSequencerTest()
	config.DelayedSequencer = TestDelayedSequencerConfig
	config.BatchPoster = TestBatchPosterConfig
	config.SeqCoordinator = TestSeqCoordinatorConfig
	config.Sequencer = true
	config.Dangerous.NoSequencerCoordinator = true

	return config
}

func ConfigDefaultL1NonSequencerTest() *Config {
	config := ConfigDefault
	config.ParentChainReader = headerreader.TestConfig
	config.InboxReader = TestInboxReaderConfig
	config.DelayedSequencer.Enable = false
	config.BatchPoster.Enable = false
	config.SeqCoordinator.Enable = false
	config.BlockValidator = staker.TestBlockValidatorConfig
	config.Staker = staker.TestL1ValidatorConfig
	config.Staker.Enable = false
	config.BlockValidator.ValidationServer.URL = ""

	return &config
}

func ConfigDefaultL2Test() *Config {
	config := ConfigDefault
	config.ParentChainReader.Enable = false
	config.SeqCoordinator = TestSeqCoordinatorConfig
	config.Feed.Input.Verify.Dangerous.AcceptMissing = true
	config.Feed.Output.Signed = false
	config.SeqCoordinator.Signer.ECDSA.AcceptSequencer = false
	config.SeqCoordinator.Signer.ECDSA.Dangerous.AcceptMissing = true
	config.Staker = staker.TestL1ValidatorConfig
	config.Staker.Enable = false
	config.BlockValidator.ValidationServer.URL = ""
	config.TransactionStreamer = DefaultTransactionStreamerConfig

	return &config
}

type DangerousConfig struct {
	NoL1Listener           bool `koanf:"no-l1-listener"`
	NoSequencerCoordinator bool `koanf:"no-sequencer-coordinator"`
}

var DefaultDangerousConfig = DangerousConfig{
	NoL1Listener:           false,
	NoSequencerCoordinator: false,
}

func DangerousConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".no-l1-listener", DefaultDangerousConfig.NoL1Listener, "DANGEROUS! disables listening to L1. To be used in test nodes only")
	f.Bool(prefix+".no-sequencer-coordinator", DefaultDangerousConfig.NoSequencerCoordinator, "DANGEROUS! allows sequencing without sequencer-coordinator")
}

type Node struct {
	ArbDB                   ethdb.Database
	Stack                   *node.Node
	Execution               execution.FullExecutionClient
	L1Reader                *headerreader.HeaderReader
	TxStreamer              *TransactionStreamer
	DeployInfo              *chaininfo.RollupAddresses
	InboxReader             *InboxReader
	InboxTracker            *InboxTracker
	DelayedSequencer        *DelayedSequencer
	BatchPoster             *BatchPoster
	MessagePruner           *MessagePruner
	BlockValidator          *staker.BlockValidator
	StatelessBlockValidator *staker.StatelessBlockValidator
	Staker                  *staker.Staker
	BroadcastServer         *broadcaster.Broadcaster
	BroadcastClients        *broadcastclients.BroadcastClients
	SeqCoordinator          *SeqCoordinator
	MaintenanceRunner       *MaintenanceRunner
	DASLifecycleManager     *das.LifecycleManager
	ClassicOutboxRetriever  *ClassicOutboxRetriever
	SyncMonitor             *SyncMonitor
	configFetcher           ConfigFetcher
	ctx                     context.Context
}

type ConfigFetcher interface {
	Get() *Config
	Start(context.Context)
	StopAndWait()
	Started() bool
}

func checkArbDbSchemaVersion(arbDb ethdb.Database) error {
	var version uint64
	hasVersion, err := arbDb.Has(dbSchemaVersion)
	if err != nil {
		return err
	}
	if hasVersion {
		versionBytes, err := arbDb.Get(dbSchemaVersion)
		if err != nil {
			return err
		}
		version = binary.BigEndian.Uint64(versionBytes)
	}
	for version != currentDbSchemaVersion {
		batch := arbDb.NewBatch()
		switch version {
		case 0:
			// No database updates are necessary for database format version 0->1.
			// This version adds a new format for delayed messages in the inbox tracker,
			// but it can still read the old format for old messages.
		default:
			return fmt.Errorf("unsupported database format version %v", version)
		}

		// Increment version and flush the batch
		version++
		versionBytes := make([]uint8, 8)
		binary.BigEndian.PutUint64(versionBytes, version)
		err = batch.Put(dbSchemaVersion, versionBytes)
		if err != nil {
			return err
		}
		err = batch.Write()
		if err != nil {
			return err
		}
	}
	return nil
}

func StakerDataposter(
	ctx context.Context, db ethdb.Database, l1Reader *headerreader.HeaderReader,
	transactOpts *bind.TransactOpts, cfgFetcher ConfigFetcher, syncMonitor *SyncMonitor,
) (*dataposter.DataPoster, error) {
	if transactOpts == nil {
		return nil, nil
	}
	cfg := cfgFetcher.Get()
	mdRetriever := func(ctx context.Context, blockNum *big.Int) ([]byte, error) {
		return nil, nil
	}
	redisC, err := redisutil.RedisClientFromURL(cfg.Staker.RedisUrl)
	if err != nil {
		return nil, fmt.Errorf("creating redis client from url: %w", err)
	}
	lockCfgFetcher := func() *redislock.SimpleCfg {
		return &cfg.Staker.RedisLock
	}
	redisLock, err := redislock.NewSimple(redisC, lockCfgFetcher, func() bool { return syncMonitor.Synced() })
	if err != nil {
		return nil, err
	}
	dpCfg := func() *dataposter.DataPosterConfig {
		return &cfg.Staker.DataPoster
	}
	return dataposter.NewDataPoster(ctx,
		&dataposter.DataPosterOpts{
			Database:          db,
			HeaderReader:      l1Reader,
			Auth:              transactOpts,
			RedisClient:       redisC,
			RedisLock:         redisLock,
			Config:            dpCfg,
			MetadataRetriever: mdRetriever,
			// transactOpts is non-nil, it's checked at the beginning.
			RedisKey: transactOpts.From.String() + ".staker-data-poster.queue",
		})
}

func createNodeImpl(
	ctx context.Context,
	stack *node.Node,
	exec execution.FullExecutionClient,
	arbDb ethdb.Database,
	configFetcher ConfigFetcher,
	l2Config *params.ChainConfig,
	l1client arbutil.L1Interface,
	deployInfo *chaininfo.RollupAddresses,
	txOptsValidator *bind.TransactOpts,
	txOptsBatchPoster *bind.TransactOpts,
	dataSigner signature.DataSignerFunc,
	fatalErrChan chan error,
) (*Node, error) {
	config := configFetcher.Get()

	err := checkArbDbSchemaVersion(arbDb)
	if err != nil {
		return nil, err
	}

	l2ChainId := l2Config.ChainID.Uint64()

	syncMonitor := NewSyncMonitor(&config.SyncMonitor)
	var classicOutbox *ClassicOutboxRetriever
	classicMsgDb, err := stack.OpenDatabase("classic-msg", 0, 0, "", true)
	if err != nil {
		if l2Config.ArbitrumChainParams.GenesisBlockNum > 0 {
			log.Warn("Classic Msg Database not found", "err", err)
		}
		classicOutbox = nil
	} else {
		classicOutbox = NewClassicOutboxRetriever(classicMsgDb)
	}

	var l1Reader *headerreader.HeaderReader
	if config.ParentChainReader.Enable {
		arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1client)
		l1Reader, err = headerreader.New(ctx, l1client, func() *headerreader.Config { return &configFetcher.Get().ParentChainReader }, arbSys)
		if err != nil {
			return nil, err
		}
	}

	var broadcastServer *broadcaster.Broadcaster
	if config.Feed.Output.Enable {
		var maybeDataSigner signature.DataSignerFunc
		if config.Feed.Output.Signed {
			if dataSigner == nil {
				return nil, errors.New("cannot sign outgoing feed")
			}
			maybeDataSigner = dataSigner
		}
		broadcastServer = broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &configFetcher.Get().Feed.Output }, l2ChainId, fatalErrChan, maybeDataSigner)
	}

	transactionStreamerConfigFetcher := func() *TransactionStreamerConfig { return &configFetcher.Get().TransactionStreamer }
	txStreamer, err := NewTransactionStreamer(arbDb, l2Config, exec, broadcastServer, fatalErrChan, transactionStreamerConfigFetcher)
	if err != nil {
		return nil, err
	}
	var coordinator *SeqCoordinator
	var bpVerifier *contracts.AddressVerifier
	if deployInfo != nil && l1client != nil {
		sequencerInboxAddr := deployInfo.SequencerInbox

		seqInboxCaller, err := bridgegen.NewSequencerInboxCaller(sequencerInboxAddr, l1client)
		if err != nil {
			return nil, err
		}
		bpVerifier = contracts.NewAddressVerifier(seqInboxCaller)
	}

	if config.SeqCoordinator.Enable {
		coordinator, err = NewSeqCoordinator(dataSigner, bpVerifier, txStreamer, exec, syncMonitor, config.SeqCoordinator)
		if err != nil {
			return nil, err
		}
	} else if config.Sequencer && !config.Dangerous.NoSequencerCoordinator {
		return nil, errors.New("sequencer must be enabled with coordinator, unless dangerous.no-sequencer-coordinator set")
	}
	dbs := []ethdb.Database{arbDb}
	maintenanceRunner, err := NewMaintenanceRunner(func() *MaintenanceConfig { return &configFetcher.Get().Maintenance }, coordinator, dbs, exec)
	if err != nil {
		return nil, err
	}

	var broadcastClients *broadcastclients.BroadcastClients
	if config.Feed.Input.Enable() {
		currentMessageCount, err := txStreamer.GetMessageCount()
		if err != nil {
			return nil, err
		}

		broadcastClients, err = broadcastclients.NewBroadcastClients(
			func() *broadcastclient.Config { return &configFetcher.Get().Feed.Input },
			l2ChainId,
			currentMessageCount,
			txStreamer,
			nil,
			fatalErrChan,
			bpVerifier,
		)
		if err != nil {
			return nil, err
		}
	}

	if !config.ParentChainReader.Enable {
		return &Node{
			ArbDB:                   arbDb,
			Stack:                   stack,
			Execution:               exec,
			L1Reader:                nil,
			TxStreamer:              txStreamer,
			DeployInfo:              nil,
			InboxReader:             nil,
			InboxTracker:            nil,
			DelayedSequencer:        nil,
			BatchPoster:             nil,
			MessagePruner:           nil,
			BlockValidator:          nil,
			StatelessBlockValidator: nil,
			Staker:                  nil,
			BroadcastServer:         broadcastServer,
			BroadcastClients:        broadcastClients,
			SeqCoordinator:          coordinator,
			MaintenanceRunner:       maintenanceRunner,
			DASLifecycleManager:     nil,
			ClassicOutboxRetriever:  classicOutbox,
			SyncMonitor:             syncMonitor,
			configFetcher:           configFetcher,
			ctx:                     ctx,
		}, nil
	}

	if deployInfo == nil {
		return nil, errors.New("deployinfo is nil")
	}
	delayedBridge, err := NewDelayedBridge(l1client, deployInfo.Bridge, deployInfo.DeployedAt)
	if err != nil {
		return nil, err
	}
	sequencerInbox, err := NewSequencerInbox(l1client, deployInfo.SequencerInbox, int64(deployInfo.DeployedAt))
	if err != nil {
		return nil, err
	}

	var daWriter das.DataAvailabilityServiceWriter
	var daReader das.DataAvailabilityServiceReader
	var dasLifecycleManager *das.LifecycleManager
	if config.DataAvailability.Enable {
		if config.BatchPoster.Enable {
			daWriter, daReader, dasLifecycleManager, err = das.CreateBatchPosterDAS(ctx, &config.DataAvailability, dataSigner, l1client, deployInfo.SequencerInbox)
			if err != nil {
				return nil, err
			}
		} else {
			daReader, dasLifecycleManager, err = das.CreateDAReaderForNode(ctx, &config.DataAvailability, l1Reader, &deployInfo.SequencerInbox)
			if err != nil {
				return nil, err
			}
		}

		daReader = das.NewReaderTimeoutWrapper(daReader, config.DataAvailability.RequestTimeout)

		if config.DataAvailability.PanicOnError {
			if daWriter != nil {
				daWriter = das.NewWriterPanicWrapper(daWriter)
			}
			daReader = das.NewReaderPanicWrapper(daReader)
		}
	} else if l2Config.ArbitrumChainParams.DataAvailabilityCommittee {
		return nil, errors.New("a data availability service is required for this chain, but it was not configured")
	}

	inboxTracker, err := NewInboxTracker(arbDb, txStreamer, daReader)
	if err != nil {
		return nil, err
	}
	inboxReader, err := NewInboxReader(inboxTracker, l1client, l1Reader, new(big.Int).SetUint64(deployInfo.DeployedAt), delayedBridge, sequencerInbox, func() *InboxReaderConfig { return &configFetcher.Get().InboxReader })
	if err != nil {
		return nil, err
	}
	txStreamer.SetInboxReaders(inboxReader, delayedBridge)

	var statelessBlockValidator *staker.StatelessBlockValidator
	if config.BlockValidator.ValidationServer.URL != "" {
		statelessBlockValidator, err = staker.NewStatelessBlockValidator(
			inboxReader,
			inboxTracker,
			txStreamer,
			exec,
			rawdb.NewTable(arbDb, storage.BlockValidatorPrefix),
			daReader,
			func() *staker.BlockValidatorConfig { return &configFetcher.Get().BlockValidator },
			stack,
		)
	} else {
		err = errors.New("no validator url specified")
	}
	if err != nil {
		if config.ValidatorRequired() || config.Staker.Enable {
			return nil, fmt.Errorf("%w: failed to init block validator", err)
		}
		log.Warn("validation not supported", "err", err)
		statelessBlockValidator = nil
	}

	var blockValidator *staker.BlockValidator
	if config.ValidatorRequired() {
		blockValidator, err = staker.NewBlockValidator(
			statelessBlockValidator,
			inboxTracker,
			txStreamer,
			func() *staker.BlockValidatorConfig { return &configFetcher.Get().BlockValidator },
			fatalErrChan,
		)
		if err != nil {
			return nil, err
		}
	}

	var stakerObj *staker.Staker
	var messagePruner *MessagePruner

	if config.Staker.Enable {
		dp, err := StakerDataposter(
			ctx,
			rawdb.NewTable(arbDb, storage.StakerPrefix),
			l1Reader,
			txOptsValidator,
			configFetcher,
			syncMonitor,
		)
		if err != nil {
			return nil, err
		}
		getExtraGas := func() uint64 { return configFetcher.Get().Staker.ExtraGas }
		// TODO: factor this out into separate helper, and split rest of node
		// creation into multiple helpers.
		var wallet staker.ValidatorWalletInterface = validatorwallet.NewNoOp(l1client, deployInfo.Rollup)
		if !strings.EqualFold(config.Staker.Strategy, "watchtower") {
			if config.Staker.UseSmartContractWallet || txOptsValidator == nil {
				var existingWalletAddress *common.Address
				if len(config.Staker.ContractWalletAddress) > 0 {
					if !common.IsHexAddress(config.Staker.ContractWalletAddress) {
						log.Error("invalid validator smart contract wallet", "addr", config.Staker.ContractWalletAddress)
						return nil, errors.New("invalid validator smart contract wallet address")
					}
					tmpAddress := common.HexToAddress(config.Staker.ContractWalletAddress)
					existingWalletAddress = &tmpAddress
				}
				wallet, err = validatorwallet.NewContract(dp, existingWalletAddress, deployInfo.ValidatorWalletCreator, deployInfo.Rollup, l1Reader, txOptsValidator, int64(deployInfo.DeployedAt), func(common.Address) {}, getExtraGas)
				if err != nil {
					return nil, err
				}
			} else {
				if len(config.Staker.ContractWalletAddress) > 0 {
					return nil, errors.New("validator contract wallet specified but flag to use a smart contract wallet was not specified")
				}
				wallet, err = validatorwallet.NewEOA(dp, deployInfo.Rollup, l1client, txOptsValidator, getExtraGas)
				if err != nil {
					return nil, err
				}
			}
		}

		var confirmedNotifiers []staker.LatestConfirmedNotifier
		if config.MessagePruner.Enable {
			messagePruner = NewMessagePruner(txStreamer, inboxTracker, func() *MessagePrunerConfig { return &configFetcher.Get().MessagePruner })
			confirmedNotifiers = append(confirmedNotifiers, messagePruner)
		}

		stakerObj, err = staker.NewStaker(l1Reader, wallet, bind.CallOpts{}, config.Staker, blockValidator, statelessBlockValidator, nil, confirmedNotifiers, deployInfo.ValidatorUtils, fatalErrChan)
		if err != nil {
			return nil, err
		}
		if stakerObj.Strategy() == staker.WatchtowerStrategy {
			if err := wallet.Initialize(ctx); err != nil {
				return nil, err
			}
		}
		var txValidatorSenderPtr *common.Address
		if txOptsValidator != nil {
			txValidatorSenderPtr = &txOptsValidator.From
		}
		whitelisted, err := stakerObj.IsWhitelisted(ctx)
		if err != nil {
			return nil, err
		}
		log.Info("running as validator", "txSender", txValidatorSenderPtr, "actingAsWallet", wallet.Address(), "whitelisted", whitelisted, "strategy", config.Staker.Strategy)
	}

	var batchPoster *BatchPoster
	var delayedSequencer *DelayedSequencer
	if config.BatchPoster.Enable {
		if txOptsBatchPoster == nil {
			return nil, errors.New("batchposter, but no TxOpts")
		}
		batchPoster, err = NewBatchPoster(ctx, &BatchPosterOpts{
			DataPosterDB: rawdb.NewTable(arbDb, storage.BatchPosterPrefix),
			L1Reader:     l1Reader,
			Inbox:        inboxTracker,
			Streamer:     txStreamer,
			SyncMonitor:  syncMonitor,
			Config:       func() *BatchPosterConfig { return &configFetcher.Get().BatchPoster },
			DeployInfo:   deployInfo,
			TransactOpts: txOptsBatchPoster,
			DAWriter:     daWriter,
		})
		if err != nil {
			return nil, err
		}
	}

	// always create DelayedSequencer, it won't do anything if it is disabled
	delayedSequencer, err = NewDelayedSequencer(l1Reader, inboxReader, exec, coordinator, func() *DelayedSequencerConfig { return &configFetcher.Get().DelayedSequencer })
	if err != nil {
		return nil, err
	}

	return &Node{
		ArbDB:                   arbDb,
		Stack:                   stack,
		Execution:               exec,
		L1Reader:                l1Reader,
		TxStreamer:              txStreamer,
		DeployInfo:              deployInfo,
		InboxReader:             inboxReader,
		InboxTracker:            inboxTracker,
		DelayedSequencer:        delayedSequencer,
		BatchPoster:             batchPoster,
		MessagePruner:           messagePruner,
		BlockValidator:          blockValidator,
		StatelessBlockValidator: statelessBlockValidator,
		Staker:                  stakerObj,
		BroadcastServer:         broadcastServer,
		BroadcastClients:        broadcastClients,
		SeqCoordinator:          coordinator,
		MaintenanceRunner:       maintenanceRunner,
		DASLifecycleManager:     dasLifecycleManager,
		ClassicOutboxRetriever:  classicOutbox,
		SyncMonitor:             syncMonitor,
		configFetcher:           configFetcher,
		ctx:                     ctx,
	}, nil
}

func (n *Node) OnConfigReload(_ *Config, _ *Config) error {
	// TODO
	return nil
}

func CreateNode(
	ctx context.Context,
	stack *node.Node,
	exec execution.FullExecutionClient,
	arbDb ethdb.Database,
	configFetcher ConfigFetcher,
	l2Config *params.ChainConfig,
	l1client arbutil.L1Interface,
	deployInfo *chaininfo.RollupAddresses,
	txOptsValidator *bind.TransactOpts,
	txOptsBatchPoster *bind.TransactOpts,
	dataSigner signature.DataSignerFunc,
	fatalErrChan chan error,
) (*Node, error) {
	currentNode, err := createNodeImpl(ctx, stack, exec, arbDb, configFetcher, l2Config, l1client, deployInfo, txOptsValidator, txOptsBatchPoster, dataSigner, fatalErrChan)
	if err != nil {
		return nil, err
	}
	var apis []rpc.API
	if currentNode.BlockValidator != nil {
		apis = append(apis, rpc.API{
			Namespace: "arb",
			Version:   "1.0",
			Service:   &BlockValidatorAPI{val: currentNode.BlockValidator},
			Public:    false,
		})
	}
	if currentNode.StatelessBlockValidator != nil {
		apis = append(apis, rpc.API{
			Namespace: "arbvalidator",
			Version:   "1.0",
			Service: &BlockValidatorDebugAPI{
				val: currentNode.StatelessBlockValidator,
			},
			Public: false,
		})
	}

	stack.RegisterAPIs(apis)

	return currentNode, nil
}

func (n *Node) Start(ctx context.Context) error {
	// config is the static config at start, not a dynamic config
	config := n.configFetcher.Get()
	execClient, ok := n.Execution.(*gethexec.ExecutionNode)
	if !ok {
		execClient = nil
	}
	if execClient != nil {
		err := execClient.Initialize(ctx, n, n.SyncMonitor)
		if err != nil {
			return fmt.Errorf("error initializing exec client: %w", err)
		}
	}
	n.SyncMonitor.Initialize(n.InboxReader, n.TxStreamer, n.SeqCoordinator, n.Execution)
	err := n.Stack.Start()
	if err != nil {
		return fmt.Errorf("error starting geth stack: %w", err)
	}
	err = n.Execution.Start(ctx)
	if err != nil {
		return fmt.Errorf("error starting exec client: %w", err)
	}
	if n.InboxTracker != nil {
		err = n.InboxTracker.Initialize()
		if err != nil {
			return fmt.Errorf("error initializing inbox tracker: %w", err)
		}
	}
	if n.BroadcastServer != nil {
		err = n.BroadcastServer.Initialize()
		if err != nil {
			return fmt.Errorf("error initializing feed broadcast server: %w", err)
		}
	}
	if n.InboxTracker != nil && n.BroadcastServer != nil && config.Sequencer && !config.SeqCoordinator.Enable {
		// Normally, the sequencer would populate the feed backlog when it acquires the lockout.
		// However, if the sequencer coordinator is not enabled, we must populate the backlog on startup.
		err = n.InboxTracker.PopulateFeedBacklog(n.BroadcastServer)
		if err != nil {
			return fmt.Errorf("error populating feed backlog on startup: %w", err)
		}
	}
	err = n.TxStreamer.Start(ctx)
	if err != nil {
		return fmt.Errorf("error starting transaction streamer: %w", err)
	}
	if n.InboxReader != nil {
		err = n.InboxReader.Start(ctx)
		if err != nil {
			return fmt.Errorf("error starting inbox reader: %w", err)
		}
	}
	if n.SeqCoordinator != nil {
		n.SeqCoordinator.Start(ctx)
	}
	if n.MaintenanceRunner != nil {
		n.MaintenanceRunner.Start(ctx)
	}
	if n.DelayedSequencer != nil {
		n.DelayedSequencer.Start(ctx)
	}
	if n.BatchPoster != nil {
		n.BatchPoster.Start(ctx)
	}
	if n.MessagePruner != nil {
		n.MessagePruner.Start(ctx)
	}
	if n.Staker != nil {
		err = n.Staker.Initialize(ctx)
		if err != nil {
			return fmt.Errorf("error initializing staker: %w", err)
		}
	}
	if n.StatelessBlockValidator != nil {
		err = n.StatelessBlockValidator.Start(ctx)
		if err != nil {
			if n.configFetcher.Get().ValidatorRequired() {
				return fmt.Errorf("error initializing stateless block validator: %w", err)
			}
			log.Info("validation not set up", "err", err)
			n.StatelessBlockValidator = nil
			n.BlockValidator = nil
		}
	}
	if n.BlockValidator != nil {
		err = n.BlockValidator.Initialize(ctx)
		if err != nil {
			return fmt.Errorf("error initializing block validator: %w", err)
		}
		err = n.BlockValidator.Start(ctx)
		if err != nil {
			return fmt.Errorf("error starting block validator: %w", err)
		}
	}
	if n.Staker != nil {
		n.Staker.Start(ctx)
	}
	if n.L1Reader != nil {
		n.L1Reader.Start(ctx)
	}
	if n.BroadcastServer != nil {
		err = n.BroadcastServer.Start(ctx)
		if err != nil {
			return fmt.Errorf("error starting feed broadcast server: %w", err)
		}
	}
	if n.BroadcastClients != nil {
		go func() {
			if n.InboxReader != nil {
				select {
				case <-n.InboxReader.CaughtUp():
				case <-ctx.Done():
					return
				}
			}
			n.BroadcastClients.Start(ctx)
		}()
	}
	if n.configFetcher != nil {
		n.configFetcher.Start(ctx)
	}
	return nil
}

func (n *Node) StopAndWait() {
	if n.Execution != nil {
		n.Execution.StopAndWait()
	}
	if n.MaintenanceRunner != nil && n.MaintenanceRunner.Started() {
		n.MaintenanceRunner.StopAndWait()
	}
	if n.configFetcher != nil && n.configFetcher.Started() {
		n.configFetcher.StopAndWait()
	}
	if n.SeqCoordinator != nil && n.SeqCoordinator.Started() {
		// Releases the chosen sequencer lockout,
		// and stops the background thread but not the redis client.
		n.SeqCoordinator.PrepareForShutdown()
	}
	n.Stack.StopRPC() // does nothing if not running
	if n.DelayedSequencer != nil && n.DelayedSequencer.Started() {
		n.DelayedSequencer.StopAndWait()
	}
	if n.BatchPoster != nil && n.BatchPoster.Started() {
		n.BatchPoster.StopAndWait()
	}
	if n.MessagePruner != nil && n.MessagePruner.Started() {
		n.MessagePruner.StopAndWait()
	}
	if n.BroadcastServer != nil && n.BroadcastServer.Started() {
		n.BroadcastServer.StopAndWait()
	}
	if n.BroadcastClients != nil {
		n.BroadcastClients.StopAndWait()
	}
	if n.BlockValidator != nil && n.BlockValidator.Started() {
		n.BlockValidator.StopAndWait()
	}
	if n.Staker != nil {
		n.Staker.StopAndWait()
	}
	if n.StatelessBlockValidator != nil {
		n.StatelessBlockValidator.Stop()
	}
	if n.InboxReader != nil && n.InboxReader.Started() {
		n.InboxReader.StopAndWait()
	}
	if n.L1Reader != nil && n.L1Reader.Started() {
		n.L1Reader.StopAndWait()
	}
	if n.TxStreamer.Started() {
		n.TxStreamer.StopAndWait()
	}
	if n.SeqCoordinator != nil && n.SeqCoordinator.Started() {
		// Just stops the redis client (most other stuff was stopped earlier)
		n.SeqCoordinator.StopAndWait()
	}
	if n.DASLifecycleManager != nil {
		n.DASLifecycleManager.StopAndWaitUntil(2 * time.Second)
	}
	if err := n.Stack.Close(); err != nil {
		log.Error("error on stak close", "err", err)
	}
}

'''
'''--- arbnode/redislock/redis.go ---
package redislock

import (
	"context"
	"crypto/rand"
	"errors"
	"math"
	"math/big"
	"strconv"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/go-redis/redis/v8"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

type Simple struct {
	stopwaiter.StopWaiter
	client      redis.UniversalClient
	config      SimpleCfgFetcher
	lockedUntil int64
	mutex       sync.Mutex
	stopping    bool
	readyToLock func() bool
	myId        string
}

type SimpleCfg struct {
	MyId            string        `koanf:"my-id"`
	LockoutDuration time.Duration `koanf:"lockout-duration" reload:"hot"`
	RefreshDuration time.Duration `koanf:"refresh-duration" reload:"hot"`
	Key             string        `koanf:"key"`
	BackgroundLock  bool          `koanf:"background-lock"`
}

type SimpleCfgFetcher func() *SimpleCfg

func AddConfigOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".my-id", "", "this node's id prefix when acquiring the lock (optional)")
	f.Duration(prefix+".lockout-duration", DefaultCfg.LockoutDuration, "how long lock is held")
	f.Duration(prefix+".refresh-duration", DefaultCfg.RefreshDuration, "how long between consecutive calls to redis")
	f.String(prefix+".key", DefaultCfg.Key, "key for lock")
	f.Bool(prefix+".background-lock", DefaultCfg.BackgroundLock, "should node always try grabing lock in background")
}

func NewSimple(client redis.UniversalClient, config SimpleCfgFetcher, readyToLock func() bool) (*Simple, error) {
	randBig, err := rand.Int(rand.Reader, big.NewInt(math.MaxInt64))
	if err != nil {
		return nil, err
	}
	return &Simple{
		myId:        config().MyId + "-" + strconv.FormatInt(randBig.Int64(), 16), // unique even if config is not
		client:      client,
		config:      config,
		readyToLock: readyToLock,
	}, nil
}

var DefaultCfg = SimpleCfg{
	LockoutDuration: time.Minute,
	RefreshDuration: time.Second * 10,
	Key:             "",
	BackgroundLock:  false,
}

func (l *Simple) attemptLock(ctx context.Context) (bool, error) {
	l.mutex.Lock()
	defer l.mutex.Unlock()
	if l.stopping || l.client == nil {
		return false, nil
	}
	if !l.readyToLock() {
		return false, nil
	}
	gotLock := false
	config := l.config()
	timeAtStart := time.Now()

	err := l.client.Watch(ctx, func(tx *redis.Tx) error {
		current, err := tx.Get(ctx, config.Key).Result()
		if errors.Is(err, redis.Nil) {
			current = ""
			err = nil
		}
		if err != nil {
			return err
		}
		if current != "" && (current != l.myId) {
			return nil
		}
		pipe := tx.TxPipeline()
		pipe.Set(ctx, config.Key, l.myId, config.LockoutDuration)
		pipe.PExpireAt(ctx, config.Key, timeAtStart.Add(config.LockoutDuration))
		err = execTestPipe(pipe, ctx)
		if errors.Is(err, redis.TxFailedErr) {
			return nil
		}
		if err != nil {
			return err
		}
		gotLock = true
		return nil
	}, config.Key)

	if !gotLock {
		atomicTimeWrite(&l.lockedUntil, time.Time{})
	}
	if err != nil {
		return false, err
	}
	if gotLock {
		if config.BackgroundLock {
			atomicTimeWrite(&l.lockedUntil, timeAtStart.Add(config.LockoutDuration))
		} else {
			atomicTimeWrite(&l.lockedUntil, timeAtStart.Add(config.RefreshDuration))
		}
	}
	return gotLock, nil
}

func (l *Simple) AttemptLock(ctx context.Context) bool {
	if l.Locked() {
		return true
	}
	if l.config().BackgroundLock {
		return false
	}
	res, err := l.attemptLock(ctx)
	if err != nil {
		log.Error("attemptLock returned error", "err", err)
		return false
	}
	return res
}

func (l *Simple) Locked() bool {
	if l.client == nil {
		return true
	}
	return time.Now().Before(atomicTimeRead(&l.lockedUntil))
}

func (l *Simple) Release(ctx context.Context) {
	l.mutex.Lock()
	defer l.mutex.Unlock()

	if l.client == nil {
		return
	}

	config := l.config()
	err := l.client.Watch(ctx, func(tx *redis.Tx) error {
		current, err := tx.Get(ctx, config.Key).Result()
		if errors.Is(err, redis.Nil) {
			return nil
		}
		if err != nil {
			return err
		}
		if current != l.myId {
			return nil
		}
		pipe := tx.TxPipeline()
		pipe.Del(ctx, config.Key, l.myId)
		err = execTestPipe(pipe, ctx)
		if errors.Is(err, redis.TxFailedErr) {
			return nil
		}
		if err != nil {
			return err
		}
		return nil
	}, config.Key)

	if err != nil {
		log.Error("release returned error", "err", err)
	}
}

func (l *Simple) Start(ctxin context.Context) {
	l.StopWaiter.Start(ctxin, l)
	if l.config().BackgroundLock && l.client != nil {
		l.CallIteratively(func(ctx context.Context) time.Duration {
			_, err := l.attemptLock(ctx)
			if err != nil {
				log.Error("attemptLock returned error", "err", err)
			}
			return l.config().RefreshDuration
		})
	}
}

func (l *Simple) StopAndWait() {
	l.mutex.Lock()
	l.stopping = true
	l.mutex.Unlock()
	l.Release(l.GetContext())
	l.StopWaiter.StopAndWait()
}

func execTestPipe(pipe redis.Pipeliner, ctx context.Context) error {
	cmders, err := pipe.Exec(ctx)
	if err != nil {
		return err
	}
	for _, cmder := range cmders {
		if err := cmder.Err(); err != nil {
			return err
		}
	}
	return nil
}

// notice: It is possible for two consecutive reads to get decreasing values. That shouldn't matter.
func atomicTimeRead(addr *int64) time.Time {
	asint64 := atomic.LoadInt64(addr)
	return time.UnixMilli(asint64)
}

func atomicTimeWrite(addr *int64, t time.Time) {
	asint64 := t.UnixMilli()
	atomic.StoreInt64(addr, asint64)
}

'''
'''--- arbnode/resourcemanager/resource_management.go ---
// Copyright 2023, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package resourcemanager

import (
	"bufio"
	"errors"
	"fmt"
	"net/http"
	"os"
	"regexp"
	"strconv"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/node"
	"github.com/spf13/pflag"
)

var (
	limitCheckDurationHistogram = metrics.NewRegisteredHistogram("arb/rpc/limitcheck/duration", nil, metrics.NewBoundedHistogramSample())
	limitCheckSuccessCounter    = metrics.NewRegisteredCounter("arb/rpc/limitcheck/success", nil)
	limitCheckFailureCounter    = metrics.NewRegisteredCounter("arb/rpc/limitcheck/failure", nil)
	nitroMemLimit               = metrics.GetOrRegisterGauge("arb/memory/limit", nil)
	nitroMemUsage               = metrics.GetOrRegisterGauge("arb/memory/usage", nil)
	errNotSupported             = errors.New("not supported")
)

// Init adds the resource manager's httpServer to a custom hook in geth.
// Geth will add it to the stack of http.Handlers so that it is run
// prior to RPC request handling.
//
// Must be run before the go-ethereum stack is set up (ethereum/go-ethereum/node.New).
func Init(conf *Config) error {
	if conf.MemFreeLimit == "" {
		return nil
	}

	limit, err := parseMemLimit(conf.MemFreeLimit)
	if err != nil {
		return err
	}

	node.WrapHTTPHandler = func(srv http.Handler) (http.Handler, error) {
		var c limitChecker
		c, err := newCgroupsMemoryLimitCheckerIfSupported(limit)
		if errors.Is(err, errNotSupported) {
			log.Error("No method for determining memory usage and limits was discovered, disabled memory limit RPC throttling")
			c = &trivialLimitChecker{}
		}

		return newHttpServer(srv, c), nil
	}
	return nil
}

func parseMemLimit(limitStr string) (int, error) {
	var (
		limit int = 1
		s     string
	)
	if _, err := fmt.Sscanf(limitStr, "%d%s", &limit, &s); err != nil {
		return 0, err
	}

	switch strings.ToUpper(s) {
	case "K", "KB":
		limit <<= 10
	case "M", "MB":
		limit <<= 20
	case "G", "GB":
		limit <<= 30
	case "T", "TB":
		limit <<= 40
	case "B":
	default:
		return 0, fmt.Errorf("unsupported memory limit suffix string %s", s)
	}

	return limit, nil
}

// Config contains the configuration for resourcemanager functionality.
// Currently only a memory limit is supported, other limits may be added
// in the future.
type Config struct {
	MemFreeLimit string `koanf:"mem-free-limit" reload:"hot"`
}

// DefaultConfig has the defaul resourcemanager configuration,
// all limits are disabled.
var DefaultConfig = Config{
	MemFreeLimit: "",
}

// ConfigAddOptions adds the configuration options for resourcemanager.
func ConfigAddOptions(prefix string, f *pflag.FlagSet) {
	f.String(prefix+".mem-free-limit", DefaultConfig.MemFreeLimit, "RPC calls are throttled if free system memory excluding the page cache is below this amount, expressed in bytes or multiples of bytes with suffix B, K, M, G. The limit should be set such that sufficient free memory is left for the page cache in order for the system to be performant")
}

// httpServer implements http.Handler and wraps calls to inner with a resource
// limit check.
type httpServer struct {
	inner http.Handler
	c     limitChecker
}

func newHttpServer(inner http.Handler, c limitChecker) *httpServer {
	return &httpServer{inner: inner, c: c}
}

// ServeHTTP passes req to inner unless any configured system resource
// limit is exceeded, in which case it returns a HTTP 429 error.
func (s *httpServer) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	start := time.Now()
	exceeded, err := s.c.isLimitExceeded()
	limitCheckDurationHistogram.Update(time.Since(start).Nanoseconds())
	if err != nil {
		log.Error("Error checking memory limit", "err", err, "checker", s.c.String())
	} else if exceeded {
		http.Error(w, "Too many requests", http.StatusTooManyRequests)
		limitCheckFailureCounter.Inc(1)
		return
	}

	limitCheckSuccessCounter.Inc(1)
	s.inner.ServeHTTP(w, req)
}

type limitChecker interface {
	isLimitExceeded() (bool, error)
	String() string
}

func isSupported(c limitChecker) bool {
	_, err := c.isLimitExceeded()
	return err == nil
}

// newCgroupsMemoryLimitCheckerIfSupported attempts to auto-discover whether
// Cgroups V1 or V2 is supported for checking system memory limits.
func newCgroupsMemoryLimitCheckerIfSupported(memLimitBytes int) (*cgroupsMemoryLimitChecker, error) {
	c := newCgroupsMemoryLimitChecker(cgroupsV1MemoryFiles, memLimitBytes)
	if isSupported(c) {
		log.Info("Cgroups v1 detected, enabling memory limit RPC throttling")
		return c, nil
	}

	c = newCgroupsMemoryLimitChecker(cgroupsV2MemoryFiles, memLimitBytes)
	if isSupported(c) {
		log.Info("Cgroups v2 detected, enabling memory limit RPC throttling")
		return c, nil
	}

	return nil, errNotSupported
}

// trivialLimitChecker checks no limits, so its limits are never exceeded.
type trivialLimitChecker struct{}

func (_ trivialLimitChecker) isLimitExceeded() (bool, error) {
	return false, nil
}

func (_ trivialLimitChecker) String() string { return "trivial" }

type cgroupsMemoryFiles struct {
	limitFile, usageFile, statsFile string
	activeRe, inactiveRe            *regexp.Regexp
}

const defaultCgroupsV1MemoryDirectory = "/sys/fs/cgroup/memory/"
const defaultCgroupsV2MemoryDirectory = "/sys/fs/cgroup/"

var cgroupsV1MemoryFiles = cgroupsMemoryFiles{
	limitFile:  defaultCgroupsV1MemoryDirectory + "/memory.limit_in_bytes",
	usageFile:  defaultCgroupsV1MemoryDirectory + "/memory.usage_in_bytes",
	statsFile:  defaultCgroupsV1MemoryDirectory + "/memory.stat",
	activeRe:   regexp.MustCompile(`^total_active_file (\d+)`),
	inactiveRe: regexp.MustCompile(`^total_inactive_file (\d+)`),
}
var cgroupsV2MemoryFiles = cgroupsMemoryFiles{
	limitFile:  defaultCgroupsV2MemoryDirectory + "/memory.max",
	usageFile:  defaultCgroupsV2MemoryDirectory + "/memory.current",
	statsFile:  defaultCgroupsV2MemoryDirectory + "/memory.stat",
	activeRe:   regexp.MustCompile(`^active_file (\d+)`),
	inactiveRe: regexp.MustCompile(`^inactive_file (\d+)`),
}

type cgroupsMemoryLimitChecker struct {
	files         cgroupsMemoryFiles
	memLimitBytes int
}

func newCgroupsMemoryLimitChecker(files cgroupsMemoryFiles, memLimitBytes int) *cgroupsMemoryLimitChecker {
	return &cgroupsMemoryLimitChecker{
		files:         files,
		memLimitBytes: memLimitBytes,
	}
}

// isLimitExceeded checks if the system memory free is less than the limit.
// It returns true if the limit is exceeded.
//
// container_memory_working_set_bytes in prometheus is calculated as
// memory.usage_in_bytes - inactive page cache bytes, see
// https://mihai-albert.com/2022/02/13/out-of-memory-oom-in-kubernetes-part-3-memory-metrics-sources-and-tools-to-collect-them/
// This metric is used by kubernetes to report memory in use by the pod,
// but memory.usage_in_bytes also includes the active page cache, which
// can be evicted by the kernel when more memory is needed, see
// https://github.com/kubernetes/kubernetes/issues/43916
// The kernel cannot be guaranteed to move a page from a file from
// active to inactive even when the file is closed, or Nitro is exited.
// For larger chains, Nitro's page cache can grow quite large due to
// the large amount of state that is randomly accessed from disk as each
// block is added. So in checking the limit we also include the active
// page cache.
//
// The limit should be set such that the system has a reasonable amount of
// free memory for the page cache, to avoid cache thrashing on chain state
// access. How much "reasonable" is will depend on access patterns, state
// size, and your application's tolerance for latency.
func (c *cgroupsMemoryLimitChecker) isLimitExceeded() (bool, error) {
	var limit, usage, active, inactive int
	var err error
	if limit, err = readIntFromFile(c.files.limitFile); err != nil {
		return false, err
	}
	if usage, err = readIntFromFile(c.files.usageFile); err != nil {
		return false, err
	}
	if active, err = readFromMemStats(c.files.statsFile, c.files.activeRe); err != nil {
		return false, err
	}
	if inactive, err = readFromMemStats(c.files.statsFile, c.files.inactiveRe); err != nil {
		return false, err
	}

	memLimit := limit - c.memLimitBytes
	memUsage := usage - (active + inactive)
	nitroMemLimit.Update(int64(memLimit))
	nitroMemUsage.Update(int64(memUsage))

	return memUsage >= memLimit, nil
}

func (c cgroupsMemoryLimitChecker) String() string {
	return "CgroupsMemoryLimitChecker"
}

func readIntFromFile(fileName string) (int, error) {
	file, err := os.Open(fileName)
	if err != nil {
		return 0, err
	}

	var limit int
	if _, err = fmt.Fscanf(file, "%d", &limit); err != nil {
		return 0, err
	}
	return limit, nil
}

func readFromMemStats(fileName string, re *regexp.Regexp) (int, error) {
	file, err := os.Open(fileName)
	if err != nil {
		return 0, err
	}

	scanner := bufio.NewScanner(file)
	for scanner.Scan() {
		line := scanner.Text()

		matches := re.FindStringSubmatch(line)

		if len(matches) >= 2 {
			inactive, err := strconv.Atoi(matches[1])
			if err != nil {
				return 0, err
			}
			return inactive, nil
		}
	}

	return 0, errors.New("total_inactive_file not found in " + fileName)
}

'''
'''--- arbnode/resourcemanager/resource_management_test.go ---
// Copyright 2023, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package resourcemanager

import (
	"fmt"
	"os"
	"regexp"
	"testing"
)

func updateFakeCgroupFiles(c *cgroupsMemoryLimitChecker, limit, usage, inactive, active int) error {
	limitFile, err := os.Create(c.files.limitFile)
	if err != nil {
		return err
	}
	if _, err = fmt.Fprintf(limitFile, "%d\n", limit); err != nil {
		return err
	}

	usageFile, err := os.Create(c.files.usageFile)
	if err != nil {
		return err
	}
	if _, err = fmt.Fprintf(usageFile, "%d\n", usage); err != nil {
		return err
	}

	statsFile, err := os.Create(c.files.statsFile)
	if err != nil {
		return err
	}
	_, err = fmt.Fprintf(statsFile, `total_cache 1029980160
total_rss 1016209408
total_inactive_file %d
total_active_file %d
`, inactive, active)
	return err
}

func makeCgroupsTestDir(cgroupDir string) cgroupsMemoryFiles {
	return cgroupsMemoryFiles{
		limitFile:  cgroupDir + "/memory.limit_in_bytes",
		usageFile:  cgroupDir + "/memory.usage_in_bytes",
		statsFile:  cgroupDir + "/memory.stat",
		activeRe:   regexp.MustCompile(`^total_active_file (\d+)`),
		inactiveRe: regexp.MustCompile(`^total_inactive_file (\d+)`),
	}
}

func TestCgroupsFailIfCantOpen(t *testing.T) {
	testFiles := makeCgroupsTestDir(t.TempDir())
	c := newCgroupsMemoryLimitChecker(testFiles, 1024*1024*512)
	if _, err := c.isLimitExceeded(); err == nil {
		t.Fatal("Should fail open if can't read files")
	}
}

func TestCgroupsMemoryLimit(t *testing.T) {
	for _, tc := range []struct {
		desc     string
		sysLimit int
		inactive int
		active   int
		usage    int
		memLimit string
		want     bool
	}{
		{
			desc:     "limit should be exceeded",
			sysLimit: 1000,
			inactive: 50,
			active:   25,
			usage:    1000,
			memLimit: "75B",
			want:     true,
		},
		{
			desc:     "limit should not be exceeded",
			sysLimit: 1000,
			inactive: 51,
			active:   25,
			usage:    1000,
			memLimit: "75b",
			want:     false,
		},
		{
			desc:     "limit (MB) should be exceeded",
			sysLimit: 1000 * 1024 * 1024,
			inactive: 50 * 1024 * 1024,
			active:   25 * 1024 * 1024,
			usage:    1000 * 1024 * 1024,
			memLimit: "75MB",
			want:     true,
		},
		{
			desc:     "limit (MB) should not be exceeded",
			sysLimit: 1000 * 1024 * 1024,
			inactive: 1 + 50*1024*1024,
			active:   25 * 1024 * 1024,
			usage:    1000 * 1024 * 1024,
			memLimit: "75m",
			want:     false,
		},
		{
			desc:     "limit (GB) should be exceeded",
			sysLimit: 1000 * 1024 * 1024 * 1024,
			inactive: 50 * 1024 * 1024 * 1024,
			active:   25 * 1024 * 1024 * 1024,
			usage:    1000 * 1024 * 1024 * 1024,
			memLimit: "75G",
			want:     true,
		},
		{
			desc:     "limit (GB) should not be exceeded",
			sysLimit: 1000 * 1024 * 1024 * 1024,
			inactive: 1 + 50*1024*1024*1024,
			active:   25 * 1024 * 1024 * 1024,
			usage:    1000 * 1024 * 1024 * 1024,
			memLimit: "75gb",
			want:     false,
		},
	} {
		t.Run(tc.desc, func(t *testing.T) {
			testFiles := makeCgroupsTestDir(t.TempDir())
			memLimit, err := parseMemLimit(tc.memLimit)
			if err != nil {
				t.Fatalf("Parsing memory limit failed: %v", err)
			}
			c := newCgroupsMemoryLimitChecker(testFiles, memLimit)
			if err := updateFakeCgroupFiles(c, tc.sysLimit, tc.usage, tc.inactive, tc.active); err != nil {
				t.Fatalf("Updating cgroup files: %v", err)
			}
			exceeded, err := c.isLimitExceeded()
			if err != nil {
				t.Fatalf("Checking if limit exceeded: %v", err)
			}
			if exceeded != tc.want {
				t.Errorf("isLimitExceeded() = %t, want %t", exceeded, tc.want)
			}
		},
		)
	}
}

'''
'''--- arbnode/schema.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

var (
	messagePrefix                []byte = []byte("m") // maps a message sequence number to a message
	legacyDelayedMessagePrefix   []byte = []byte("d") // maps a delayed sequence number to an accumulator and a message as serialized on L1
	rlpDelayedMessagePrefix      []byte = []byte("e") // maps a delayed sequence number to an accumulator and an RLP encoded message
	parentChainBlockNumberPrefix []byte = []byte("p") // maps a delayed sequence number to a parent chain block number
	sequencerBatchMetaPrefix     []byte = []byte("s") // maps a batch sequence number to BatchMetadata
	delayedSequencedPrefix       []byte = []byte("a") // maps a delayed message count to the first sequencer batch sequence number with this delayed count

	messageCountKey        []byte = []byte("_messageCount")        // contains the current message count
	delayedMessageCountKey []byte = []byte("_delayedMessageCount") // contains the current delayed message count
	sequencerBatchCountKey []byte = []byte("_sequencerBatchCount") // contains the current sequencer message count
	dbSchemaVersion        []byte = []byte("_schemaVersion")       // contains a uint64 representing the database schema version
)

const currentDbSchemaVersion uint64 = 1

'''
'''--- arbnode/seq_coordinator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"encoding/binary"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"sync"
	"sync/atomic"
	"time"

	"github.com/go-redis/redis/v8"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

var (
	isActiveSequencer = metrics.NewRegisteredGauge("arb/sequencer/active", nil)
)

type SeqCoordinator struct {
	stopwaiter.StopWaiter

	redisutil.RedisCoordinator

	sync             *SyncMonitor
	streamer         *TransactionStreamer
	sequencer        execution.ExecutionSequencer
	delayedSequencer *DelayedSequencer
	signer           *signature.SignVerify
	config           SeqCoordinatorConfig // warning: static, don't use for hot reloadable fields

	prevChosenSequencer  string
	reportedWantsLockout bool

	lockoutUntil int64 // atomic

	wantsLockoutMutex sync.Mutex // manages access to acquireLockoutAndWriteMessage and generally the wants lockout key
	avoidLockout      int        // If > 0, prevents acquiring the lockout but not extending the lockout if no alternative sequencer wants the lockout. Protected by chosenUpdateMutex.

	redisErrors int // error counter, from workthread
}

type SeqCoordinatorConfig struct {
	Enable                bool          `koanf:"enable"`
	ChosenHealthcheckAddr string        `koanf:"chosen-healthcheck-addr"`
	RedisUrl              string        `koanf:"redis-url"`
	LockoutDuration       time.Duration `koanf:"lockout-duration"`
	LockoutSpare          time.Duration `koanf:"lockout-spare"`
	SeqNumDuration        time.Duration `koanf:"seq-num-duration"`
	UpdateInterval        time.Duration `koanf:"update-interval"`
	RetryInterval         time.Duration `koanf:"retry-interval"`
	HandoffTimeout        time.Duration `koanf:"handoff-timeout"`
	SafeShutdownDelay     time.Duration `koanf:"safe-shutdown-delay"`
	ReleaseRetries        int           `koanf:"release-retries"`
	// Max message per poll.
	MsgPerPoll arbutil.MessageIndex       `koanf:"msg-per-poll"`
	MyUrl      string                     `koanf:"my-url"`
	Signer     signature.SignVerifyConfig `koanf:"signer"`
}

func (c *SeqCoordinatorConfig) Url() string {
	if c.MyUrl == "" {
		return redisutil.INVALID_URL
	}
	return c.MyUrl
}

func SeqCoordinatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultSeqCoordinatorConfig.Enable, "enable sequence coordinator")
	f.String(prefix+".redis-url", DefaultSeqCoordinatorConfig.RedisUrl, "the Redis URL to coordinate via")
	f.String(prefix+".chosen-healthcheck-addr", DefaultSeqCoordinatorConfig.ChosenHealthcheckAddr, "if non-empty, launch an HTTP service binding to this address that returns status code 200 when chosen and 503 otherwise")
	f.Duration(prefix+".lockout-duration", DefaultSeqCoordinatorConfig.LockoutDuration, "")
	f.Duration(prefix+".lockout-spare", DefaultSeqCoordinatorConfig.LockoutSpare, "")
	f.Duration(prefix+".seq-num-duration", DefaultSeqCoordinatorConfig.SeqNumDuration, "")
	f.Duration(prefix+".update-interval", DefaultSeqCoordinatorConfig.UpdateInterval, "")
	f.Duration(prefix+".retry-interval", DefaultSeqCoordinatorConfig.RetryInterval, "")
	f.Duration(prefix+".handoff-timeout", DefaultSeqCoordinatorConfig.HandoffTimeout, "the maximum amount of time to spend waiting for another sequencer to accept the lockout when handing it off on shutdown or db compaction")
	f.Duration(prefix+".safe-shutdown-delay", DefaultSeqCoordinatorConfig.SafeShutdownDelay, "if non-zero will add delay after transferring control")
	f.Int(prefix+".release-retries", DefaultSeqCoordinatorConfig.ReleaseRetries, "the number of times to retry releasing the wants lockout and chosen one status on shutdown")
	f.Uint64(prefix+".msg-per-poll", uint64(DefaultSeqCoordinatorConfig.MsgPerPoll), "will only be marked as wanting the lockout if not too far behind")
	f.String(prefix+".my-url", DefaultSeqCoordinatorConfig.MyUrl, "url for this sequencer if it is the chosen")
	signature.SignVerifyConfigAddOptions(prefix+".signer", f)
}

var DefaultSeqCoordinatorConfig = SeqCoordinatorConfig{
	Enable:                false,
	ChosenHealthcheckAddr: "",
	RedisUrl:              "",
	LockoutDuration:       time.Minute,
	LockoutSpare:          30 * time.Second,
	SeqNumDuration:        24 * time.Hour,
	UpdateInterval:        250 * time.Millisecond,
	HandoffTimeout:        30 * time.Second,
	SafeShutdownDelay:     5 * time.Second,
	ReleaseRetries:        4,
	RetryInterval:         50 * time.Millisecond,
	MsgPerPoll:            2000,
	MyUrl:                 redisutil.INVALID_URL,
	Signer:                signature.DefaultSignVerifyConfig,
}

var TestSeqCoordinatorConfig = SeqCoordinatorConfig{
	Enable:            false,
	RedisUrl:          "",
	LockoutDuration:   time.Second * 2,
	LockoutSpare:      time.Millisecond * 10,
	SeqNumDuration:    time.Minute * 10,
	UpdateInterval:    time.Millisecond * 10,
	HandoffTimeout:    time.Millisecond * 200,
	SafeShutdownDelay: time.Millisecond * 100,
	ReleaseRetries:    4,
	RetryInterval:     time.Millisecond * 3,
	MsgPerPoll:        20,
	MyUrl:             redisutil.INVALID_URL,
	Signer:            signature.DefaultSignVerifyConfig,
}

func NewSeqCoordinator(
	dataSigner signature.DataSignerFunc,
	bpvalidator *contracts.AddressVerifier,
	streamer *TransactionStreamer,
	sequencer execution.ExecutionSequencer,
	sync *SyncMonitor,
	config SeqCoordinatorConfig,
) (*SeqCoordinator, error) {
	redisCoordinator, err := redisutil.NewRedisCoordinator(config.RedisUrl)
	if err != nil {
		return nil, err
	}
	signer, err := signature.NewSignVerify(&config.Signer, dataSigner, bpvalidator)
	if err != nil {
		return nil, err
	}
	coordinator := &SeqCoordinator{
		RedisCoordinator: *redisCoordinator,
		sync:             sync,
		streamer:         streamer,
		sequencer:        sequencer,
		config:           config,
		signer:           signer,
	}
	if sequencer != nil {
		sequencer.Pause()
	}
	streamer.SetSeqCoordinator(coordinator)
	return coordinator, nil
}

func (c *SeqCoordinator) SetDelayedSequencer(delayedSequencer *DelayedSequencer) {
	if c.Started() {
		panic("trying to set delayed sequencer after start")
	}
	if c.delayedSequencer != nil {
		panic("trying to set delayed sequencer when already set")
	}
	c.delayedSequencer = delayedSequencer
}

func StandaloneSeqCoordinatorInvalidateMsgIndex(ctx context.Context, redisClient redis.UniversalClient, keyConfig string, msgIndex arbutil.MessageIndex) error {
	signerConfig := signature.EmptySimpleHmacConfig
	if keyConfig == "" {
		signerConfig.Dangerous.DisableSignatureVerification = true
	} else {
		signerConfig.SigningKey = keyConfig
	}
	signer, err := signature.NewSimpleHmac(&signerConfig)
	if err != nil {
		return err
	}
	var msgIndexBytes [8]byte
	binary.BigEndian.PutUint64(msgIndexBytes[:], uint64(msgIndex))
	msg := []byte(redisutil.INVALID_VAL)
	sig, err := signer.SignMessage(msgIndexBytes[:], msg)
	if err != nil {
		return err
	}
	redisClient.Set(ctx, redisutil.MessageKeyFor(msgIndex), msg, DefaultSeqCoordinatorConfig.SeqNumDuration)
	redisClient.Set(ctx, redisutil.MessageSigKeyFor(msgIndex), sig, DefaultSeqCoordinatorConfig.SeqNumDuration)
	return nil
}

func atomicTimeWrite(addr *int64, t time.Time) {
	asint64 := t.UnixMilli()
	atomic.StoreInt64(addr, asint64)
}

// notice: It is possible for two consecutive reads to get decreasing values. That shouldn't matter.
func atomicTimeRead(addr *int64) time.Time {
	asint64 := atomic.LoadInt64(addr)
	return time.UnixMilli(asint64)
}

func execTestPipe(pipe redis.Pipeliner, ctx context.Context) error {
	cmders, err := pipe.Exec(ctx)
	if err != nil {
		return err
	}
	for _, cmder := range cmders {
		if err := cmder.Err(); err != nil {
			return err
		}
	}
	return nil
}

func (c *SeqCoordinator) msgCountToSignedBytes(msgCount arbutil.MessageIndex) ([]byte, error) {
	var msgCountBytes [8]byte
	binary.BigEndian.PutUint64(msgCountBytes[:], uint64(msgCount))
	sig, err := c.signer.SignMessage(msgCountBytes[:])
	if err != nil {
		return nil, err
	}
	return append(sig, msgCountBytes[:]...), nil
}

func (c *SeqCoordinator) signedBytesToMsgCount(ctx context.Context, data []byte) (arbutil.MessageIndex, error) {
	datalen := len(data)
	if datalen < 8 {
		return 0, errors.New("msgcount value too short")
	}
	msgCountBytes := data[datalen-8:]
	sig := data[:datalen-8]
	err := c.signer.VerifySignature(ctx, sig, msgCountBytes)
	if err != nil {
		return 0, err
	}
	return arbutil.MessageIndex(binary.BigEndian.Uint64(msgCountBytes)), nil
}

// Acquires or refreshes the chosen one lockout and optionally writes a message into redis atomically.
func (c *SeqCoordinator) acquireLockoutAndWriteMessage(ctx context.Context, msgCountExpected, msgCountToWrite arbutil.MessageIndex, lastmsg *arbostypes.MessageWithMetadata) error {
	var messageData *string
	var messageSigData *string
	if lastmsg != nil {
		msgBytes, err := json.Marshal(lastmsg)
		if err != nil {
			return err
		}
		msgSig, err := c.signer.SignMessage(arbmath.UintToBytes(uint64(msgCountToWrite-1)), msgBytes)
		if err != nil {
			return err
		}
		if c.config.Signer.SymmetricSign {
			messageString := string(append(msgSig, msgBytes...))
			messageData = &messageString
		} else {
			messageString := string(msgBytes)
			sigString := string(msgSig)
			messageData = &messageString
			messageSigData = &sigString
		}
	}
	msgCountMsg, err := c.msgCountToSignedBytes(msgCountToWrite)
	if err != nil {
		return err
	}
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	setWantsLockout := c.avoidLockout <= 0
	lockoutUntil := time.Now().Add(c.config.LockoutDuration)
	err = c.Client.Watch(ctx, func(tx *redis.Tx) error {
		current, err := tx.Get(ctx, redisutil.CHOSENSEQ_KEY).Result()
		var wasEmpty bool
		if errors.Is(err, redis.Nil) {
			wasEmpty = true
			err = nil
		}
		if err != nil {
			return err
		}
		if !wasEmpty && (current != c.config.Url()) {
			return fmt.Errorf("%w: failed to catch lock. redis shows chosen: %s", execution.ErrRetrySequencer, current)
		}
		remoteMsgCount, err := c.getRemoteMsgCountImpl(ctx, tx)
		if err != nil {
			return err
		}
		if remoteMsgCount > msgCountExpected {
			if messageData == nil && c.CurrentlyChosen() {
				// this was called from update(), while msgCount was changed by a call from SequencingMessage
				// no need to do anything
				return nil
			}
			log.Info("coordinator failed to become main", "expected", msgCountExpected, "found", remoteMsgCount, "message is nil?", messageData == nil)
			return fmt.Errorf("%w: failed to catch lock. expected msg %d found %d", execution.ErrRetrySequencer, msgCountExpected, remoteMsgCount)
		}
		pipe := tx.TxPipeline()
		initialDuration := c.config.LockoutDuration
		if initialDuration < 2*time.Second {
			initialDuration = 2 * time.Second
		}
		if wasEmpty {
			pipe.Set(ctx, redisutil.CHOSENSEQ_KEY, c.config.Url(), initialDuration)
		}
		pipe.Set(ctx, redisutil.MSG_COUNT_KEY, msgCountMsg, c.config.SeqNumDuration)
		if messageData != nil {
			pipe.Set(ctx, redisutil.MessageKeyFor(msgCountToWrite-1), *messageData, c.config.SeqNumDuration)
			if messageSigData != nil {
				pipe.Set(ctx, redisutil.MessageSigKeyFor(msgCountToWrite-1), *messageSigData, c.config.SeqNumDuration)
			}
		}
		pipe.PExpireAt(ctx, redisutil.CHOSENSEQ_KEY, lockoutUntil)
		if setWantsLockout {
			myWantsLockoutKey := redisutil.WantsLockoutKeyFor(c.config.Url())
			pipe.Set(ctx, myWantsLockoutKey, redisutil.WANTS_LOCKOUT_VAL, initialDuration)
			pipe.PExpireAt(ctx, myWantsLockoutKey, lockoutUntil)
		}
		err = execTestPipe(pipe, ctx)
		if errors.Is(err, redis.TxFailedErr) {
			return fmt.Errorf("%w: failed to catch sequencer lock", execution.ErrRetrySequencer)
		}
		if err != nil {
			return fmt.Errorf("chosen sequencer failed to update redis: %w", err)
		}
		return nil
	}, redisutil.CHOSENSEQ_KEY, redisutil.MSG_COUNT_KEY)

	if err != nil {
		return err
	}
	if setWantsLockout {
		c.reportedWantsLockout = true
	}
	isActiveSequencer.Update(1)
	atomicTimeWrite(&c.lockoutUntil, lockoutUntil.Add(-c.config.LockoutSpare))
	return nil
}

func (c *SeqCoordinator) getRemoteMsgCountImpl(ctx context.Context, r redis.Cmdable) (arbutil.MessageIndex, error) {
	resStr, err := r.Get(ctx, redisutil.MSG_COUNT_KEY).Result()
	if errors.Is(err, redis.Nil) {
		return 0, nil
	}
	if err != nil {
		return 0, err
	}
	return c.signedBytesToMsgCount(ctx, []byte(resStr))
}

func (c *SeqCoordinator) GetRemoteMsgCount() (arbutil.MessageIndex, error) {
	return c.getRemoteMsgCountImpl(c.GetContext(), c.Client)
}

func (c *SeqCoordinator) wantsLockoutUpdate(ctx context.Context) error {
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	return c.wantsLockoutUpdateWithMutex(ctx)
}

// Requires the caller hold the wantsLockoutMutex
func (c *SeqCoordinator) wantsLockoutUpdateWithMutex(ctx context.Context) error {
	if c.avoidLockout > 0 {
		return nil
	}
	myWantsLockoutKey := redisutil.WantsLockoutKeyFor(c.config.Url())
	wantsLockoutUntil := time.Now().Add(c.config.LockoutDuration)
	pipe := c.Client.TxPipeline()
	initialDuration := c.config.LockoutDuration
	if initialDuration < 2*time.Second {
		initialDuration = 2 * time.Second
	}
	pipe.Set(ctx, myWantsLockoutKey, redisutil.WANTS_LOCKOUT_VAL, initialDuration)
	pipe.PExpireAt(ctx, myWantsLockoutKey, wantsLockoutUntil)
	err := execTestPipe(pipe, ctx)
	if err != nil {
		return fmt.Errorf("failed to update wants lockout key in redis: %w", err)
	}
	c.reportedWantsLockout = true
	return nil
}

func (c *SeqCoordinator) chosenOneRelease(ctx context.Context) error {
	atomicTimeWrite(&c.lockoutUntil, time.Time{})
	isActiveSequencer.Update(0)
	releaseErr := c.Client.Watch(ctx, func(tx *redis.Tx) error {
		current, err := tx.Get(ctx, redisutil.CHOSENSEQ_KEY).Result()
		if errors.Is(err, redis.Nil) {
			return nil
		}
		if err != nil {
			return err
		}
		if current != c.config.Url() {
			return nil
		}
		pipe := tx.TxPipeline()
		pipe.Del(ctx, redisutil.CHOSENSEQ_KEY)
		err = execTestPipe(pipe, ctx)
		if err != nil {
			return fmt.Errorf("chosen sequencer failed to update redis: %w", err)
		}
		return nil
	}, redisutil.CHOSENSEQ_KEY)
	if releaseErr == nil {
		return nil
	}
	// got error - was it still released?
	current, readErr := c.Client.Get(ctx, redisutil.CHOSENSEQ_KEY).Result()
	if errors.Is(readErr, redis.Nil) {
		return nil
	}
	if current != c.config.Url() {
		return nil
	}
	return releaseErr
}

func (c *SeqCoordinator) wantsLockoutRelease(ctx context.Context) error {
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	if !c.reportedWantsLockout {
		return nil
	}
	myWantsLockoutKey := redisutil.WantsLockoutKeyFor(c.config.Url())
	releaseErr := c.Client.Del(ctx, myWantsLockoutKey).Err()
	if releaseErr != nil {
		// got error - was it still deleted?
		readErr := c.Client.Get(ctx, myWantsLockoutKey).Err()
		if !errors.Is(readErr, redis.Nil) {
			return releaseErr
		}
	}
	c.reportedWantsLockout = false
	return nil
}

func (c *SeqCoordinator) retryAfterRedisError() time.Duration {
	c.redisErrors++
	retryIn := c.config.RetryInterval * time.Duration(c.redisErrors)
	if retryIn > c.config.UpdateInterval {
		retryIn = c.config.UpdateInterval
	}
	return retryIn
}

func (c *SeqCoordinator) noRedisError() time.Duration {
	c.redisErrors = 0
	return c.config.UpdateInterval
}

// update for the prev known-chosen sequencer (no need to load new messages)
func (c *SeqCoordinator) updateWithLockout(ctx context.Context, nextChosen string) time.Duration {
	if nextChosen != "" && nextChosen != c.config.Url() {
		// was the active sequencer, but no longer
		// we maintain chosen status if we had it and nobody in the priorities wants the lockout
		setPrevChosenTo := nextChosen
		if c.sequencer != nil {
			err := c.sequencer.ForwardTo(nextChosen)
			if err != nil {
				// The error was already logged in ForwardTo, just clean up state.
				// Setting prevChosenSequencer to an empty string will cause the next update to attempt to reconnect.
				setPrevChosenTo = ""
			}
		}
		if err := c.chosenOneRelease(ctx); err != nil {
			log.Warn("coordinator failed chosen one release", "err", err)
			return c.retryAfterRedisError()
		}
		c.prevChosenSequencer = setPrevChosenTo
		log.Info("released chosen-coordinator lock", "myUrl", c.config.Url(), "nextChosen", nextChosen)
		return c.noRedisError()
	}
	// Was, and still is, the active sequencer
	// We leave a margin of error of either a five times the update interval or a fifth of the lockout duration, whichever is greater.
	marginOfError := arbmath.MaxInt(c.config.LockoutDuration/5, c.config.UpdateInterval*5)
	if time.Now().Add(marginOfError).Before(atomicTimeRead(&c.lockoutUntil)) {
		// if we recently sequenced - no need for an update
		return c.noRedisError()
	}
	localMsgCount, err := c.streamer.GetMessageCount()
	if err != nil {
		log.Error("coordinator cannot read message count", "err", err)
		return c.config.UpdateInterval
	}
	err = c.acquireLockoutAndWriteMessage(ctx, localMsgCount, localMsgCount, nil)
	if err != nil {
		log.Warn("coordinator failed chosen-one keepalive", "err", err)
		return c.retryAfterRedisError()
	}
	return c.noRedisError()
}

func (c *SeqCoordinator) update(ctx context.Context) time.Duration {
	chosenSeq, err := c.RecommendSequencerWantingLockout(ctx)
	if err != nil {
		log.Warn("coordinator failed finding sequencer wanting lockout", "err", err)
		return c.retryAfterRedisError()
	}
	if c.prevChosenSequencer == c.config.Url() {
		return c.updateWithLockout(ctx, chosenSeq)
	}
	if chosenSeq != c.config.Url() && chosenSeq != c.prevChosenSequencer {
		var err error
		if c.sequencer != nil {
			err = c.sequencer.ForwardTo(chosenSeq)
		}
		if err == nil {
			c.prevChosenSequencer = chosenSeq
			log.Info("chosen sequencer changing", "recommended", chosenSeq)
		} else {
			// The error was already logged in ForwardTo, just clean up state.
			// Next run this will attempt to reconnect.
			c.prevChosenSequencer = ""
		}
	}

	// read messages from redis
	localMsgCount, err := c.streamer.GetMessageCount()
	if err != nil {
		log.Error("cannot read message count", "err", err)
		return c.config.UpdateInterval
	}
	remoteMsgCount, err := c.GetRemoteMsgCount()
	if err != nil {
		log.Warn("cannot get remote message count", "err", err)
		return c.retryAfterRedisError()
	}
	readUntil := remoteMsgCount
	if readUntil > localMsgCount+c.config.MsgPerPoll {
		readUntil = localMsgCount + c.config.MsgPerPoll
	}
	var messages []arbostypes.MessageWithMetadata
	msgToRead := localMsgCount
	var msgReadErr error
	for msgToRead < readUntil {
		var resString string
		resString, msgReadErr = c.Client.Get(ctx, redisutil.MessageKeyFor(msgToRead)).Result()
		if msgReadErr != nil {
			log.Warn("coordinator failed reading message", "pos", msgToRead, "err", msgReadErr)
			break
		}
		rsBytes := []byte(resString)
		var sigString string
		var sigBytes []byte
		sigSeparateKey := true
		sigString, msgReadErr = c.Client.Get(ctx, redisutil.MessageSigKeyFor(msgToRead)).Result()
		if errors.Is(msgReadErr, redis.Nil) {
			// no separate signature. Try reading old-style sig
			if len(rsBytes) < 32 {
				log.Warn("signature not found for msg", "pos", msgToRead)
				msgReadErr = errors.New("signature not found")
				break
			}
			sigBytes = rsBytes[:32]
			rsBytes = rsBytes[32:]
			sigSeparateKey = false
		} else if msgReadErr != nil {
			log.Warn("coordinator failed reading sig", "pos", msgToRead, "err", msgReadErr)
			break
		} else {
			sigBytes = []byte(sigString)
		}
		msgReadErr = c.signer.VerifySignature(ctx, sigBytes, arbmath.UintToBytes(uint64(msgToRead)), rsBytes)
		if msgReadErr != nil {
			log.Warn("coordinator failed verifying message signature", "pos", msgToRead, "err", msgReadErr, "separate-key", sigSeparateKey)
			break
		}
		var message arbostypes.MessageWithMetadata
		err = json.Unmarshal(rsBytes, &message)
		if err != nil {
			log.Warn("coordinator failed to parse message from redis", "pos", msgToRead, "err", err)
			msgReadErr = fmt.Errorf("failed to parse message: %w", err)
			// redis messages spelled "INVALID" will be parsed as invalid L1 message, but only one at a time
			if len(messages) > 0 || string(rsBytes) != redisutil.INVALID_VAL {
				break
			}
			lastDelayedMsg := uint64(0)
			if msgToRead > 0 {
				prevMsg, err := c.streamer.GetMessage(msgToRead - 1)
				if err != nil {
					log.Error("coordinator failed to get msg", "pos", msgToRead-1)
					break
				}
				lastDelayedMsg = prevMsg.DelayedMessagesRead
			}
			message = arbostypes.MessageWithMetadata{
				Message:             arbostypes.InvalidL1Message,
				DelayedMessagesRead: lastDelayedMsg,
			}
		}
		messages = append(messages, message)
		msgToRead++
	}
	if len(messages) > 0 {
		if err := c.streamer.AddMessages(localMsgCount, false, messages); err != nil {
			log.Warn("coordinator failed to add messages", "err", err, "pos", localMsgCount, "length", len(messages))
		} else {
			localMsgCount = msgToRead
		}
	}

	if c.config.Url() == redisutil.INVALID_URL {
		return c.noRedisError()
	}

	syncProgress := c.sync.SyncProgressMap()
	synced := len(syncProgress) == 0
	if !synced {
		var detailsList []interface{}
		for key, value := range syncProgress {
			detailsList = append(detailsList, key, value)
		}
		log.Warn("sequencer is not synced", detailsList...)
	}

	// can take over as main sequencer?
	if synced && localMsgCount >= remoteMsgCount && chosenSeq == c.config.Url() {
		if c.sequencer == nil {
			log.Error("myurl main sequencer, but no sequencer exists")
			return c.noRedisError()
		}
		processedMessages, err := c.streamer.GetProcessedMessageCount()
		if err != nil {
			log.Warn("coordinator: failed to read processed message count", "err", err)
			processedMessages = 0
		}
		if processedMessages >= localMsgCount {
			// we're here because we don't currently hold the lock
			// sequencer is already either paused or forwarding
			c.sequencer.Pause()
			err := c.acquireLockoutAndWriteMessage(ctx, localMsgCount, localMsgCount, nil)
			if err != nil {
				// this could be just new messages we didn't get yet - even then, we should retry soon
				log.Info("sequencer failed to become chosen", "err", err, "msgcount", localMsgCount)
				// make sure we're marked as wanting the lockout
				if err := c.wantsLockoutUpdate(ctx); err != nil {
					log.Warn("failed to update wants lockout key", "err", err)
				}
				c.prevChosenSequencer = ""
				return c.retryAfterRedisError()
			}
			log.Info("caught chosen-coordinator lock", "myUrl", c.config.Url())
			if c.delayedSequencer != nil {
				err = c.delayedSequencer.ForceSequenceDelayed(ctx)
				if err != nil {
					log.Warn("failed sequencing delayed messages after catching lock", "err", err)
				}
			}
			err = c.streamer.PopulateFeedBacklog()
			if err != nil {
				log.Warn("failed to populate the feed backlog on lockout acquisition", "err", err)
			}
			c.sequencer.Activate()
			c.prevChosenSequencer = c.config.Url()
			return c.noRedisError()
		}
	}

	// update wanting the lockout
	var wantsLockoutErr error
	if synced && !c.AvoidingLockout() {
		wantsLockoutErr = c.wantsLockoutUpdate(ctx)
	} else {
		wantsLockoutErr = c.wantsLockoutRelease(ctx)
	}
	if wantsLockoutErr != nil {
		log.Warn("coordinator failed to update its wanting lockout status", "err", wantsLockoutErr)
	}

	if (wantsLockoutErr != nil) || (msgReadErr != nil) {
		return c.retryAfterRedisError()
	}
	return c.noRedisError()
}

// Warning: acquires the wantsLockoutMutex
func (c *SeqCoordinator) AvoidingLockout() bool {
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	return c.avoidLockout > 0
}

// Warning: acquires the wantsLockoutMutex
func (c *SeqCoordinator) DebugPrint() string {
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	return fmt.Sprint("Url:", c.config.Url(),
		" prevChosenSequencer:", c.prevChosenSequencer,
		" reportedWantsLockout:", c.reportedWantsLockout,
		" lockoutUntil:", c.lockoutUntil,
		" redisErrors:", c.redisErrors)
}

type seqCoordinatorChosenHealthcheck struct {
	c *SeqCoordinator
}

func (h seqCoordinatorChosenHealthcheck) ServeHTTP(response http.ResponseWriter, _ *http.Request) {
	if h.c.CurrentlyChosen() {
		response.WriteHeader(http.StatusOK)
	} else {
		response.WriteHeader(http.StatusServiceUnavailable)
	}
}

func (c *SeqCoordinator) launchHealthcheckServer(ctx context.Context) {
	server := &http.Server{
		Addr:              c.config.ChosenHealthcheckAddr,
		Handler:           seqCoordinatorChosenHealthcheck{c},
		ReadHeaderTimeout: 5 * time.Second,
	}

	go func() {
		<-ctx.Done()
		err := server.Shutdown(ctx)
		if err != nil && !errors.Is(err, context.Canceled) && !errors.Is(err, context.DeadlineExceeded) {
			log.Warn("error shutting down coordinator chosen healthcheck server", "err", err)
		}
	}()

	err := server.ListenAndServe()
	if err != nil && !errors.Is(err, http.ErrServerClosed) {
		log.Warn("error serving coordinator chosen healthcheck server", "err", err)
	}
}

func (c *SeqCoordinator) Start(ctxIn context.Context) {
	c.StopWaiter.Start(ctxIn, c)
	c.CallIteratively(c.update)
	if c.config.ChosenHealthcheckAddr != "" {
		c.StopWaiter.LaunchThread(c.launchHealthcheckServer)
	}
}

// Calls check() every c.config.RetryInterval until it returns true, or the context times out.
func (c *SeqCoordinator) waitFor(ctx context.Context, check func() bool) bool {
	for {
		result := check()
		if result {
			return true
		}
		select {
		case <-ctx.Done():
			// The caller should've already logged an info line with context about what it's waiting on
			return false
		case <-time.After(c.config.RetryInterval):
		}
	}
}

func (c *SeqCoordinator) PrepareForShutdown() {
	ctx := c.StopWaiter.GetContext()
	// Any errors/failures here are logged in these methods
	c.AvoidLockout(ctx)
	c.TryToHandoffChosenOne(ctx)
}

func (c *SeqCoordinator) StopAndWait() {
	c.StopWaiter.StopAndWait()
	// We've just stopped our normal context so we need to use our parent's context.
	parentCtx := c.StopWaiter.GetParentContext()
	for i := 0; i <= c.config.ReleaseRetries || c.config.ReleaseRetries < 0; i++ {
		log.Info("releasing wants lockout key", "myUrl", c.config.Url(), "attempt", i)
		err := c.wantsLockoutRelease(parentCtx)
		if err == nil {
			c.noRedisError()
			break
		} else {
			log.Error("failed to release wanting the lockout on shutdown", "err", err)
			time.Sleep(c.retryAfterRedisError())
		}
	}
	for i := 0; i < c.config.ReleaseRetries || c.config.ReleaseRetries < 0; i++ {
		log.Info("releasing chosen one", "myUrl", c.config.Url(), "attempt", i)
		err := c.chosenOneRelease(parentCtx)
		if err == nil {
			c.noRedisError()
			break
		} else {
			log.Error("failed to release chosen one status on shutdown", "err", err)
			time.Sleep(c.retryAfterRedisError())
		}
	}
	_ = c.Client.Close()
}

func (c *SeqCoordinator) CurrentlyChosen() bool {
	return time.Now().Before(atomicTimeRead(&c.lockoutUntil))
}

func (c *SeqCoordinator) SequencingMessage(pos arbutil.MessageIndex, msg *arbostypes.MessageWithMetadata) error {
	if !c.CurrentlyChosen() {
		return fmt.Errorf("%w: not main sequencer", execution.ErrRetrySequencer)
	}
	if err := c.acquireLockoutAndWriteMessage(c.GetContext(), pos, pos+1, msg); err != nil {
		return err
	}
	return nil
}

// Returns true if the wanting the lockout key was released.
// The seq coordinator is internally marked as disliking the lockout regardless, so you might want to call SeekLockout on error.
func (c *SeqCoordinator) AvoidLockout(ctx context.Context) bool {
	c.wantsLockoutMutex.Lock()
	c.avoidLockout++
	c.wantsLockoutMutex.Unlock()
	log.Info("avoiding lockout", "myUrl", c.config.Url())
	err := c.wantsLockoutRelease(ctx)
	if err != nil {
		log.Error("failed to release wanting the lockout in redis", "err", err)
		return false
	}
	return true
}

// Returns true on success.
func (c *SeqCoordinator) TryToHandoffChosenOne(ctx context.Context) bool {
	ctx, cancel := context.WithTimeout(ctx, c.config.HandoffTimeout)
	defer cancel()
	if c.CurrentlyChosen() {
		log.Info("waiting for another sequencer to become chosen...", "timeout", c.config.HandoffTimeout, "myUrl", c.config.Url())
		success := c.waitFor(ctx, func() bool {
			return !c.CurrentlyChosen()
		})
		if success {
			wantsLockout, err := c.RecommendSequencerWantingLockout(ctx)
			if err == nil {
				log.Info("released chosen one status; a new sequencer hopefully wants to acquire it", "delay", c.config.SafeShutdownDelay, "wantsLockout", wantsLockout)
			} else {
				log.Warn("succeeded in releasing chosen one status but failed to get sequencer wanting lockout", "err", err)
			}
		} else {
			log.Error("timed out waiting for another sequencer to become chosen", "timeout", c.config.HandoffTimeout)
		}
		return success
	}
	return true
}

// Undoes the effects of AvoidLockout. AvoidLockout must've been called before an equal number of times.
func (c *SeqCoordinator) SeekLockout(ctx context.Context) {
	c.wantsLockoutMutex.Lock()
	defer c.wantsLockoutMutex.Unlock()
	c.avoidLockout--
	log.Info("seeking lockout", "myUrl", c.config.Url())
	if c.sync.Synced() {
		// Even if this errors we still internally marked ourselves as wanting the lockout
		err := c.wantsLockoutUpdateWithMutex(ctx)
		if err != nil {
			log.Warn("failed to set wants lockout key in redis after seeking lockout again", "err", err)
		}
	}
}

'''
'''--- arbnode/seq_coordinator_atomic_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"fmt"
	"math/rand"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/signature"
)

const messagesPerRound = 20

type CoordinatorTestData struct {
	messageCount uint64

	sequencer []string
	err       error
	mutex     sync.Mutex

	waitForCoords  sync.WaitGroup
	testStartRound int32
}

func coordinatorTestThread(ctx context.Context, coord *SeqCoordinator, data *CoordinatorTestData) {
	nextRound := int32(0)
	for {
		sequenced := make([]bool, messagesPerRound)
		for atomic.LoadInt32(&data.testStartRound) < nextRound {
			if ctx.Err() != nil {
				return
			}
		}
		atomicTimeWrite(&coord.lockoutUntil, time.Time{})
		nextRound++
		var execError error
		for {
			messageCount := atomic.LoadUint64(&data.messageCount)
			if messageCount >= messagesPerRound {
				break
			}
			asIndex := arbutil.MessageIndex(messageCount)
			holdingLockout := atomicTimeRead(&coord.lockoutUntil)
			err := coord.acquireLockoutAndWriteMessage(ctx, asIndex, asIndex+1, &arbostypes.EmptyTestMessageWithMetadata)
			if err == nil {
				sequenced[messageCount] = true
				atomic.StoreUint64(&data.messageCount, messageCount+1)
				randNr := rand.Intn(20)
				if randNr > 15 {
					execError = coord.chosenOneRelease(ctx)
					if execError != nil {
						break
					}
					atomicTimeWrite(&coord.lockoutUntil, time.Time{})
				} else {
					time.Sleep(coord.config.LockoutDuration * time.Duration(randNr) / 10)
				}
				continue
			}
			timeLaunching := time.Now()
			// didn't sequence.. should we have succeeded?
			if timeLaunching.Before(holdingLockout) {
				execError = fmt.Errorf("failed while holding lock %s err %w", coord.config.Url(), err)
				break
			}
		}
		data.mutex.Lock()
		for i, me := range sequenced {
			if !me {
				continue
			}
			if data.sequencer[i] != "" {
				execError = fmt.Errorf("two sequencers for same msg: submsg %d, success for %s, %s", i, data.sequencer[i], coord.config.Url())
			}
			data.sequencer[i] = coord.config.Url()
		}
		if execError != nil {
			data.err = execError
		}
		data.mutex.Unlock()
		data.waitForCoords.Done()
	}
}

func TestRedisSeqCoordinatorAtomic(t *testing.T) {
	NumOfThreads := 10
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	coordConfig := TestSeqCoordinatorConfig
	coordConfig.LockoutDuration = time.Millisecond * 100
	coordConfig.LockoutSpare = time.Millisecond * 10
	coordConfig.Signer.ECDSA.AcceptSequencer = false
	coordConfig.Signer.SymmetricFallback = true
	coordConfig.Signer.SymmetricSign = true
	coordConfig.Signer.Symmetric.Dangerous.DisableSignatureVerification = true
	coordConfig.Signer.Symmetric.SigningKey = ""
	testData := CoordinatorTestData{
		testStartRound: -1,
		sequencer:      make([]string, messagesPerRound),
	}
	nullSigner, err := signature.NewSignVerify(&coordConfig.Signer, nil, nil)
	Require(t, err)

	redisUrl := redisutil.CreateTestRedis(ctx, t)
	coordConfig.RedisUrl = redisUrl
	redisClient, err := redisutil.RedisClientFromURL(redisUrl)
	Require(t, err)
	if redisClient == nil {
		t.Fatal("redisClient is nil")
	}

	for i := 0; i < NumOfThreads; i++ {
		config := coordConfig
		config.MyUrl = fmt.Sprint(i)
		redisCoordinator, err := redisutil.NewRedisCoordinator(config.RedisUrl)
		Require(t, err)
		coordinator := &SeqCoordinator{
			RedisCoordinator: *redisCoordinator,
			config:           config,
			signer:           nullSigner,
		}
		go coordinatorTestThread(ctx, coordinator, &testData)
	}

	for round := int32(0); round < 10; round++ {
		redisClient.Del(ctx, redisutil.CHOSENSEQ_KEY, redisutil.MSG_COUNT_KEY)
		testData.messageCount = 0
		for i := 0; i < messagesPerRound; i++ {
			testData.sequencer[i] = ""
		}
		testData.waitForCoords.Add(NumOfThreads)
		atomic.StoreInt32(&testData.testStartRound, round)
		testData.waitForCoords.Wait()
		Require(t, testData.err)
		seqList := ""
		for i := 0; i < messagesPerRound; i++ {
			if testData.sequencer[i] == "" {
				Fail(t, "no sequencer succeeded", "round", round, "message", i)
			}
			seqList = seqList + testData.sequencer[i] + ","
		}

		t.Log("Round", round, "sequencers", seqList)
		// wait out the current lock
		time.Sleep(time.Millisecond * 20)
	}

}

'''
'''--- arbnode/sequencer_inbox.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbutil"

	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

var sequencerBridgeABI *abi.ABI
var batchDeliveredID common.Hash
var addSequencerL2BatchFromOriginCallABI abi.Method
var sequencerBatchDataABI abi.Event

const sequencerBatchDataEvent = "SequencerBatchData"

type batchDataLocation uint8

const (
	batchDataTxInput batchDataLocation = iota
	batchDataSeparateEvent
	batchDataNone
)

func init() {
	var err error
	sequencerBridgeABI, err = bridgegen.SequencerInboxMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	batchDeliveredID = sequencerBridgeABI.Events["SequencerBatchDelivered"].ID
	sequencerBatchDataABI = sequencerBridgeABI.Events[sequencerBatchDataEvent]
	addSequencerL2BatchFromOriginCallABI = sequencerBridgeABI.Methods["addSequencerL2BatchFromOrigin"]
}

type SequencerInbox struct {
	con       *bridgegen.SequencerInbox
	address   common.Address
	fromBlock int64
	client    arbutil.L1Interface
}

func NewSequencerInbox(client arbutil.L1Interface, addr common.Address, fromBlock int64) (*SequencerInbox, error) {
	con, err := bridgegen.NewSequencerInbox(addr, client)
	if err != nil {
		return nil, err
	}

	return &SequencerInbox{
		con:       con,
		address:   addr,
		fromBlock: fromBlock,
		client:    client,
	}, nil
}

func (i *SequencerInbox) GetBatchCount(ctx context.Context, blockNumber *big.Int) (uint64, error) {
	if blockNumber.IsInt64() && blockNumber.Int64() < i.fromBlock {
		return 0, nil
	}
	opts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: blockNumber,
	}
	count, err := i.con.BatchCount(opts)
	if err != nil {
		return 0, err
	}
	if !count.IsUint64() {
		return 0, errors.New("sequencer inbox returned non-uint64 batch count")
	}
	return count.Uint64(), nil
}

func (i *SequencerInbox) GetAccumulator(ctx context.Context, sequenceNumber uint64, blockNumber *big.Int) (common.Hash, error) {
	opts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: blockNumber,
	}
	acc, err := i.con.InboxAccs(opts, new(big.Int).SetUint64(sequenceNumber))
	return acc, err
}

type SequencerInboxBatch struct {
	BlockHash              common.Hash
	ParentChainBlockNumber uint64
	SequenceNumber         uint64
	BeforeInboxAcc         common.Hash
	AfterInboxAcc          common.Hash
	AfterDelayedAcc        common.Hash
	AfterDelayedCount      uint64
	TimeBounds             bridgegen.ISequencerInboxTimeBounds
	rawLog                 types.Log
	dataLocation           batchDataLocation
	bridgeAddress          common.Address
	serialized             []byte // nil if serialization isn't cached yet
}

func (m *SequencerInboxBatch) getSequencerData(ctx context.Context, client arbutil.L1Interface) ([]byte, error) {
	switch m.dataLocation {
	case batchDataTxInput:
		data, err := arbutil.GetLogEmitterTxData(ctx, client, m.rawLog)
		if err != nil {
			return nil, err
		}
		args := make(map[string]interface{})
		err = addSequencerL2BatchFromOriginCallABI.Inputs.UnpackIntoMap(args, data[4:])
		if err != nil {
			return nil, err
		}
		return args["data"].([]byte), nil
	case batchDataSeparateEvent:
		var numberAsHash common.Hash
		binary.BigEndian.PutUint64(numberAsHash[(32-8):], m.SequenceNumber)
		query := ethereum.FilterQuery{
			BlockHash: &m.BlockHash,
			Addresses: []common.Address{m.bridgeAddress},
			Topics:    [][]common.Hash{{sequencerBatchDataABI.ID}, {numberAsHash}},
		}
		logs, err := client.FilterLogs(ctx, query)
		if err != nil {
			return nil, err
		}
		if len(logs) == 0 {
			return nil, errors.New("expected to find sequencer batch data")
		}
		if len(logs) > 1 {
			return nil, errors.New("expected to find only one matching sequencer batch data")
		}
		event := new(bridgegen.SequencerInboxSequencerBatchData)
		err = sequencerBridgeABI.UnpackIntoInterface(event, sequencerBatchDataEvent, logs[0].Data)
		if err != nil {
			return nil, err
		}
		return event.Data, nil
	case batchDataNone:
		// No data when in a force inclusion batch
		return nil, nil
	default:
		return nil, fmt.Errorf("batch has invalid data location %v", m.dataLocation)
	}
}

func (m *SequencerInboxBatch) Serialize(ctx context.Context, client arbutil.L1Interface) ([]byte, error) {
	if m.serialized != nil {
		return m.serialized, nil
	}

	var fullData []byte

	// Serialize the header
	headerVals := []uint64{
		m.TimeBounds.MinTimestamp,
		m.TimeBounds.MaxTimestamp,
		m.TimeBounds.MinBlockNumber,
		m.TimeBounds.MaxBlockNumber,
		m.AfterDelayedCount,
	}
	for _, bound := range headerVals {
		var intData [8]byte
		binary.BigEndian.PutUint64(intData[:], bound)
		fullData = append(fullData, intData[:]...)
	}

	// Append the batch data
	data, err := m.getSequencerData(ctx, client)
	if err != nil {
		return nil, err
	}
	fullData = append(fullData, data...)

	m.serialized = fullData
	return fullData, nil
}

func (i *SequencerInbox) LookupBatchesInRange(ctx context.Context, from, to *big.Int) ([]*SequencerInboxBatch, error) {
	query := ethereum.FilterQuery{
		FromBlock: from,
		ToBlock:   to,
		Addresses: []common.Address{i.address},
		Topics:    [][]common.Hash{{batchDeliveredID}},
	}
	logs, err := i.client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	messages := make([]*SequencerInboxBatch, 0, len(logs))
	var lastSeqNum *uint64
	for _, log := range logs {
		if log.Topics[0] != batchDeliveredID {
			return nil, errors.New("unexpected log selector")
		}
		parsedLog, err := i.con.ParseSequencerBatchDelivered(log)
		if err != nil {
			return nil, err
		}
		if !parsedLog.BatchSequenceNumber.IsUint64() {
			return nil, errors.New("sequencer inbox event has non-uint64 sequence number")
		}
		if !parsedLog.AfterDelayedMessagesRead.IsUint64() {
			return nil, errors.New("sequencer inbox event has non-uint64 delayed messages read")
		}

		seqNum := parsedLog.BatchSequenceNumber.Uint64()
		if lastSeqNum != nil {
			if seqNum != *lastSeqNum+1 {
				return nil, fmt.Errorf("sequencer batches out of order; after batch %v got batch %v", lastSeqNum, seqNum)
			}
		}
		lastSeqNum = &seqNum
		batch := &SequencerInboxBatch{
			BlockHash:              log.BlockHash,
			ParentChainBlockNumber: log.BlockNumber,
			SequenceNumber:         seqNum,
			BeforeInboxAcc:         parsedLog.BeforeAcc,
			AfterInboxAcc:          parsedLog.AfterAcc,
			AfterDelayedAcc:        parsedLog.DelayedAcc,
			AfterDelayedCount:      parsedLog.AfterDelayedMessagesRead.Uint64(),
			rawLog:                 log,
			TimeBounds:             parsedLog.TimeBounds,
			dataLocation:           batchDataLocation(parsedLog.DataLocation),
			bridgeAddress:          log.Address,
		}
		messages = append(messages, batch)
	}
	return messages, nil
}

'''
'''--- arbnode/simple_redis_lock_test.go ---
package arbnode

import (
	"context"
	"math/rand"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbnode/redislock"
	"github.com/offchainlabs/nitro/util/redisutil"
)

func prepareTrue() bool  { return true }
func prepareFalse() bool { return false }

const test_attempts = 10
const test_threads = 10
const test_release_frac = 5
const test_delay = time.Millisecond
const test_redisKey_prefix = "__TEMP_SimpleRedisLockTest__"

func attemptLock(ctx context.Context, s *redislock.Simple, flag *int32, wg *sync.WaitGroup) {
	defer wg.Done()
	for i := 0; i < test_attempts; i++ {
		if s.AttemptLock(ctx) {
			atomic.AddInt32(flag, 1)
		} else if rand.Intn(test_release_frac) == 0 {
			s.Release(ctx)
		}
		select {
		case <-time.After(test_delay):
		case <-ctx.Done():
			return
		}
	}
}

func simpleRedisLockTest(t *testing.T, redisKeySuffix string, chosen int, backgound bool) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	redisKey := test_redisKey_prefix + redisKeySuffix
	redisUrl := redisutil.CreateTestRedis(ctx, t)
	redisClient, err := redisutil.RedisClientFromURL(redisUrl)
	Require(t, err)
	Require(t, redisClient.Del(ctx, redisKey).Err())

	conf := &redislock.SimpleCfg{
		LockoutDuration: test_delay * test_attempts * 10,
		RefreshDuration: test_delay * 2,
		Key:             redisKey,
		BackgroundLock:  backgound,
	}
	confFetcher := func() *redislock.SimpleCfg { return conf }

	locks := make([]*redislock.Simple, 0)
	for i := 0; i < test_threads; i++ {
		var err error
		var lock *redislock.Simple
		if chosen < 0 || chosen == i {
			lock, err = redislock.NewSimple(redisClient, confFetcher, prepareTrue)
		} else {
			lock, err = redislock.NewSimple(redisClient, confFetcher, prepareFalse)
		}
		if err != nil {
			t.Fatal(err)
		}
		lock.Start(ctx)
		defer lock.StopAndWait()
		locks = append(locks, lock)
	}
	if backgound {
		<-time.After(time.Second)
	}
	wg := sync.WaitGroup{}
	counters := make([]int32, test_threads)
	for i, lock := range locks {
		wg.Add(1)
		go attemptLock(ctx, lock, &counters[i], &wg)
	}
	wg.Wait()
	successful := -1
	for i, counter := range counters {
		if counter != 0 {
			if counter != test_attempts {
				t.Fatalf("counter %d value %d", i, counter)
			}
			if successful > 0 {
				t.Fatalf("counter %d and %d both positive", i, successful)
			}
			successful = i
		}
	}
	if successful < 0 {
		t.Fatal("no counter succeeded")
	}
	if chosen >= 0 && chosen != successful {
		t.Fatalf("counter %d succeeded, should have been %d", successful, chosen)
	}
}

func TestRedisLock0(t *testing.T) {
	simpleRedisLockTest(t, "0", 0, false)
}

func TestRedisLock0Bg(t *testing.T) {
	simpleRedisLockTest(t, "0bg", 0, true)
}

func TestRedisLock7(t *testing.T) {
	simpleRedisLockTest(t, "7", 7, false)
}

func TestRedisLockAny(t *testing.T) {
	simpleRedisLockTest(t, "a", -1, false)
}

func TestRedisLockAnyBg(t *testing.T) {
	simpleRedisLockTest(t, "abg", -1, true)
}

'''
'''--- arbnode/sync_monitor.go ---
package arbnode

import (
	"context"
	"errors"
	"sync/atomic"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	flag "github.com/spf13/pflag"
)

type SyncMonitor struct {
	config      *SyncMonitorConfig
	inboxReader *InboxReader
	txStreamer  *TransactionStreamer
	coordinator *SeqCoordinator
	exec        execution.FullExecutionClient
	initialized bool
}

func NewSyncMonitor(config *SyncMonitorConfig) *SyncMonitor {
	return &SyncMonitor{
		config: config,
	}
}

type SyncMonitorConfig struct {
	BlockBuildLag               uint64 `koanf:"block-build-lag"`
	BlockBuildSequencerInboxLag uint64 `koanf:"block-build-sequencer-inbox-lag"`
	CoordinatorMsgLag           uint64 `koanf:"coordinator-msg-lag"`
}

var DefaultSyncMonitorConfig = SyncMonitorConfig{
	BlockBuildLag:               20,
	BlockBuildSequencerInboxLag: 0,
	CoordinatorMsgLag:           15,
}

func SyncMonitorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".block-build-lag", DefaultSyncMonitorConfig.BlockBuildLag, "allowed lag between messages read and blocks built")
	f.Uint64(prefix+".block-build-sequencer-inbox-lag", DefaultSyncMonitorConfig.BlockBuildSequencerInboxLag, "allowed lag between messages read from sequencer inbox and blocks built")
	f.Uint64(prefix+".coordinator-msg-lag", DefaultSyncMonitorConfig.CoordinatorMsgLag, "allowed lag between local and remote messages")
}

func (s *SyncMonitor) Initialize(inboxReader *InboxReader, txStreamer *TransactionStreamer, coordinator *SeqCoordinator, exec execution.FullExecutionClient) {
	s.inboxReader = inboxReader
	s.txStreamer = txStreamer
	s.coordinator = coordinator
	s.exec = exec
	s.initialized = true
}

func (s *SyncMonitor) SyncProgressMap() map[string]interface{} {
	syncing := false
	res := make(map[string]interface{})

	if !s.initialized {
		res["err"] = "uninitialized"
		return res
	}

	broadcasterQueuedMessagesPos := atomic.LoadUint64(&(s.txStreamer.broadcasterQueuedMessagesPos))

	if broadcasterQueuedMessagesPos != 0 { // unprocessed feed
		syncing = true
	}
	res["broadcasterQueuedMessagesPos"] = broadcasterQueuedMessagesPos

	builtMessageCount, err := s.exec.HeadMessageNumber()
	if err != nil {
		res["builtMessageCountError"] = err.Error()
		syncing = true
		builtMessageCount = 0
	} else {
		blockNum := s.exec.MessageIndexToBlockNumber(builtMessageCount)
		res["blockNum"] = blockNum
		builtMessageCount++
		res["messageOfLastBlock"] = builtMessageCount
	}

	msgCount, err := s.txStreamer.GetMessageCount()
	if err != nil {
		res["msgCountError"] = err.Error()
		syncing = true
	} else {
		res["msgCount"] = msgCount
		if builtMessageCount+arbutil.MessageIndex(s.config.BlockBuildLag) < msgCount {
			syncing = true
		}
	}

	if s.inboxReader != nil {
		batchSeen := s.inboxReader.GetLastSeenBatchCount()
		_, batchProcessed := s.inboxReader.GetLastReadBlockAndBatchCount()

		if (batchSeen == 0) || // error or not yet read inbox
			(batchProcessed < batchSeen) { // unprocessed inbox messages
			syncing = true
		}
		res["batchSeen"] = batchSeen
		res["batchProcessed"] = batchProcessed

		processedMetadata, err := s.inboxReader.Tracker().GetBatchMetadata(batchProcessed - 1)
		if err != nil {
			res["batchMetadataError"] = err.Error()
			syncing = true
		} else {
			res["messageOfProcessedBatch"] = processedMetadata.MessageCount
			if builtMessageCount+arbutil.MessageIndex(s.config.BlockBuildSequencerInboxLag) < processedMetadata.MessageCount {
				syncing = true
			}
		}

		l1reader := s.inboxReader.l1Reader
		if l1reader != nil {
			header, err := l1reader.LastHeaderWithError()
			if err != nil {
				res["lastL1HeaderErr"] = err
			}
			if header != nil {
				res["lastL1BlockNum"] = header.Number
				res["lastl1BlockHash"] = header.Hash()
			}
		}
	}

	if s.coordinator != nil {
		coordinatorMessageCount, err := s.coordinator.GetRemoteMsgCount() //NOTE: this creates a remote call
		if err != nil {
			res["coordinatorMsgCountError"] = err.Error()
			syncing = true
		} else {
			res["coordinatorMessageCount"] = coordinatorMessageCount
			if msgCount+arbutil.MessageIndex(s.config.CoordinatorMsgLag) < coordinatorMessageCount {
				syncing = true
			}
		}
	}

	if !syncing {
		return make(map[string]interface{})
	}

	return res
}

func (s *SyncMonitor) SafeBlockNumber(ctx context.Context) (uint64, error) {
	if s.inboxReader == nil || !s.initialized {
		return 0, errors.New("not set up for safeblock")
	}
	msg, err := s.inboxReader.GetSafeMsgCount(ctx)
	if err != nil {
		return 0, err
	}
	block := s.exec.MessageIndexToBlockNumber(msg - 1)
	return block, nil
}

func (s *SyncMonitor) FinalizedBlockNumber(ctx context.Context) (uint64, error) {
	if s.inboxReader == nil || !s.initialized {
		return 0, errors.New("not set up for safeblock")
	}
	msg, err := s.inboxReader.GetFinalizedMsgCount(ctx)
	if err != nil {
		return 0, err
	}
	block := s.exec.MessageIndexToBlockNumber(msg - 1)
	return block, nil
}

func (s *SyncMonitor) Synced() bool {
	return len(s.SyncProgressMap()) == 0
}

'''
'''--- arbnode/transaction_streamer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbnode

import (
	"bytes"
	"context"
	"encoding/binary"
	"encoding/json"
	"fmt"
	"math/big"
	"reflect"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"errors"

	"github.com/cockroachdb/pebble"
	flag "github.com/spf13/pflag"
	"github.com/syndtr/goleveldb/leveldb"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/util/sharedmetrics"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

// TransactionStreamer produces blocks from a node's L1 messages, storing the results in the blockchain and recording their positions
// The streamer is notified when there's new batches to process
type TransactionStreamer struct {
	stopwaiter.StopWaiter

	chainConfig      *params.ChainConfig
	exec             execution.ExecutionSequencer
	execLastMsgCount arbutil.MessageIndex
	validator        *staker.BlockValidator

	db           ethdb.Database
	fatalErrChan chan<- error
	config       TransactionStreamerConfigFetcher

	insertionMutex     sync.Mutex // cannot be acquired while reorgMutex is held
	reorgMutex         sync.RWMutex
	newMessageNotifier chan struct{}

	nextAllowedFeedReorgLog time.Time

	broadcasterQueuedMessages            []arbostypes.MessageWithMetadata
	broadcasterQueuedMessagesPos         uint64
	broadcasterQueuedMessagesActiveReorg bool

	coordinator     *SeqCoordinator
	broadcastServer *broadcaster.Broadcaster
	inboxReader     *InboxReader
	delayedBridge   *DelayedBridge
}

type TransactionStreamerConfig struct {
	MaxBroadcasterQueueSize int           `koanf:"max-broadcaster-queue-size"`
	MaxReorgResequenceDepth int64         `koanf:"max-reorg-resequence-depth" reload:"hot"`
	ExecuteMessageLoopDelay time.Duration `koanf:"execute-message-loop-delay" reload:"hot"`
}

type TransactionStreamerConfigFetcher func() *TransactionStreamerConfig

var DefaultTransactionStreamerConfig = TransactionStreamerConfig{
	MaxBroadcasterQueueSize: 1024,
	MaxReorgResequenceDepth: 1024,
	ExecuteMessageLoopDelay: time.Millisecond * 100,
}

var TestTransactionStreamerConfig = TransactionStreamerConfig{
	MaxBroadcasterQueueSize: 10_000,
	MaxReorgResequenceDepth: 128 * 1024,
	ExecuteMessageLoopDelay: time.Millisecond,
}

func TransactionStreamerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int(prefix+".max-broadcaster-queue-size", DefaultTransactionStreamerConfig.MaxBroadcasterQueueSize, "maximum cache of pending broadcaster messages")
	f.Int64(prefix+".max-reorg-resequence-depth", DefaultTransactionStreamerConfig.MaxReorgResequenceDepth, "maximum number of messages to attempt to resequence on reorg (0 = never resequence, -1 = always resequence)")
	f.Duration(prefix+".execute-message-loop-delay", DefaultTransactionStreamerConfig.ExecuteMessageLoopDelay, "delay when polling calls to execute messages")
}

func NewTransactionStreamer(
	db ethdb.Database,
	chainConfig *params.ChainConfig,
	exec execution.ExecutionSequencer,
	broadcastServer *broadcaster.Broadcaster,
	fatalErrChan chan<- error,
	config TransactionStreamerConfigFetcher,
) (*TransactionStreamer, error) {
	streamer := &TransactionStreamer{
		exec:               exec,
		chainConfig:        chainConfig,
		db:                 db,
		newMessageNotifier: make(chan struct{}, 1),
		broadcastServer:    broadcastServer,
		fatalErrChan:       fatalErrChan,
		config:             config,
	}
	streamer.exec.SetTransactionStreamer(streamer)
	err := streamer.cleanupInconsistentState()
	if err != nil {
		return nil, err
	}
	return streamer, nil
}

// Encodes a uint64 as bytes in a lexically sortable manner for database iteration.
// Generally this is only used for database keys, which need sorted.
// A shorter RLP encoding is usually used for database values.
func uint64ToKey(x uint64) []byte {
	data := make([]byte, 8)
	binary.BigEndian.PutUint64(data, x)
	return data
}

func (s *TransactionStreamer) SetBlockValidator(validator *staker.BlockValidator) {
	if s.Started() {
		panic("trying to set coordinator after start")
	}
	if s.validator != nil {
		panic("trying to set coordinator when already set")
	}
	s.validator = validator
}

func (s *TransactionStreamer) SetSeqCoordinator(coordinator *SeqCoordinator) {
	if s.Started() {
		panic("trying to set coordinator after start")
	}
	if s.coordinator != nil {
		panic("trying to set coordinator when already set")
	}
	s.coordinator = coordinator
}

func (s *TransactionStreamer) SetInboxReaders(inboxReader *InboxReader, delayedBridge *DelayedBridge) {
	if s.Started() {
		panic("trying to set inbox reader after start")
	}
	if s.inboxReader != nil || s.delayedBridge != nil {
		panic("trying to set inbox reader when already set")
	}
	s.inboxReader = inboxReader
	s.delayedBridge = delayedBridge
}

func (s *TransactionStreamer) cleanupInconsistentState() error {
	// If it doesn't exist yet, set the message count to 0
	hasMessageCount, err := s.db.Has(messageCountKey)
	if err != nil {
		return err
	}
	if !hasMessageCount {
		err := setMessageCount(s.db, 0)
		if err != nil {
			return err
		}
	}
	// TODO remove trailing messageCountToMessage and messageCountToBlockPrefix entries
	return nil
}

func (s *TransactionStreamer) ReorgTo(count arbutil.MessageIndex) error {
	return s.ReorgToAndEndBatch(s.db.NewBatch(), count)
}

func (s *TransactionStreamer) ReorgToAndEndBatch(batch ethdb.Batch, count arbutil.MessageIndex) error {
	s.insertionMutex.Lock()
	defer s.insertionMutex.Unlock()
	err := s.reorg(batch, count, nil)
	if err != nil {
		return err
	}
	err = batch.Write()
	if err != nil {
		return err
	}
	return nil
}

func deleteStartingAt(db ethdb.Database, batch ethdb.Batch, prefix []byte, minKey []byte) error {
	iter := db.NewIterator(prefix, minKey)
	defer iter.Release()
	for iter.Next() {
		err := batch.Delete(iter.Key())
		if err != nil {
			return err
		}
	}
	return iter.Error()
}

// deleteFromRange deletes key ranging from startMinKey(inclusive) to endMinKey(exclusive)
// might have deleted some keys even if returning an error
func deleteFromRange(ctx context.Context, db ethdb.Database, prefix []byte, startMinKey uint64, endMinKey uint64) ([]uint64, error) {
	batch := db.NewBatch()
	startIter := db.NewIterator(prefix, uint64ToKey(startMinKey))
	defer startIter.Release()
	var prunedKeysRange []uint64
	for startIter.Next() {
		if ctx.Err() != nil {
			return nil, ctx.Err()
		}
		currentKey := binary.BigEndian.Uint64(bytes.TrimPrefix(startIter.Key(), prefix))
		if currentKey >= endMinKey {
			break
		}
		if len(prunedKeysRange) == 0 || len(prunedKeysRange) == 1 {
			prunedKeysRange = append(prunedKeysRange, currentKey)
		} else {
			prunedKeysRange[1] = currentKey
		}
		err := batch.Delete(startIter.Key())
		if err != nil {
			return nil, err
		}
		if batch.ValueSize() >= ethdb.IdealBatchSize {
			if err := batch.Write(); err != nil {
				return nil, err
			}
			batch.Reset()
		}
	}
	if batch.ValueSize() > 0 {
		if err := batch.Write(); err != nil {
			return nil, err
		}
	}
	return prunedKeysRange, nil
}

// The insertion mutex must be held. This acquires the reorg mutex.
// Note: oldMessages will be empty if reorgHook is nil
func (s *TransactionStreamer) reorg(batch ethdb.Batch, count arbutil.MessageIndex, newMessages []arbostypes.MessageWithMetadata) error {
	if count == 0 {
		return errors.New("cannot reorg out init message")
	}
	lastDelayedSeqNum, err := s.getPrevPrevDelayedRead(count)
	if err != nil {
		return err
	}
	var oldMessages []*arbostypes.MessageWithMetadata

	targetMsgCount, err := s.GetMessageCount()
	if err != nil {
		return err
	}
	config := s.config()
	maxResequenceMsgCount := count + arbutil.MessageIndex(config.MaxReorgResequenceDepth)
	if config.MaxReorgResequenceDepth >= 0 && maxResequenceMsgCount < targetMsgCount {
		log.Error(
			"unable to re-sequence all old messages because there are too many",
			"reorgingToCount", count,
			"removingMessages", targetMsgCount-count,
			"maxReorgResequenceDepth", config.MaxReorgResequenceDepth,
		)
		targetMsgCount = maxResequenceMsgCount
	}
	for i := count; i < targetMsgCount; i++ {
		oldMessage, err := s.GetMessage(i)
		if err != nil {
			log.Error("unable to lookup old message for re-sequencing", "position", i, "err", err)
			break
		}

		if oldMessage.Message == nil || oldMessage.Message.Header == nil {
			continue
		}

		header := oldMessage.Message.Header

		if header.RequestId != nil {
			// This is a delayed message
			delayedSeqNum := header.RequestId.Big().Uint64()
			if delayedSeqNum+1 != oldMessage.DelayedMessagesRead {
				log.Error("delayed message header RequestId doesn't match database DelayedMessagesRead", "header", oldMessage.Message.Header, "delayedMessagesRead", oldMessage.DelayedMessagesRead)
				continue
			}
			if delayedSeqNum != lastDelayedSeqNum {
				// This is the wrong position for the delayed message
				continue
			}
			if s.inboxReader != nil {
				// this is a delayed message. Should be resequenced if all 3 agree:
				// oldMessage, accumulator stored in tracker, and the message re-read from l1
				expectedAcc, err := s.inboxReader.tracker.GetDelayedAcc(delayedSeqNum)
				if err != nil {
					if !strings.Contains(err.Error(), "not found") {
						log.Error("reorg-resequence: failed to read expected accumulator", "err", err)
					}
					continue
				}
				msgBlockNum := new(big.Int).SetUint64(oldMessage.Message.Header.BlockNumber)
				delayedInBlock, err := s.delayedBridge.LookupMessagesInRange(s.GetContext(), msgBlockNum, msgBlockNum, nil)
				if err != nil {
					log.Error("reorg-resequence: failed to serialize old delayed message from database", "err", err)
					continue
				}
				messageFound := false
			delayedInBlockLoop:
				for _, delayedFound := range delayedInBlock {
					if delayedFound.Message.Header.RequestId.Big().Uint64() != delayedSeqNum {
						continue delayedInBlockLoop
					}
					if expectedAcc == delayedFound.AfterInboxAcc() && delayedFound.Message.Equals(oldMessage.Message) {
						messageFound = true
					}
					break delayedInBlockLoop
				}
				if !messageFound {
					continue
				}
			}
			lastDelayedSeqNum++
		}

		oldMessages = append(oldMessages, oldMessage)
	}

	s.reorgMutex.Lock()
	defer s.reorgMutex.Unlock()

	err = s.exec.Reorg(count, newMessages, oldMessages)
	if err != nil {
		return err
	}

	if s.validator != nil {
		err = s.validator.Reorg(s.GetContext(), count)
		if err != nil {
			return err
		}
	}

	err = deleteStartingAt(s.db, batch, messagePrefix, uint64ToKey(uint64(count)))
	if err != nil {
		return err
	}

	return setMessageCount(batch, count)
}

func setMessageCount(batch ethdb.KeyValueWriter, count arbutil.MessageIndex) error {
	countBytes, err := rlp.EncodeToBytes(count)
	if err != nil {
		return err
	}
	err = batch.Put(messageCountKey, countBytes)
	if err != nil {
		return err
	}
	sharedmetrics.UpdateSequenceNumberGauge(count)

	return nil
}

func dbKey(prefix []byte, pos uint64) []byte {
	var key []byte
	key = append(key, prefix...)
	key = append(key, uint64ToKey(pos)...)
	return key
}

// Note: if changed to acquire the mutex, some internal users may need to be updated to a non-locking version.
func (s *TransactionStreamer) GetMessage(seqNum arbutil.MessageIndex) (*arbostypes.MessageWithMetadata, error) {
	key := dbKey(messagePrefix, uint64(seqNum))
	data, err := s.db.Get(key)
	if err != nil {
		return nil, err
	}
	var message arbostypes.MessageWithMetadata
	err = rlp.DecodeBytes(data, &message)
	if err != nil {
		return nil, err
	}

	return &message, nil
}

// Note: if changed to acquire the mutex, some internal users may need to be updated to a non-locking version.
func (s *TransactionStreamer) GetMessageCount() (arbutil.MessageIndex, error) {
	posBytes, err := s.db.Get(messageCountKey)
	if err != nil {
		return 0, err
	}
	var pos uint64
	err = rlp.DecodeBytes(posBytes, &pos)
	if err != nil {
		return 0, err
	}
	return arbutil.MessageIndex(pos), nil
}

func (s *TransactionStreamer) GetProcessedMessageCount() (arbutil.MessageIndex, error) {
	msgCount, err := s.GetMessageCount()
	if err != nil {
		return 0, err
	}
	digestedHead, err := s.exec.HeadMessageNumber()
	if err != nil {
		return 0, err
	}
	if msgCount > digestedHead+1 {
		return digestedHead + 1, nil
	}
	return msgCount, nil
}

func (s *TransactionStreamer) AddMessages(pos arbutil.MessageIndex, messagesAreConfirmed bool, messages []arbostypes.MessageWithMetadata) error {
	return s.AddMessagesAndEndBatch(pos, messagesAreConfirmed, messages, nil)
}

func (s *TransactionStreamer) AddBroadcastMessages(feedMessages []*broadcaster.BroadcastFeedMessage) error {
	if len(feedMessages) == 0 {
		return nil
	}
	broadcastStartPos := feedMessages[0].SequenceNumber
	var messages []arbostypes.MessageWithMetadata
	broadcastAfterPos := broadcastStartPos
	for _, feedMessage := range feedMessages {
		if broadcastAfterPos != feedMessage.SequenceNumber {
			return fmt.Errorf("invalid sequence number %v, expected %v", feedMessage.SequenceNumber, broadcastAfterPos)
		}
		if feedMessage.Message.Message == nil || feedMessage.Message.Message.Header == nil {
			return fmt.Errorf("invalid feed message at sequence number %v", feedMessage.SequenceNumber)
		}
		messages = append(messages, feedMessage.Message)
		broadcastAfterPos++
	}

	s.insertionMutex.Lock()
	defer s.insertionMutex.Unlock()

	var feedReorg bool
	var err error
	// Skip any messages already in the database
	// prevDelayedRead set to 0 because it's only used to compute the output prevDelayedRead which is not used here
	// Messages from feed are not confirmed, so confirmedMessageCount is 0 and confirmedReorg can be ignored
	dups, feedReorg, oldMsg, err := s.countDuplicateMessages(broadcastStartPos, messages, nil)
	if err != nil {
		return err
	}
	messages = messages[dups:]
	broadcastStartPos += arbutil.MessageIndex(dups)
	if oldMsg != nil {
		s.logReorg(broadcastStartPos, oldMsg, &messages[0], false)
	}
	if len(messages) == 0 {
		// No new messages received
		return nil
	}

	if len(s.broadcasterQueuedMessages) == 0 || (feedReorg && !s.broadcasterQueuedMessagesActiveReorg) {
		// Empty cache or feed different from database, save current feed messages until confirmed L1 messages catch up.
		s.broadcasterQueuedMessages = messages
		atomic.StoreUint64(&s.broadcasterQueuedMessagesPos, uint64(broadcastStartPos))
		s.broadcasterQueuedMessagesActiveReorg = feedReorg
	} else {
		broadcasterQueuedMessagesPos := arbutil.MessageIndex(atomic.LoadUint64(&s.broadcasterQueuedMessagesPos))
		if broadcasterQueuedMessagesPos >= broadcastStartPos {
			// Feed messages older than cache
			s.broadcasterQueuedMessages = messages
			atomic.StoreUint64(&s.broadcasterQueuedMessagesPos, uint64(broadcastStartPos))
			s.broadcasterQueuedMessagesActiveReorg = feedReorg
		} else if broadcasterQueuedMessagesPos+arbutil.MessageIndex(len(s.broadcasterQueuedMessages)) == broadcastStartPos {
			// Feed messages can be added directly to end of cache
			maxQueueSize := s.config().MaxBroadcasterQueueSize
			if maxQueueSize == 0 || len(s.broadcasterQueuedMessages) <= maxQueueSize {
				s.broadcasterQueuedMessages = append(s.broadcasterQueuedMessages, messages...)
			}
			broadcastStartPos = broadcasterQueuedMessagesPos
			// Do not change existing reorg state
		} else {
			if len(s.broadcasterQueuedMessages) > 0 {
				log.Warn(
					"broadcaster queue jumped positions",
					"queuedMessages", len(s.broadcasterQueuedMessages),
					"expectedNextPos", broadcasterQueuedMessagesPos+arbutil.MessageIndex(len(s.broadcasterQueuedMessages)),
					"gotPos", broadcastStartPos,
				)
			}
			s.broadcasterQueuedMessages = messages
			atomic.StoreUint64(&s.broadcasterQueuedMessagesPos, uint64(broadcastStartPos))
			s.broadcasterQueuedMessagesActiveReorg = feedReorg
		}
	}

	if s.broadcasterQueuedMessagesActiveReorg || len(s.broadcasterQueuedMessages) == 0 {
		// Broadcaster never triggered reorg or no messages to add
		return nil
	}

	if broadcastStartPos > 0 {
		_, err := s.GetMessage(broadcastStartPos - 1)
		if err != nil {
			if !errors.Is(err, leveldb.ErrNotFound) && !errors.Is(err, pebble.ErrNotFound) {
				return err
			}
			// Message before current message doesn't exist in database, so don't add current messages yet
			return nil
		}
	}

	err = s.addMessagesAndEndBatchImpl(broadcastStartPos, false, nil, nil)
	if err != nil {
		return fmt.Errorf("error adding pending broadcaster messages: %w", err)
	}

	return nil
}

// AddFakeInitMessage should only be used for testing or running a local dev node
func (s *TransactionStreamer) AddFakeInitMessage() error {
	chainConfigJson, err := json.Marshal(s.chainConfig)
	if err != nil {
		return fmt.Errorf("failed to serialize chain config: %w", err)
	}
	msg := append(append(math.U256Bytes(s.chainConfig.ChainID), 0), chainConfigJson...)
	return s.AddMessages(0, false, []arbostypes.MessageWithMetadata{{
		Message: &arbostypes.L1IncomingMessage{
			Header: &arbostypes.L1IncomingMessageHeader{
				Kind:      arbostypes.L1MessageType_Initialize,
				RequestId: &common.Hash{},
				L1BaseFee: common.Big0,
			},
			L2msg: msg,
		},
		DelayedMessagesRead: 1,
	}})
}

// Used in redis tests
func (s *TransactionStreamer) GetMessageCountSync(t *testing.T) (arbutil.MessageIndex, error) {
	s.insertionMutex.Lock()
	defer s.insertionMutex.Unlock()
	return s.GetMessageCount()
}

func endBatch(batch ethdb.Batch) error {
	if batch == nil {
		return nil
	}
	return batch.Write()
}

func (s *TransactionStreamer) AddMessagesAndEndBatch(pos arbutil.MessageIndex, messagesAreConfirmed bool, messages []arbostypes.MessageWithMetadata, batch ethdb.Batch) error {
	if messagesAreConfirmed {
		s.reorgMutex.RLock()
		dups, _, _, err := s.countDuplicateMessages(pos, messages, nil)
		s.reorgMutex.RUnlock()
		if err != nil {
			return err
		}
		if dups == len(messages) {
			return endBatch(batch)
		}
		// cant keep reorg lock when catching insertionMutex.
		// we have to re-evaluate all messages
		// happy cases for confirmed messages:
		// 1: were previously in feed. We saved work
		// 2: are new (syncing). We wasted very little work.
	}
	s.insertionMutex.Lock()
	defer s.insertionMutex.Unlock()

	return s.addMessagesAndEndBatchImpl(pos, messagesAreConfirmed, messages, batch)
}

func (s *TransactionStreamer) getPrevPrevDelayedRead(pos arbutil.MessageIndex) (uint64, error) {
	var prevDelayedRead uint64
	if pos > 0 {
		prevMsg, err := s.GetMessage(pos - 1)
		if err != nil {
			return 0, fmt.Errorf("failed to get previous message for pos %d: %w", pos, err)
		}
		prevDelayedRead = prevMsg.DelayedMessagesRead
	}

	return prevDelayedRead, nil
}

func (s *TransactionStreamer) countDuplicateMessages(
	pos arbutil.MessageIndex,
	messages []arbostypes.MessageWithMetadata,
	batch *ethdb.Batch,
) (int, bool, *arbostypes.MessageWithMetadata, error) {
	curMsg := 0
	for {
		if len(messages) == curMsg {
			break
		}
		key := dbKey(messagePrefix, uint64(pos))
		hasMessage, err := s.db.Has(key)
		if err != nil {
			return 0, false, nil, err
		}
		if !hasMessage {
			break
		}
		haveMessage, err := s.db.Get(key)
		if err != nil {
			return 0, false, nil, err
		}
		nextMessage := messages[curMsg]
		wantMessage, err := rlp.EncodeToBytes(nextMessage)
		if err != nil {
			return 0, false, nil, err
		}
		if !bytes.Equal(haveMessage, wantMessage) {
			// Current message does not exactly match message in database
			var dbMessageParsed arbostypes.MessageWithMetadata

			if err := rlp.DecodeBytes(haveMessage, &dbMessageParsed); err != nil {
				log.Warn("TransactionStreamer: Reorg detected! (failed parsing db message)",
					"pos", pos,
					"err", err,
				)
				return curMsg, true, nil, nil
			}
			var duplicateMessage bool
			if nextMessage.Message != nil {
				if dbMessageParsed.Message.BatchGasCost == nil || nextMessage.Message.BatchGasCost == nil {
					// Remove both of the batch gas costs and see if the messages still differ
					nextMessageCopy := nextMessage
					nextMessageCopy.Message = new(arbostypes.L1IncomingMessage)
					*nextMessageCopy.Message = *nextMessage.Message
					batchGasCostBkup := dbMessageParsed.Message.BatchGasCost
					dbMessageParsed.Message.BatchGasCost = nil
					nextMessageCopy.Message.BatchGasCost = nil
					if reflect.DeepEqual(dbMessageParsed, nextMessageCopy) {
						// Actually this isn't a reorg; only the batch gas costs differed
						duplicateMessage = true
						// If possible - update the message in the database to add the gas cost cache.
						if batch != nil && nextMessage.Message.BatchGasCost != nil {
							if *batch == nil {
								*batch = s.db.NewBatch()
							}
							if err := s.writeMessage(pos, nextMessage, *batch); err != nil {
								return 0, false, nil, err
							}
						}
					}
					dbMessageParsed.Message.BatchGasCost = batchGasCostBkup
				}
			}

			if !duplicateMessage {
				return curMsg, true, &dbMessageParsed, nil
			}
		}

		curMsg++
		pos++
	}

	return curMsg, false, nil, nil
}

func (s *TransactionStreamer) logReorg(pos arbutil.MessageIndex, dbMsg *arbostypes.MessageWithMetadata, newMsg *arbostypes.MessageWithMetadata, confirmed bool) {
	sendLog := confirmed
	if time.Now().After(s.nextAllowedFeedReorgLog) {
		sendLog = true
	}
	if sendLog {
		s.nextAllowedFeedReorgLog = time.Now().Add(time.Minute)
		log.Warn("TransactionStreamer: Reorg detected!",
			"confirmed", confirmed,
			"pos", pos,
			"got-delayed", newMsg.DelayedMessagesRead,
			"got-header", newMsg.Message.Header,
			"db-delayed", dbMsg.DelayedMessagesRead,
			"db-header", dbMsg.Message.Header,
		)
	}

}

func (s *TransactionStreamer) addMessagesAndEndBatchImpl(messageStartPos arbutil.MessageIndex, messagesAreConfirmed bool, messages []arbostypes.MessageWithMetadata, batch ethdb.Batch) error {
	var confirmedReorg bool
	var oldMsg *arbostypes.MessageWithMetadata
	var lastDelayedRead uint64
	var hasNewConfirmedMessages bool
	var cacheClearLen int

	messagesAfterPos := messageStartPos + arbutil.MessageIndex(len(messages))
	broadcastStartPos := arbutil.MessageIndex(atomic.LoadUint64(&s.broadcasterQueuedMessagesPos))

	if messagesAreConfirmed {
		var duplicates int
		var err error
		duplicates, confirmedReorg, oldMsg, err = s.countDuplicateMessages(messageStartPos, messages, &batch)
		if err != nil {
			return err
		}
		if duplicates > 0 {
			lastDelayedRead = messages[duplicates-1].DelayedMessagesRead
			messages = messages[duplicates:]
			messageStartPos += arbutil.MessageIndex(duplicates)
		}
		if len(messages) > 0 {
			hasNewConfirmedMessages = true
		}
	}

	clearQueueOnSuccess := false
	if (s.broadcasterQueuedMessagesActiveReorg && messageStartPos <= broadcastStartPos) ||
		(!s.broadcasterQueuedMessagesActiveReorg && broadcastStartPos <= messagesAfterPos) {
		// Active broadcast reorg and L1 messages at or before start of broadcast messages
		// Or no active broadcast reorg and broadcast messages start before or immediately after last L1 message
		if messagesAfterPos >= broadcastStartPos {
			broadcastSliceIndex := int(messagesAfterPos - broadcastStartPos)
			messagesOldLen := len(messages)
			if broadcastSliceIndex < len(s.broadcasterQueuedMessages) {
				// Some cached feed messages can be used
				messages = append(messages, s.broadcasterQueuedMessages[broadcastSliceIndex:]...)
			}
			// This calculation gives the exact length of cache which was appended to messages
			cacheClearLen = broadcastSliceIndex + len(messages) - messagesOldLen
		}

		// L1 used or replaced broadcast cache items
		clearQueueOnSuccess = true
	}

	var feedReorg bool
	if !hasNewConfirmedMessages {
		var duplicates int
		var err error
		duplicates, feedReorg, oldMsg, err = s.countDuplicateMessages(messageStartPos, messages, nil)
		if err != nil {
			return err
		}
		if duplicates > 0 {
			lastDelayedRead = messages[duplicates-1].DelayedMessagesRead
			messages = messages[duplicates:]
			messageStartPos += arbutil.MessageIndex(duplicates)
		}
	}
	if oldMsg != nil {
		s.logReorg(messageStartPos, oldMsg, &messages[0], confirmedReorg)
	}

	if feedReorg {
		// Never allow feed to reorg confirmed messages
		// Note that any remaining messages must be feed messages, so we're done here
		return endBatch(batch)
	}

	if lastDelayedRead == 0 {
		var err error
		lastDelayedRead, err = s.getPrevPrevDelayedRead(messageStartPos)
		if err != nil {
			return err
		}
	}

	// Validate delayed message counts of remaining messages
	for i, msg := range messages {
		msgPos := messageStartPos + arbutil.MessageIndex(i)
		diff := msg.DelayedMessagesRead - lastDelayedRead
		if diff != 0 && diff != 1 {
			return fmt.Errorf("attempted to insert jump from %v delayed messages read to %v delayed messages read at message index %v", lastDelayedRead, msg.DelayedMessagesRead, msgPos)
		}
		lastDelayedRead = msg.DelayedMessagesRead
		if msg.Message == nil {
			return fmt.Errorf("attempted to insert nil message at position %v", msgPos)
		}
	}

	if confirmedReorg {
		reorgBatch := s.db.NewBatch()
		err := s.reorg(reorgBatch, messageStartPos, messages)
		if err != nil {
			return err
		}
		err = reorgBatch.Write()
		if err != nil {
			return err
		}
	}
	if len(messages) == 0 {
		return endBatch(batch)
	}

	err := s.writeMessages(messageStartPos, messages, batch)
	if err != nil {
		return err
	}

	if clearQueueOnSuccess {
		// Check if new messages were added at the end of cache, if they were, then dont remove those particular messages
		if len(s.broadcasterQueuedMessages) > cacheClearLen {
			s.broadcasterQueuedMessages = s.broadcasterQueuedMessages[cacheClearLen:]
			atomic.StoreUint64(&s.broadcasterQueuedMessagesPos, uint64(broadcastStartPos)+uint64(cacheClearLen))
		} else {
			s.broadcasterQueuedMessages = s.broadcasterQueuedMessages[:0]
			atomic.StoreUint64(&s.broadcasterQueuedMessagesPos, 0)
		}
		s.broadcasterQueuedMessagesActiveReorg = false
	}

	return nil
}

func (s *TransactionStreamer) FetchBatch(batchNum uint64) ([]byte, error) {
	return s.inboxReader.GetSequencerMessageBytes(context.TODO(), batchNum)
}

// The caller must hold the insertionMutex
func (s *TransactionStreamer) ExpectChosenSequencer() error {
	if s.coordinator != nil {
		if !s.coordinator.CurrentlyChosen() {
			return fmt.Errorf("%w: not main sequencer", execution.ErrRetrySequencer)
		}
	}
	return nil
}

func (s *TransactionStreamer) WriteMessageFromSequencer(pos arbutil.MessageIndex, msgWithMeta arbostypes.MessageWithMetadata) error {
	if err := s.ExpectChosenSequencer(); err != nil {
		return err
	}
	if !s.insertionMutex.TryLock() {
		return execution.ErrSequencerInsertLockTaken
	}
	defer s.insertionMutex.Unlock()

	msgCount, err := s.GetMessageCount()
	if err != nil {
		return err
	}

	if msgCount != pos {
		return fmt.Errorf("wrong pos got %d expected %d", pos, msgCount)
	}

	if s.coordinator != nil {
		if err := s.coordinator.SequencingMessage(pos, &msgWithMeta); err != nil {
			return err
		}
	}

	if err := s.writeMessages(pos, []arbostypes.MessageWithMetadata{msgWithMeta}, nil); err != nil {
		return err
	}

	if s.broadcastServer != nil {
		if err := s.broadcastServer.BroadcastSingle(msgWithMeta, pos); err != nil {
			log.Error("failed broadcasting message", "pos", pos, "err", err)
		}
	}

	return nil
}

func (s *TransactionStreamer) GenesisBlockNumber() uint64 {
	return s.chainConfig.ArbitrumChainParams.GenesisBlockNum
}

// PauseReorgs until a matching call to ResumeReorgs (may be called concurrently)
func (s *TransactionStreamer) PauseReorgs() {
	s.reorgMutex.RLock()
}

func (s *TransactionStreamer) ResumeReorgs() {
	s.reorgMutex.RUnlock()
}

func (s *TransactionStreamer) PopulateFeedBacklog() error {
	if s.broadcastServer == nil {
		return nil
	}
	return s.inboxReader.tracker.PopulateFeedBacklog(s.broadcastServer)
}

func (s *TransactionStreamer) writeMessage(pos arbutil.MessageIndex, msg arbostypes.MessageWithMetadata, batch ethdb.Batch) error {
	key := dbKey(messagePrefix, uint64(pos))
	msgBytes, err := rlp.EncodeToBytes(msg)
	if err != nil {
		return err
	}
	return batch.Put(key, msgBytes)
}

// The mutex must be held, and pos must be the latest message count.
// `batch` may be nil, which initializes a new batch. The batch is closed out in this function.
func (s *TransactionStreamer) writeMessages(pos arbutil.MessageIndex, messages []arbostypes.MessageWithMetadata, batch ethdb.Batch) error {
	if batch == nil {
		batch = s.db.NewBatch()
	}
	for i, msg := range messages {
		err := s.writeMessage(pos+arbutil.MessageIndex(i), msg, batch)
		if err != nil {
			return err
		}
	}

	err := setMessageCount(batch, pos+arbutil.MessageIndex(len(messages)))
	if err != nil {
		return err
	}
	err = batch.Write()
	if err != nil {
		return err
	}

	select {
	case s.newMessageNotifier <- struct{}{}:
	default:
	}

	return nil
}

// TODO: eventually there will be a table maintained by txStreamer itself
func (s *TransactionStreamer) ResultAtCount(count arbutil.MessageIndex) (*execution.MessageResult, error) {
	if count == 0 {
		return &execution.MessageResult{}, nil
	}
	return s.exec.ResultAtPos(count - 1)
}

// return value: true if should be called again immediately
func (s *TransactionStreamer) executeNextMsg(ctx context.Context, exec execution.ExecutionSequencer) bool {
	if ctx.Err() != nil {
		return false
	}
	if !s.reorgMutex.TryRLock() {
		return false
	}
	defer s.reorgMutex.RUnlock()
	prevMessageCount := s.execLastMsgCount
	msgCount, err := s.GetMessageCount()
	if err != nil {
		log.Error("feedOneMsg failed to get message count", "err", err)
		return false
	}
	s.execLastMsgCount = msgCount
	pos, err := s.exec.HeadMessageNumber()
	if err != nil {
		log.Error("feedOneMsg failed to get exec engine message count", "err", err)
		return false
	}
	pos++
	if pos >= msgCount {
		return false
	}
	msg, err := s.GetMessage(pos)
	if err != nil {
		log.Error("feedOneMsg failed to readMessage", "err", err, "pos", pos)
		return false
	}
	err = s.exec.DigestMessage(pos, msg)
	if err != nil {
		logger := log.Warn
		if prevMessageCount < msgCount {
			logger = log.Debug
		}
		logger("feedOneMsg failed to send message to execEngine", "err", err, "pos", pos)
		return false
	}
	return pos+1 < msgCount
}

func (s *TransactionStreamer) executeMessages(ctx context.Context, ignored struct{}) time.Duration {
	if s.executeNextMsg(ctx, s.exec) {
		return 0
	}
	return s.config().ExecuteMessageLoopDelay
}

func (s *TransactionStreamer) Start(ctxIn context.Context) error {
	s.StopWaiter.Start(ctxIn, s)
	return stopwaiter.CallIterativelyWith[struct{}](&s.StopWaiterSafe, s.executeMessages, s.newMessageNotifier)
}

'''
'''--- arbos/addressSet/addressSet.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package addressSet

// TODO lowercase this package name

import (
	"errors"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
)

// AddressSet represents a set of addresses
// size is stored at position 0
// members of the set are stored sequentially from 1 onward
type AddressSet struct {
	backingStorage *storage.Storage
	size           storage.StorageBackedUint64
	byAddress      *storage.Storage
}

func Initialize(sto *storage.Storage) error {
	return sto.SetUint64ByUint64(0, 0)
}

func OpenAddressSet(sto *storage.Storage) *AddressSet {
	return &AddressSet{
		backingStorage: sto.WithoutCache(),
		size:           sto.OpenStorageBackedUint64(0),
		byAddress:      sto.OpenSubStorage([]byte{0}),
	}
}

func (as *AddressSet) Size() (uint64, error) {
	return as.size.Get()
}

func (as *AddressSet) IsMember(addr common.Address) (bool, error) {
	value, err := as.byAddress.Get(util.AddressToHash(addr))
	return value != (common.Hash{}), err
}

func (as *AddressSet) GetAnyMember() (*common.Address, error) {
	size, err := as.size.Get()
	if err != nil || size == 0 {
		return nil, err
	}
	sba := as.backingStorage.OpenStorageBackedAddressOrNil(1)
	addr, err := sba.Get()
	return addr, err
}

func (as *AddressSet) Clear() error {
	size, err := as.size.Get()
	if err != nil || size == 0 {
		return err
	}
	for i := uint64(1); i <= size; i++ {
		contents, _ := as.backingStorage.GetByUint64(i)
		_ = as.backingStorage.ClearByUint64(i)
		err = as.byAddress.Clear(contents)
		if err != nil {
			return err
		}
	}
	return as.size.Clear()
}

func (as *AddressSet) AllMembers(maxNumToReturn uint64) ([]common.Address, error) {
	size, err := as.size.Get()
	if err != nil {
		return nil, err
	}
	if size > maxNumToReturn {
		size = maxNumToReturn
	}
	ret := make([]common.Address, size)
	for i := range ret {
		sba := as.backingStorage.OpenStorageBackedAddress(uint64(i + 1))
		ret[i], err = sba.Get()
		if err != nil {
			return nil, err
		}
	}
	return ret, nil
}

func (as *AddressSet) ClearList() error {
	size, err := as.size.Get()
	if err != nil || size == 0 {
		return err
	}
	for i := uint64(1); i <= size; i++ {
		err = as.backingStorage.ClearByUint64(i)
		if err != nil {
			return err
		}
	}
	return as.size.Clear()
}

func (as *AddressSet) RectifyMapping(addr common.Address) error {
	isOwner, err := as.IsMember(addr)
	if !isOwner || err != nil {
		return errors.New("RectifyMapping: Address is not an owner")
	}

	// If the mapping is correct, RectifyMapping shouldn't do anything
	// Additional safety check to avoid corruption of mapping after the initial fix
	addrAsHash := common.BytesToHash(addr.Bytes())
	slot, err := as.byAddress.GetUint64(addrAsHash)
	if err != nil {
		return err
	}
	atSlot, err := as.backingStorage.GetByUint64(slot)
	if err != nil {
		return err
	}
	size, err := as.size.Get()
	if err != nil {
		return err
	}
	if atSlot == addrAsHash && slot <= size {
		return errors.New("RectifyMapping: Owner address is correctly mapped")
	}

	// Remove the owner from map and add them as a new owner
	err = as.byAddress.Clear(addrAsHash)
	if err != nil {
		return err
	}

	return as.Add(addr)
}

func (as *AddressSet) Add(addr common.Address) error {
	present, err := as.IsMember(addr)
	if present || err != nil {
		return err
	}
	size, err := as.size.Get()
	if err != nil {
		return err
	}
	slot := util.UintToHash(1 + size)
	addrAsHash := common.BytesToHash(addr.Bytes())
	err = as.byAddress.Set(addrAsHash, slot)
	if err != nil {
		return err
	}
	sba := as.backingStorage.OpenStorageBackedAddress(1 + size)
	err = sba.Set(addr)
	if err != nil {
		return err
	}
	_, err = as.size.Increment()
	return err
}

func (as *AddressSet) Remove(addr common.Address, arbosVersion uint64) error {
	addrAsHash := common.BytesToHash(addr.Bytes())
	slot, err := as.byAddress.GetUint64(addrAsHash)
	if slot == 0 || err != nil {
		return err
	}
	err = as.byAddress.Clear(addrAsHash)
	if err != nil {
		return err
	}
	size, err := as.size.Get()
	if err != nil {
		return err
	}
	if slot < size {
		atSize, err := as.backingStorage.GetByUint64(size)
		if err != nil {
			return err
		}
		err = as.backingStorage.SetByUint64(slot, atSize)
		if err != nil {
			return err
		}
		if arbosVersion >= 11 {
			err = as.byAddress.Set(atSize, util.UintToHash(slot))
			if err != nil {
				return err
			}
		}
	}
	err = as.backingStorage.ClearByUint64(size)
	if err != nil {
		return err
	}
	_, err = as.size.Decrement()
	return err
}

'''
'''--- arbos/addressSet/addressSet_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package addressSet

import (
	"fmt"
	"math/rand"
	"testing"

	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/params"
	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestEmptyAddressSet(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	Require(t, Initialize(sto))
	aset := OpenAddressSet(sto)
	version := params.ArbitrumDevTestParams().InitialArbOSVersion

	if size(t, aset) != 0 {
		Fail(t)
	}
	if isMember(t, aset, common.Address{}) {
		Fail(t)
	}
	err := aset.Remove(common.Address{}, version)
	Require(t, err)
	if size(t, aset) != 0 {
		Fail(t)
	}
	if isMember(t, aset, common.Address{}) {
		Fail(t)
	}
}

func TestAddressSet(t *testing.T) {
	db := storage.NewMemoryBackedStateDB()
	sto := storage.NewGeth(db, burn.NewSystemBurner(nil, false))
	Require(t, Initialize(sto))
	aset := OpenAddressSet(sto)
	version := params.ArbitrumDevTestParams().InitialArbOSVersion

	statedb, _ := (db).(*state.StateDB)
	stateHashBeforeChanges := statedb.IntermediateRoot(false)

	addr1 := testhelpers.RandomAddress()
	addr2 := testhelpers.RandomAddress()
	addr3 := testhelpers.RandomAddress()
	possibleAddresses := []common.Address{addr1, addr2, addr3}

	Require(t, aset.Add(addr1))
	if size(t, aset) != 1 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Add(addr2))
	if size(t, aset) != 2 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Add(addr1))
	if size(t, aset) != 2 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)
	if !isMember(t, aset, addr1) {
		Fail(t)
	}
	if !isMember(t, aset, addr2) {
		Fail(t)
	}
	if isMember(t, aset, addr3) {
		Fail(t)
	}

	Require(t, aset.Remove(addr1, version))
	if size(t, aset) != 1 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)
	if isMember(t, aset, addr1) {
		Fail(t)
	}
	if !isMember(t, aset, addr2) {
		Fail(t)
	}

	Require(t, aset.Add(addr3))
	if size(t, aset) != 2 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Remove(addr3, version))
	if size(t, aset) != 1 {
		Fail(t)
	}
	checkAllMembers(t, aset, possibleAddresses)

	Require(t, aset.Add(addr1))
	all, err := aset.AllMembers(math.MaxUint64)
	Require(t, err)
	if len(all) != 2 {
		Fail(t)
	}
	if all[0] == addr1 {
		if all[1] != addr2 {
			Fail(t)
		}
	} else {
		if (all[0] != addr2) || (all[1] != addr1) {
			Fail(t)
		}
	}

	stateHashAfterChanges := statedb.IntermediateRoot(false)
	Require(t, aset.Clear())
	stateHashAfterClear := statedb.IntermediateRoot(false)

	colors.PrintBlue("prior ", stateHashBeforeChanges)
	colors.PrintGrey("after ", stateHashAfterChanges)
	colors.PrintBlue("clear ", stateHashAfterClear)

	if stateHashAfterClear != stateHashBeforeChanges {
		Fail(t, "Clear() left data in the statedb")
	}
	if stateHashAfterChanges == stateHashBeforeChanges {
		Fail(t, "set-operations didn't change the underlying statedb")
	}
}

func TestAddressSetAllMembers(t *testing.T) {
	db := storage.NewMemoryBackedStateDB()
	sto := storage.NewGeth(db, burn.NewSystemBurner(nil, false))
	Require(t, Initialize(sto))
	aset := OpenAddressSet(sto)
	version := params.ArbitrumDevTestParams().InitialArbOSVersion

	addr1 := testhelpers.RandomAddress()
	addr2 := testhelpers.RandomAddress()
	addr3 := testhelpers.RandomAddress()
	possibleAddresses := []common.Address{addr1, addr2, addr3}

	Require(t, aset.Add(addr1))
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Add(addr2))
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Remove(addr1, version))
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Add(addr3))
	checkAllMembers(t, aset, possibleAddresses)
	Require(t, aset.Remove(addr2, version))
	checkAllMembers(t, aset, possibleAddresses)

	for i := 0; i < 512; i++ {
		rem := rand.Intn(2) == 1
		addr := possibleAddresses[rand.Intn(len(possibleAddresses))]
		if rem {
			fmt.Printf("removing %v\n", addr)
			Require(t, aset.Remove(addr, version))
		} else {
			fmt.Printf("adding %v\n", addr)
			Require(t, aset.Add(addr))
		}
		checkAllMembers(t, aset, possibleAddresses)
	}
}

func TestRectifyMappingAgainstHistory(t *testing.T) {
	db := storage.NewMemoryBackedStateDB()
	sto := storage.NewGeth(db, burn.NewSystemBurner(nil, false))
	Require(t, Initialize(sto))
	aset := OpenAddressSet(sto)
	version := uint64(10)

	// Test Nova history
	addr1 := common.HexToAddress("0x9C040726F2A657226Ed95712245DeE84b650A1b5")
	addr2 := common.HexToAddress("0xd345e41ae2cb00311956aa7109fc801ae8c81a52")
	addr3 := common.HexToAddress("0xd0749b3e537ed52de4e6a3ae1eb6fc26059d0895")
	addr4 := common.HexToAddress("0x86a02dd71363c440b21f4c0e5b2ad01ffe1a7482")
	// Follow logs
	Require(t, aset.Add(addr1))
	Require(t, aset.Add(addr2))
	Require(t, aset.Remove(addr1, version))
	Require(t, aset.Add(addr3))
	Require(t, aset.Add(addr4))
	Require(t, aset.Remove(addr2, version))
	Require(t, aset.Remove(addr3, version))
	// Check if history's correct
	CurrentOwner, _ := aset.backingStorage.GetByUint64(uint64(1))
	isOwner, _ := aset.IsMember(addr2)
	correctOwner, _ := aset.IsMember(addr4)
	if size(t, aset) != uint64(1) || CurrentOwner != common.BytesToHash(addr2.Bytes()) || isOwner || !correctOwner {
		Fail(t, "Logs and current state did not match")
	}
	// Run RectifyMapping to fix the issue
	checkIfRectifyMappingWorks(t, aset, []common.Address{addr4}, true)
	Require(t, aset.Clear())

	// Test Arb1 history
	addr1 = common.HexToAddress("0xd345e41ae2cb00311956aa7109fc801ae8c81a52")
	addr2 = common.HexToAddress("0x98e4db7e07e584f89a2f6043e7b7c89dc27769ed")
	addr3 = common.HexToAddress("0xcf57572261c7c2bcf21ffd220ea7d1a27d40a827")
	// Follow logs
	Require(t, aset.Add(addr1))
	Require(t, aset.Add(addr2))
	Require(t, aset.Add(addr3))
	Require(t, aset.Remove(addr1, version))
	Require(t, aset.Remove(addr2, version))
	// Check if history's correct
	CurrentOwner, _ = aset.backingStorage.GetByUint64(uint64(1))
	correctOwner, _ = aset.IsMember(addr3)
	index, _ := aset.byAddress.GetUint64(common.BytesToHash(addr3.Bytes()))
	if size(t, aset) != uint64(1) || index == 1 || CurrentOwner != common.BytesToHash(addr3.Bytes()) || !correctOwner {
		Fail(t, "Logs and current state did not match")
	}
	// Run RectifyMapping to fix the issue
	checkIfRectifyMappingWorks(t, aset, []common.Address{addr3}, true)
	Require(t, aset.Clear())

	// Test Goerli history
	addr1 = common.HexToAddress("0x186B56023d42B2B4E7616589a5C62EEf5FCa21DD")
	addr2 = common.HexToAddress("0xc8efdb677afeb775ce1617dd976b56b3a6e95bba")
	addr3 = common.HexToAddress("0xc3f86bb81e32295d29c288ffb4828936538cf326")
	addr4 = common.HexToAddress("0x67acb531a05160a81dcd03079347f264c4fa2da3")
	// Follow logs
	Require(t, aset.Add(addr1))
	Require(t, aset.Add(addr2))
	Require(t, aset.Add(addr3))
	Require(t, aset.Remove(addr1, version))
	Require(t, aset.Add(addr4))
	Require(t, aset.Remove(addr3, version))
	Require(t, aset.Remove(addr2, version))
	// Check if history's correct
	CurrentOwner, _ = aset.backingStorage.GetByUint64(uint64(1))
	isOwner, _ = aset.IsMember(addr3)
	correctOwner, _ = aset.IsMember(addr4)
	if size(t, aset) != uint64(1) || CurrentOwner != common.BytesToHash(addr3.Bytes()) || isOwner || !correctOwner {
		Fail(t, "Logs and current state did not match")
	}
	// Run RectifyMapping to fix the issue
	checkIfRectifyMappingWorks(t, aset, []common.Address{addr4}, true)
}

func TestRectifyMapping(t *testing.T) {
	db := storage.NewMemoryBackedStateDB()
	sto := storage.NewGeth(db, burn.NewSystemBurner(nil, false))
	Require(t, Initialize(sto))
	aset := OpenAddressSet(sto)

	addr1 := testhelpers.RandomAddress()
	addr2 := testhelpers.RandomAddress()
	addr3 := testhelpers.RandomAddress()
	possibleAddresses := []common.Address{addr1, addr2, addr3}

	Require(t, aset.Add(addr1))
	Require(t, aset.Add(addr2))
	Require(t, aset.Add(addr3))

	// Non owner's should not be able to call RectifyMapping
	err := aset.RectifyMapping(testhelpers.RandomAddress())
	if err == nil {
		Fail(t, "RectifyMapping was successfully called by non owner")
	}

	// Corrupt the list and verify if RectifyMapping fixes it
	addrHash := common.BytesToHash(addr2.Bytes())
	Require(t, aset.backingStorage.SetByUint64(uint64(1), addrHash))
	checkIfRectifyMappingWorks(t, aset, possibleAddresses, true)

	// Corrupt the map and verify if RectifyMapping fixes it
	addrHash = common.BytesToHash(addr2.Bytes())
	Require(t, aset.byAddress.Set(addrHash, util.UintToHash(uint64(6))))
	checkIfRectifyMappingWorks(t, aset, possibleAddresses, true)

	// Add a new owner to the map and verify if RectifyMapping syncs list with the map
	// to check for the case where list has fewer owners than expected
	addr4 := testhelpers.RandomAddress()
	addrHash = common.BytesToHash(addr4.Bytes())
	Require(t, aset.byAddress.Set(addrHash, util.UintToHash(uint64(1))))
	checkIfRectifyMappingWorks(t, aset, possibleAddresses, true)

	// RectifyMapping should not do anything if the mapping is correct
	// Check to verify functionality post fix
	err = aset.RectifyMapping(addr1)
	if err == nil {
		Fail(t, "RectifyMapping called by a correctly mapped owner")
	}

}

func checkIfRectifyMappingWorks(t *testing.T, aset *AddressSet, owners []common.Address, clearList bool) {
	t.Helper()
	if clearList {
		Require(t, aset.ClearList())
	}
	for index, owner := range owners {
		Require(t, aset.RectifyMapping(owner))

		addrAsHash := common.BytesToHash(owner.Bytes())
		slot, err := aset.byAddress.GetUint64(addrAsHash)
		Require(t, err)
		atSlot, err := aset.backingStorage.GetByUint64(slot)
		Require(t, err)
		if slot == 0 || atSlot != addrAsHash {
			Fail(t, "RectifyMapping did not fix the mismatch")
		}

		if clearList && int(size(t, aset)) != index+1 {
			Fail(t, "RectifyMapping did not fix the mismatch")
		}
	}
	allMembers, err := aset.AllMembers(size(t, aset))
	Require(t, err)
	less := func(a, b common.Address) bool { return a.String() < b.String() }
	if cmp.Diff(owners, allMembers, cmpopts.SortSlices(less)) != "" {
		Fail(t, "RectifyMapping did not fix the mismatch")
	}
}

func checkAllMembers(t *testing.T, aset *AddressSet, possibleAddresses []common.Address) {
	allMembers, err := aset.AllMembers(1024)
	Require(t, err)

	allMembersSet := make(map[common.Address]struct{})
	for _, addr := range allMembers {
		allMembersSet[addr] = struct{}{}
	}

	if len(allMembers) != len(allMembersSet) {
		Fail(t, "allMembers contains duplicates:", allMembers)
	}

	possibleAddressSet := make(map[common.Address]struct{})
	for _, addr := range possibleAddresses {
		possibleAddressSet[addr] = struct{}{}
	}
	for _, addr := range allMembers {
		_, isPossible := possibleAddressSet[addr]
		if !isPossible {
			Fail(t, "allMembers contains impossible address", addr)
		}
	}

	for _, possible := range possibleAddresses {
		isMember, err := aset.IsMember(possible)
		Require(t, err)
		_, inSet := allMembersSet[possible]
		if isMember != inSet {
			Fail(t, "IsMember", isMember, "does not match whether it's in the allMembers list", inSet)
		}
	}
}

func isMember(t *testing.T, aset *AddressSet, address common.Address) bool {
	t.Helper()
	present, err := aset.IsMember(address)
	Require(t, err)
	return present
}

func size(t *testing.T, aset *AddressSet) uint64 {
	t.Helper()
	size, err := aset.Size()
	Require(t, err)
	return size
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/addressTable/addressTable.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package addressTable

// TODO lowercase this package name

import (
	"bytes"
	"errors"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
)

type AddressTable struct {
	backingStorage *storage.Storage
	byAddress      *storage.Storage // 0 means item isn't in the table; n > 0 means it's in the table at slot n-1
	numItems       storage.StorageBackedUint64
}

func Initialize(sto *storage.Storage) {
	// No initialization needed.
}

func Open(sto *storage.Storage) *AddressTable {
	numItems := sto.OpenStorageBackedUint64(0)
	return &AddressTable{sto.WithoutCache(), sto.OpenSubStorage([]byte{}), numItems}
}

func (atab *AddressTable) Register(addr common.Address) (uint64, error) {
	addrAsHash := common.BytesToHash(addr.Bytes())
	rev, err := atab.byAddress.Get(addrAsHash)
	if err != nil {
		return 0, err
	}

	if rev != (common.Hash{}) {
		return rev.Big().Uint64() - 1, nil
	}
	// Addr isn't in the table, so add it.
	newNumItems, err := atab.numItems.Increment()
	if err != nil {
		return 0, err
	}
	if err := atab.backingStorage.SetByUint64(newNumItems, addrAsHash); err != nil {
		return 0, err
	}
	if err := atab.byAddress.Set(addrAsHash, util.UintToHash(newNumItems)); err != nil {
		return 0, err
	}
	return newNumItems - 1, nil
}

func (atab *AddressTable) Lookup(addr common.Address) (uint64, bool, error) {
	addrAsHash := common.BytesToHash(addr.Bytes())
	res, err := atab.byAddress.GetUint64(addrAsHash)
	if res == 0 || err != nil {
		return 0, false, err
	} else {
		return res - 1, true, nil
	}
}

func (atab *AddressTable) AddressExists(addr common.Address) (bool, error) {
	_, ret, err := atab.Lookup(addr)
	return ret, err
}

func (atab *AddressTable) Size() (uint64, error) {
	return atab.numItems.Get()
}

func (atab *AddressTable) LookupIndex(index uint64) (common.Address, bool, error) {
	items, err := atab.numItems.Get()
	if index >= items || err != nil {
		return common.Address{}, false, err
	}
	value, err := atab.backingStorage.GetByUint64(index + 1)
	return common.BytesToAddress(value.Bytes()), true, err
}

func (atab *AddressTable) Compress(addr common.Address) ([]byte, error) {
	index, exists, err := atab.Lookup(addr)
	if exists || err != nil {
		return rlp.AppendUint64([]byte{}, index), err
	} else {
		buf, err := rlp.EncodeToBytes(addr.Bytes())
		if err != nil {
			panic(err)
		}
		return buf, nil
	}
}

func (atab *AddressTable) Decompress(buf []byte) (common.Address, uint64, error) {
	rd := bytes.NewReader(buf)
	decoder := rlp.NewStream(rd, 21)
	input, err := decoder.Bytes()
	if err != nil {
		return common.Address{}, 0, err
	}
	if len(input) == 20 {
		numBytesRead := uint64(rd.Size() - int64(rd.Len()))
		return common.BytesToAddress(input), numBytesRead, nil
	} else {
		rd = bytes.NewReader(buf)
		index, err := rlp.NewStream(rd, 9).Uint64()
		if err != nil {
			return common.Address{}, 0, err
		}
		addr, exists, err := atab.LookupIndex(index)
		if err != nil {
			return common.Address{}, 0, err
		}
		if !exists {
			return common.Address{}, 0, errors.New("invalid index in compressed address")
		}
		numBytesRead := uint64(rd.Size() - int64(rd.Len()))
		return addr, numBytesRead, nil
	}
}

'''
'''--- arbos/addressTable/addressTable_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package addressTable

import (
	"bytes"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestAddressTableInit(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	Initialize(sto)
	atab := Open(sto)
	if size(t, atab) != 0 {
		Fail(t)
	}

	_, found, err := atab.Lookup(common.Address{})
	Require(t, err)
	if found {
		Fail(t)
	}
	_, found, err = atab.LookupIndex(0)
	Require(t, err)
	if found {
		Fail(t)
	}
}

func TestAddressTable1(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	Initialize(sto)
	atab := Open(sto)
	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])
	_, err := atab.Register(addr)
	Require(t, err)
	if size(t, atab) != 1 {
		Fail(t)
	}

	atab = Open(sto)
	if size(t, atab) != 1 {
		Fail(t)
	}
	idx, found, err := atab.Lookup(addr)
	Require(t, err)
	if !found {
		Fail(t)
	}
	if idx != 0 {
		Fail(t)
	}

	_, found, err = atab.Lookup(common.Address{})
	Require(t, err)
	if found {
		Fail(t)
	}

	addr2, found, err := atab.LookupIndex(0)
	Require(t, err)
	if !found {
		Fail(t)
	}
	if addr2 != addr {
		Fail(t)
	}

	_, found, err = atab.LookupIndex(1)
	Require(t, err)
	if found {
		Fail(t)
	}
}

func TestAddressTableCompressNotInTable(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	Initialize(sto)
	atab := Open(sto)
	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	res, err := atab.Compress(addr)
	Require(t, err)
	if len(res) != 21 {
		Fail(t)
	}
	if !bytes.Equal(addr.Bytes(), res[1:]) {
		Fail(t)
	}

	dec, nbytes, err := atab.Decompress(res)
	if err != nil {
		Fail(t, err)
	}
	if nbytes != 21 {
		Fail(t, nbytes)
	}
	if dec != addr {
		Fail(t)
	}
}

func TestAddressTableCompressInTable(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	Initialize(sto)
	atab := Open(sto)
	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	_, err := atab.Register(addr)
	Require(t, err)

	res, err := atab.Compress(addr)
	Require(t, err)
	if len(res) > 9 {
		Fail(t, len(res))
	}

	dec, nbytes, err := atab.Decompress(res)
	if err != nil {
		Fail(t, err)
	}
	if nbytes > 9 {
		Fail(t, nbytes)
	}
	if dec != addr {
		Fail(t)
	}
}

func size(t *testing.T, atab *AddressTable) uint64 {
	size, err := atab.Size()
	Require(t, err)
	return size
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/arbosState/arbosstate.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbosState

import (
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbos/addressSet"
	"github.com/offchainlabs/nitro/arbos/addressTable"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/blockhash"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbos/merkleAccumulator"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
)

// ArbosState contains ArbOS-related state. It is backed by ArbOS's storage in the persistent stateDB.
// Modifications to the ArbosState are written through to the underlying StateDB so that the StateDB always
// has the definitive state, stored persistently. (Note that some tests use memory-backed StateDB's that aren't
// persisted beyond the end of the test.)

type ArbosState struct {
	arbosVersion           uint64                      // version of the ArbOS storage format and semantics
	upgradeVersion         storage.StorageBackedUint64 // version we're planning to upgrade to, or 0 if not planning to upgrade
	upgradeTimestamp       storage.StorageBackedUint64 // when to do the planned upgrade
	networkFeeAccount      storage.StorageBackedAddress
	l1PricingState         *l1pricing.L1PricingState
	l2PricingState         *l2pricing.L2PricingState
	retryableState         *retryables.RetryableState
	addressTable           *addressTable.AddressTable
	chainOwners            *addressSet.AddressSet
	sendMerkle             *merkleAccumulator.MerkleAccumulator
	blockhashes            *blockhash.Blockhashes
	chainId                storage.StorageBackedBigInt
	chainConfig            storage.StorageBackedBytes
	genesisBlockNum        storage.StorageBackedUint64
	infraFeeAccount        storage.StorageBackedAddress
	brotliCompressionLevel storage.StorageBackedUint64 // brotli compression level used for pricing
	backingStorage         *storage.Storage
	Burner                 burn.Burner
}

var ErrUninitializedArbOS = errors.New("ArbOS uninitialized")
var ErrAlreadyInitialized = errors.New("ArbOS is already initialized")

func OpenArbosState(stateDB vm.StateDB, burner burn.Burner) (*ArbosState, error) {
	backingStorage := storage.NewGeth(stateDB, burner)
	arbosVersion, err := backingStorage.GetUint64ByUint64(uint64(versionOffset))
	if err != nil {
		return nil, err
	}
	if arbosVersion == 0 {
		return nil, ErrUninitializedArbOS
	}
	return &ArbosState{
		arbosVersion,
		backingStorage.OpenStorageBackedUint64(uint64(upgradeVersionOffset)),
		backingStorage.OpenStorageBackedUint64(uint64(upgradeTimestampOffset)),
		backingStorage.OpenStorageBackedAddress(uint64(networkFeeAccountOffset)),
		l1pricing.OpenL1PricingState(backingStorage.OpenCachedSubStorage(l1PricingSubspace)),
		l2pricing.OpenL2PricingState(backingStorage.OpenCachedSubStorage(l2PricingSubspace)),
		retryables.OpenRetryableState(backingStorage.OpenCachedSubStorage(retryablesSubspace), stateDB),
		addressTable.Open(backingStorage.OpenCachedSubStorage(addressTableSubspace)),
		addressSet.OpenAddressSet(backingStorage.OpenCachedSubStorage(chainOwnerSubspace)),
		merkleAccumulator.OpenMerkleAccumulator(backingStorage.OpenCachedSubStorage(sendMerkleSubspace)),
		blockhash.OpenBlockhashes(backingStorage.OpenCachedSubStorage(blockhashesSubspace)),
		backingStorage.OpenStorageBackedBigInt(uint64(chainIdOffset)),
		backingStorage.OpenStorageBackedBytes(chainConfigSubspace),
		backingStorage.OpenStorageBackedUint64(uint64(genesisBlockNumOffset)),
		backingStorage.OpenStorageBackedAddress(uint64(infraFeeAccountOffset)),
		backingStorage.OpenStorageBackedUint64(uint64(brotliCompressionLevelOffset)),
		backingStorage,
		burner,
	}, nil
}

func OpenSystemArbosState(stateDB vm.StateDB, tracingInfo *util.TracingInfo, readOnly bool) (*ArbosState, error) {
	burner := burn.NewSystemBurner(tracingInfo, readOnly)
	newState, err := OpenArbosState(stateDB, burner)
	burner.Restrict(err)
	return newState, err
}

func OpenSystemArbosStateOrPanic(stateDB vm.StateDB, tracingInfo *util.TracingInfo, readOnly bool) *ArbosState {
	newState, err := OpenSystemArbosState(stateDB, tracingInfo, readOnly)
	if err != nil {
		panic(err)
	}
	return newState
}

// NewArbosMemoryBackedArbOSState creates and initializes a memory-backed ArbOS state (for testing only)
func NewArbosMemoryBackedArbOSState() (*ArbosState, *state.StateDB) {
	raw := rawdb.NewMemoryDatabase()
	db := state.NewDatabase(raw)
	statedb, err := state.New(common.Hash{}, db, nil)
	if err != nil {
		log.Crit("failed to init empty statedb", "error", err)
	}
	burner := burn.NewSystemBurner(nil, false)
	chainConfig := params.ArbitrumDevTestChainConfig()
	newState, err := InitializeArbosState(statedb, burner, chainConfig, arbostypes.TestInitMessage)
	if err != nil {
		log.Crit("failed to open the ArbOS state", "error", err)
	}
	return newState, statedb
}

// ArbOSVersion returns the ArbOS version
func ArbOSVersion(stateDB vm.StateDB) uint64 {
	backingStorage := storage.NewGeth(stateDB, burn.NewSystemBurner(nil, false))
	arbosVersion, err := backingStorage.GetUint64ByUint64(uint64(versionOffset))
	if err != nil {
		log.Crit("failed to get the ArbOS version", "error", err)
	}
	return arbosVersion
}

type Offset uint64

const (
	versionOffset Offset = iota
	upgradeVersionOffset
	upgradeTimestampOffset
	networkFeeAccountOffset
	chainIdOffset
	genesisBlockNumOffset
	infraFeeAccountOffset
	brotliCompressionLevelOffset
)

type SubspaceID []byte

var (
	l1PricingSubspace    SubspaceID = []byte{0}
	l2PricingSubspace    SubspaceID = []byte{1}
	retryablesSubspace   SubspaceID = []byte{2}
	addressTableSubspace SubspaceID = []byte{3}
	chainOwnerSubspace   SubspaceID = []byte{4}
	sendMerkleSubspace   SubspaceID = []byte{5}
	blockhashesSubspace  SubspaceID = []byte{6}
	chainConfigSubspace  SubspaceID = []byte{7}
)

// Returns a list of precompiles that only appear in Arbitrum chains (i.e. ArbOS precompiles) at the genesis block
func getArbitrumOnlyGenesisPrecompiles(chainConfig *params.ChainConfig) []common.Address {
	rules := chainConfig.Rules(big.NewInt(0), false, 0, chainConfig.ArbitrumChainParams.InitialArbOSVersion)
	arbPrecompiles := vm.ActivePrecompiles(rules)
	rules.IsArbitrum = false
	ethPrecompiles := vm.ActivePrecompiles(rules)

	ethPrecompilesSet := make(map[common.Address]bool)
	for _, addr := range ethPrecompiles {
		ethPrecompilesSet[addr] = true
	}

	var arbOnlyPrecompiles []common.Address
	for _, addr := range arbPrecompiles {
		if !ethPrecompilesSet[addr] {
			arbOnlyPrecompiles = append(arbOnlyPrecompiles, addr)
		}
	}
	return arbOnlyPrecompiles
}

// During early development we sometimes change the storage format of version 1, for convenience. But as soon as we
// start running long-lived chains, every change to the storage format will require defining a new version and
// providing upgrade code.

func InitializeArbosState(stateDB vm.StateDB, burner burn.Burner, chainConfig *params.ChainConfig, initMessage *arbostypes.ParsedInitMessage) (*ArbosState, error) {
	sto := storage.NewGeth(stateDB, burner)
	arbosVersion, err := sto.GetUint64ByUint64(uint64(versionOffset))
	if err != nil {
		return nil, err
	}
	if arbosVersion != 0 {
		return nil, ErrAlreadyInitialized
	}

	desiredArbosVersion := chainConfig.ArbitrumChainParams.InitialArbOSVersion
	if desiredArbosVersion == 0 {
		return nil, errors.New("cannot initialize to ArbOS version 0")
	}

	// Solidity requires call targets have code, but precompiles don't.
	// To work around this, we give precompiles fake code.
	for _, genesisPrecompile := range getArbitrumOnlyGenesisPrecompiles(chainConfig) {
		stateDB.SetCode(genesisPrecompile, []byte{byte(vm.INVALID)})
	}

	// may be the zero address
	initialChainOwner := chainConfig.ArbitrumChainParams.InitialChainOwner

	_ = sto.SetUint64ByUint64(uint64(versionOffset), 1) // initialize to version 1; upgrade at end of this func if needed
	_ = sto.SetUint64ByUint64(uint64(upgradeVersionOffset), 0)
	_ = sto.SetUint64ByUint64(uint64(upgradeTimestampOffset), 0)
	if desiredArbosVersion >= 2 {
		_ = sto.SetByUint64(uint64(networkFeeAccountOffset), util.AddressToHash(initialChainOwner))
	} else {
		_ = sto.SetByUint64(uint64(networkFeeAccountOffset), common.Hash{}) // the 0 address until an owner sets it
	}
	_ = sto.SetByUint64(uint64(chainIdOffset), common.BigToHash(chainConfig.ChainID))
	chainConfigStorage := sto.OpenStorageBackedBytes(chainConfigSubspace)
	_ = chainConfigStorage.Set(initMessage.SerializedChainConfig)
	_ = sto.SetUint64ByUint64(uint64(genesisBlockNumOffset), chainConfig.ArbitrumChainParams.GenesisBlockNum)
	_ = sto.SetUint64ByUint64(uint64(brotliCompressionLevelOffset), 0) // default brotliCompressionLevel for fast compression is 0

	initialRewardsRecipient := l1pricing.BatchPosterAddress
	if desiredArbosVersion >= 2 {
		initialRewardsRecipient = initialChainOwner
	}
	_ = l1pricing.InitializeL1PricingState(sto.OpenCachedSubStorage(l1PricingSubspace), initialRewardsRecipient, initMessage.InitialL1BaseFee)
	_ = l2pricing.InitializeL2PricingState(sto.OpenCachedSubStorage(l2PricingSubspace))
	_ = retryables.InitializeRetryableState(sto.OpenCachedSubStorage(retryablesSubspace))
	addressTable.Initialize(sto.OpenCachedSubStorage(addressTableSubspace))
	merkleAccumulator.InitializeMerkleAccumulator(sto.OpenCachedSubStorage(sendMerkleSubspace))
	blockhash.InitializeBlockhashes(sto.OpenCachedSubStorage(blockhashesSubspace))

	ownersStorage := sto.OpenCachedSubStorage(chainOwnerSubspace)
	_ = addressSet.Initialize(ownersStorage)
	_ = addressSet.OpenAddressSet(ownersStorage).Add(initialChainOwner)

	aState, err := OpenArbosState(stateDB, burner)
	if err != nil {
		return nil, err
	}
	if desiredArbosVersion > 1 {
		err = aState.UpgradeArbosVersion(desiredArbosVersion, true, stateDB, chainConfig)
		if err != nil {
			return nil, err
		}
	}
	return aState, nil
}

func (state *ArbosState) UpgradeArbosVersionIfNecessary(
	currentTimestamp uint64, stateDB vm.StateDB, chainConfig *params.ChainConfig,
) error {
	upgradeTo, err := state.upgradeVersion.Get()
	state.Restrict(err)
	flagday, _ := state.upgradeTimestamp.Get()
	if state.arbosVersion < upgradeTo && currentTimestamp >= flagday {
		return state.UpgradeArbosVersion(upgradeTo, false, stateDB, chainConfig)
	}
	return nil
}

var ErrFatalNodeOutOfDate = errors.New("please upgrade to the latest version of the node software")

func (state *ArbosState) UpgradeArbosVersion(
	upgradeTo uint64, firstTime bool, stateDB vm.StateDB, chainConfig *params.ChainConfig,
) error {
	for state.arbosVersion < upgradeTo {
		ensure := func(err error) {
			if err != nil {
				message := fmt.Sprintf(
					"Failed to upgrade ArbOS version %v to version %v: %v",
					state.arbosVersion, state.arbosVersion+1, err,
				)
				panic(message)
			}
		}

		switch state.arbosVersion {
		case 1:
			ensure(state.l1PricingState.SetLastSurplus(common.Big0, 1))
		case 2:
			ensure(state.l1PricingState.SetPerBatchGasCost(0))
			ensure(state.l1PricingState.SetAmortizedCostCapBips(math.MaxUint64))
		case 3:
			// no state changes needed
		case 4:
			// no state changes needed
		case 5:
			// no state changes needed
		case 6:
			// no state changes needed
		case 7:
			// no state changes needed
		case 8:
			// no state changes needed
		case 9:
			ensure(state.l1PricingState.SetL1FeesAvailable(stateDB.GetBalance(
				l1pricing.L1PricerFundsPoolAddress,
			)))
		case 10:
			if !chainConfig.DebugMode() {
				// This upgrade isn't finalized so we only want to support it for testing
				return fmt.Errorf(
					"the chain is upgrading to unsupported ArbOS version %v, %w",
					state.arbosVersion+1,
					ErrFatalNodeOutOfDate,
				)
			}
			// Update the PerBatchGasCost to a more accurate value compared to the old v6 default.
			ensure(state.l1PricingState.SetPerBatchGasCost(l1pricing.InitialPerBatchGasCostV12))

			// We had mistakenly initialized AmortizedCostCapBips to math.MaxUint64 in older versions,
			// but the correct value to disable the amortization cap is 0.
			oldAmortizationCap, err := state.l1PricingState.AmortizedCostCapBips()
			ensure(err)
			if oldAmortizationCap == math.MaxUint64 {
				ensure(state.l1PricingState.SetAmortizedCostCapBips(0))
			}

			// Clear chainOwners list to allow rectification of the mapping.
			if !firstTime {
				ensure(state.chainOwners.ClearList())
			}
		case 11:
			if !chainConfig.DebugMode() {
				// This upgrade isn't finalized so we only want to support it for testing
				return fmt.Errorf(
					"the chain is upgrading to unsupported ArbOS version %v, %w",
					state.arbosVersion+1,
					ErrFatalNodeOutOfDate,
				)
			}
			// Update Brotli compression level for fast compression from 0 to 1
			ensure(state.SetBrotliCompressionLevel(1))
		default:
			return fmt.Errorf(
				"the chain is upgrading to unsupported ArbOS version %v, %w",
				state.arbosVersion+1,
				ErrFatalNodeOutOfDate,
			)
		}
		state.arbosVersion++
	}

	if firstTime && upgradeTo >= 6 {
		if upgradeTo < 11 {
			state.Restrict(state.l1PricingState.SetPerBatchGasCost(l1pricing.InitialPerBatchGasCostV6))
		}
		state.Restrict(state.l1PricingState.SetEquilibrationUnits(l1pricing.InitialEquilibrationUnitsV6))
		state.Restrict(state.l2PricingState.SetSpeedLimitPerSecond(l2pricing.InitialSpeedLimitPerSecondV6))
		state.Restrict(state.l2PricingState.SetMaxPerBlockGasLimit(l2pricing.InitialPerBlockGasLimitV6))
	}

	state.Restrict(state.backingStorage.SetUint64ByUint64(uint64(versionOffset), state.arbosVersion))

	return nil
}

func (state *ArbosState) ScheduleArbOSUpgrade(newVersion uint64, timestamp uint64) error {
	err := state.upgradeVersion.Set(newVersion)
	if err != nil {
		return err
	}
	return state.upgradeTimestamp.Set(timestamp)
}

func (state *ArbosState) GetScheduledUpgrade() (uint64, uint64, error) {
	version, err := state.upgradeVersion.Get()
	if err != nil {
		return 0, 0, err
	}
	timestamp, err := state.upgradeTimestamp.Get()
	if err != nil {
		return 0, 0, err
	}
	return version, timestamp, nil
}

func (state *ArbosState) BackingStorage() *storage.Storage {
	return state.backingStorage
}

func (state *ArbosState) Restrict(err error) {
	state.Burner.Restrict(err)
}

func (state *ArbosState) ArbOSVersion() uint64 {
	return state.arbosVersion
}

func (state *ArbosState) SetFormatVersion(val uint64) {
	state.arbosVersion = val
	state.Restrict(state.backingStorage.SetUint64ByUint64(uint64(versionOffset), val))
}

func (state *ArbosState) BrotliCompressionLevel() (uint64, error) {
	return state.brotliCompressionLevel.Get()
}

func (state *ArbosState) SetBrotliCompressionLevel(val uint64) error {
	if val <= arbcompress.LEVEL_WELL {
		return state.brotliCompressionLevel.Set(val)
	}
	return errors.New("invalid brotli compression level")
}

func (state *ArbosState) RetryableState() *retryables.RetryableState {
	return state.retryableState
}

func (state *ArbosState) L1PricingState() *l1pricing.L1PricingState {
	return state.l1PricingState
}

func (state *ArbosState) L2PricingState() *l2pricing.L2PricingState {
	return state.l2PricingState
}

func (state *ArbosState) AddressTable() *addressTable.AddressTable {
	return state.addressTable
}

func (state *ArbosState) ChainOwners() *addressSet.AddressSet {
	return state.chainOwners
}

func (state *ArbosState) SendMerkleAccumulator() *merkleAccumulator.MerkleAccumulator {
	if state.sendMerkle == nil {
		state.sendMerkle = merkleAccumulator.OpenMerkleAccumulator(state.backingStorage.OpenCachedSubStorage(sendMerkleSubspace))
	}
	return state.sendMerkle
}

func (state *ArbosState) Blockhashes() *blockhash.Blockhashes {
	return state.blockhashes
}

func (state *ArbosState) NetworkFeeAccount() (common.Address, error) {
	return state.networkFeeAccount.Get()
}

func (state *ArbosState) SetNetworkFeeAccount(account common.Address) error {
	return state.networkFeeAccount.Set(account)
}

func (state *ArbosState) InfraFeeAccount() (common.Address, error) {
	return state.infraFeeAccount.Get()
}

func (state *ArbosState) SetInfraFeeAccount(account common.Address) error {
	return state.infraFeeAccount.Set(account)
}

func (state *ArbosState) Keccak(data ...[]byte) ([]byte, error) {
	return state.backingStorage.Keccak(data...)
}

func (state *ArbosState) KeccakHash(data ...[]byte) (common.Hash, error) {
	return state.backingStorage.KeccakHash(data...)
}

func (state *ArbosState) ChainId() (*big.Int, error) {
	return state.chainId.Get()
}

func (state *ArbosState) ChainConfig() ([]byte, error) {
	return state.chainConfig.Get()
}

func (state *ArbosState) SetChainConfig(serializedChainConfig []byte) error {
	return state.chainConfig.Set(serializedChainConfig)
}

func (state *ArbosState) GenesisBlockNum() (uint64, error) {
	return state.genesisBlockNum.Get()
}

'''
'''--- arbos/arbosState/arbosstate_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbosState

import (
	"bytes"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/colors"
)

func TestStorageOpenFromEmpty(t *testing.T) {
	NewArbosMemoryBackedArbOSState()
}

func TestMemoryBackingEvmStorage(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	value, err := sto.Get(common.Hash{})
	Require(t, err)
	if value != (common.Hash{}) {
		Fail(t)
	}

	loc1 := util.UintToHash(99)
	val1 := util.UintToHash(1351908)

	Require(t, sto.Set(loc1, val1))
	value, err = sto.Get(common.Hash{})
	Require(t, err)
	if value != (common.Hash{}) {
		Fail(t)
	}

	value, err = sto.Get(loc1)
	Require(t, err)
	if value != val1 {
		Fail(t)
	}
}

func TestStorageBackedInt64(t *testing.T) {
	state, _ := NewArbosMemoryBackedArbOSState()
	storage := state.backingStorage
	offset := uint64(7895463)

	valuesToTry := []int64{0, 7, -7, 56487423567, -7586427647}

	for _, val := range valuesToTry {
		sbi := storage.OpenStorageBackedInt64(offset)
		Require(t, sbi.Set(val))
		sbi = storage.OpenStorageBackedInt64(offset)
		res, err := sbi.Get()
		Require(t, err)
		if val != res {
			Fail(t, val, res)
		}
	}
}

func TestStorageSlots(t *testing.T) {
	state, _ := NewArbosMemoryBackedArbOSState()
	sto := state.BackingStorage().OpenCachedSubStorage([]byte{})

	println("nil address", colors.Blue, storage.NilAddressRepresentation.String(), colors.Clear)

	a := sto.GetStorageSlot(util.IntToHash(0))
	b := sto.GetStorageSlot(util.IntToHash(1))
	c := sto.GetStorageSlot(util.IntToHash(255))
	d := sto.GetStorageSlot(util.IntToHash(256)) // should be in its own page

	if !bytes.Equal(a[:31], b[:31]) {
		Fail(t, "upper bytes are unequal", a.String(), b.String())
	}
	if !bytes.Equal(a[:31], c[:31]) {
		Fail(t, "upper bytes are unequal", a.String(), c.String())
	}
	if bytes.Equal(a[:31], d[:31]) {
		Fail(t, "upper bytes should be different", a.String(), d.String())
	}

	if a[31] != 0 || b[31] != 1 || c[31] != 255 || d[31] != 0 {
		println("offset 0\t", colors.Red, a.String(), colors.Clear)
		println("offset 1\t", colors.Red, b.String(), colors.Clear)
		println("offset 255\t", colors.Red, c.String(), colors.Clear)
		println("offset 256\t", colors.Red, d.String(), colors.Clear)
		Fail(t, "page offset mismatch")
	}
}

'''
'''--- arbos/arbosState/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbosState

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/arbosState/initialization_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbosState

import (
	"bytes"
	"encoding/json"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/statetransfer"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestJsonMarshalUnmarshal(t *testing.T) {
	prand := testhelpers.NewPseudoRandomDataSource(t, 1)
	tryMarshalUnmarshal(
		&statetransfer.ArbosInitializationInfo{
			AddressTableContents: []common.Address{prand.GetAddress()},
			RetryableData:        []statetransfer.InitializationDataForRetryable{pseudorandomRetryableInitForTesting(prand)},
			Accounts:             []statetransfer.AccountInitializationInfo{pseudorandomAccountInitInfoForTesting(prand)},
		},
		t,
	)
}

func tryMarshalUnmarshal(input *statetransfer.ArbosInitializationInfo, t *testing.T) {
	marshaled, err := json.Marshal(input)
	if err != nil {
		t.Fatal(err)
	}
	if !json.Valid(marshaled) {
		t.Fatal()
	}
	if len(marshaled) == 0 {
		t.Fatal()
	}

	output := statetransfer.ArbosInitializationInfo{}
	err = json.Unmarshal(marshaled, &output)
	if err != nil {
		t.Fatal(err)
	}
	if len(output.AddressTableContents) != 1 {
		t.Fatal(output)
	}

	var initData statetransfer.ArbosInitializationInfo
	err = json.Unmarshal(marshaled, &initData)
	Require(t, err)

	raw := rawdb.NewMemoryDatabase()

	initReader := statetransfer.NewMemoryInitDataReader(&initData)
	chainConfig := params.ArbitrumDevTestChainConfig()
	stateroot, err := InitializeArbosInDatabase(raw, initReader, chainConfig, arbostypes.TestInitMessage, 0, 0)
	Require(t, err)

	stateDb, err := state.New(stateroot, state.NewDatabase(raw), nil)
	Require(t, err)

	arbState, err := OpenArbosState(stateDb, &burn.SystemBurner{})
	Require(t, err)
	checkAddressTable(arbState, input.AddressTableContents, t)
	checkRetryables(arbState, input.RetryableData, t)
	checkAccounts(stateDb, arbState, input.Accounts, t)
}

func pseudorandomRetryableInitForTesting(prand *testhelpers.PseudoRandomDataSource) statetransfer.InitializationDataForRetryable {
	return statetransfer.InitializationDataForRetryable{
		Id:          prand.GetHash(),
		Timeout:     prand.GetUint64(),
		From:        prand.GetAddress(),
		To:          prand.GetAddress(),
		Callvalue:   new(big.Int).SetBytes(prand.GetHash().Bytes()[1:]),
		Beneficiary: prand.GetAddress(),
		Calldata:    prand.GetData(256),
	}
}

func pseudorandomAccountInitInfoForTesting(prand *testhelpers.PseudoRandomDataSource) statetransfer.AccountInitializationInfo {
	aggToPay := prand.GetAddress()
	return statetransfer.AccountInitializationInfo{
		Addr:       prand.GetAddress(),
		Nonce:      prand.GetUint64(),
		EthBalance: prand.GetHash().Big(),
		ContractInfo: &statetransfer.AccountInitContractInfo{
			Code:            prand.GetData(256),
			ContractStorage: pseudorandomHashHashMapForTesting(prand, 16),
		},
		AggregatorInfo: &statetransfer.AccountInitAggregatorInfo{
			FeeCollector: prand.GetAddress(),
			BaseFeeL1Gas: prand.GetHash().Big(),
		},
		AggregatorToPay: &aggToPay,
	}
}

func pseudorandomHashHashMapForTesting(prand *testhelpers.PseudoRandomDataSource, maxItems uint64) map[common.Hash]common.Hash {
	size := int(prand.GetUint64() % maxItems)
	ret := make(map[common.Hash]common.Hash)
	for i := 0; i < size; i++ {
		ret[prand.GetHash()] = prand.GetHash()
	}
	return ret
}

func checkAddressTable(arbState *ArbosState, addrTable []common.Address, t *testing.T) {
	atab := arbState.AddressTable()
	atabSize, err := atab.Size()
	Require(t, err)
	if atabSize != uint64(len(addrTable)) {
		Fail(t)
	}
	for i, addr := range addrTable {
		res, exists, err := atab.LookupIndex(uint64(i))
		Require(t, err)
		if !exists {
			Fail(t)
		}
		if res != addr {
			Fail(t)
		}
	}
}

func checkRetryables(arbState *ArbosState, expected []statetransfer.InitializationDataForRetryable, t *testing.T) {
	ret := arbState.RetryableState()
	for _, exp := range expected {
		found, err := ret.OpenRetryable(exp.Id, 0)
		Require(t, err)
		if found == nil {
			Fail(t)
		}
		// TODO: detailed comparison
	}
}

func checkAccounts(db *state.StateDB, arbState *ArbosState, accts []statetransfer.AccountInitializationInfo, t *testing.T) {
	l1p := arbState.L1PricingState()
	posterTable := l1p.BatchPosterTable()
	for _, acct := range accts {
		addr := acct.Addr
		if db.GetNonce(addr) != acct.Nonce {
			t.Fatal()
		}
		if db.GetBalance(addr).Cmp(acct.EthBalance) != 0 {
			t.Fatal()
		}
		if acct.ContractInfo != nil {
			if !bytes.Equal(acct.ContractInfo.Code, db.GetCode(addr)) {
				t.Fatal()
			}
			err := db.ForEachStorage(addr, func(key common.Hash, value common.Hash) bool {
				if key == (common.Hash{}) {
					// Unfortunately, geth doesn't seem capable of giving us storage keys any more.
					// Even with the triedb Preimages set to true, it doesn't record the necessary
					// hashed storage key -> raw storage key mapping. This means that geth will always
					// give us an empty storage key when iterating, which we can't validate.
					return true
				}
				val2, exists := acct.ContractInfo.ContractStorage[key]
				if !exists {
					t.Fatal("address", addr, "key", key, "found in storage as", value, "but not in initialization data")
				}
				if value != val2 {
					t.Fatal("address", addr, "key", key, "value", val2, "isn't what was specified in initialization data", value)
				}
				return true
			})
			if err != nil {
				t.Fatal(err)
			}
		}
		isPoster, err := posterTable.ContainsPoster(addr)
		Require(t, err)
		if acct.AggregatorInfo != nil && isPoster {
			posterInfo, err := posterTable.OpenPoster(addr, false)
			Require(t, err)
			fc, err := posterInfo.PayTo()
			Require(t, err)
			if fc != acct.AggregatorInfo.FeeCollector {
				t.Fatal()
			}
		}
	}
	_ = l1p
}

'''
'''--- arbos/arbosState/initialize.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbosState

import (
	"errors"
	"math/big"
	"sort"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/trie"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/statetransfer"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func MakeGenesisBlock(parentHash common.Hash, blockNumber uint64, timestamp uint64, stateRoot common.Hash, chainConfig *params.ChainConfig) *types.Block {
	head := &types.Header{
		Number:     new(big.Int).SetUint64(blockNumber),
		Nonce:      types.EncodeNonce(1), // the genesis block reads the init message
		Time:       timestamp,
		ParentHash: parentHash,
		Extra:      nil,
		GasLimit:   l2pricing.GethBlockGasLimit,
		GasUsed:    0,
		BaseFee:    big.NewInt(l2pricing.InitialBaseFeeWei),
		Difficulty: big.NewInt(1),
		MixDigest:  common.Hash{},
		Coinbase:   common.Address{},
		Root:       stateRoot,
	}

	genesisHeaderInfo := types.HeaderInfo{
		SendRoot:           common.Hash{},
		SendCount:          0,
		L1BlockNumber:      0,
		ArbOSFormatVersion: chainConfig.ArbitrumChainParams.InitialArbOSVersion,
	}
	genesisHeaderInfo.UpdateHeaderWithInfo(head)

	return types.NewBlock(head, nil, nil, nil, trie.NewStackTrie(nil))
}

func InitializeArbosInDatabase(db ethdb.Database, initData statetransfer.InitDataReader, chainConfig *params.ChainConfig, initMessage *arbostypes.ParsedInitMessage, timestamp uint64, accountsPerSync uint) (common.Hash, error) {
	stateDatabase := state.NewDatabase(db)
	statedb, err := state.New(common.Hash{}, stateDatabase, nil)
	if err != nil {
		log.Crit("failed to init empty statedb", "error", err)
	}

	commit := func() (common.Hash, error) {
		root, err := statedb.Commit(chainConfig.ArbitrumChainParams.GenesisBlockNum, true)
		if err != nil {
			return common.Hash{}, err
		}
		err = stateDatabase.TrieDB().Commit(root, true)
		if err != nil {
			return common.Hash{}, err
		}
		statedb, err = state.New(root, stateDatabase, nil)
		if err != nil {
			return common.Hash{}, err
		}
		return root, nil
	}

	burner := burn.NewSystemBurner(nil, false)
	arbosState, err := InitializeArbosState(statedb, burner, chainConfig, initMessage)
	if err != nil {
		log.Crit("failed to open the ArbOS state", "error", err)
	}

	addrTable := arbosState.AddressTable()
	addrTableSize, err := addrTable.Size()
	if err != nil {
		return common.Hash{}, err
	}
	if addrTableSize != 0 {
		return common.Hash{}, errors.New("address table must be empty")
	}
	addressReader, err := initData.GetAddressTableReader()
	if err != nil {
		return common.Hash{}, err
	}
	for i := 0; addressReader.More(); i++ {
		addr, err := addressReader.GetNext()
		if err != nil {
			return common.Hash{}, err
		}
		slot, err := addrTable.Register(*addr)
		if err != nil {
			return common.Hash{}, err
		}
		if uint64(i) != slot {
			return common.Hash{}, errors.New("address table slot mismatch")
		}
	}
	if err := addressReader.Close(); err != nil {
		return common.Hash{}, err
	}

	log.Info("addresss table import complete")

	retryableReader, err := initData.GetRetryableDataReader()
	if err != nil {
		return common.Hash{}, err
	}
	err = initializeRetryables(statedb, arbosState.RetryableState(), retryableReader, timestamp)
	if err != nil {
		return common.Hash{}, err
	}

	log.Info("retryables import complete")

	if accountsPerSync > 0 {
		_, err := commit()
		if err != nil {
			return common.Hash{}, err
		}
	}

	accountDataReader, err := initData.GetAccountDataReader()
	if err != nil {
		return common.Hash{}, err
	}
	accountsRead := uint(0)
	for accountDataReader.More() {
		account, err := accountDataReader.GetNext()
		if err != nil {
			return common.Hash{}, err
		}
		err = initializeArbosAccount(statedb, arbosState, *account)
		if err != nil {
			return common.Hash{}, err
		}
		statedb.SetBalance(account.Addr, account.EthBalance)
		statedb.SetNonce(account.Addr, account.Nonce)
		if account.ContractInfo != nil {
			statedb.SetCode(account.Addr, account.ContractInfo.Code)
			for k, v := range account.ContractInfo.ContractStorage {
				statedb.SetState(account.Addr, k, v)
			}
		}
		accountsRead++
		if accountsPerSync > 0 && (accountsRead%accountsPerSync == 0) {
			log.Info("imported accounts", "count", accountsRead)
			_, err := commit()
			if err != nil {
				return common.Hash{}, err
			}
		}
	}
	if err := accountDataReader.Close(); err != nil {
		return common.Hash{}, err
	}
	return commit()
}

func initializeRetryables(statedb *state.StateDB, rs *retryables.RetryableState, initData statetransfer.RetryableDataReader, currentTimestamp uint64) error {
	var retryablesList []*statetransfer.InitializationDataForRetryable
	for initData.More() {
		r, err := initData.GetNext()
		if err != nil {
			return err
		}
		if r.Timeout <= currentTimestamp {
			statedb.AddBalance(r.Beneficiary, r.Callvalue)
			continue
		}
		retryablesList = append(retryablesList, r)
	}
	sort.Slice(retryablesList, func(i, j int) bool {
		a := retryablesList[i]
		b := retryablesList[j]
		if a.Timeout == b.Timeout {
			return arbmath.BigLessThan(a.Id.Big(), b.Id.Big())
		}
		return a.Timeout < b.Timeout
	})
	for _, r := range retryablesList {
		var to *common.Address
		if r.To != (common.Address{}) {
			addr := r.To
			to = &addr
		}
		statedb.AddBalance(retryables.RetryableEscrowAddress(r.Id), r.Callvalue)
		_, err := rs.CreateRetryable(r.Id, r.Timeout, r.From, to, r.Callvalue, r.Beneficiary, r.Calldata)
		if err != nil {
			return err
		}
	}
	return initData.Close()
}

func initializeArbosAccount(_ *state.StateDB, arbosState *ArbosState, account statetransfer.AccountInitializationInfo) error {
	l1pState := arbosState.L1PricingState()
	posterTable := l1pState.BatchPosterTable()
	if account.AggregatorInfo != nil {
		isPoster, err := posterTable.ContainsPoster(account.Addr)
		if err != nil {
			return err
		}
		if isPoster {
			// poster is already authorized, just set its fee collector
			poster, err := posterTable.OpenPoster(account.Addr, false)
			if err != nil {
				return err
			}
			err = poster.SetPayTo(account.AggregatorInfo.FeeCollector)
			if err != nil {
				return err
			}
		}
	}
	return nil
}

'''
'''--- arbos/arbostypes/incomingmessage.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbostypes

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
)

const (
	L1MessageType_L2Message             = 3
	L1MessageType_EndOfBlock            = 6
	L1MessageType_L2FundedByL1          = 7
	L1MessageType_RollupEvent           = 8
	L1MessageType_SubmitRetryable       = 9
	L1MessageType_BatchForGasEstimation = 10 // probably won't use this in practice
	L1MessageType_Initialize            = 11
	L1MessageType_EthDeposit            = 12
	L1MessageType_BatchPostingReport    = 13
	L1MessageType_Invalid               = 0xFF
)

const MaxL2MessageSize = 256 * 1024

type L1IncomingMessageHeader struct {
	Kind        uint8          `json:"kind"`
	Poster      common.Address `json:"sender"`
	BlockNumber uint64         `json:"blockNumber"`
	Timestamp   uint64         `json:"timestamp"`
	RequestId   *common.Hash   `json:"requestId" rlp:"nilList"`
	L1BaseFee   *big.Int       `json:"baseFeeL1"`
}

func (h L1IncomingMessageHeader) SeqNum() (uint64, error) {
	if h.RequestId == nil {
		return 0, errors.New("no requestId")
	}
	seqNumBig := h.RequestId.Big()
	if !seqNumBig.IsUint64() {
		return 0, errors.New("bad requestId")
	}
	return seqNumBig.Uint64(), nil
}

type L1IncomingMessage struct {
	Header *L1IncomingMessageHeader `json:"header"`
	L2msg  []byte                   `json:"l2Msg"`

	// Only used for `L1MessageType_BatchPostingReport`
	BatchGasCost *uint64 `json:"batchGasCost,omitempty" rlp:"optional"`
}

var EmptyTestIncomingMessage = L1IncomingMessage{
	Header: &L1IncomingMessageHeader{},
}

var TestIncomingMessageWithRequestId = L1IncomingMessage{
	Header: &L1IncomingMessageHeader{
		Kind:      L1MessageType_Invalid,
		RequestId: &common.Hash{},
		L1BaseFee: big.NewInt(0),
	},
}

var InvalidL1Message = &L1IncomingMessage{
	Header: &L1IncomingMessageHeader{
		Kind: L1MessageType_Invalid,
	},
	L2msg: []byte{},
}

func (msg *L1IncomingMessage) Serialize() ([]byte, error) {
	wr := &bytes.Buffer{}
	if err := wr.WriteByte(msg.Header.Kind); err != nil {
		return nil, err
	}

	if err := util.AddressTo256ToWriter(msg.Header.Poster, wr); err != nil {
		return nil, err
	}

	if err := util.Uint64ToWriter(msg.Header.BlockNumber, wr); err != nil {
		return nil, err
	}

	if err := util.Uint64ToWriter(msg.Header.Timestamp, wr); err != nil {
		return nil, err
	}

	if msg.Header.RequestId == nil {
		return nil, errors.New("cannot serialize L1IncomingMessage without RequestId")
	}
	requestId := *msg.Header.RequestId
	if err := util.HashToWriter(requestId, wr); err != nil {
		return nil, err
	}

	var l1BaseFeeHash common.Hash
	if msg.Header.L1BaseFee == nil {
		return nil, errors.New("cannot serialize L1IncomingMessage without L1BaseFee")
	}
	l1BaseFeeHash = common.BigToHash(msg.Header.L1BaseFee)
	if err := util.HashToWriter(l1BaseFeeHash, wr); err != nil {
		return nil, err
	}

	if _, err := wr.Write(msg.L2msg); err != nil {
		return nil, err
	}

	return wr.Bytes(), nil
}

func (msg *L1IncomingMessage) Equals(other *L1IncomingMessage) bool {
	return msg.Header.Equals(other.Header) && bytes.Equal(msg.L2msg, other.L2msg)
}

func hashesEqual(ha, hb *common.Hash) bool {
	if (ha == nil) != (hb == nil) {
		return false
	}
	return (ha == nil && hb == nil) || *ha == *hb
}

func (h *L1IncomingMessageHeader) Equals(other *L1IncomingMessageHeader) bool {
	// These are all non-pointer types so it's safe to use the == operator
	return h.Kind == other.Kind &&
		h.Poster == other.Poster &&
		h.BlockNumber == other.BlockNumber &&
		h.Timestamp == other.Timestamp &&
		hashesEqual(h.RequestId, other.RequestId) &&
		arbmath.BigEquals(h.L1BaseFee, other.L1BaseFee)
}

func ComputeBatchGasCost(data []byte) uint64 {
	var gas uint64
	for _, b := range data {
		if b == 0 {
			gas += params.TxDataZeroGas
		} else {
			gas += params.TxDataNonZeroGasEIP2028
		}
	}

	// the poster also pays to keccak the batch and place it and a batch-posting report into the inbox
	keccakWords := arbmath.WordsForBytes(uint64(len(data)))
	gas += params.Keccak256Gas + (keccakWords * params.Keccak256WordGas)
	gas += 2 * params.SstoreSetGasEIP2200
	return gas
}

func (msg *L1IncomingMessage) FillInBatchGasCost(batchFetcher FallibleBatchFetcher) error {
	if batchFetcher == nil || msg.Header.Kind != L1MessageType_BatchPostingReport || msg.BatchGasCost != nil {
		return nil
	}
	_, _, batchHash, batchNum, _, _, err := ParseBatchPostingReportMessageFields(bytes.NewReader(msg.L2msg))
	if err != nil {
		return fmt.Errorf("failed to parse batch posting report: %w", err)
	}
	batchData, err := batchFetcher(batchNum)
	if err != nil {
		return fmt.Errorf("failed to fetch batch mentioned by batch posting report: %w", err)
	}
	gotHash := crypto.Keccak256Hash(batchData)
	if gotHash != batchHash {
		return fmt.Errorf("batch fetcher returned incorrect data hash %v (wanted %v for batch %v)", gotHash, batchHash, batchNum)
	}
	gas := ComputeBatchGasCost(batchData)
	msg.BatchGasCost = &gas
	return nil
}

func ParseIncomingL1Message(rd io.Reader, batchFetcher FallibleBatchFetcher) (*L1IncomingMessage, error) {
	var kindBuf [1]byte
	_, err := rd.Read(kindBuf[:])
	if err != nil {
		return nil, err
	}
	kind := kindBuf[0]

	sender, err := util.AddressFrom256FromReader(rd)
	if err != nil {
		return nil, err
	}

	blockNumber, err := util.Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}

	timestamp, err := util.Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}

	requestId, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}

	baseFeeL1, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}

	data, err := io.ReadAll(rd)
	if err != nil {
		return nil, err
	}

	msg := &L1IncomingMessage{
		&L1IncomingMessageHeader{
			kind,
			sender,
			blockNumber,
			timestamp,
			&requestId,
			baseFeeL1.Big(),
		},
		data,
		nil,
	}
	err = msg.FillInBatchGasCost(batchFetcher)
	if err != nil {
		return nil, err
	}
	return msg, nil
}

type FallibleBatchFetcher func(batchNum uint64) ([]byte, error)

type ParsedInitMessage struct {
	ChainId          *big.Int
	InitialL1BaseFee *big.Int

	// These may be nil
	ChainConfig           *params.ChainConfig
	SerializedChainConfig []byte
}

// The initial L1 pricing basefee starts at 50 GWei unless set in the init message
var DefaultInitialL1BaseFee = big.NewInt(50 * params.GWei)

var TestInitMessage = &ParsedInitMessage{
	ChainId:          params.ArbitrumDevTestChainConfig().ChainID,
	InitialL1BaseFee: DefaultInitialL1BaseFee,
}

// ParseInitMessage returns the chain id on success
func (msg *L1IncomingMessage) ParseInitMessage() (*ParsedInitMessage, error) {
	if msg.Header.Kind != L1MessageType_Initialize {
		return nil, fmt.Errorf("invalid init message kind %v", msg.Header.Kind)
	}
	basefee := new(big.Int).Set(DefaultInitialL1BaseFee)
	var chainConfig params.ChainConfig
	var chainId *big.Int
	if len(msg.L2msg) == 32 {
		chainId = new(big.Int).SetBytes(msg.L2msg[:32])
		return &ParsedInitMessage{chainId, basefee, nil, nil}, nil
	}
	if len(msg.L2msg) > 32 {
		chainId = new(big.Int).SetBytes(msg.L2msg[:32])
		version := msg.L2msg[32]
		reader := bytes.NewReader(msg.L2msg[33:])
		switch version {
		case 1:
			var err error
			basefee, err = util.Uint256FromReader(reader)
			if err != nil {
				return nil, err
			}
			fallthrough
		case 0:
			serializedChainConfig, err := io.ReadAll(reader)
			if err != nil {
				return nil, err
			}
			err = json.Unmarshal(serializedChainConfig, &chainConfig)
			if err != nil {
				return nil, fmt.Errorf("failed to parse init message, err: %w, message data: %v", err, string(msg.L2msg))
			}
			return &ParsedInitMessage{chainId, basefee, &chainConfig, serializedChainConfig}, nil
		}
	}
	return nil, fmt.Errorf("invalid init message data %v", string(msg.L2msg))
}

func ParseBatchPostingReportMessageFields(rd io.Reader) (*big.Int, common.Address, common.Hash, uint64, *big.Int, uint64, error) {
	batchTimestamp, err := util.HashFromReader(rd)
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	batchPosterAddr, err := util.AddressFromReader(rd)
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	dataHash, err := util.HashFromReader(rd)
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	batchNum, err := util.HashFromReader(rd)
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	l1BaseFee, err := util.HashFromReader(rd)
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	extraGas, err := util.Uint64FromReader(rd)
	if errors.Is(err, io.EOF) {
		// This field isn't always present
		extraGas = 0
		err = nil
	}
	if err != nil {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, err
	}
	batchNumBig := batchNum.Big()
	if !batchNumBig.IsUint64() {
		return nil, common.Address{}, common.Hash{}, 0, nil, 0, fmt.Errorf("batch number %v is not a uint64", batchNumBig)
	}
	return batchTimestamp.Big(), batchPosterAddr, dataHash, batchNumBig.Uint64(), l1BaseFee.Big(), extraGas, nil
}

'''
'''--- arbos/arbostypes/messagewithmeta.go ---
package arbostypes

import (
	"context"
	"encoding/binary"
	"fmt"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/offchainlabs/nitro/arbutil"
)

var uniquifyingPrefix = []byte("Arbitrum Nitro Feed:")

type MessageWithMetadata struct {
	Message             *L1IncomingMessage `json:"message"`
	DelayedMessagesRead uint64             `json:"delayedMessagesRead"`
}

var EmptyTestMessageWithMetadata = MessageWithMetadata{
	Message: &EmptyTestIncomingMessage,
}

// TestMessageWithMetadataAndRequestId message signature is only verified if requestId defined
var TestMessageWithMetadataAndRequestId = MessageWithMetadata{
	Message: &TestIncomingMessageWithRequestId,
}

func (m *MessageWithMetadata) Hash(sequenceNumber arbutil.MessageIndex, chainId uint64) (common.Hash, error) {
	serializedExtraData := make([]byte, 24)
	binary.BigEndian.PutUint64(serializedExtraData[:8], uint64(sequenceNumber))
	binary.BigEndian.PutUint64(serializedExtraData[8:16], chainId)
	binary.BigEndian.PutUint64(serializedExtraData[16:], m.DelayedMessagesRead)

	serializedMessage, err := rlp.EncodeToBytes(m.Message)
	if err != nil {
		return common.Hash{}, fmt.Errorf("unable to serialize message %v: %w", sequenceNumber, err)
	}

	return crypto.Keccak256Hash(uniquifyingPrefix, serializedExtraData, serializedMessage), nil
}

type InboxMultiplexer interface {
	Pop(context.Context) (*MessageWithMetadata, error)
	DelayedMessagesRead() uint64
}

'''
'''--- arbos/block_processor.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"encoding/binary"
	"errors"
	"fmt"
	"math"
	"math/big"

	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/trie"
)

// set by the precompile module, to avoid a package dependence cycle
var ArbRetryableTxAddress common.Address
var ArbSysAddress common.Address
var InternalTxStartBlockMethodID [4]byte
var InternalTxBatchPostingReportMethodID [4]byte
var RedeemScheduledEventID common.Hash
var L2ToL1TransactionEventID common.Hash
var L2ToL1TxEventID common.Hash
var EmitReedeemScheduledEvent func(*vm.EVM, uint64, uint64, [32]byte, [32]byte, common.Address, *big.Int, *big.Int) error
var EmitTicketCreatedEvent func(*vm.EVM, [32]byte) error
var gasUsedSinceStartupCounter = metrics.NewRegisteredCounter("arb/gas_used", nil)

type L1Info struct {
	poster        common.Address
	l1BlockNumber uint64
	l1Timestamp   uint64
}

func (info *L1Info) Equals(o *L1Info) bool {
	return info.poster == o.poster && info.l1BlockNumber == o.l1BlockNumber && info.l1Timestamp == o.l1Timestamp
}

func (info *L1Info) L1BlockNumber() uint64 {
	return info.l1BlockNumber
}

func createNewHeader(prevHeader *types.Header, l1info *L1Info, state *arbosState.ArbosState, chainConfig *params.ChainConfig) *types.Header {
	l2Pricing := state.L2PricingState()
	baseFee, err := l2Pricing.BaseFeeWei()
	state.Restrict(err)

	var lastBlockHash common.Hash
	blockNumber := big.NewInt(0)
	timestamp := uint64(0)
	coinbase := common.Address{}
	if l1info != nil {
		timestamp = l1info.l1Timestamp
		coinbase = l1info.poster
	}
	extra := common.Hash{}.Bytes()
	mixDigest := common.Hash{}
	if prevHeader != nil {
		lastBlockHash = prevHeader.Hash()
		blockNumber.Add(prevHeader.Number, big.NewInt(1))
		if timestamp < prevHeader.Time {
			timestamp = prevHeader.Time
		}
		copy(extra, prevHeader.Extra)
		mixDigest = prevHeader.MixDigest
	}
	header := &types.Header{
		ParentHash:  lastBlockHash,
		UncleHash:   types.EmptyUncleHash, // Post-merge Ethereum will require this to be types.EmptyUncleHash
		Coinbase:    coinbase,
		Root:        [32]byte{},    // Filled in later
		TxHash:      [32]byte{},    // Filled in later
		ReceiptHash: [32]byte{},    // Filled in later
		Bloom:       [256]byte{},   // Filled in later
		Difficulty:  big.NewInt(1), // Eventually, Ethereum plans to require this to be zero
		Number:      blockNumber,
		GasLimit:    l2pricing.GethBlockGasLimit,
		GasUsed:     0,
		Time:        timestamp,
		Extra:       extra,     // used by NewEVMBlockContext
		MixDigest:   mixDigest, // used by NewEVMBlockContext
		Nonce:       [8]byte{}, // Filled in later; post-merge Ethereum will require this to be zero
		BaseFee:     baseFee,
	}
	return header
}

type ConditionalOptionsForTx []*arbitrum_types.ConditionalOptions

type SequencingHooks struct {
	TxErrors                []error
	DiscardInvalidTxsEarly  bool
	PreTxFilter             func(*params.ChainConfig, *types.Header, *state.StateDB, *arbosState.ArbosState, *types.Transaction, *arbitrum_types.ConditionalOptions, common.Address, *L1Info) error
	PostTxFilter            func(*types.Header, *arbosState.ArbosState, *types.Transaction, common.Address, uint64, *core.ExecutionResult) error
	ConditionalOptionsForTx []*arbitrum_types.ConditionalOptions
}

func NoopSequencingHooks() *SequencingHooks {
	return &SequencingHooks{
		[]error{},
		false,
		func(*params.ChainConfig, *types.Header, *state.StateDB, *arbosState.ArbosState, *types.Transaction, *arbitrum_types.ConditionalOptions, common.Address, *L1Info) error {
			return nil
		},
		func(*types.Header, *arbosState.ArbosState, *types.Transaction, common.Address, uint64, *core.ExecutionResult) error {
			return nil
		},
		nil,
	}
}

func ProduceBlock(
	message *arbostypes.L1IncomingMessage,
	delayedMessagesRead uint64,
	lastBlockHeader *types.Header,
	statedb *state.StateDB,
	chainContext core.ChainContext,
	chainConfig *params.ChainConfig,
	batchFetcher arbostypes.FallibleBatchFetcher,
) (*types.Block, types.Receipts, error) {
	var batchFetchErr error
	txes, err := ParseL2Transactions(message, chainConfig.ChainID, func(batchNum uint64, batchHash common.Hash) []byte {
		data, err := batchFetcher(batchNum)
		if err != nil {
			batchFetchErr = err
			return nil
		}
		dataHash := crypto.Keccak256Hash(data)
		if dataHash != batchHash {
			batchFetchErr = fmt.Errorf("expecting batch %v hash %v but got data with hash %v", batchNum, batchHash, dataHash)
			return nil
		}
		return data
	})
	if batchFetchErr != nil {
		return nil, nil, batchFetchErr
	}
	if err != nil {
		log.Warn("error parsing incoming message", "err", err)
		txes = types.Transactions{}
	}

	hooks := NoopSequencingHooks()
	return ProduceBlockAdvanced(
		message.Header, txes, delayedMessagesRead, lastBlockHeader, statedb, chainContext, chainConfig, hooks,
	)
}

// A bit more flexible than ProduceBlock for use in the sequencer.
func ProduceBlockAdvanced(
	l1Header *arbostypes.L1IncomingMessageHeader,
	txes types.Transactions,
	delayedMessagesRead uint64,
	lastBlockHeader *types.Header,
	statedb *state.StateDB,
	chainContext core.ChainContext,
	chainConfig *params.ChainConfig,
	sequencingHooks *SequencingHooks,
) (*types.Block, types.Receipts, error) {

	state, err := arbosState.OpenSystemArbosState(statedb, nil, true)
	if err != nil {
		return nil, nil, err
	}

	if statedb.GetUnexpectedBalanceDelta().BitLen() != 0 {
		return nil, nil, errors.New("ProduceBlock called with dirty StateDB (non-zero unexpected balance delta)")
	}

	poster := l1Header.Poster

	l1Info := &L1Info{
		poster:        poster,
		l1BlockNumber: l1Header.BlockNumber,
		l1Timestamp:   l1Header.Timestamp,
	}

	header := createNewHeader(lastBlockHeader, l1Info, state, chainConfig)
	signer := types.MakeSigner(chainConfig, header.Number, header.Time)
	// Note: blockGasLeft will diverge from the actual gas left during execution in the event of invalid txs,
	// but it's only used as block-local representation limiting the amount of work done in a block.
	blockGasLeft, _ := state.L2PricingState().PerBlockGasLimit()
	l1BlockNum := l1Info.l1BlockNumber

	// Prepend a tx before all others to touch up the state (update the L1 block num, pricing pools, etc)
	startTx := InternalTxStartBlock(chainConfig.ChainID, l1Header.L1BaseFee, l1BlockNum, header, lastBlockHeader)
	txes = append(types.Transactions{types.NewTx(startTx)}, txes...)

	complete := types.Transactions{}
	receipts := types.Receipts{}
	basefee := header.BaseFee
	time := header.Time
	expectedBalanceDelta := new(big.Int)
	redeems := types.Transactions{}
	userTxsProcessed := 0

	// We'll check that the block can fit each message, so this pool is set to not run out
	gethGas := core.GasPool(l2pricing.GethBlockGasLimit)

	for len(txes) > 0 || len(redeems) > 0 {
		// repeatedly process the next tx, doing redeems created along the way in FIFO order

		var tx *types.Transaction
		var options *arbitrum_types.ConditionalOptions
		hooks := NoopSequencingHooks()
		isUserTx := false
		if len(redeems) > 0 {
			tx = redeems[0]
			redeems = redeems[1:]

			retry, ok := (tx.GetInner()).(*types.ArbitrumRetryTx)
			if !ok {
				return nil, nil, errors.New("retryable tx is somehow not a retryable")
			}
			retryable, _ := state.RetryableState().OpenRetryable(retry.TicketId, time)
			if retryable == nil {
				// retryable was already deleted
				continue
			}
		} else {
			tx = txes[0]
			txes = txes[1:]
			if tx.Type() != types.ArbitrumInternalTxType {
				hooks = sequencingHooks // the sequencer has the ability to drop this tx
				isUserTx = true
				if len(hooks.ConditionalOptionsForTx) > 0 {
					options = hooks.ConditionalOptionsForTx[0]
					hooks.ConditionalOptionsForTx = hooks.ConditionalOptionsForTx[1:]
				}
			}
		}

		startRefund := statedb.GetRefund()
		if startRefund != 0 {
			return nil, nil, fmt.Errorf("at beginning of tx statedb has non-zero refund %v", startRefund)
		}

		var sender common.Address
		var dataGas uint64 = 0
		preTxHeaderGasUsed := header.GasUsed
		receipt, result, err := (func() (*types.Receipt, *core.ExecutionResult, error) {
			// If we've done too much work in this block, discard the tx as early as possible
			if blockGasLeft < params.TxGas && isUserTx {
				return nil, nil, core.ErrGasLimitReached
			}

			sender, err = signer.Sender(tx)
			if err != nil {
				return nil, nil, err
			}

			if err = hooks.PreTxFilter(chainConfig, header, statedb, state, tx, options, sender, l1Info); err != nil {
				return nil, nil, err
			}

			if basefee.Sign() > 0 {
				dataGas = math.MaxUint64
				brotliCompressionLevel, err := state.BrotliCompressionLevel()
				if err != nil {
					return nil, nil, fmt.Errorf("failed to get brotli compression level: %w", err)
				}
				posterCost, _ := state.L1PricingState().GetPosterInfo(tx, poster, brotliCompressionLevel)
				posterCostInL2Gas := arbmath.BigDiv(posterCost, basefee)

				if posterCostInL2Gas.IsUint64() {
					dataGas = posterCostInL2Gas.Uint64()
				} else {
					log.Error("Could not get poster cost in L2 terms", "posterCost", posterCost, "basefee", basefee)
				}
			}

			if dataGas > tx.Gas() {
				// this txn is going to be rejected later
				dataGas = tx.Gas()
			}

			computeGas := tx.Gas() - dataGas
			if computeGas < params.TxGas {
				if hooks.DiscardInvalidTxsEarly {
					return nil, nil, core.ErrIntrinsicGas
				}
				// ensure at least TxGas is left in the pool before trying a state transition
				computeGas = params.TxGas
			}

			if computeGas > blockGasLeft && isUserTx && userTxsProcessed > 0 {
				return nil, nil, core.ErrGasLimitReached
			}

			snap := statedb.Snapshot()
			statedb.SetTxContext(tx.Hash(), len(receipts)) // the number of successful state transitions

			gasPool := gethGas
			receipt, result, err := core.ApplyTransactionWithResultFilter(
				chainConfig,
				chainContext,
				&header.Coinbase,
				&gasPool,
				statedb,
				header,
				tx,
				&header.GasUsed,
				vm.Config{},
				func(result *core.ExecutionResult) error {
					return hooks.PostTxFilter(header, state, tx, sender, dataGas, result)
				},
			)
			if err != nil {
				// Ignore this transaction if it's invalid under the state transition function
				statedb.RevertToSnapshot(snap)
				return nil, nil, err
			}

			return receipt, result, nil
		})()

		if tx.Type() == types.ArbitrumInternalTxType {
			// ArbOS might have upgraded to a new version, so we need to refresh our state
			state, err = arbosState.OpenSystemArbosState(statedb, nil, true)
			if err != nil {
				return nil, nil, err
			}
			// Update the ArbOS version in the header (if it changed)
			extraInfo := types.DeserializeHeaderExtraInformation(header)
			extraInfo.ArbOSFormatVersion = state.ArbOSVersion()
			extraInfo.UpdateHeaderWithInfo(header)
		}

		// append the err, even if it is nil
		hooks.TxErrors = append(hooks.TxErrors, err)

		if err != nil {
			log.Debug("error applying transaction", "tx", tx, "err", err)
			if !hooks.DiscardInvalidTxsEarly {
				// we'll still deduct a TxGas's worth from the block-local rate limiter even if the tx was invalid
				blockGasLeft = arbmath.SaturatingUSub(blockGasLeft, params.TxGas)
				if isUserTx {
					userTxsProcessed++
				}
			}
			continue
		}

		if tx.Type() == types.ArbitrumInternalTxType && result.Err != nil {
			return nil, nil, fmt.Errorf("failed to apply internal transaction: %w", result.Err)
		}

		if preTxHeaderGasUsed > header.GasUsed {
			return nil, nil, fmt.Errorf("ApplyTransaction() used -%v gas", preTxHeaderGasUsed-header.GasUsed)
		}
		txGasUsed := header.GasUsed - preTxHeaderGasUsed

		// Update expectedTotalBalanceDelta (also done in logs loop)
		switch txInner := tx.GetInner().(type) {
		case *types.ArbitrumDepositTx:
			// L1->L2 deposits add eth to the system
			expectedBalanceDelta.Add(expectedBalanceDelta, txInner.Value)
		case *types.ArbitrumSubmitRetryableTx:
			// Retryable submission can include a deposit which adds eth to the system
			expectedBalanceDelta.Add(expectedBalanceDelta, txInner.DepositValue)
		}

		computeUsed := txGasUsed - dataGas
		if txGasUsed < dataGas {
			log.Error("ApplyTransaction() used less gas than it should have", "delta", dataGas-txGasUsed)
			computeUsed = params.TxGas
		} else if computeUsed < params.TxGas {
			computeUsed = params.TxGas
		}

		if txGasUsed > tx.Gas() {
			return nil, nil, fmt.Errorf("ApplyTransaction() used %v more gas than it should have", txGasUsed-tx.Gas())
		}

		// append any scheduled redeems
		redeems = append(redeems, result.ScheduledTxes...)

		for _, txLog := range receipt.Logs {
			if txLog.Address == ArbSysAddress {
				// L2ToL1TransactionEventID is deprecated in upgrade 4, but it should to safe to make this code handle
				// both events ignoring the version.
				// TODO: Remove L2ToL1Transaction handling on next chain reset
				// L2->L1 withdrawals remove eth from the system
				switch txLog.Topics[0] {
				case L2ToL1TransactionEventID:
					event := &precompilesgen.ArbSysL2ToL1Transaction{}
					err := util.ParseL2ToL1TransactionLog(event, txLog)
					if err != nil {
						log.Error("Failed to parse L2ToL1Transaction log", "err", err)
					} else {
						expectedBalanceDelta.Sub(expectedBalanceDelta, event.Callvalue)
					}
				case L2ToL1TxEventID:
					event := &precompilesgen.ArbSysL2ToL1Tx{}
					err := util.ParseL2ToL1TxLog(event, txLog)
					if err != nil {
						log.Error("Failed to parse L2ToL1Tx log", "err", err)
					} else {
						expectedBalanceDelta.Sub(expectedBalanceDelta, event.Callvalue)
					}
				}
			}
		}

		blockGasLeft = arbmath.SaturatingUSub(blockGasLeft, computeUsed)

		// Add gas used since startup to prometheus metric.
		gasUsed := arbmath.SaturatingUSub(receipt.GasUsed, receipt.GasUsedForL1)
		gasUsedSinceStartupCounter.Inc(arbmath.SaturatingCast(gasUsed))

		complete = append(complete, tx)
		receipts = append(receipts, receipt)

		if isUserTx {
			userTxsProcessed++
		}
	}

	binary.BigEndian.PutUint64(header.Nonce[:], delayedMessagesRead)

	FinalizeBlock(header, complete, statedb, chainConfig)

	// Touch up the block hashes in receipts
	tmpBlock := types.NewBlock(header, complete, nil, receipts, trie.NewStackTrie(nil))
	blockHash := tmpBlock.Hash()

	for _, receipt := range receipts {
		receipt.BlockHash = blockHash
		for _, txLog := range receipt.Logs {
			txLog.BlockHash = blockHash
		}
	}

	block := types.NewBlock(header, complete, nil, receipts, trie.NewStackTrie(nil))

	if len(block.Transactions()) != len(receipts) {
		return nil, nil, fmt.Errorf("block has %d txes but %d receipts", len(block.Transactions()), len(receipts))
	}

	balanceDelta := statedb.GetUnexpectedBalanceDelta()
	if !arbmath.BigEquals(balanceDelta, expectedBalanceDelta) {
		// Fail if funds have been minted or debug mode is enabled (i.e. this is a test)
		if balanceDelta.Cmp(expectedBalanceDelta) > 0 || chainConfig.DebugMode() {
			return nil, nil, fmt.Errorf("unexpected total balance delta %v (expected %v)", balanceDelta, expectedBalanceDelta)
		}
		// This is a real chain and funds were burnt, not minted, so only log an error and don't panic
		log.Error("Unexpected total balance delta", "delta", balanceDelta, "expected", expectedBalanceDelta)
	}

	return block, receipts, nil
}

// Also sets header.Root
func FinalizeBlock(header *types.Header, txs types.Transactions, statedb *state.StateDB, chainConfig *params.ChainConfig) {
	if header != nil {
		if header.Number.Uint64() < chainConfig.ArbitrumChainParams.GenesisBlockNum {
			panic("cannot finalize blocks before genesis")
		}

		var sendRoot common.Hash
		var sendCount uint64
		var nextL1BlockNumber uint64
		var arbosVersion uint64

		if header.Number.Uint64() == chainConfig.ArbitrumChainParams.GenesisBlockNum {
			arbosVersion = chainConfig.ArbitrumChainParams.InitialArbOSVersion
		} else {
			state, err := arbosState.OpenSystemArbosState(statedb, nil, true)
			if err != nil {
				newErr := fmt.Errorf("%w while opening arbos state. Block: %d root: %v", err, header.Number, header.Root)
				panic(newErr)
			}
			// Add outbox info to the header for client-side proving
			acc := state.SendMerkleAccumulator()
			sendRoot, _ = acc.Root()
			sendCount, _ = acc.Size()
			nextL1BlockNumber, _ = state.Blockhashes().L1BlockNumber()
			arbosVersion = state.ArbOSVersion()
		}
		arbitrumHeader := types.HeaderInfo{
			SendRoot:           sendRoot,
			SendCount:          sendCount,
			L1BlockNumber:      nextL1BlockNumber,
			ArbOSFormatVersion: arbosVersion,
		}
		arbitrumHeader.UpdateHeaderWithInfo(header)
		header.Root = statedb.IntermediateRoot(true)
	}
}

'''
'''--- arbos/blockhash/blockhash.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package blockhash

import (
	"encoding/binary"
	"errors"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/storage"
)

type Blockhashes struct {
	backingStorage *storage.Storage
	l1BlockNumber  storage.StorageBackedUint64
}

func InitializeBlockhashes(backingStorage *storage.Storage) {
	// no need to do anything, nextBlockNumber is already zero and no hashes are needed when nextBlockNumber is zero
}

func OpenBlockhashes(backingStorage *storage.Storage) *Blockhashes {
	return &Blockhashes{backingStorage.WithoutCache(), backingStorage.OpenStorageBackedUint64(0)}
}

func (bh *Blockhashes) L1BlockNumber() (uint64, error) {
	return bh.l1BlockNumber.Get()
}

func (bh *Blockhashes) BlockHash(number uint64) (common.Hash, error) {
	currentNumber, err := bh.l1BlockNumber.Get()
	if err != nil {
		return common.Hash{}, err
	}
	if number >= currentNumber || number+256 < currentNumber {
		return common.Hash{}, errors.New("invalid block number for BlockHash")
	}
	return bh.backingStorage.GetByUint64(1 + (number % 256))
}

func (bh *Blockhashes) RecordNewL1Block(number uint64, blockHash common.Hash, arbosVersion uint64) error {
	nextNumber, err := bh.l1BlockNumber.Get()
	if err != nil {
		return err
	}
	if number < nextNumber {
		// we already have a stored hash for the block, so just return
		return nil
	}
	if nextNumber+256 < number {
		nextNumber = number - 256 // no need to record hashes that we're just going to discard
	}
	for nextNumber+1 < number {
		// fill in hashes for any "skipped over" blocks
		nextNumber++
		var nextNumBuf [8]byte
		if arbosVersion >= 8 {
			binary.LittleEndian.PutUint64(nextNumBuf[:], nextNumber)
		}

		fill, err := bh.backingStorage.KeccakHash(blockHash.Bytes(), nextNumBuf[:])
		if err != nil {
			return err
		}
		err = bh.backingStorage.SetByUint64(1+(nextNumber%256), fill)
		if err != nil {
			return err
		}
	}

	err = bh.backingStorage.SetByUint64(1+(number%256), blockHash)
	if err != nil {
		return err
	}
	return bh.l1BlockNumber.Set(number + 1)
}

'''
'''--- arbos/blockhash/blockhash_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package blockhash

import (
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestBlockhash(t *testing.T) {
	arbosVersion := uint64(8)

	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	InitializeBlockhashes(sto)

	bh := OpenBlockhashes(sto)
	bnum, err := bh.L1BlockNumber()
	Require(t, err, "failed to read blocknum in new Blockhashes")
	if bnum != 0 {
		Fail(t, "incorrect blocknum in new Blockhashes")
	}
	_, err = bh.BlockHash(0)
	if err == nil {
		Fail(t, "should have generated error on Blockhash(0) in new Blockhashes")
	}
	_, err = bh.BlockHash(4242)
	if err == nil {
		Fail(t, "should have generated error on Blockhash(4242) in new Blockhashes")
	}

	hash0 := common.BytesToHash(crypto.Keccak256([]byte{0}))
	err = bh.RecordNewL1Block(0, hash0, arbosVersion)
	Require(t, err)
	bnum, err = bh.L1BlockNumber()
	Require(t, err)
	if bnum != 1 {
		Fail(t, "incorrect NextBlockNumber after initial Blockhash(0)")
	}
	h, err := bh.BlockHash(0)
	Require(t, err)
	if h != hash0 {
		Fail(t, "incorrect hash return for initial Blockhash(0)")
	}

	hash4242 := common.BytesToHash(crypto.Keccak256([]byte{42, 42}))
	err = bh.RecordNewL1Block(4242, hash4242, arbosVersion)
	Require(t, err)
	bnum, err = bh.L1BlockNumber()
	Require(t, err)
	if bnum != 4243 {
		Fail(t, "incorrect NextBlockNumber after big jump")
	}
	_, err = bh.BlockHash(4243)
	if err == nil {
		Fail(t, "BlockHash for future block should generate error")
	}
	h, err = bh.BlockHash(4242)
	Require(t, err)
	if h != hash4242 {
		Fail(t, "incorrect BlockHash(4242)")
	}
	h2, err := bh.BlockHash(4242 - 1)
	Require(t, err)
	if h2 == h {
		Fail(t, "same blockhash at different blocknums")
	}
	h3, err := bh.BlockHash(4242 - 2)
	Require(t, err)
	if h3 == h2 || h3 == h {
		Fail(t, "same blockhash at different blocknums")
	}
	h255, err := bh.BlockHash(4242 - 255)
	Require(t, err)
	if h255 == h || h255 == h3 {
		Fail(t, "same blockhash at different blocknums")
	}
	_, err = bh.BlockHash(4242 - 256)
	if err == nil {
		Fail(t, "old blockhash should give error")
	}

}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/burn/burn.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package burn

import (
	"fmt"

	glog "github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbos/util"
)

type Burner interface {
	Burn(amount uint64) error
	Burned() uint64
	Restrict(err error)
	HandleError(err error) error
	ReadOnly() bool
	TracingInfo() *util.TracingInfo
}

type SystemBurner struct {
	gasBurnt    uint64
	tracingInfo *util.TracingInfo
	readOnly    bool
}

func NewSystemBurner(tracingInfo *util.TracingInfo, readOnly bool) *SystemBurner {
	return &SystemBurner{
		tracingInfo: tracingInfo,
		readOnly:    readOnly,
	}
}

func (burner *SystemBurner) Burn(amount uint64) error {
	burner.gasBurnt += amount
	return nil
}

func (burner *SystemBurner) Burned() uint64 {
	return burner.gasBurnt
}

func (burner *SystemBurner) Restrict(err error) {
	if err != nil {
		glog.Error("Restrict() received an error", "err", err)
	}
}

func (burner *SystemBurner) HandleError(err error) error {
	panic(fmt.Sprintf("fatal error in system burner: %v", err))
}

func (burner *SystemBurner) ReadOnly() bool {
	return burner.readOnly
}

func (burner *SystemBurner) TracingInfo() *util.TracingInfo {
	return burner.tracingInfo
}

'''
'''--- arbos/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/engine.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/consensus"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/ethereum/go-ethereum/trie"
)

type Engine struct {
	IsSequencer bool
}

func (e Engine) Author(header *types.Header) (common.Address, error) {
	return header.Coinbase, nil
}

func (e Engine) VerifyHeader(chain consensus.ChainHeaderReader, header *types.Header) error {
	// TODO what verification should be done here?
	return nil
}

func (e Engine) VerifyHeaders(chain consensus.ChainHeaderReader, headers []*types.Header) (chan<- struct{}, <-chan error) {
	errors := make(chan error, len(headers))
	for i := range headers {
		errors <- e.VerifyHeader(chain, headers[i])
	}
	return make(chan struct{}), errors
}

func (e Engine) VerifyUncles(chain consensus.ChainReader, block *types.Block) error {
	if len(block.Uncles()) != 0 {
		return errors.New("uncles not supported")
	}
	return nil
}

func (e Engine) Prepare(chain consensus.ChainHeaderReader, header *types.Header) error {
	header.Difficulty = big.NewInt(1)
	return nil
}

func (e Engine) Finalize(chain consensus.ChainHeaderReader, header *types.Header, state *state.StateDB, txs []*types.Transaction, uncles []*types.Header, withdrawals []*types.Withdrawal) {
	FinalizeBlock(header, txs, state, chain.Config())
}

func (e Engine) FinalizeAndAssemble(chain consensus.ChainHeaderReader, header *types.Header, state *state.StateDB, txs []*types.Transaction,
	uncles []*types.Header, receipts []*types.Receipt, withdrawals []*types.Withdrawal) (*types.Block, error) {

	e.Finalize(chain, header, state, txs, uncles, withdrawals)

	block := types.NewBlock(header, txs, nil, receipts, trie.NewStackTrie(nil))
	return block, nil
}

func (e Engine) Seal(chain consensus.ChainHeaderReader, block *types.Block, results chan<- *types.Block, stop <-chan struct{}) error {
	if !e.IsSequencer {
		return errors.New("sealing not supported")
	}
	if len(block.Transactions()) == 0 {
		return nil
	}
	results <- block
	return nil
}

func (e Engine) SealHash(header *types.Header) common.Hash {
	return header.Hash()
}

func (e Engine) CalcDifficulty(chain consensus.ChainHeaderReader, time uint64, parent *types.Header) *big.Int {
	return big.NewInt(1)
}

func (e Engine) APIs(chain consensus.ChainHeaderReader) []rpc.API {
	return nil
}

func (e Engine) Close() error {
	return nil
}

'''
'''--- arbos/incomingmessage_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"bytes"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
)

func TestSerializeAndParseL1Message(t *testing.T) {
	chainId := big.NewInt(6345634)
	requestId := common.BigToHash(big.NewInt(3))
	header := arbostypes.L1IncomingMessageHeader{
		Kind:        arbostypes.L1MessageType_EndOfBlock,
		Poster:      common.BigToAddress(big.NewInt(4684)),
		BlockNumber: 864513,
		Timestamp:   8794561564,
		RequestId:   &requestId,
		L1BaseFee:   big.NewInt(10000000000000),
	}
	msg := arbostypes.L1IncomingMessage{
		Header:       &header,
		L2msg:        []byte{3, 2, 1},
		BatchGasCost: nil,
	}
	serialized, err := msg.Serialize()
	if err != nil {
		t.Error(err)
	}
	newMsg, err := arbostypes.ParseIncomingL1Message(bytes.NewReader(serialized), nil)
	if err != nil {
		t.Error(err)
	}
	txes, err := ParseL2Transactions(newMsg, chainId, nil)
	if err != nil {
		t.Error(err)
	}
	if len(txes) != 0 {
		Fail(t, "unexpected tx count")
	}
}

'''
'''--- arbos/internal_tx.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"encoding/hex"
	"fmt"
	"math/big"

	"github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/log"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/util"
)

func InternalTxStartBlock(
	chainId,
	l1BaseFee *big.Int,
	l1BlockNum uint64,
	header,
	lastHeader *types.Header,
) *types.ArbitrumInternalTx {

	l2BlockNum := header.Number.Uint64()
	timePassed := header.Time - lastHeader.Time

	if l1BaseFee == nil {
		l1BaseFee = big.NewInt(0)
	}
	data, err := util.PackInternalTxDataStartBlock(l1BaseFee, l1BlockNum, l2BlockNum, timePassed)
	if err != nil {
		panic(fmt.Sprintf("Failed to pack internal tx %v", err))
	}
	return &types.ArbitrumInternalTx{
		ChainId: chainId,
		Data:    data,
	}
}

func ApplyInternalTxUpdate(tx *types.ArbitrumInternalTx, state *arbosState.ArbosState, evm *vm.EVM) error {
	if len(tx.Data) < 4 {
		return fmt.Errorf("internal tx data is too short (only %v bytes, at least 4 required)", len(tx.Data))
	}
	selector := *(*[4]byte)(tx.Data[:4])
	switch selector {
	case InternalTxStartBlockMethodID:
		inputs, err := util.UnpackInternalTxDataStartBlock(tx.Data)
		if err != nil {
			return err
		}

		l1BlockNumber := util.SafeMapGet[uint64](inputs, "l1BlockNumber")
		timePassed := util.SafeMapGet[uint64](inputs, "timePassed")
		if state.ArbOSVersion() < 3 {
			// (incorrectly) use the L2 block number instead
			timePassed = util.SafeMapGet[uint64](inputs, "l2BlockNumber")
		}
		if state.ArbOSVersion() < 8 {
			// in old versions we incorrectly used an L1 block number one too high
			l1BlockNumber++
		}

		oldL1BlockNumber, err := state.Blockhashes().L1BlockNumber()
		state.Restrict(err)

		l2BaseFee, err := state.L2PricingState().BaseFeeWei()
		state.Restrict(err)

		if l1BlockNumber > oldL1BlockNumber {
			var prevHash common.Hash
			if evm.Context.BlockNumber.Sign() > 0 {
				prevHash = evm.Context.GetHash(evm.Context.BlockNumber.Uint64() - 1)
			}
			state.Restrict(state.Blockhashes().RecordNewL1Block(l1BlockNumber-1, prevHash, state.ArbOSVersion()))
		}

		currentTime := evm.Context.Time

		// Try to reap 2 retryables
		_ = state.RetryableState().TryToReapOneRetryable(currentTime, evm, util.TracingDuringEVM)
		_ = state.RetryableState().TryToReapOneRetryable(currentTime, evm, util.TracingDuringEVM)

		state.L2PricingState().UpdatePricingModel(l2BaseFee, timePassed, false)

		return state.UpgradeArbosVersionIfNecessary(currentTime, evm.StateDB, evm.ChainConfig())
	case InternalTxBatchPostingReportMethodID:
		inputs, err := util.UnpackInternalTxDataBatchPostingReport(tx.Data)
		if err != nil {
			return err
		}
		batchTimestamp := util.SafeMapGet[*big.Int](inputs, "batchTimestamp")
		batchPosterAddress := util.SafeMapGet[common.Address](inputs, "batchPosterAddress")
		batchDataGas := util.SafeMapGet[uint64](inputs, "batchDataGas")
		l1BaseFeeWei := util.SafeMapGet[*big.Int](inputs, "l1BaseFeeWei")

		l1p := state.L1PricingState()
		perBatchGas, err := l1p.PerBatchGasCost()
		if err != nil {
			log.Warn("L1Pricing PerBatchGas failed", "err", err)
		}
		gasSpent := arbmath.SaturatingAdd(perBatchGas, arbmath.SaturatingCast(batchDataGas))
		weiSpent := arbmath.BigMulByUint(l1BaseFeeWei, arbmath.SaturatingUCast(gasSpent))
		err = l1p.UpdateForBatchPosterSpending(
			evm.StateDB,
			evm,
			state.ArbOSVersion(),
			batchTimestamp.Uint64(),
			evm.Context.Time,
			batchPosterAddress,
			weiSpent,
			l1BaseFeeWei,
			util.TracingDuringEVM,
		)
		if err != nil {
			log.Warn("L1Pricing UpdateForSequencerSpending failed", "err", err)
		}
		return nil
	default:
		return fmt.Errorf("unknown internal tx method selector: %v", hex.EncodeToString(tx.Data[:4]))
	}
}

'''
'''--- arbos/l1pricing/batchPoster.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"errors"
	"math"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/addressSet"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/arbmath"
)

const totalFundsDueOffset = 0

var (
	PosterAddrsKey = []byte{0}
	PosterInfoKey  = []byte{1}

	ErrAlreadyExists = errors.New("tried to add a batch poster that already exists")
	ErrNotExist      = errors.New("tried to open a batch poster that does not exist")
)

// BatchPostersTable is the layout of storage in the table
type BatchPostersTable struct {
	posterAddrs   *addressSet.AddressSet
	posterInfo    *storage.Storage
	totalFundsDue storage.StorageBackedBigInt
}

type BatchPosterState struct {
	fundsDue     storage.StorageBackedBigInt
	payTo        storage.StorageBackedAddress
	postersTable *BatchPostersTable
}

func InitializeBatchPostersTable(storage *storage.Storage) error {
	totalFundsDue := storage.OpenStorageBackedBigInt(totalFundsDueOffset)
	if err := totalFundsDue.SetChecked(common.Big0); err != nil {
		return err
	}
	return addressSet.Initialize(storage.OpenCachedSubStorage(PosterAddrsKey))
}

func OpenBatchPostersTable(storage *storage.Storage) *BatchPostersTable {
	return &BatchPostersTable{
		posterAddrs:   addressSet.OpenAddressSet(storage.OpenCachedSubStorage(PosterAddrsKey)),
		posterInfo:    storage.OpenSubStorage(PosterInfoKey),
		totalFundsDue: storage.OpenStorageBackedBigInt(totalFundsDueOffset),
	}
}

func (bpt *BatchPostersTable) OpenPoster(poster common.Address, createIfNotExist bool) (*BatchPosterState, error) {
	isBatchPoster, err := bpt.posterAddrs.IsMember(poster)
	if err != nil {
		return nil, err
	}
	if !isBatchPoster {
		if !createIfNotExist {
			return nil, ErrNotExist
		}
		return bpt.AddPoster(poster, poster)
	}
	return bpt.internalOpen(poster), nil
}

func (bpt *BatchPostersTable) internalOpen(poster common.Address) *BatchPosterState {
	bpStorage := bpt.posterInfo.OpenSubStorage(poster.Bytes())
	return &BatchPosterState{
		fundsDue:     bpStorage.OpenStorageBackedBigInt(0),
		payTo:        bpStorage.OpenStorageBackedAddress(1),
		postersTable: bpt,
	}
}

func (bpt *BatchPostersTable) ContainsPoster(poster common.Address) (bool, error) {
	return bpt.posterAddrs.IsMember(poster)
}

func (bpt *BatchPostersTable) AddPoster(posterAddress common.Address, payTo common.Address) (*BatchPosterState, error) {
	isBatchPoster, err := bpt.posterAddrs.IsMember(posterAddress)
	if err != nil {
		return nil, err
	}
	if isBatchPoster {
		return nil, ErrAlreadyExists
	}
	bpState := bpt.internalOpen(posterAddress)
	if err := bpState.fundsDue.SetChecked(common.Big0); err != nil {
		return nil, err
	}
	if err := bpState.payTo.Set(payTo); err != nil {
		return nil, err
	}

	if err := bpt.posterAddrs.Add(posterAddress); err != nil {
		return nil, err
	}

	return bpState, nil
}

func (bpt *BatchPostersTable) AllPosters(maxNumToGet uint64) ([]common.Address, error) {
	return bpt.posterAddrs.AllMembers(maxNumToGet)
}

func (bpt *BatchPostersTable) TotalFundsDue() (*big.Int, error) {
	return bpt.totalFundsDue.Get()
}

func (bps *BatchPosterState) FundsDue() (*big.Int, error) {
	return bps.fundsDue.Get()
}

func (bps *BatchPosterState) SetFundsDue(val *big.Int) error {
	fundsDue := bps.fundsDue
	totalFundsDue := bps.postersTable.totalFundsDue
	prev, err := fundsDue.Get()
	if err != nil {
		return err
	}
	prevTotal, err := totalFundsDue.Get()
	if err != nil {
		return err
	}
	if err := totalFundsDue.SetSaturatingWithWarning(arbmath.BigSub(arbmath.BigAdd(prevTotal, val), prev), "batch poster total funds due"); err != nil {
		return err
	}
	return bps.fundsDue.SetSaturatingWithWarning(val, "batch poster funds due")
}

func (bps *BatchPosterState) PayTo() (common.Address, error) {
	return bps.payTo.Get()
}

func (bps *BatchPosterState) SetPayTo(addr common.Address) error {
	return bps.payTo.Set(addr)
}

type FundsDueItem struct {
	dueTo   common.Address
	balance *big.Int
}

func (bpt *BatchPostersTable) GetFundsDueList() ([]FundsDueItem, error) {
	ret := []FundsDueItem{}
	allPosters, err := bpt.AllPosters(math.MaxUint64)
	if err != nil {
		return nil, err
	}
	for _, posterAddr := range allPosters {
		poster, err := bpt.OpenPoster(posterAddr, false)
		if err != nil {
			return nil, err
		}
		due, err := poster.FundsDue()
		if err != nil {
			return nil, err
		}
		if due.Sign() > 0 {
			ret = append(ret, FundsDueItem{
				dueTo:   posterAddr,
				balance: due,
			})
		}
	}
	return ret, nil
}

'''
'''--- arbos/l1pricing/batchPoster_test.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
)

func TestBatchPosterTable(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	err := InitializeBatchPostersTable(sto)
	Require(t, err)

	bpTable := OpenBatchPostersTable(sto)

	addr1 := common.Address{1, 2, 3}
	pay1 := common.Address{4, 5, 6, 7}
	addr2 := common.Address{2, 4, 6}
	pay2 := common.Address{8, 10, 12, 14}

	// test creation and counting of bps
	allPosters, err := bpTable.AllPosters(math.MaxUint64)
	Require(t, err)
	if len(allPosters) != 0 {
		t.Fatal()
	}
	exists, err := bpTable.ContainsPoster(addr1)
	Require(t, err)
	if exists {
		t.Fatal()
	}

	bp1, err := bpTable.AddPoster(addr1, pay1)
	Require(t, err)
	getPay1, err := bp1.PayTo()
	Require(t, err)
	if getPay1 != pay1 {
		t.Fatal()
	}
	getDue1, err := bp1.FundsDue()
	Require(t, err)
	if getDue1.Sign() != 0 {
		t.Fatal()
	}
	exists, err = bpTable.ContainsPoster(addr1)
	Require(t, err)
	if !exists {
		t.Fatal()
	}

	bp2, err := bpTable.AddPoster(addr2, pay2)
	Require(t, err)
	_ = bp2
	getPay2, err := bp2.PayTo()
	Require(t, err)
	if getPay2 != pay2 {
		t.Fatal()
	}
	getDue2, err := bp2.FundsDue()
	Require(t, err)
	if getDue2.Sign() != 0 {
		t.Fatal()
	}
	exists, err = bpTable.ContainsPoster(addr2)
	Require(t, err)
	if !exists {
		t.Fatal()
	}

	allPosters, err = bpTable.AllPosters(math.MaxUint64)
	Require(t, err)
	if len(allPosters) != 2 {
		t.Fatal()
	}

	// test get/set of BP fields
	bp1, err = bpTable.OpenPoster(addr1, false)
	Require(t, err)
	err = bp1.SetPayTo(addr2)
	Require(t, err)
	getPay1, err = bp1.PayTo()
	Require(t, err)
	if getPay1 != addr2 {
		t.Fatal()
	}
	err = bp1.SetFundsDue(big.NewInt(13))
	Require(t, err)
	getDue1, err = bp1.FundsDue()
	Require(t, err)
	if getDue1.Uint64() != 13 {
		t.Fatal()
	}

	// test adding up the fundsDue
	err = bp2.SetFundsDue(big.NewInt(42))
	Require(t, err)
	getDue2, err = bp2.FundsDue()
	Require(t, err)
	if getDue2.Uint64() != 42 {
		t.Fatal()
	}

	totalDue, err := bpTable.TotalFundsDue()
	Require(t, err)
	if totalDue.Uint64() != 13+42 {
		t.Fatal()
	}
}

'''
'''--- arbos/l1pricing/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/l1pricing/l1PricingOldVersions.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/offchainlabs/nitro/arbos/util"
	am "github.com/offchainlabs/nitro/util/arbmath"
	"math"
	"math/big"
)

func (ps *L1PricingState) _preversion10_UpdateForBatchPosterSpending(
	statedb vm.StateDB,
	evm *vm.EVM,
	arbosVersion uint64,
	updateTime, currentTime uint64,
	batchPoster common.Address,
	weiSpent *big.Int,
	l1Basefee *big.Int,
	scenario util.TracingScenario,
) error {
	if arbosVersion < 2 {
		return ps._preVersion2_UpdateForBatchPosterSpending(statedb, evm, updateTime, currentTime, batchPoster, weiSpent, scenario)
	}

	batchPosterTable := ps.BatchPosterTable()
	posterState, err := batchPosterTable.OpenPoster(batchPoster, true)
	if err != nil {
		return err
	}

	fundsDueForRewards, err := ps.FundsDueForRewards()
	if err != nil {
		return err
	}

	// compute allocation fraction -- will allocate updateTimeDelta/timeDelta fraction of units and funds to this update
	lastUpdateTime, err := ps.LastUpdateTime()
	if err != nil {
		return err
	}
	if lastUpdateTime == 0 && updateTime > 0 { // it's the first update, so there isn't a last update time
		lastUpdateTime = updateTime - 1
	}
	if updateTime > currentTime || updateTime < lastUpdateTime {
		return ErrInvalidTime
	}
	allocationNumerator := updateTime - lastUpdateTime
	allocationDenominator := currentTime - lastUpdateTime
	if allocationDenominator == 0 {
		allocationNumerator = 1
		allocationDenominator = 1
	}

	// allocate units to this update
	unitsSinceUpdate, err := ps.UnitsSinceUpdate()
	if err != nil {
		return err
	}
	unitsAllocated := am.SaturatingUMul(unitsSinceUpdate, allocationNumerator) / allocationDenominator
	unitsSinceUpdate -= unitsAllocated
	if err := ps.SetUnitsSinceUpdate(unitsSinceUpdate); err != nil {
		return err
	}

	// impose cap on amortized cost, if there is one
	if arbosVersion >= 3 {
		amortizedCostCapBips, err := ps.AmortizedCostCapBips()
		if err != nil {
			return err
		}
		if amortizedCostCapBips != 0 {
			weiSpentCap := am.BigMulByBips(
				am.BigMulByUint(l1Basefee, unitsAllocated),
				am.SaturatingCastToBips(amortizedCostCapBips),
			)
			if am.BigLessThan(weiSpentCap, weiSpent) {
				// apply the cap on assignment of amortized cost;
				// the difference will be a loss for the batch poster
				weiSpent = weiSpentCap
			}
		}
	}

	dueToPoster, err := posterState.FundsDue()
	if err != nil {
		return err
	}
	err = posterState.SetFundsDue(am.BigAdd(dueToPoster, weiSpent))
	if err != nil {
		return err
	}
	perUnitReward, err := ps.PerUnitReward()
	if err != nil {
		return err
	}
	fundsDueForRewards = am.BigAdd(fundsDueForRewards, am.BigMulByUint(am.UintToBig(unitsAllocated), perUnitReward))
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}

	// pay rewards, as much as possible
	paymentForRewards := am.BigMulByUint(am.UintToBig(perUnitReward), unitsAllocated)
	availableFunds := statedb.GetBalance(L1PricerFundsPoolAddress)
	if am.BigLessThan(availableFunds, paymentForRewards) {
		paymentForRewards = availableFunds
	}
	fundsDueForRewards = am.BigSub(fundsDueForRewards, paymentForRewards)
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}
	payRewardsTo, err := ps.PayRewardsTo()
	if err != nil {
		return err
	}
	err = util.TransferBalance(
		&L1PricerFundsPoolAddress, &payRewardsTo, paymentForRewards, evm, scenario, "batchPosterReward",
	)
	if err != nil {
		return err
	}
	availableFunds = statedb.GetBalance(L1PricerFundsPoolAddress)

	// settle up payments owed to the batch poster, as much as possible
	balanceDueToPoster, err := posterState.FundsDue()
	if err != nil {
		return err
	}
	balanceToTransfer := balanceDueToPoster
	if am.BigLessThan(availableFunds, balanceToTransfer) {
		balanceToTransfer = availableFunds
	}
	if balanceToTransfer.Sign() > 0 {
		addrToPay, err := posterState.PayTo()
		if err != nil {
			return err
		}
		err = util.TransferBalance(
			&L1PricerFundsPoolAddress, &addrToPay, balanceToTransfer, evm, scenario, "batchPosterRefund",
		)
		if err != nil {
			return err
		}
		balanceDueToPoster = am.BigSub(balanceDueToPoster, balanceToTransfer)
		err = posterState.SetFundsDue(balanceDueToPoster)
		if err != nil {
			return err
		}
	}

	// update time
	if err := ps.SetLastUpdateTime(updateTime); err != nil {
		return err
	}

	// adjust the price
	if unitsAllocated > 0 {
		totalFundsDue, err := batchPosterTable.TotalFundsDue()
		if err != nil {
			return err
		}
		fundsDueForRewards, err = ps.FundsDueForRewards()
		if err != nil {
			return err
		}
		surplus := am.BigSub(statedb.GetBalance(L1PricerFundsPoolAddress), am.BigAdd(totalFundsDue, fundsDueForRewards))

		inertia, err := ps.Inertia()
		if err != nil {
			return err
		}
		equilUnits, err := ps.EquilibrationUnits()
		if err != nil {
			return err
		}
		inertiaUnits := am.BigDivByUint(equilUnits, inertia)
		price, err := ps.PricePerUnit()
		if err != nil {
			return err
		}

		allocPlusInert := am.BigAddByUint(inertiaUnits, unitsAllocated)
		oldSurplus, err := ps.LastSurplus()
		if err != nil {
			return err
		}

		desiredDerivative := am.BigDiv(new(big.Int).Neg(surplus), equilUnits)
		actualDerivative := am.BigDivByUint(am.BigSub(surplus, oldSurplus), unitsAllocated)
		changeDerivativeBy := am.BigSub(desiredDerivative, actualDerivative)
		priceChange := am.BigDiv(am.BigMulByUint(changeDerivativeBy, unitsAllocated), allocPlusInert)

		if err := ps.SetLastSurplus(surplus, arbosVersion); err != nil {
			return err
		}
		newPrice := am.BigAdd(price, priceChange)
		if newPrice.Sign() < 0 {
			newPrice = common.Big0
		}
		if err := ps.SetPricePerUnit(newPrice); err != nil {
			return err
		}
	}
	return nil
}

func (ps *L1PricingState) _preVersion2_UpdateForBatchPosterSpending(
	statedb vm.StateDB,
	evm *vm.EVM,
	updateTime, currentTime uint64,
	batchPoster common.Address,
	weiSpent *big.Int,
	scenario util.TracingScenario,
) error {
	batchPosterTable := ps.BatchPosterTable()
	posterState, err := batchPosterTable.OpenPoster(batchPoster, true)
	if err != nil {
		return err
	}

	// compute previous shortfall
	totalFundsDue, err := batchPosterTable.TotalFundsDue()
	if err != nil {
		return err
	}
	fundsDueForRewards, err := ps.FundsDueForRewards()
	if err != nil {
		return err
	}
	oldSurplus := am.BigSub(statedb.GetBalance(L1PricerFundsPoolAddress), am.BigAdd(totalFundsDue, fundsDueForRewards))

	// compute allocation fraction -- will allocate updateTimeDelta/timeDelta fraction of units and funds to this update
	lastUpdateTime, err := ps.LastUpdateTime()
	if err != nil {
		return err
	}
	if lastUpdateTime == 0 && currentTime > 0 { // it's the first update, so there isn't a last update time
		lastUpdateTime = updateTime - 1
	}
	if updateTime >= currentTime || updateTime < lastUpdateTime {
		return nil // historically this returned an error
	}
	allocationNumerator := updateTime - lastUpdateTime
	allocationDenominator := currentTime - lastUpdateTime
	if allocationDenominator == 0 {
		allocationNumerator = 1
		allocationDenominator = 1
	}

	// allocate units to this update
	unitsSinceUpdate, err := ps.UnitsSinceUpdate()
	if err != nil {
		return err
	}
	unitsAllocated := unitsSinceUpdate * allocationNumerator / allocationDenominator
	unitsSinceUpdate -= unitsAllocated
	if err := ps.SetUnitsSinceUpdate(unitsSinceUpdate); err != nil {
		return err
	}

	dueToPoster, err := posterState.FundsDue()
	if err != nil {
		return err
	}
	err = posterState.SetFundsDue(am.BigAdd(dueToPoster, weiSpent))
	if err != nil {
		return err
	}
	perUnitReward, err := ps.PerUnitReward()
	if err != nil {
		return err
	}
	fundsDueForRewards = am.BigAdd(fundsDueForRewards, am.BigMulByUint(am.UintToBig(unitsAllocated), perUnitReward))
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}

	// allocate funds to this update
	collectedSinceUpdate := statedb.GetBalance(L1PricerFundsPoolAddress)
	availableFunds := am.BigDivByUint(am.BigMulByUint(collectedSinceUpdate, allocationNumerator), allocationDenominator)

	// pay rewards, as much as possible
	paymentForRewards := am.BigMulByUint(am.UintToBig(perUnitReward), unitsAllocated)
	if am.BigLessThan(availableFunds, paymentForRewards) {
		paymentForRewards = availableFunds
	}
	fundsDueForRewards = am.BigSub(fundsDueForRewards, paymentForRewards)
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}
	payRewardsTo, err := ps.PayRewardsTo()
	if err != nil {
		return err
	}
	err = util.TransferBalance(
		&L1PricerFundsPoolAddress, &payRewardsTo, paymentForRewards, evm, scenario, "batchPosterReward",
	)
	if err != nil {
		return err
	}
	availableFunds = am.BigSub(availableFunds, paymentForRewards)

	// settle up our batch poster payments owed, as much as possible
	allPosterAddrs, err := batchPosterTable.AllPosters(math.MaxUint64)
	if err != nil {
		return err
	}
	for _, posterAddr := range allPosterAddrs {
		poster, err := batchPosterTable.OpenPoster(posterAddr, false)
		if err != nil {
			return err
		}
		balanceDueToPoster, err := poster.FundsDue()
		if err != nil {
			return err
		}
		balanceToTransfer := balanceDueToPoster
		if am.BigLessThan(availableFunds, balanceToTransfer) {
			balanceToTransfer = availableFunds
		}
		if balanceToTransfer.Sign() > 0 {
			addrToPay, err := poster.PayTo()
			if err != nil {
				return err
			}
			err = util.TransferBalance(
				&L1PricerFundsPoolAddress, &addrToPay, balanceToTransfer, evm, scenario, "batchPosterRefund",
			)
			if err != nil {
				return err
			}
			availableFunds = am.BigSub(availableFunds, balanceToTransfer)
			balanceDueToPoster = am.BigSub(balanceDueToPoster, balanceToTransfer)
			err = poster.SetFundsDue(balanceDueToPoster)
			if err != nil {
				return err
			}
		}
	}

	// update time
	if err := ps.SetLastUpdateTime(updateTime); err != nil {
		return err
	}

	// adjust the price
	if unitsAllocated > 0 {
		totalFundsDue, err = batchPosterTable.TotalFundsDue()
		if err != nil {
			return err
		}
		fundsDueForRewards, err = ps.FundsDueForRewards()
		if err != nil {
			return err
		}
		surplus := am.BigSub(statedb.GetBalance(L1PricerFundsPoolAddress), am.BigAdd(totalFundsDue, fundsDueForRewards))

		inertia, err := ps.Inertia()
		if err != nil {
			return err
		}
		equilUnits, err := ps.EquilibrationUnits()
		if err != nil {
			return err
		}
		inertiaUnits := am.BigDivByUint(equilUnits, inertia)
		price, err := ps.PricePerUnit()
		if err != nil {
			return err
		}

		allocPlusInert := am.BigAddByUint(inertiaUnits, unitsAllocated)
		priceChange := am.BigDiv(
			am.BigSub(
				am.BigMul(surplus, am.BigSub(equilUnits, common.Big1)),
				am.BigMul(oldSurplus, equilUnits),
			),
			am.BigMul(equilUnits, allocPlusInert),
		)

		newPrice := am.BigAdd(price, priceChange)
		if newPrice.Sign() < 0 {
			newPrice = common.Big0
		}
		if err := ps.SetPricePerUnit(newPrice); err != nil {
			return err
		}
	}
	return nil
}

'''
'''--- arbos/l1pricing/l1pricing.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"
	"sync/atomic"

	"github.com/ethereum/go-ethereum/crypto"

	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/util/arbmath"
	am "github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
)

type L1PricingState struct {
	storage *storage.Storage

	// parameters
	batchPosterTable   *BatchPostersTable
	payRewardsTo       storage.StorageBackedAddress
	equilibrationUnits storage.StorageBackedBigUint
	inertia            storage.StorageBackedUint64
	perUnitReward      storage.StorageBackedUint64
	// variables
	lastUpdateTime     storage.StorageBackedUint64 // timestamp of the last update from L1 that we processed
	fundsDueForRewards storage.StorageBackedBigInt
	// funds collected since update are recorded as the balance in account L1PricerFundsPoolAddress
	unitsSinceUpdate     storage.StorageBackedUint64  // calldata units collected for since last update
	pricePerUnit         storage.StorageBackedBigUint // current price per calldata unit
	lastSurplus          storage.StorageBackedBigInt  // introduced in ArbOS version 2
	perBatchGasCost      storage.StorageBackedInt64   // introduced in ArbOS version 3
	amortizedCostCapBips storage.StorageBackedUint64  // in basis points; introduced in ArbOS version 3
	l1FeesAvailable      storage.StorageBackedBigUint
}

var (
	BatchPosterTableKey      = []byte{0}
	BatchPosterAddress       = common.HexToAddress("0xA4B000000000000000000073657175656e636572")
	BatchPosterPayToAddress  = BatchPosterAddress
	L1PricerFundsPoolAddress = common.HexToAddress("0xA4B00000000000000000000000000000000000f6")

	ErrInvalidTime = errors.New("invalid timestamp")
)

const (
	payRewardsToOffset uint64 = iota
	equilibrationUnitsOffset
	inertiaOffset
	perUnitRewardOffset
	lastUpdateTimeOffset
	fundsDueForRewardsOffset
	unitsSinceOffset
	pricePerUnitOffset
	lastSurplusOffset
	perBatchGasCostOffset
	amortizedCostCapBipsOffset
	l1FeesAvailableOffset
)

const (
	InitialInertia            = 10
	InitialPerUnitReward      = 10
	InitialPerBatchGasCostV6  = 100_000
	InitialPerBatchGasCostV12 = 210_000 // overridden as part of the upgrade
)

// one minute at 100000 bytes / sec
var InitialEquilibrationUnitsV0 = arbmath.UintToBig(60 * params.TxDataNonZeroGasEIP2028 * 100000)
var InitialEquilibrationUnitsV6 = arbmath.UintToBig(params.TxDataNonZeroGasEIP2028 * 10000000)

func InitializeL1PricingState(sto *storage.Storage, initialRewardsRecipient common.Address, initialL1BaseFee *big.Int) error {
	bptStorage := sto.OpenCachedSubStorage(BatchPosterTableKey)
	if err := InitializeBatchPostersTable(bptStorage); err != nil {
		return err
	}
	bpTable := OpenBatchPostersTable(bptStorage)
	if _, err := bpTable.AddPoster(BatchPosterAddress, BatchPosterPayToAddress); err != nil {
		return err
	}
	if err := sto.SetByUint64(payRewardsToOffset, util.AddressToHash(initialRewardsRecipient)); err != nil {
		return err
	}
	equilibrationUnits := sto.OpenStorageBackedBigUint(equilibrationUnitsOffset)
	if err := equilibrationUnits.SetChecked(InitialEquilibrationUnitsV0); err != nil {
		return err
	}
	if err := sto.SetUint64ByUint64(inertiaOffset, InitialInertia); err != nil {
		return err
	}
	fundsDueForRewards := sto.OpenStorageBackedBigInt(fundsDueForRewardsOffset)
	if err := fundsDueForRewards.SetChecked(common.Big0); err != nil {
		return err
	}
	if err := sto.SetUint64ByUint64(perUnitRewardOffset, InitialPerUnitReward); err != nil {
		return err
	}
	pricePerUnit := sto.OpenStorageBackedBigInt(pricePerUnitOffset)
	if err := pricePerUnit.SetSaturatingWithWarning(initialL1BaseFee, "initial L1 base fee (storing in price per unit)"); err != nil {
		return err
	}
	return nil
}

func OpenL1PricingState(sto *storage.Storage) *L1PricingState {
	return &L1PricingState{
		sto,
		OpenBatchPostersTable(sto.OpenCachedSubStorage(BatchPosterTableKey)),
		sto.OpenStorageBackedAddress(payRewardsToOffset),
		sto.OpenStorageBackedBigUint(equilibrationUnitsOffset),
		sto.OpenStorageBackedUint64(inertiaOffset),
		sto.OpenStorageBackedUint64(perUnitRewardOffset),
		sto.OpenStorageBackedUint64(lastUpdateTimeOffset),
		sto.OpenStorageBackedBigInt(fundsDueForRewardsOffset),
		sto.OpenStorageBackedUint64(unitsSinceOffset),
		sto.OpenStorageBackedBigUint(pricePerUnitOffset),
		sto.OpenStorageBackedBigInt(lastSurplusOffset),
		sto.OpenStorageBackedInt64(perBatchGasCostOffset),
		sto.OpenStorageBackedUint64(amortizedCostCapBipsOffset),
		sto.OpenStorageBackedBigUint(l1FeesAvailableOffset),
	}
}

func (ps *L1PricingState) BatchPosterTable() *BatchPostersTable {
	return ps.batchPosterTable
}

func (ps *L1PricingState) PayRewardsTo() (common.Address, error) {
	return ps.payRewardsTo.Get()
}

func (ps *L1PricingState) SetPayRewardsTo(addr common.Address) error {
	return ps.payRewardsTo.Set(addr)
}

func (ps *L1PricingState) GetRewardsRecepient() (common.Address, error) {
	return ps.payRewardsTo.Get()
}

func (ps *L1PricingState) EquilibrationUnits() (*big.Int, error) {
	return ps.equilibrationUnits.Get()
}

func (ps *L1PricingState) SetEquilibrationUnits(equilUnits *big.Int) error {
	return ps.equilibrationUnits.SetChecked(equilUnits)
}

func (ps *L1PricingState) Inertia() (uint64, error) {
	return ps.inertia.Get()
}

func (ps *L1PricingState) SetInertia(inertia uint64) error {
	return ps.inertia.Set(inertia)
}

func (ps *L1PricingState) PerUnitReward() (uint64, error) {
	return ps.perUnitReward.Get()
}

func (ps *L1PricingState) SetPerUnitReward(weiPerUnit uint64) error {
	return ps.perUnitReward.Set(weiPerUnit)
}

func (ps *L1PricingState) GetRewardsRate() (uint64, error) {
	return ps.perUnitReward.Get()
}

func (ps *L1PricingState) LastUpdateTime() (uint64, error) {
	return ps.lastUpdateTime.Get()
}

func (ps *L1PricingState) SetLastUpdateTime(t uint64) error {
	return ps.lastUpdateTime.Set(t)
}

func (ps *L1PricingState) FundsDueForRewards() (*big.Int, error) {
	return ps.fundsDueForRewards.Get()
}

func (ps *L1PricingState) SetFundsDueForRewards(amt *big.Int) error {
	return ps.fundsDueForRewards.SetSaturatingWithWarning(amt, "L1 pricer funds due for rewards")

}

func (ps *L1PricingState) UnitsSinceUpdate() (uint64, error) {
	return ps.unitsSinceUpdate.Get()
}

func (ps *L1PricingState) SetUnitsSinceUpdate(units uint64) error {
	return ps.unitsSinceUpdate.Set(units)
}

func (ps *L1PricingState) LastSurplus() (*big.Int, error) {
	return ps.lastSurplus.Get()
}

func (ps *L1PricingState) SetLastSurplus(val *big.Int, arbosVersion uint64) error {
	if arbosVersion < 7 {
		return ps.lastSurplus.Set_preVersion7(val)
	}
	return ps.lastSurplus.SetSaturatingWithWarning(val, "L1 pricer last surplus")
}

func (ps *L1PricingState) AddToUnitsSinceUpdate(units uint64) error {
	oldUnits, err := ps.unitsSinceUpdate.Get()
	if err != nil {
		return err
	}
	return ps.unitsSinceUpdate.Set(oldUnits + units)
}

func (ps *L1PricingState) PricePerUnit() (*big.Int, error) {
	return ps.pricePerUnit.Get()
}

func (ps *L1PricingState) SetPricePerUnit(price *big.Int) error {
	return ps.pricePerUnit.SetChecked(price)
}

func (ps *L1PricingState) PerBatchGasCost() (int64, error) {
	return ps.perBatchGasCost.Get()
}

func (ps *L1PricingState) SetPerBatchGasCost(cost int64) error {
	return ps.perBatchGasCost.Set(cost)
}

func (ps *L1PricingState) AmortizedCostCapBips() (uint64, error) {
	return ps.amortizedCostCapBips.Get()
}

func (ps *L1PricingState) SetAmortizedCostCapBips(cap uint64) error {
	return ps.amortizedCostCapBips.Set(cap)
}

func (ps *L1PricingState) L1FeesAvailable() (*big.Int, error) {
	return ps.l1FeesAvailable.Get()
}

func (ps *L1PricingState) SetL1FeesAvailable(val *big.Int) error {
	return ps.l1FeesAvailable.SetChecked(val)
}

func (ps *L1PricingState) AddToL1FeesAvailable(delta *big.Int) (*big.Int, error) {
	old, err := ps.L1FeesAvailable()
	if err != nil {
		return nil, err
	}
	new := new(big.Int).Add(old, delta)
	if err := ps.SetL1FeesAvailable(new); err != nil {
		return nil, err
	}
	return new, nil
}

func (ps *L1PricingState) TransferFromL1FeesAvailable(
	recipient common.Address,
	amount *big.Int,
	evm *vm.EVM,
	scenario util.TracingScenario,
	purpose string,
) (*big.Int, error) {
	if err := util.TransferBalance(&L1PricerFundsPoolAddress, &recipient, amount, evm, scenario, purpose); err != nil {
		return nil, err
	}
	old, err := ps.L1FeesAvailable()
	if err != nil {
		return nil, err
	}
	updated := new(big.Int).Sub(old, amount)
	if updated.Sign() < 0 {
		return nil, core.ErrInsufficientFunds
	}
	if err := ps.SetL1FeesAvailable(updated); err != nil {
		return nil, err
	}
	return updated, nil
}

// UpdateForBatchPosterSpending updates the pricing model based on a payment by a batch poster
func (ps *L1PricingState) UpdateForBatchPosterSpending(
	statedb vm.StateDB,
	evm *vm.EVM,
	arbosVersion uint64,
	updateTime, currentTime uint64,
	batchPoster common.Address,
	weiSpent *big.Int,
	l1Basefee *big.Int,
	scenario util.TracingScenario,
) error {
	if arbosVersion < 10 {
		return ps._preversion10_UpdateForBatchPosterSpending(statedb, evm, arbosVersion, updateTime, currentTime, batchPoster, weiSpent, l1Basefee, scenario)
	}

	batchPosterTable := ps.BatchPosterTable()
	posterState, err := batchPosterTable.OpenPoster(batchPoster, true)
	if err != nil {
		return err
	}

	fundsDueForRewards, err := ps.FundsDueForRewards()
	if err != nil {
		return err
	}

	l1FeesAvailable, err := ps.L1FeesAvailable()
	if err != nil {
		return err
	}

	// compute allocation fraction -- will allocate updateTimeDelta/timeDelta fraction of units and funds to this update
	lastUpdateTime, err := ps.LastUpdateTime()
	if err != nil {
		return err
	}
	if lastUpdateTime == 0 && updateTime > 0 { // it's the first update, so there isn't a last update time
		lastUpdateTime = updateTime - 1
	}
	if updateTime > currentTime || updateTime < lastUpdateTime {
		return ErrInvalidTime
	}
	allocationNumerator := updateTime - lastUpdateTime
	allocationDenominator := currentTime - lastUpdateTime
	if allocationDenominator == 0 {
		allocationNumerator = 1
		allocationDenominator = 1
	}

	// allocate units to this update
	unitsSinceUpdate, err := ps.UnitsSinceUpdate()
	if err != nil {
		return err
	}
	unitsAllocated := am.SaturatingUMul(unitsSinceUpdate, allocationNumerator) / allocationDenominator
	unitsSinceUpdate -= unitsAllocated
	if err := ps.SetUnitsSinceUpdate(unitsSinceUpdate); err != nil {
		return err
	}

	// impose cap on amortized cost, if there is one
	if arbosVersion >= 3 {
		amortizedCostCapBips, err := ps.AmortizedCostCapBips()
		if err != nil {
			return err
		}
		if amortizedCostCapBips != 0 {
			weiSpentCap := am.BigMulByBips(
				am.BigMulByUint(l1Basefee, unitsAllocated),
				am.SaturatingCastToBips(amortizedCostCapBips),
			)
			if am.BigLessThan(weiSpentCap, weiSpent) {
				// apply the cap on assignment of amortized cost;
				// the difference will be a loss for the batch poster
				weiSpent = weiSpentCap
			}
		}
	}

	dueToPoster, err := posterState.FundsDue()
	if err != nil {
		return err
	}
	err = posterState.SetFundsDue(am.BigAdd(dueToPoster, weiSpent))
	if err != nil {
		return err
	}
	perUnitReward, err := ps.PerUnitReward()
	if err != nil {
		return err
	}
	fundsDueForRewards = am.BigAdd(fundsDueForRewards, am.BigMulByUint(am.UintToBig(unitsAllocated), perUnitReward))
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}

	// pay rewards, as much as possible
	paymentForRewards := am.BigMulByUint(am.UintToBig(perUnitReward), unitsAllocated)
	if am.BigLessThan(l1FeesAvailable, paymentForRewards) {
		paymentForRewards = l1FeesAvailable
	}
	fundsDueForRewards = am.BigSub(fundsDueForRewards, paymentForRewards)
	if err := ps.SetFundsDueForRewards(fundsDueForRewards); err != nil {
		return err
	}
	payRewardsTo, err := ps.PayRewardsTo()
	if err != nil {
		return err
	}
	l1FeesAvailable, err = ps.TransferFromL1FeesAvailable(
		payRewardsTo, paymentForRewards, evm, scenario, "batchPosterReward",
	)
	if err != nil {
		return err
	}

	// settle up payments owed to the batch poster, as much as possible
	balanceDueToPoster, err := posterState.FundsDue()
	if err != nil {
		return err
	}
	balanceToTransfer := balanceDueToPoster
	if am.BigLessThan(l1FeesAvailable, balanceToTransfer) {
		balanceToTransfer = l1FeesAvailable
	}
	if balanceToTransfer.Sign() > 0 {
		addrToPay, err := posterState.PayTo()
		if err != nil {
			return err
		}
		l1FeesAvailable, err = ps.TransferFromL1FeesAvailable(
			addrToPay, balanceToTransfer, evm, scenario, "batchPosterRefund",
		)
		if err != nil {
			return err
		}
		balanceDueToPoster = am.BigSub(balanceDueToPoster, balanceToTransfer)
		err = posterState.SetFundsDue(balanceDueToPoster)
		if err != nil {
			return err
		}
	}

	// update time
	if err := ps.SetLastUpdateTime(updateTime); err != nil {
		return err
	}

	// adjust the price
	if unitsAllocated > 0 {
		totalFundsDue, err := batchPosterTable.TotalFundsDue()
		if err != nil {
			return err
		}
		fundsDueForRewards, err = ps.FundsDueForRewards()
		if err != nil {
			return err
		}
		surplus := am.BigSub(l1FeesAvailable, am.BigAdd(totalFundsDue, fundsDueForRewards))

		inertia, err := ps.Inertia()
		if err != nil {
			return err
		}
		equilUnits, err := ps.EquilibrationUnits()
		if err != nil {
			return err
		}
		inertiaUnits := am.BigDivByUint(equilUnits, inertia)
		price, err := ps.PricePerUnit()
		if err != nil {
			return err
		}

		allocPlusInert := am.BigAddByUint(inertiaUnits, unitsAllocated)
		oldSurplus, err := ps.LastSurplus()
		if err != nil {
			return err
		}

		desiredDerivative := am.BigDiv(new(big.Int).Neg(surplus), equilUnits)
		actualDerivative := am.BigDivByUint(am.BigSub(surplus, oldSurplus), unitsAllocated)
		changeDerivativeBy := am.BigSub(desiredDerivative, actualDerivative)
		priceChange := am.BigDiv(am.BigMulByUint(changeDerivativeBy, unitsAllocated), allocPlusInert)

		if err := ps.SetLastSurplus(surplus, arbosVersion); err != nil {
			return err
		}
		newPrice := am.BigAdd(price, priceChange)
		if newPrice.Sign() < 0 {
			newPrice = common.Big0
		}
		if err := ps.SetPricePerUnit(newPrice); err != nil {
			return err
		}
	}
	return nil
}

func (ps *L1PricingState) getPosterUnitsWithoutCache(tx *types.Transaction, posterAddr common.Address, brotliCompressionLevel uint64) uint64 {

	if posterAddr != BatchPosterAddress {
		return 0
	}
	txBytes, merr := tx.MarshalBinary()
	txType := tx.Type()
	if !util.TxTypeHasPosterCosts(txType) || merr != nil {
		return 0
	}

	l1Bytes, err := byteCountAfterBrotliLevel(txBytes, int(brotliCompressionLevel))
	if err != nil {
		panic(fmt.Sprintf("failed to compress tx: %v", err))
	}
	return l1Bytes * params.TxDataNonZeroGasEIP2028
}

// GetPosterInfo returns the poster cost and the calldata units for a transaction
func (ps *L1PricingState) GetPosterInfo(tx *types.Transaction, poster common.Address, brotliCompressionLevel uint64) (*big.Int, uint64) {
	if poster != BatchPosterAddress {
		return common.Big0, 0
	}
	units := atomic.LoadUint64(&tx.CalldataUnits)
	if units == 0 {
		units = ps.getPosterUnitsWithoutCache(tx, poster, brotliCompressionLevel)
		atomic.StoreUint64(&tx.CalldataUnits, units)
	}

	// Approximate the l1 fee charged for posting this tx's calldata
	pricePerUnit, _ := ps.PricePerUnit()
	return am.BigMulByUint(pricePerUnit, units), units
}

// We don't have the full tx in gas estimation, so we assume it might be a bit bigger in practice.
const estimationPaddingUnits = 16 * params.TxDataNonZeroGasEIP2028
const estimationPaddingBasisPoints = 100

var randomNonce = binary.BigEndian.Uint64(crypto.Keccak256([]byte("Nonce"))[:8])
var randomGasTipCap = new(big.Int).SetBytes(crypto.Keccak256([]byte("GasTipCap"))[:4])
var randomGasFeeCap = new(big.Int).SetBytes(crypto.Keccak256([]byte("GasFeeCap"))[:4])
var RandomGas = uint64(binary.BigEndian.Uint32(crypto.Keccak256([]byte("Gas"))[:4]))
var randV = arbmath.BigMulByUint(params.ArbitrumOneChainConfig().ChainID, 3)
var randR = crypto.Keccak256Hash([]byte("R")).Big()
var randS = crypto.Keccak256Hash([]byte("S")).Big()

// The returned tx will be invalid, likely for a number of reasons such as an invalid signature.
// It's only used to check how large it is after brotli level 0 compression.
func makeFakeTxForMessage(message *core.Message) *types.Transaction {
	nonce := message.Nonce
	if nonce == 0 {
		nonce = randomNonce
	}
	gasTipCap := message.GasTipCap
	if gasTipCap.Sign() == 0 {
		gasTipCap = randomGasTipCap
	}
	gasFeeCap := message.GasFeeCap
	if gasFeeCap.Sign() == 0 {
		gasFeeCap = randomGasFeeCap
	}
	// During gas estimation, we don't want the gas limit variability to change the L1 cost.
	gas := message.GasLimit
	if gas == 0 || message.TxRunMode == core.MessageGasEstimationMode {
		gas = RandomGas
	}
	return types.NewTx(&types.DynamicFeeTx{
		Nonce:      nonce,
		GasTipCap:  gasTipCap,
		GasFeeCap:  gasFeeCap,
		Gas:        gas,
		To:         message.To,
		Value:      message.Value,
		Data:       message.Data,
		AccessList: message.AccessList,
		V:          randV,
		R:          randR,
		S:          randS,
	})
}

func (ps *L1PricingState) PosterDataCost(message *core.Message, poster common.Address, brotliCompressionLevel uint64) (*big.Int, uint64) {
	tx := message.Tx
	if tx != nil {
		return ps.GetPosterInfo(tx, poster, brotliCompressionLevel)
	}

	// Otherwise, we don't have an underlying transaction, so we're likely in gas estimation.
	// We'll instead make a fake tx from the message info we do have, and then pad our cost a bit to be safe.
	tx = makeFakeTxForMessage(message)
	units := ps.getPosterUnitsWithoutCache(tx, poster, brotliCompressionLevel)
	units = arbmath.UintMulByBips(units+estimationPaddingUnits, arbmath.OneInBips+estimationPaddingBasisPoints)
	pricePerUnit, _ := ps.PricePerUnit()
	return am.BigMulByUint(pricePerUnit, units), units
}

func byteCountAfterBrotliLevel(input []byte, level int) (uint64, error) {
	compressed, err := arbcompress.CompressLevel(input, level)
	if err != nil {
		return 0, err
	}
	return uint64(len(compressed)), nil
}

'''
'''--- arbos/l1pricing/l1pricing_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l1pricing

import (
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
)

func TestL1PriceUpdate(t *testing.T) {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	initialPriceEstimate := big.NewInt(123 * params.GWei)
	err := InitializeL1PricingState(sto, common.Address{}, initialPriceEstimate)
	Require(t, err)
	ps := OpenL1PricingState(sto)

	tyme, err := ps.LastUpdateTime()
	Require(t, err)
	if tyme != 0 {
		Fail(t)
	}

	priceEstimate, err := ps.PricePerUnit()
	Require(t, err)
	if priceEstimate.Cmp(initialPriceEstimate) != 0 {
		Fail(t)
	}
}

'''
'''--- arbos/l1pricing_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/burn"
)

type l1PricingTest struct {
	unitReward              uint64
	unitsPerSecond          uint64
	fundsCollectedPerSecond uint64
	fundsSpent              uint64
	amortizationCapBips     uint64
	l1BasefeeGwei           uint64
}

type l1TestExpectedResults struct {
	rewardRecipientBalance *big.Int
	unitsRemaining         uint64
	fundsReceived          *big.Int
	fundsStillHeld         *big.Int
}

func TestL1Pricing(t *testing.T) {
	inputs := []*l1PricingTest{
		{
			unitReward:              10,
			unitsPerSecond:          78,
			fundsCollectedPerSecond: 7800,
			fundsSpent:              3000,
			amortizationCapBips:     math.MaxUint64,
			l1BasefeeGwei:           10,
		},
		{
			unitReward:              10,
			unitsPerSecond:          78,
			fundsCollectedPerSecond: 1313,
			fundsSpent:              3000,
			amortizationCapBips:     math.MaxUint64,
			l1BasefeeGwei:           10,
		},
		{
			unitReward:              10,
			unitsPerSecond:          78,
			fundsCollectedPerSecond: 31,
			fundsSpent:              3000,
			amortizationCapBips:     math.MaxUint64,
			l1BasefeeGwei:           10,
		},
		{
			unitReward:              10,
			unitsPerSecond:          78,
			fundsCollectedPerSecond: 7800,
			fundsSpent:              3000,
			amortizationCapBips:     100,
			l1BasefeeGwei:           10,
		},
		{
			unitReward:              0,
			unitsPerSecond:          78,
			fundsCollectedPerSecond: 7800 * params.GWei,
			fundsSpent:              3000 * params.GWei,
			amortizationCapBips:     100,
			l1BasefeeGwei:           10,
		},
	}
	for _, input := range inputs {
		expectedResult := expectedResultsForL1Test(input)
		_testL1PricingFundsDue(t, input, expectedResult)
	}
}

func expectedResultsForL1Test(input *l1PricingTest) *l1TestExpectedResults {
	ret := &l1TestExpectedResults{}
	availableFunds := arbmath.UintToBig(3 * input.fundsCollectedPerSecond)
	uncappedAvailableFunds := availableFunds
	if input.amortizationCapBips != 0 {
		availableFundsCap := arbmath.BigMulByBips(arbmath.BigMulByUint(
			arbmath.UintToBig(input.unitsPerSecond),
			input.l1BasefeeGwei*params.GWei),
			arbmath.SaturatingCastToBips(input.amortizationCapBips),
		)
		if arbmath.BigLessThan(availableFundsCap, availableFunds) {
			availableFunds = availableFundsCap
		}
	}
	fundsWantedForRewards := big.NewInt(int64(input.unitReward * input.unitsPerSecond))
	unitsAllocated := arbmath.UintToBig(input.unitsPerSecond)
	if arbmath.BigLessThan(availableFunds, fundsWantedForRewards) {
		ret.rewardRecipientBalance = availableFunds
	} else {
		ret.rewardRecipientBalance = fundsWantedForRewards
	}
	availableFunds = arbmath.BigSub(availableFunds, ret.rewardRecipientBalance)
	uncappedAvailableFunds = arbmath.BigSub(uncappedAvailableFunds, ret.rewardRecipientBalance)
	ret.unitsRemaining = (3 * input.unitsPerSecond) - unitsAllocated.Uint64()

	maxCollectable := big.NewInt(int64(input.fundsSpent))
	if arbmath.BigLessThan(availableFunds, maxCollectable) {
		maxCollectable = availableFunds
	}
	ret.fundsReceived = maxCollectable
	uncappedAvailableFunds = arbmath.BigSub(uncappedAvailableFunds, maxCollectable)
	ret.fundsStillHeld = uncappedAvailableFunds

	return ret
}

func _testL1PricingFundsDue(t *testing.T, testParams *l1PricingTest, expectedResults *l1TestExpectedResults) {
	evm := newMockEVMForTesting()
	burner := burn.NewSystemBurner(nil, false)
	arbosSt, err := arbosState.OpenArbosState(evm.StateDB, burner)
	Require(t, err)

	l1p := arbosSt.L1PricingState()
	err = l1p.SetPerUnitReward(testParams.unitReward)
	Require(t, err)
	rewardAddress := common.Address{137}
	err = l1p.SetPayRewardsTo(rewardAddress)
	Require(t, err)

	posterTable := l1p.BatchPosterTable()

	// check initial funds state
	rewardsDue, err := l1p.FundsDueForRewards()
	Require(t, err)
	if rewardsDue.Sign() != 0 {
		Fail(t)
	}
	if evm.StateDB.GetBalance(rewardAddress).Sign() != 0 {
		Fail(t)
	}
	posterAddrs, err := posterTable.AllPosters(math.MaxUint64)
	Require(t, err)
	if len(posterAddrs) != 1 {
		Fail(t)
	}
	firstPoster := posterAddrs[0]
	firstPayTo := common.Address{1, 2}
	poster, err := posterTable.OpenPoster(firstPoster, true)
	Require(t, err)
	due, err := poster.FundsDue()
	Require(t, err)
	if due.Sign() != 0 {
		Fail(t)
	}
	err = poster.SetPayTo(firstPayTo)
	Require(t, err)

	// add another poster
	secondPoster := common.Address{3, 4, 5}
	secondPayTo := common.Address{6, 7}
	_, err = posterTable.AddPoster(secondPoster, secondPayTo)
	Require(t, err)

	// create some fake collection
	balanceAdded := big.NewInt(int64(testParams.fundsCollectedPerSecond * 3))
	unitsAdded := testParams.unitsPerSecond * 3
	evm.StateDB.AddBalance(l1pricing.L1PricerFundsPoolAddress, balanceAdded)
	err = l1p.SetL1FeesAvailable(balanceAdded)
	Require(t, err)
	err = l1p.SetUnitsSinceUpdate(unitsAdded)
	Require(t, err)

	// submit a fake spending update, then check that balances are correct
	err = l1p.SetAmortizedCostCapBips(testParams.amortizationCapBips)
	Require(t, err)
	version := arbosSt.ArbOSVersion()
	scenario := util.TracingDuringEVM
	err = l1p.UpdateForBatchPosterSpending(
		evm.StateDB, evm, version, 1, 3, firstPoster, arbmath.UintToBig(testParams.fundsSpent), arbmath.UintToBig(testParams.l1BasefeeGwei*params.GWei), scenario,
	)
	Require(t, err)
	rewardRecipientBalance := evm.StateDB.GetBalance(rewardAddress)
	if !arbmath.BigEquals(rewardRecipientBalance, expectedResults.rewardRecipientBalance) {
		Fail(t, rewardRecipientBalance, expectedResults.rewardRecipientBalance)
	}
	unitsRemaining, err := l1p.UnitsSinceUpdate()
	Require(t, err)
	if unitsRemaining != expectedResults.unitsRemaining {
		Fail(t, unitsRemaining, expectedResults.unitsRemaining)
	}
	fundsReceived := evm.StateDB.GetBalance(firstPayTo)
	if !arbmath.BigEquals(fundsReceived, expectedResults.fundsReceived) {
		Fail(t, fundsReceived, expectedResults.fundsReceived)
	}
	fundsStillHeld := evm.StateDB.GetBalance(l1pricing.L1PricerFundsPoolAddress)
	if !arbmath.BigEquals(fundsStillHeld, expectedResults.fundsStillHeld) {
		Fail(t, fundsStillHeld, expectedResults.fundsStillHeld)
	}
	fundsAvail, err := l1p.L1FeesAvailable()
	Require(t, err)
	if fundsStillHeld.Cmp(fundsAvail) != 0 {
		Fail(t, fundsStillHeld, fundsAvail)
	}
}

func TestUpdateTimeUpgradeBehavior(t *testing.T) {
	evm := newMockEVMForTesting()
	burner := burn.NewSystemBurner(nil, false)
	arbosSt, err := arbosState.OpenArbosState(evm.StateDB, burner)
	Require(t, err)

	l1p := arbosSt.L1PricingState()
	amount := arbmath.UintToBig(10 * params.GWei)
	poster := common.Address{3, 4, 5}
	_, err = l1p.BatchPosterTable().AddPoster(poster, poster)
	Require(t, err)

	// In the past this would have errored due to an invalid timestamp.
	// We don't want to error since it'd create noise in the console,
	// so instead let's check that nothing happened
	statedb, ok := evm.StateDB.(*state.StateDB)
	if !ok {
		panic("not a statedb")
	}
	stateCheck(t, statedb, false, "uh oh, nothing should have happened", func() {
		Require(t, l1p.UpdateForBatchPosterSpending(
			evm.StateDB, evm, 1, 1, 1, poster, common.Big1, amount, util.TracingDuringEVM,
		))
	})

	Require(t, l1p.UpdateForBatchPosterSpending(
		evm.StateDB, evm, 3, 1, 1, poster, common.Big1, amount, util.TracingDuringEVM,
	))
}

func TestL1PriceEquilibrationUp(t *testing.T) {
	_testL1PriceEquilibration(t, big.NewInt(1_000_000_000), big.NewInt(5_000_000_000))
}

func TestL1PriceEquilibrationDown(t *testing.T) {
	_testL1PriceEquilibration(t, big.NewInt(5_000_000_000), big.NewInt(1_000_000_000))
}

func TestL1PriceEquilibrationConstant(t *testing.T) {
	_testL1PriceEquilibration(t, big.NewInt(2_000_000_000), big.NewInt(2_000_000_000))
}

func _testL1PriceEquilibration(t *testing.T, initialL1BasefeeEstimate *big.Int, equilibriumL1BasefeeEstimate *big.Int) {
	evm := newMockEVMForTesting()
	stateDb := evm.StateDB
	state, err := arbosState.OpenArbosState(stateDb, burn.NewSystemBurner(nil, false))
	Require(t, err)

	l1p := state.L1PricingState()
	Require(t, l1p.SetPerUnitReward(0))
	Require(t, l1p.SetPricePerUnit(initialL1BasefeeEstimate))

	bpAddr := common.Address{3, 4, 5, 6}
	l1PoolAddress := l1pricing.L1PricerFundsPoolAddress
	for i := 0; i < 10; i++ {
		unitsToAdd := l1pricing.InitialEquilibrationUnitsV6.Uint64()
		oldUnits, err := l1p.UnitsSinceUpdate()
		Require(t, err)
		err = l1p.SetUnitsSinceUpdate(oldUnits + unitsToAdd)
		Require(t, err)
		currentPricePerUnit, err := l1p.PricePerUnit()
		Require(t, err)
		feesToAdd := arbmath.BigMulByUint(currentPricePerUnit, unitsToAdd)
		util.MintBalance(&l1PoolAddress, feesToAdd, evm, util.TracingBeforeEVM, "test")
		err = l1p.UpdateForBatchPosterSpending(
			evm.StateDB,
			evm,
			3,
			uint64(10*(i+1)),
			uint64(10*(i+1)+5),
			bpAddr,
			arbmath.BigMulByUint(equilibriumL1BasefeeEstimate, unitsToAdd),
			equilibriumL1BasefeeEstimate,
			util.TracingBeforeEVM,
		)
		Require(t, err)
	}
	expectedMovement := arbmath.BigSub(equilibriumL1BasefeeEstimate, initialL1BasefeeEstimate)
	actualPricePerUnit, err := l1p.PricePerUnit()
	Require(t, err)
	actualMovement := arbmath.BigSub(actualPricePerUnit, initialL1BasefeeEstimate)
	if expectedMovement.Sign() != actualMovement.Sign() {
		Fail(t, "L1 data fee moved in wrong direction", initialL1BasefeeEstimate, equilibriumL1BasefeeEstimate, actualPricePerUnit)
	}
	expectedMovement = new(big.Int).Abs(expectedMovement)
	actualMovement = new(big.Int).Abs(actualMovement)
	if !_withinOnePercent(expectedMovement, actualMovement) {
		Fail(t, "Expected vs actual movement are too far apart", expectedMovement, actualMovement)
	}
}

func _withinOnePercent(v1, v2 *big.Int) bool {
	if arbmath.BigMulByUint(v1, 100).Cmp(arbmath.BigMulByUint(v2, 101)) > 0 {
		return false
	}
	if arbmath.BigMulByUint(v2, 100).Cmp(arbmath.BigMulByUint(v1, 101)) > 0 {
		return false
	}
	return true
}

func newMockEVMForTesting() *vm.EVM {
	chainConfig := params.ArbitrumDevTestChainConfig()
	_, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	context := vm.BlockContext{
		BlockNumber: big.NewInt(0),
		GasLimit:    ^uint64(0),
		Time:        0,
	}
	evm := vm.NewEVM(context, vm.TxContext{}, statedb, chainConfig, vm.Config{})
	evm.ProcessingHook = &TxProcessor{}
	return evm
}

'''
'''--- arbos/l2pricing/l2pricing.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l2pricing

import (
	"math/big"

	"github.com/offchainlabs/nitro/arbos/storage"
)

type L2PricingState struct {
	storage             *storage.Storage
	speedLimitPerSecond storage.StorageBackedUint64
	perBlockGasLimit    storage.StorageBackedUint64
	baseFeeWei          storage.StorageBackedBigUint
	minBaseFeeWei       storage.StorageBackedBigUint
	gasBacklog          storage.StorageBackedUint64
	pricingInertia      storage.StorageBackedUint64
	backlogTolerance    storage.StorageBackedUint64
}

const (
	speedLimitPerSecondOffset uint64 = iota
	perBlockGasLimitOffset
	baseFeeWeiOffset
	minBaseFeeWeiOffset
	gasBacklogOffset
	pricingInertiaOffset
	backlogToleranceOffset
)

const GethBlockGasLimit = 1 << 50

func InitializeL2PricingState(sto *storage.Storage) error {
	_ = sto.SetUint64ByUint64(speedLimitPerSecondOffset, InitialSpeedLimitPerSecondV0)
	_ = sto.SetUint64ByUint64(perBlockGasLimitOffset, InitialPerBlockGasLimitV0)
	_ = sto.SetUint64ByUint64(baseFeeWeiOffset, InitialBaseFeeWei)
	_ = sto.SetUint64ByUint64(gasBacklogOffset, 0)
	_ = sto.SetUint64ByUint64(pricingInertiaOffset, InitialPricingInertia)
	_ = sto.SetUint64ByUint64(backlogToleranceOffset, InitialBacklogTolerance)
	return sto.SetUint64ByUint64(minBaseFeeWeiOffset, InitialMinimumBaseFeeWei)
}

func OpenL2PricingState(sto *storage.Storage) *L2PricingState {
	return &L2PricingState{
		sto,
		sto.OpenStorageBackedUint64(speedLimitPerSecondOffset),
		sto.OpenStorageBackedUint64(perBlockGasLimitOffset),
		sto.OpenStorageBackedBigUint(baseFeeWeiOffset),
		sto.OpenStorageBackedBigUint(minBaseFeeWeiOffset),
		sto.OpenStorageBackedUint64(gasBacklogOffset),
		sto.OpenStorageBackedUint64(pricingInertiaOffset),
		sto.OpenStorageBackedUint64(backlogToleranceOffset),
	}
}

func (ps *L2PricingState) BaseFeeWei() (*big.Int, error) {
	return ps.baseFeeWei.Get()
}

func (ps *L2PricingState) SetBaseFeeWei(val *big.Int) error {
	return ps.baseFeeWei.SetSaturatingWithWarning(val, "L2 base fee")
}

func (ps *L2PricingState) MinBaseFeeWei() (*big.Int, error) {
	return ps.minBaseFeeWei.Get()
}

func (ps *L2PricingState) SetMinBaseFeeWei(val *big.Int) error {
	// This modifies the "minimum basefee" parameter, but doesn't modify the current basefee.
	// If this increases the minimum basefee, then the basefee might be below the minimum for a little while.
	// If so, the basefee will increase by up to a factor of two per block, until it reaches the minimum.
	return ps.minBaseFeeWei.SetChecked(val)
}

func (ps *L2PricingState) SpeedLimitPerSecond() (uint64, error) {
	return ps.speedLimitPerSecond.Get()
}

func (ps *L2PricingState) SetSpeedLimitPerSecond(limit uint64) error {
	return ps.speedLimitPerSecond.Set(limit)
}

func (ps *L2PricingState) PerBlockGasLimit() (uint64, error) {
	return ps.perBlockGasLimit.Get()
}

func (ps *L2PricingState) SetMaxPerBlockGasLimit(limit uint64) error {
	return ps.perBlockGasLimit.Set(limit)
}

func (ps *L2PricingState) GasBacklog() (uint64, error) {
	return ps.gasBacklog.Get()
}

func (ps *L2PricingState) SetGasBacklog(backlog uint64) error {
	return ps.gasBacklog.Set(backlog)
}

func (ps *L2PricingState) PricingInertia() (uint64, error) {
	return ps.pricingInertia.Get()
}

func (ps *L2PricingState) SetPricingInertia(val uint64) error {
	return ps.pricingInertia.Set(val)
}

func (ps *L2PricingState) BacklogTolerance() (uint64, error) {
	return ps.backlogTolerance.Get()
}

func (ps *L2PricingState) SetBacklogTolerance(val uint64) error {
	return ps.backlogTolerance.Set(val)
}

func (ps *L2PricingState) Restrict(err error) {
	ps.storage.Burner().Restrict(err)
}

'''
'''--- arbos/l2pricing/l2pricing_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l2pricing

import (
	"fmt"
	"testing"

	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func PricingForTest(t *testing.T) *L2PricingState {
	storage := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	err := InitializeL2PricingState(storage)
	Require(t, err)
	return OpenL2PricingState(storage)
}

func fakeBlockUpdate(t *testing.T, pricing *L2PricingState, gasUsed int64, timePassed uint64) {
	basefee := getPrice(t, pricing)
	pricing.storage.Burner().Restrict(pricing.AddToGasPool(-gasUsed))
	pricing.UpdatePricingModel(arbmath.UintToBig(basefee), timePassed, true)
}

func TestPricingModelExp(t *testing.T) {
	pricing := PricingForTest(t)
	minPrice := getMinPrice(t, pricing)
	price := getPrice(t, pricing)
	limit := getSpeedLimit(t, pricing)

	if price != minPrice {
		Fail(t, "price not minimal", price, minPrice)
	}

	// show that running at the speed limit with a full pool is a steady-state
	colors.PrintBlue("full pool & speed limit")
	for seconds := 0; seconds < 4; seconds++ {
		fakeBlockUpdate(t, pricing, int64(seconds)*int64(limit), uint64(seconds))
		if getPrice(t, pricing) != minPrice {
			Fail(t, "price changed when it shouldn't have")
		}
	}

	// show that running at the speed limit with a target pool is close to a steady-state
	// note that for large enough spans of time the price will rise a miniscule amount due to the pool's avg
	colors.PrintBlue("pool target & speed limit")
	for seconds := 0; seconds < 4; seconds++ {
		fakeBlockUpdate(t, pricing, int64(seconds)*int64(limit), uint64(seconds))
		if getPrice(t, pricing) != minPrice {
			Fail(t, "price changed when it shouldn't have")
		}
	}

	// show that running over the speed limit escalates the price before the pool drains
	colors.PrintBlue("exceeding the speed limit")
	for {
		fakeBlockUpdate(t, pricing, 8*int64(limit), 1)
		newPrice := getPrice(t, pricing)
		if newPrice < price {
			Fail(t, "the price shouldn't have fallen")
		}
		if newPrice > price {
			break
		}
		price = newPrice
	}

	// empty the pool
	price = getPrice(t, pricing)
	Require(t, pricing.SetGasBacklog(100000000))

	// show that nothing happens when no time has passed and no gas has been burnt
	colors.PrintBlue("nothing should happen")
	fakeBlockUpdate(t, pricing, 0, 0)

	// show that the pool will escalate the price
	colors.PrintBlue("gas pool is empty")
	fakeBlockUpdate(t, pricing, 0, 1)
	if getPrice(t, pricing) <= price {
		fmt.Println(price, getPrice(t, pricing))
		Fail(t, "price should have risen")
	}
}

func getPrice(t *testing.T, pricing *L2PricingState) uint64 {
	value, err := pricing.BaseFeeWei()
	Require(t, err)
	return arbmath.BigToUintOrPanic(value)
}

func getMinPrice(t *testing.T, pricing *L2PricingState) uint64 {
	value, err := pricing.MinBaseFeeWei()
	Require(t, err)
	return arbmath.BigToUintOrPanic(value)
}

func getSpeedLimit(t *testing.T, pricing *L2PricingState) uint64 {
	value, err := pricing.SpeedLimitPerSecond()
	Require(t, err)
	return value
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- arbos/l2pricing/model.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package l2pricing

import (
	"math/big"

	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/util/arbmath"
)

const InitialSpeedLimitPerSecondV0 = 1000000
const InitialPerBlockGasLimitV0 uint64 = 20 * 1000000
const InitialSpeedLimitPerSecondV6 = 7000000
const InitialPerBlockGasLimitV6 uint64 = 32 * 1000000
const InitialMinimumBaseFeeWei = params.GWei / 10
const InitialBaseFeeWei = InitialMinimumBaseFeeWei
const InitialGasPoolSeconds = 10 * 60
const InitialRateEstimateInertia = 60
const InitialPricingInertia = 102
const InitialBacklogTolerance = 10

var InitialGasPoolTargetBips = arbmath.PercentToBips(80)
var InitialGasPoolWeightBips = arbmath.PercentToBips(60)

func (ps *L2PricingState) AddToGasPool(gas int64) error {
	backlog, err := ps.GasBacklog()
	if err != nil {
		return err
	}
	// pay off some of the backlog with the added gas, stopping at 0
	backlog = arbmath.SaturatingUCast(arbmath.SaturatingSub(int64(backlog), gas))
	return ps.SetGasBacklog(backlog)
}

// UpdatePricingModel updates the pricing model with info from the last block
func (ps *L2PricingState) UpdatePricingModel(l2BaseFee *big.Int, timePassed uint64, debug bool) {
	speedLimit, _ := ps.SpeedLimitPerSecond()
	_ = ps.AddToGasPool(int64(timePassed * speedLimit))
	inertia, _ := ps.PricingInertia()
	tolerance, _ := ps.BacklogTolerance()
	backlog, _ := ps.GasBacklog()
	minBaseFee, _ := ps.MinBaseFeeWei()
	baseFee := minBaseFee
	if backlog > tolerance*speedLimit {
		excess := int64(backlog - tolerance*speedLimit)
		exponentBips := arbmath.NaturalToBips(excess) / arbmath.Bips(inertia*speedLimit)
		baseFee = arbmath.BigMulByBips(minBaseFee, arbmath.ApproxExpBasisPoints(exponentBips))
	}
	_ = ps.SetBaseFeeWei(baseFee)
}

'''
'''--- arbos/merkleAccumulator/merkleAccumulator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkleAccumulator

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/arbmath"
)

type MerkleAccumulator struct {
	backingStorage *storage.Storage
	size           storage.WrappedUint64
	partials       []*common.Hash // nil if we are using backingStorage (in that case we access partials in backingStorage
}

func InitializeMerkleAccumulator(sto *storage.Storage) {
	// no initialization needed
}

func OpenMerkleAccumulator(sto *storage.Storage) *MerkleAccumulator {
	size := sto.OpenStorageBackedUint64(0)
	return &MerkleAccumulator{sto, &size, nil}
}

func NewNonpersistentMerkleAccumulator() *MerkleAccumulator {
	return &MerkleAccumulator{nil, &storage.MemoryBackedUint64{}, make([]*common.Hash, 0)}
}

func CalcNumPartials(size uint64) uint64 {
	return arbmath.Log2ceil(size)
}

func NewNonpersistentMerkleAccumulatorFromPartials(partials []*common.Hash) (*MerkleAccumulator, error) {
	size := uint64(0)
	levelSize := uint64(1)
	for i := range partials {
		if *partials[i] != (common.Hash{}) {
			size += levelSize
		}
		levelSize *= 2
	}
	mbu := &storage.MemoryBackedUint64{}
	return &MerkleAccumulator{nil, mbu, partials}, mbu.Set(size)
}

func (acc *MerkleAccumulator) NonPersistentClone() (*MerkleAccumulator, error) {
	size, err := acc.size.Get()
	if err != nil {
		return nil, err
	}
	numPartials := CalcNumPartials(size)
	partials := make([]*common.Hash, numPartials)
	for i := uint64(0); i < numPartials; i++ {
		partial, err := acc.getPartial(i)
		if err != nil {
			return nil, err
		}
		partials[i] = partial
	}
	mbu := &storage.MemoryBackedUint64{}
	return &MerkleAccumulator{nil, mbu, partials}, mbu.Set(size)
}

func (acc *MerkleAccumulator) Keccak(data ...[]byte) ([]byte, error) {
	if acc.backingStorage != nil {
		return acc.backingStorage.Keccak(data...)
	}
	return crypto.Keccak256(data...), nil
}

func (acc *MerkleAccumulator) KeccakHash(data ...[]byte) (common.Hash, error) {
	if acc.backingStorage != nil {
		return acc.backingStorage.KeccakHash(data...)
	}
	return crypto.Keccak256Hash(data...), nil
}

func (acc *MerkleAccumulator) getPartial(level uint64) (*common.Hash, error) {
	if acc.backingStorage == nil {
		if acc.partials[level] == nil {
			h := common.Hash{}
			acc.partials[level] = &h
		}
		return acc.partials[level], nil
	}
	ret, err := acc.backingStorage.GetByUint64(2 + level)
	return &ret, err
}

func (acc *MerkleAccumulator) GetPartials() ([]*common.Hash, error) {
	size, err := acc.size.Get()
	if err != nil {
		return nil, err
	}
	partials := make([]*common.Hash, CalcNumPartials(size))
	for i := range partials {
		p, err := acc.getPartial(uint64(i))
		if err != nil {
			return nil, err
		}
		partials[i] = p
	}
	return partials, nil
}

func (acc *MerkleAccumulator) setPartial(level uint64, val *common.Hash) error {
	if acc.backingStorage != nil {
		err := acc.backingStorage.SetByUint64(2+level, *val)
		if err != nil {
			return err
		}
	} else if level == uint64(len(acc.partials)) {
		acc.partials = append(acc.partials, val)
	} else {
		acc.partials[level] = val
	}
	return nil
}

// Note: itemHash is hashed before being included in the tree, to prevent confusing leafs with branches.
func (acc *MerkleAccumulator) Append(itemHash common.Hash) ([]MerkleTreeNodeEvent, error) {
	size, err := acc.size.Increment()
	if err != nil {
		return nil, err
	}
	events := []MerkleTreeNodeEvent{}

	level := uint64(0)
	soFar := crypto.Keccak256(itemHash.Bytes())
	for {
		if level == CalcNumPartials(size-1) { // -1 to counteract the acc.size++ at top of this function
			h := common.BytesToHash(soFar)
			err := acc.setPartial(level, &h)
			return events, err
		}
		thisLevel, err := acc.getPartial(level)
		if err != nil {
			return nil, err
		}
		if *thisLevel == (common.Hash{}) {
			h := common.BytesToHash(soFar)
			err := acc.setPartial(level, &h)
			return events, err
		}
		soFar, err = acc.Keccak(thisLevel.Bytes(), soFar)
		if err != nil {
			return nil, err
		}
		h := common.Hash{}
		err = acc.setPartial(level, &h)
		if err != nil {
			return nil, err
		}
		level += 1
		events = append(events, MerkleTreeNodeEvent{level, size - 1, common.BytesToHash(soFar)})
	}
}

func (acc *MerkleAccumulator) Size() (uint64, error) {
	return acc.size.Get()
}

func (acc *MerkleAccumulator) Root() (common.Hash, error) {
	size, err := acc.size.Get()
	if size == 0 || err != nil {
		return common.Hash{}, err
	}

	var hashSoFar *common.Hash
	var capacityInHash uint64
	capacity := uint64(1)
	for level := uint64(0); level < CalcNumPartials(size); level++ {
		partial, err := acc.getPartial(level)
		if err != nil {
			return common.Hash{}, err
		}
		if *partial != (common.Hash{}) {
			if hashSoFar == nil {
				hashSoFar = partial
				capacityInHash = capacity
			} else {
				for capacityInHash < capacity {
					h, err := acc.KeccakHash(hashSoFar.Bytes(), make([]byte, 32))
					if err != nil {
						return common.Hash{}, err
					}
					hashSoFar = &h
					capacityInHash *= 2
				}
				h, err := acc.KeccakHash(partial.Bytes(), hashSoFar.Bytes())
				if err != nil {
					return common.Hash{}, err
				}
				hashSoFar = &h
				capacityInHash = 2 * capacity
			}
		}
		capacity *= 2
	}
	return *hashSoFar, nil
}

func (acc *MerkleAccumulator) StateForExport() (uint64, common.Hash, []common.Hash, error) {
	root, err := acc.Root()
	if err != nil {
		return 0, common.Hash{}, nil, err
	}
	size, err := acc.size.Get()
	if err != nil {
		return 0, common.Hash{}, nil, err
	}
	numPartials := CalcNumPartials(size)
	partials := make([]common.Hash, numPartials)
	for i := uint64(0); i < numPartials; i++ {
		partial, err := acc.getPartial(i)
		if err != nil {
			return 0, common.Hash{}, nil, err
		}
		partials[i] = *partial
	}
	return size, root, partials, nil
}

type MerkleTreeNodeEvent struct {
	Level     uint64
	NumLeaves uint64
	Hash      common.Hash
}

'''
'''--- arbos/parse_l2.go ---
package arbos

import (
	"bytes"
	"errors"
	"fmt"
	"io"
	"math/big"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
)

type InfallibleBatchFetcher func(batchNum uint64, batchHash common.Hash) []byte

func ParseL2Transactions(msg *arbostypes.L1IncomingMessage, chainId *big.Int, batchFetcher InfallibleBatchFetcher) (types.Transactions, error) {
	if len(msg.L2msg) > arbostypes.MaxL2MessageSize {
		// ignore the message if l2msg is too large
		return nil, errors.New("message too large")
	}
	switch msg.Header.Kind {
	case arbostypes.L1MessageType_L2Message:
		return parseL2Message(bytes.NewReader(msg.L2msg), msg.Header.Poster, msg.Header.Timestamp, msg.Header.RequestId, chainId, 0)
	case arbostypes.L1MessageType_Initialize:
		return nil, errors.New("ParseL2Transactions encounted initialize message (should've been handled explicitly at genesis)")
	case arbostypes.L1MessageType_EndOfBlock:
		return nil, nil
	case arbostypes.L1MessageType_L2FundedByL1:
		if len(msg.L2msg) < 1 {
			return nil, errors.New("L2FundedByL1 message has no data")
		}
		if msg.Header.RequestId == nil {
			return nil, errors.New("cannot issue L2 funded by L1 tx without L1 request id")
		}
		kind := msg.L2msg[0]
		depositRequestId := crypto.Keccak256Hash(msg.Header.RequestId[:], math.U256Bytes(common.Big0))
		unsignedRequestId := crypto.Keccak256Hash(msg.Header.RequestId[:], math.U256Bytes(common.Big1))
		tx, err := parseUnsignedTx(bytes.NewReader(msg.L2msg[1:]), msg.Header.Poster, &unsignedRequestId, chainId, kind)
		if err != nil {
			return nil, err
		}
		deposit := types.NewTx(&types.ArbitrumDepositTx{
			ChainId:     chainId,
			L1RequestId: depositRequestId,
			// Matches the From of parseUnsignedTx
			To:    msg.Header.Poster,
			Value: tx.Value(),
		})
		return types.Transactions{deposit, tx}, nil
	case arbostypes.L1MessageType_SubmitRetryable:
		tx, err := parseSubmitRetryableMessage(bytes.NewReader(msg.L2msg), msg.Header, chainId)
		if err != nil {
			return nil, err
		}
		return types.Transactions{tx}, nil
	case arbostypes.L1MessageType_BatchForGasEstimation:
		return nil, errors.New("L1 message type BatchForGasEstimation is unimplemented")
	case arbostypes.L1MessageType_EthDeposit:
		tx, err := parseEthDepositMessage(bytes.NewReader(msg.L2msg), msg.Header, chainId)
		if err != nil {
			return nil, err
		}
		return types.Transactions{tx}, nil
	case arbostypes.L1MessageType_RollupEvent:
		log.Debug("ignoring rollup event message")
		return types.Transactions{}, nil
	case arbostypes.L1MessageType_BatchPostingReport:
		tx, err := parseBatchPostingReportMessage(bytes.NewReader(msg.L2msg), chainId, msg.BatchGasCost, batchFetcher)
		if err != nil {
			return nil, err
		}
		return types.Transactions{tx}, nil
	case arbostypes.L1MessageType_Invalid:
		// intentionally invalid message
		return nil, errors.New("invalid message")
	default:
		// invalid message, just ignore it
		return nil, fmt.Errorf("invalid message type %v", msg.Header.Kind)
	}
}

const (
	L2MessageKind_UnsignedUserTx  = 0
	L2MessageKind_ContractTx      = 1
	L2MessageKind_NonmutatingCall = 2
	L2MessageKind_Batch           = 3
	L2MessageKind_SignedTx        = 4
	// 5 is reserved
	L2MessageKind_Heartbeat          = 6 // deprecated
	L2MessageKind_SignedCompressedTx = 7
	// 8 is reserved for BLS signed batch
)

// Warning: this does not validate the day of the week or if DST is being observed
func parseTimeOrPanic(format string, value string) time.Time {
	t, err := time.Parse(format, value)
	if err != nil {
		panic(err)
	}
	return t
}

var HeartbeatsDisabledAt = uint64(parseTimeOrPanic(time.RFC1123, "Mon, 08 Aug 2022 16:00:00 GMT").Unix())

func parseL2Message(rd io.Reader, poster common.Address, timestamp uint64, requestId *common.Hash, chainId *big.Int, depth int) (types.Transactions, error) {
	var l2KindBuf [1]byte
	if _, err := rd.Read(l2KindBuf[:]); err != nil {
		return nil, err
	}

	switch l2KindBuf[0] {
	case L2MessageKind_UnsignedUserTx:
		tx, err := parseUnsignedTx(rd, poster, requestId, chainId, L2MessageKind_UnsignedUserTx)
		if err != nil {
			return nil, err
		}
		return types.Transactions{tx}, nil
	case L2MessageKind_ContractTx:
		tx, err := parseUnsignedTx(rd, poster, requestId, chainId, L2MessageKind_ContractTx)
		if err != nil {
			return nil, err
		}
		return types.Transactions{tx}, nil
	case L2MessageKind_NonmutatingCall:
		return nil, errors.New("L2 message kind NonmutatingCall is unimplemented")
	case L2MessageKind_Batch:
		if depth >= 16 {
			return nil, errors.New("L2 message batches have a max depth of 16")
		}
		segments := make(types.Transactions, 0)
		index := big.NewInt(0)
		for {
			nextMsg, err := util.BytestringFromReader(rd, arbostypes.MaxL2MessageSize)
			if err != nil {
				// an error here means there are no further messages in the batch
				// nolint:nilerr
				return segments, nil
			}

			var nextRequestId *common.Hash
			if requestId != nil {
				subRequestId := crypto.Keccak256Hash(requestId[:], math.U256Bytes(index))
				nextRequestId = &subRequestId
			}
			nestedSegments, err := parseL2Message(bytes.NewReader(nextMsg), poster, timestamp, nextRequestId, chainId, depth+1)
			if err != nil {
				return nil, err
			}
			segments = append(segments, nestedSegments...)
			index.Add(index, big.NewInt(1))
		}
	case L2MessageKind_SignedTx:
		newTx := new(types.Transaction)
		// Safe to read in its entirety, as all input readers are limited
		readBytes, err := io.ReadAll(rd)
		if err != nil {
			return nil, err
		}
		if err := newTx.UnmarshalBinary(readBytes); err != nil {
			return nil, err
		}
		if newTx.Type() >= types.ArbitrumDepositTxType || newTx.Type() == types.BlobTxType {
			// Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs
			// and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet.
			return nil, types.ErrTxTypeNotSupported
		}
		return types.Transactions{newTx}, nil
	case L2MessageKind_Heartbeat:
		if timestamp >= HeartbeatsDisabledAt {
			return nil, errors.New("heartbeat messages have been disabled")
		}
		// do nothing
		return nil, nil
	case L2MessageKind_SignedCompressedTx:
		return nil, errors.New("L2 message kind SignedCompressedTx is unimplemented")
	default:
		// ignore invalid message kind
		return nil, fmt.Errorf("unkown L2 message kind %v", l2KindBuf[0])
	}
}

func parseUnsignedTx(rd io.Reader, poster common.Address, requestId *common.Hash, chainId *big.Int, txKind byte) (*types.Transaction, error) {
	gasLimitHash, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	gasLimitBig := gasLimitHash.Big()
	if !gasLimitBig.IsUint64() {
		return nil, errors.New("unsigned user tx gas limit >= 2^64")
	}
	gasLimit := gasLimitBig.Uint64()

	maxFeePerGas, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}

	var nonce uint64
	if txKind == L2MessageKind_UnsignedUserTx {
		nonceAsHash, err := util.HashFromReader(rd)
		if err != nil {
			return nil, err
		}
		nonceAsBig := nonceAsHash.Big()
		if !nonceAsBig.IsUint64() {
			return nil, errors.New("unsigned user tx nonce >= 2^64")
		}
		nonce = nonceAsBig.Uint64()
	}

	to, err := util.AddressFrom256FromReader(rd)
	if err != nil {
		return nil, err
	}
	var destination *common.Address
	if to != (common.Address{}) {
		destination = &to
	}

	value, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}

	calldata, err := io.ReadAll(rd)
	if err != nil {
		return nil, err
	}

	var inner types.TxData

	switch txKind {
	case L2MessageKind_UnsignedUserTx:
		inner = &types.ArbitrumUnsignedTx{
			ChainId:   chainId,
			From:      poster,
			Nonce:     nonce,
			GasFeeCap: maxFeePerGas.Big(),
			Gas:       gasLimit,
			To:        destination,
			Value:     value.Big(),
			Data:      calldata,
		}
	case L2MessageKind_ContractTx:
		if requestId == nil {
			return nil, errors.New("cannot issue contract tx without L1 request id")
		}
		inner = &types.ArbitrumContractTx{
			ChainId:   chainId,
			RequestId: *requestId,
			From:      poster,
			GasFeeCap: maxFeePerGas.Big(),
			Gas:       gasLimit,
			To:        destination,
			Value:     value.Big(),
			Data:      calldata,
		}
	default:
		return nil, errors.New("invalid L2 tx type in parseUnsignedTx")
	}

	return types.NewTx(inner), nil
}

func parseEthDepositMessage(rd io.Reader, header *arbostypes.L1IncomingMessageHeader, chainId *big.Int) (*types.Transaction, error) {
	to, err := util.AddressFromReader(rd)
	if err != nil {
		return nil, err
	}
	balance, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	if header.RequestId == nil {
		return nil, errors.New("cannot issue deposit tx without L1 request id")
	}
	tx := &types.ArbitrumDepositTx{
		ChainId:     chainId,
		L1RequestId: *header.RequestId,
		From:        header.Poster,
		To:          to,
		Value:       balance.Big(),
	}
	return types.NewTx(tx), nil
}

func parseSubmitRetryableMessage(rd io.Reader, header *arbostypes.L1IncomingMessageHeader, chainId *big.Int) (*types.Transaction, error) {
	retryTo, err := util.AddressFrom256FromReader(rd)
	if err != nil {
		return nil, err
	}
	pRetryTo := &retryTo
	if retryTo == (common.Address{}) {
		pRetryTo = nil
	}
	callvalue, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	depositValue, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	maxSubmissionFee, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	feeRefundAddress, err := util.AddressFrom256FromReader(rd)
	if err != nil {
		return nil, err
	}
	callvalueRefundAddress, err := util.AddressFrom256FromReader(rd)
	if err != nil {
		return nil, err
	}
	gasLimit, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	gasLimitBig := gasLimit.Big()
	if !gasLimitBig.IsUint64() {
		return nil, errors.New("gas limit too large")
	}
	maxFeePerGas, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	dataLength256, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	dataLengthBig := dataLength256.Big()
	if !dataLengthBig.IsUint64() {
		return nil, errors.New("data length field too large")
	}
	dataLength := dataLengthBig.Uint64()
	if dataLength > arbostypes.MaxL2MessageSize {
		return nil, errors.New("retryable data too large")
	}
	retryData := make([]byte, dataLength)
	if dataLength > 0 {
		if _, err := rd.Read(retryData); err != nil {
			return nil, err
		}
	}
	if header.RequestId == nil {
		return nil, errors.New("cannot issue submit retryable tx without L1 request id")
	}
	tx := &types.ArbitrumSubmitRetryableTx{
		ChainId:          chainId,
		RequestId:        *header.RequestId,
		From:             header.Poster,
		L1BaseFee:        header.L1BaseFee,
		DepositValue:     depositValue.Big(),
		GasFeeCap:        maxFeePerGas.Big(),
		Gas:              gasLimitBig.Uint64(),
		RetryTo:          pRetryTo,
		RetryValue:       callvalue.Big(),
		Beneficiary:      callvalueRefundAddress,
		MaxSubmissionFee: maxSubmissionFee.Big(),
		FeeRefundAddr:    feeRefundAddress,
		RetryData:        retryData,
	}
	return types.NewTx(tx), err
}

func parseBatchPostingReportMessage(rd io.Reader, chainId *big.Int, msgBatchGasCost *uint64, batchFetcher InfallibleBatchFetcher) (*types.Transaction, error) {
	batchTimestamp, batchPosterAddr, batchHash, batchNum, l1BaseFee, extraGas, err := arbostypes.ParseBatchPostingReportMessageFields(rd)
	if err != nil {
		return nil, err
	}
	var batchDataGas uint64
	if msgBatchGasCost != nil {
		batchDataGas = *msgBatchGasCost
	} else {
		batchData := batchFetcher(batchNum, batchHash)
		batchDataGas = arbostypes.ComputeBatchGasCost(batchData)
	}
	batchDataGas = arbmath.SaturatingUAdd(batchDataGas, extraGas)

	data, err := util.PackInternalTxDataBatchPostingReport(
		batchTimestamp, batchPosterAddr, batchNum, batchDataGas, l1BaseFee,
	)
	if err != nil {
		return nil, err
	}
	return types.NewTx(&types.ArbitrumInternalTx{
		ChainId: chainId,
		Data:    data,
		// don't need to fill in the other fields, since they exist only to ensure uniqueness, and batchNum is already unique
	}), nil
}

'''
'''--- arbos/queue_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"testing"

	"github.com/offchainlabs/nitro/arbos/arbosState"

	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
)

func TestQueue(t *testing.T) {
	state, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	sto := state.BackingStorage().OpenCachedSubStorage([]byte{})
	Require(t, storage.InitializeQueue(sto))
	q := storage.OpenQueue(sto)

	stateBefore := statedb.IntermediateRoot(false)

	empty := func() bool {
		empty, err := q.IsEmpty()
		Require(t, err)
		return empty
	}

	if !empty() {
		Fail(t)
	}

	val0 := uint64(853139508)
	for i := uint64(0); i < 150; i++ {
		val := util.UintToHash(val0 + i)
		Require(t, q.Put(val))
		if empty() {
			Fail(t)
		}
	}

	for i := uint64(0); i < 150; i++ {
		val := util.UintToHash(val0 + i)
		res, err := q.Get()
		Require(t, err)
		if res.Big().Cmp(val.Big()) != 0 {
			Fail(t)
		}
	}

	if !empty() {
		Fail(t)
	}
	cleared, err := q.Shift()
	Require(t, err)
	if !cleared || stateBefore != statedb.IntermediateRoot(false) {
		Fail(t, "Emptying & shifting didn't clear the state")
	}
}

'''
'''--- arbos/retryable_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/params"
)

func TestOpenNonexistentRetryable(t *testing.T) {
	state, _ := arbosState.NewArbosMemoryBackedArbOSState()
	id := common.BigToHash(big.NewInt(978645611142))
	retryable, err := state.RetryableState().OpenRetryable(id, 0)
	Require(t, err)
	if retryable != nil {
		Fail(t)
	}
}

func TestRetryableLifecycle(t *testing.T) {
	rand.Seed(time.Now().UTC().UnixNano())
	state, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	retryableState := state.RetryableState()

	lifetime := uint64(retryables.RetryableLifetimeSeconds)
	timestampAtCreation := uint64(rand.Int63n(1 << 16))
	timeoutAtCreation := timestampAtCreation + lifetime
	currentTime := timeoutAtCreation

	setTime := func(timestamp uint64) uint64 {
		currentTime = timestamp
		// state.SetLastTimestampSeen(currentTime)
		colors.PrintGrey("Time is now ", timestamp)
		return currentTime
	}
	proveReapingDoesNothing := func() {
		stateCheck(t, statedb, false, "reaping had an effect", func() {
			evm := vm.NewEVM(vm.BlockContext{}, vm.TxContext{}, statedb, &params.ChainConfig{}, vm.Config{})
			Require(t, retryableState.TryToReapOneRetryable(currentTime, evm, util.TracingDuringEVM))
		})
	}
	checkQueueSize := func(expected int, message string) {
		timeoutQueueSize, err := retryableState.TimeoutQueue.Size()
		Require(t, err)
		if timeoutQueueSize != uint64(expected) {
			Fail(t, currentTime, message, timeoutQueueSize)
		}
	}

	stateBeforeEverything := statedb.IntermediateRoot(true)
	setTime(timestampAtCreation)

	ids := []common.Hash{}
	for i := 0; i < 8; i++ {
		id := common.BigToHash(big.NewInt(rand.Int63n(1 << 32)))
		from := testhelpers.RandomAddress()
		to := testhelpers.RandomAddress()
		beneficiary := testhelpers.RandomAddress()
		callvalue := big.NewInt(rand.Int63n(1 << 32))
		calldata := testhelpers.RandomizeSlice(make([]byte, rand.Intn(1<<12)))

		timeout := timeoutAtCreation
		_, err := retryableState.CreateRetryable(id, timeout, from, &to, callvalue, beneficiary, calldata)
		Require(t, err)
		ids = append(ids, id)
	}
	proveReapingDoesNothing()

	// Advance half way to expiration and extend each retryable's lifetime by one period
	setTime((timestampAtCreation + timeoutAtCreation) / 2)
	for _, id := range ids {
		window := currentTime + lifetime
		newTimeout, err := retryableState.Keepalive(id, currentTime, window, lifetime)
		Require(t, err, "failed to extend the retryable's lifetime")
		proveReapingDoesNothing()
		if newTimeout != timeoutAtCreation+lifetime {
			Fail(t, "new timeout is wrong", newTimeout, timeoutAtCreation+lifetime)
		}

		// prove we need to wait before keepalive can succeed again
		_, err = retryableState.Keepalive(id, currentTime, window, lifetime)
		if err == nil {
			Fail(t, "keepalive should have failed")
		}
	}
	checkQueueSize(2*len(ids), "Queue should have twice as many entries as there are retryables")

	// Advance passed the original timeout and reap half the entries in the queue
	setTime(timeoutAtCreation + 1)
	burner, _ := state.Burner.(*burn.SystemBurner)
	for range ids {
		// check that our reap pricing is reflective of the true cost
		gasBefore := burner.Burned()
		evm := vm.NewEVM(vm.BlockContext{}, vm.TxContext{}, statedb, &params.ChainConfig{}, vm.Config{})
		Require(t, retryableState.TryToReapOneRetryable(currentTime, evm, util.TracingDuringEVM))
		gasBurnedToReap := burner.Burned() - gasBefore
		if gasBurnedToReap != retryables.RetryableReapPrice {
			Fail(t, "reaping has been mispriced", gasBurnedToReap, retryables.RetryableReapPrice)
		}
	}
	checkQueueSize(len(ids), "Queue should have only one copy of each retryable")
	proveReapingDoesNothing()

	// Advanced passed the extended timeout and reap everything
	setTime(timeoutAtCreation + lifetime + 1)
	for _, id := range ids {
		// The retryable will be reaped, so opening it should fail
		shouldBeNil, err := retryableState.OpenRetryable(id, currentTime)
		Require(t, err)
		if shouldBeNil != nil {
			timeout, _ := shouldBeNil.CalculateTimeout()
			Fail(t, err, "read retryable after expiration", timeout, currentTime)
		}

		gasBefore := burner.Burned()
		evm := vm.NewEVM(vm.BlockContext{}, vm.TxContext{}, statedb, &params.ChainConfig{}, vm.Config{})
		Require(t, retryableState.TryToReapOneRetryable(currentTime, evm, util.TracingDuringEVM))
		gasBurnedToReapAndDelete := burner.Burned() - gasBefore
		if gasBurnedToReapAndDelete <= retryables.RetryableReapPrice {
			Fail(t, "deletion was cheap", gasBurnedToReapAndDelete, retryables.RetryableReapPrice)
		}

		// The retryable has been deleted, so opening it should fail
		shouldBeNil, err = retryableState.OpenRetryable(id, currentTime)
		Require(t, err)
		if shouldBeNil != nil {
			timeout, _ := shouldBeNil.CalculateTimeout()
			Fail(t, err, "read retryable after deletion", timeout, currentTime)
		}
	}
	checkQueueSize(0, "Queue should be empty")
	proveReapingDoesNothing()

	cleared, err := retryableState.TimeoutQueue.Shift()
	Require(t, err)
	if !cleared || stateBeforeEverything != statedb.IntermediateRoot(true) {
		Fail(t, "reaping didn't reset the state", cleared)
	}
}

func TestRetryableCleanup(t *testing.T) {
	rand.Seed(time.Now().UTC().UnixNano())
	state, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	retryableState := state.RetryableState()

	id := common.BigToHash(big.NewInt(rand.Int63n(1 << 32)))
	from := testhelpers.RandomAddress()
	to := testhelpers.RandomAddress()
	beneficiary := testhelpers.RandomAddress()

	// could be non-zero because we haven't actually minted funds like going through the submit process does
	callvalue := big.NewInt(0)
	calldata := testhelpers.RandomizeSlice(make([]byte, rand.Intn(1<<12)))

	timeout := uint64(rand.Int63n(1 << 16))
	timestamp := 2 * timeout

	stateCheck(t, statedb, false, "state has changed", func() {
		_, err := retryableState.CreateRetryable(id, timeout, from, &to, callvalue, beneficiary, calldata)
		Require(t, err)
		evm := vm.NewEVM(vm.BlockContext{}, vm.TxContext{}, statedb, &params.ChainConfig{}, vm.Config{})
		Require(t, retryableState.TryToReapOneRetryable(timestamp, evm, util.TracingDuringEVM))
		cleared, err := retryableState.TimeoutQueue.Shift()
		Require(t, err)
		if !cleared {
			Fail(t, "failed to reset the queue")
		}
	})
}

func TestRetryableCreate(t *testing.T) {
	state, _ := arbosState.NewArbosMemoryBackedArbOSState()
	id := common.BigToHash(big.NewInt(978645611142))
	lastTimestamp := uint64(0)

	timeout := lastTimestamp + 10000000
	from := common.BytesToAddress([]byte{3, 4, 5})
	to := common.BytesToAddress([]byte{6, 7, 8, 9})
	callvalue := big.NewInt(0)
	beneficiary := common.BytesToAddress([]byte{3, 1, 4, 1, 5, 9, 2, 6})
	calldata := make([]byte, 42)
	for i := range calldata {
		calldata[i] = byte(i + 3)
	}
	rstate := state.RetryableState()
	retryable, err := rstate.CreateRetryable(id, timeout, from, &to, callvalue, beneficiary, calldata)
	Require(t, err)

	reread, err := rstate.OpenRetryable(id, lastTimestamp)
	Require(t, err)
	if reread == nil {
		Fail(t)
	}
	equal, err := reread.Equals(retryable)
	Require(t, err)

	if !equal {
		Fail(t)
	}
}

func stateCheck(t *testing.T, statedb *state.StateDB, change bool, message string, scope func()) {
	stateBefore := statedb.IntermediateRoot(true)
	dumpBefore := string(statedb.Dump(&state.DumpConfig{}))
	scope()
	if (stateBefore != statedb.IntermediateRoot(true)) != change {
		dumpAfter := string(statedb.Dump(&state.DumpConfig{}))
		colors.PrintRed(dumpBefore)
		colors.PrintRed(dumpAfter)
		Fail(t, message)
	}
}

'''
'''--- arbos/retryables/retryable.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package retryables

import (
	"bytes"
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
)

const RetryableLifetimeSeconds = 7 * 24 * 60 * 60 // one week
const RetryableReapPrice = 58000

type RetryableState struct {
	retryables   *storage.Storage
	TimeoutQueue *storage.Queue
}

var (
	timeoutQueueKey = []byte{0}
	calldataKey     = []byte{1}
)

func InitializeRetryableState(sto *storage.Storage) error {
	return storage.InitializeQueue(sto.OpenCachedSubStorage(timeoutQueueKey))
}

func OpenRetryableState(sto *storage.Storage, statedb vm.StateDB) *RetryableState {
	return &RetryableState{
		sto,
		storage.OpenQueue(sto.OpenCachedSubStorage(timeoutQueueKey)),
	}
}

type Retryable struct {
	id                 common.Hash // not backed by storage; this key determines where it lives in storage
	backingStorage     *storage.Storage
	numTries           storage.StorageBackedUint64
	from               storage.StorageBackedAddress
	to                 storage.StorageBackedAddressOrNil
	callvalue          storage.StorageBackedBigUint
	beneficiary        storage.StorageBackedAddress
	calldata           storage.StorageBackedBytes
	timeout            storage.StorageBackedUint64
	timeoutWindowsLeft storage.StorageBackedUint64
}

const (
	numTriesOffset uint64 = iota
	fromOffset
	toOffset
	callvalueOffset
	beneficiaryOffset
	timeoutOffset
	timeoutWindowsLeftOffset
)

func (rs *RetryableState) CreateRetryable(
	id common.Hash, // we assume that the id is unique and hasn't been used before
	timeout uint64,
	from common.Address,
	to *common.Address,
	callvalue *big.Int,
	beneficiary common.Address,
	calldata []byte,
) (*Retryable, error) {
	sto := rs.retryables.OpenSubStorage(id.Bytes())
	ret := &Retryable{
		id,
		sto,
		sto.OpenStorageBackedUint64(numTriesOffset),
		sto.OpenStorageBackedAddress(fromOffset),
		sto.OpenStorageBackedAddressOrNil(toOffset),
		sto.OpenStorageBackedBigUint(callvalueOffset),
		sto.OpenStorageBackedAddress(beneficiaryOffset),
		sto.OpenStorageBackedBytes(calldataKey),
		sto.OpenStorageBackedUint64(timeoutOffset),
		sto.OpenStorageBackedUint64(timeoutWindowsLeftOffset),
	}
	_ = ret.numTries.Set(0)
	_ = ret.from.Set(from)
	_ = ret.to.Set(to)
	_ = ret.callvalue.SetChecked(callvalue)
	_ = ret.beneficiary.Set(beneficiary)
	_ = ret.calldata.Set(calldata)
	_ = ret.timeout.Set(timeout)
	_ = ret.timeoutWindowsLeft.Set(0)

	// insert the new retryable into the queue so it can be reaped later
	return ret, rs.TimeoutQueue.Put(id)
}

func (rs *RetryableState) OpenRetryable(id common.Hash, currentTimestamp uint64) (*Retryable, error) {
	sto := rs.retryables.OpenSubStorage(id.Bytes())
	timeoutStorage := sto.OpenStorageBackedUint64(timeoutOffset)
	timeout, err := timeoutStorage.Get()
	if timeout == 0 || timeout < currentTimestamp || err != nil {
		// Either no retryable here (real retryable never has a zero timeout),
		// Or the timeout has expired and the retryable will soon be reaped,
		// Or the user is out of gas
		return nil, err
	}
	return &Retryable{
		id:                 id,
		backingStorage:     sto,
		numTries:           sto.OpenStorageBackedUint64(numTriesOffset),
		from:               sto.OpenStorageBackedAddress(fromOffset),
		to:                 sto.OpenStorageBackedAddressOrNil(toOffset),
		callvalue:          sto.OpenStorageBackedBigUint(callvalueOffset),
		beneficiary:        sto.OpenStorageBackedAddress(beneficiaryOffset),
		calldata:           sto.OpenStorageBackedBytes(calldataKey),
		timeout:            timeoutStorage,
		timeoutWindowsLeft: sto.OpenStorageBackedUint64(timeoutWindowsLeftOffset),
	}, nil
}

func (rs *RetryableState) RetryableSizeBytes(id common.Hash, currentTime uint64) (uint64, error) {
	retryable, err := rs.OpenRetryable(id, currentTime)
	if retryable == nil || err != nil {
		return 0, err
	}
	size, err := retryable.CalldataSize()
	calldata := 32 + 32*arbmath.WordsForBytes(size) // length + contents
	return 6*32 + calldata, err
}

func (rs *RetryableState) DeleteRetryable(id common.Hash, evm *vm.EVM, scenario util.TracingScenario) (bool, error) {
	retStorage := rs.retryables.OpenSubStorage(id.Bytes())
	timeout, err := retStorage.GetByUint64(timeoutOffset)
	if timeout == (common.Hash{}) || err != nil {
		return false, err
	}

	// move any funds in escrow to the beneficiary (should be none if the retry succeeded -- see EndTxHook)
	beneficiary, _ := retStorage.GetByUint64(beneficiaryOffset)
	escrowAddress := RetryableEscrowAddress(id)
	beneficiaryAddress := common.BytesToAddress(beneficiary[:])
	amount := evm.StateDB.GetBalance(escrowAddress)
	err = util.TransferBalance(&escrowAddress, &beneficiaryAddress, amount, evm, scenario, "escrow")
	if err != nil {
		return false, err
	}

	// we ignore returned error as we expect that if one ClearByUint64 fails, than all consecutive calls to ClearByUint64 will fail with the same error (not modifying state), and then ClearBytes will also fail with the same error (also not modifying state) - and this one we check and return
	_ = retStorage.ClearByUint64(numTriesOffset)
	_ = retStorage.ClearByUint64(fromOffset)
	_ = retStorage.ClearByUint64(toOffset)
	_ = retStorage.ClearByUint64(callvalueOffset)
	_ = retStorage.ClearByUint64(beneficiaryOffset)
	_ = retStorage.ClearByUint64(timeoutOffset)
	_ = retStorage.ClearByUint64(timeoutWindowsLeftOffset)
	err = retStorage.OpenSubStorage(calldataKey).ClearBytes()
	return true, err
}

func (retryable *Retryable) NumTries() (uint64, error) {
	return retryable.numTries.Get()
}

func (retryable *Retryable) IncrementNumTries() (uint64, error) {
	return retryable.numTries.Increment()
}

func (retryable *Retryable) Beneficiary() (common.Address, error) {
	return retryable.beneficiary.Get()
}

func (retryable *Retryable) CalculateTimeout() (uint64, error) {
	timeout, err := retryable.timeout.Get()
	if err != nil {
		return 0, err
	}
	windows, err := retryable.timeoutWindowsLeft.Get()
	return timeout + windows*RetryableLifetimeSeconds, err
}

func (retryable *Retryable) SetTimeout(val uint64) error {
	return retryable.timeout.Set(val)
}

func (retryable *Retryable) TimeoutWindowsLeft() (uint64, error) {
	return retryable.timeoutWindowsLeft.Get()
}

func (retryable *Retryable) From() (common.Address, error) {
	return retryable.from.Get()
}

func (retryable *Retryable) To() (*common.Address, error) {
	return retryable.to.Get()
}

func (retryable *Retryable) Callvalue() (*big.Int, error) {
	return retryable.callvalue.Get()
}

func (retryable *Retryable) Calldata() ([]byte, error) {
	return retryable.calldata.Get()
}

// CalldataSize efficiently gets size of calldata without loading all of it
func (retryable *Retryable) CalldataSize() (uint64, error) {
	return retryable.calldata.Size()
}

func (rs *RetryableState) Keepalive(
	ticketId common.Hash,
	currentTimestamp,
	limitBeforeAdd,
	timeToAdd uint64,
) (uint64, error) {
	retryable, err := rs.OpenRetryable(ticketId, currentTimestamp)
	if err != nil {
		return 0, err
	}
	if retryable == nil {
		return 0, errors.New("ticketId not found")
	}
	timeout, err := retryable.CalculateTimeout()
	if err != nil {
		return 0, err
	}
	if timeout > limitBeforeAdd {
		return 0, errors.New("timeout too far into the future")
	}

	// Add a duplicate entry to the end of the queue (only the last one deletes the retryable)
	err = rs.TimeoutQueue.Put(retryable.id)
	if err != nil {
		return 0, err
	}
	if _, err := retryable.timeoutWindowsLeft.Increment(); err != nil {
		return 0, err
	}
	newTimeout := timeout + RetryableLifetimeSeconds

	// Pay in advance for the work needed to reap the duplicate from the timeout queue
	return newTimeout, rs.retryables.Burner().Burn(RetryableReapPrice)
}

func (retryable *Retryable) Equals(other *Retryable) (bool, error) { // for testing
	if retryable.id != other.id {
		return false, nil
	}
	rTries, _ := retryable.NumTries()
	oTries, _ := other.NumTries()
	rTimeout, _ := retryable.timeout.Get()
	oTimeout, _ := other.timeout.Get()
	rWindows, _ := retryable.timeoutWindowsLeft.Get()
	oWindows, _ := other.timeoutWindowsLeft.Get()
	rFrom, _ := retryable.From()
	oFrom, _ := other.From()
	rTo, _ := retryable.To()
	oTo, _ := other.To()
	rCallvalue, _ := retryable.Callvalue()
	oCallvalue, _ := other.Callvalue()
	rBeneficiary, _ := retryable.Beneficiary()
	oBeneficiary, _ := other.Beneficiary()
	rBytes, _ := retryable.Calldata()
	oBytes, err := other.Calldata()

	diff := rTries != oTries || rTimeout != oTimeout || rWindows != oWindows
	diff = diff || rFrom != oFrom || rBeneficiary != oBeneficiary
	diff = diff || rCallvalue.Cmp(oCallvalue) != 0 || !bytes.Equal(rBytes, oBytes)
	if diff {
		return false, err
	}

	if rTo == nil {
		if oTo != nil {
			return false, err
		}
	} else if oTo == nil {
		return false, err
	} else if *rTo != *oTo {
		return false, err
	}
	return true, err
}

func (rs *RetryableState) TryToReapOneRetryable(currentTimestamp uint64, evm *vm.EVM, scenario util.TracingScenario) error {
	id, err := rs.TimeoutQueue.Peek()
	if err != nil || id == nil {
		return err
	}
	retryableStorage := rs.retryables.OpenSubStorage(id.Bytes())
	timeoutStorage := retryableStorage.OpenStorageBackedUint64(timeoutOffset)
	timeout, err := timeoutStorage.Get()
	if err != nil {
		return err
	}
	if timeout == 0 {
		// The retryable has already been deleted, so discard the peeked entry
		_, err = rs.TimeoutQueue.Get()
		return err
	}

	windowsLeftStorage := retryableStorage.OpenStorageBackedUint64(timeoutWindowsLeftOffset)
	windowsLeft, err := windowsLeftStorage.Get()
	if err != nil || timeout >= currentTimestamp {
		return err
	}

	// Either the retryable has expired, or it's lost a lifetime's worth of time
	_, err = rs.TimeoutQueue.Get()
	if err != nil {
		return err
	}

	if windowsLeft == 0 {
		// the retryable has expired, time to reap
		_, err = rs.DeleteRetryable(*id, evm, scenario)
		return err
	}

	// Consume a window, delaying the timeout one lifetime period
	if err := timeoutStorage.Set(timeout + RetryableLifetimeSeconds); err != nil {
		return err
	}
	return windowsLeftStorage.Set(windowsLeft - 1)
}

func (retryable *Retryable) MakeTx(chainId *big.Int, nonce uint64, gasFeeCap *big.Int, gas uint64, ticketId common.Hash, refundTo common.Address, maxRefund *big.Int, submissionFeeRefund *big.Int) (*types.ArbitrumRetryTx, error) {
	from, err := retryable.From()
	if err != nil {
		return nil, err
	}
	to, err := retryable.To()
	if err != nil {
		return nil, err
	}
	callvalue, err := retryable.Callvalue()
	if err != nil {
		return nil, err
	}
	calldata, err := retryable.Calldata()
	if err != nil {
		return nil, err
	}
	return &types.ArbitrumRetryTx{
		ChainId:             chainId,
		Nonce:               nonce,
		From:                from,
		GasFeeCap:           gasFeeCap,
		Gas:                 gas,
		To:                  to,
		Value:               callvalue,
		Data:                calldata,
		TicketId:            ticketId,
		RefundTo:            refundTo,
		MaxRefund:           maxRefund,
		SubmissionFeeRefund: submissionFeeRefund,
	}, nil
}

func RetryableEscrowAddress(ticketId common.Hash) common.Address {
	return common.BytesToAddress(crypto.Keccak256([]byte("retryable escrow"), ticketId.Bytes()))
}

func RetryableSubmissionFee(calldataLengthInBytes int, l1BaseFee *big.Int) *big.Int {
	return arbmath.BigMulByUint(l1BaseFee, uint64(1400+6*calldataLengthInBytes))
}

'''
'''--- arbos/storage/queue.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package storage

import (
	"github.com/offchainlabs/nitro/arbos/util"

	"github.com/ethereum/go-ethereum/common"
)

type Queue struct {
	storage       *Storage
	nextPutOffset StorageBackedUint64
	nextGetOffset StorageBackedUint64
}

func InitializeQueue(sto *Storage) error {
	err := sto.SetUint64ByUint64(0, 2)
	if err != nil {
		return err
	}
	return sto.SetUint64ByUint64(1, 2)
}

func OpenQueue(sto *Storage) *Queue {
	return &Queue{
		sto.WithoutCache(),
		sto.OpenStorageBackedUint64(0),
		sto.OpenStorageBackedUint64(1),
	}
}

func (q *Queue) IsEmpty() (bool, error) {
	put, err := q.nextPutOffset.Get()
	if err != nil {
		return false, err
	}
	get, err := q.nextGetOffset.Get()
	return put == get, err
}

func (q *Queue) Size() (uint64, error) {
	put, err := q.nextPutOffset.Get()
	if err != nil {
		return 0, err
	}
	get, err := q.nextGetOffset.Get()
	return put - get, err
}

func (q *Queue) Peek() (*common.Hash, error) { // returns nil iff queue is empty
	empty, err := q.IsEmpty()
	if empty || err != nil {
		return nil, err
	}
	next, err := q.nextGetOffset.Get()
	if err != nil {
		return nil, err
	}
	res, err := q.storage.GetByUint64(next)
	return &res, err
}

func (q *Queue) Get() (*common.Hash, error) { // returns nil iff queue is empty
	empty, err := q.IsEmpty()
	if empty || err != nil {
		return nil, err
	}
	newOffset, err := q.nextGetOffset.Increment()
	if err != nil {
		return nil, err
	}
	res, err := q.storage.Swap(util.UintToHash(newOffset-1), common.Hash{})
	return &res, err
}

func (q *Queue) Put(val common.Hash) error {
	newOffset, err := q.nextPutOffset.Increment()
	if err != nil {
		return err
	}
	return q.storage.SetByUint64(newOffset-1, val)
}

// Shift reset the queue to its starting state
// If the queue is empty, this method will return true and reset its state
// This is useful for testing
func (q *Queue) Shift() (bool, error) {
	put, err := q.nextPutOffset.Get()
	if err != nil {
		return false, err
	}
	get, err := q.nextGetOffset.Get()
	if err != nil || put != get {
		return false, err
	}
	if err := q.nextGetOffset.Set(2); err != nil {
		return false, err
	}
	return true, q.nextPutOffset.Set(2)
}

// ForEach apply a closure on the enumerated elements element of the queue
func (q *Queue) ForEach(closure func(uint64, common.Hash) (bool, error)) error {

	size, err := q.Size()
	if err != nil {
		return err
	}
	offset, err := q.nextGetOffset.Get()
	if err != nil {
		return err
	}

	for index := uint64(0); index < size; index++ {
		entry, err := q.storage.GetByUint64(offset + index)
		if err != nil {
			return err
		}
		done, err := closure(index, entry)
		if err != nil {
			return err
		}
		if done {
			return nil
		}
	}
	return nil
}

'''
'''--- arbos/storage/storage.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package storage

import (
	"bytes"
	"fmt"
	"math/big"
	"sync/atomic"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/lru"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
)

// Storage allows ArbOS to store data persistently in the Ethereum-compatible stateDB. This is represented in
// the stateDB as the storage of a fictional Ethereum account at address 0xA4B05FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF.
//
// The storage is logically a tree of storage spaces which can be nested hierarchically, with each storage space
// containing a key-value store with 256-bit keys and values. Uninitialized storage spaces and uninitialized keys
// within initialized storage spaces are deemed to be filled with zeroes (consistent with the behavior of Ethereum
// account storage). Logically, when a chain is launched, all possible storage spaces and all possible keys within
// them exist and contain zeroes.
//
// A storage space (represented by a Storage object) has a byte-slice storageKey which distinguishes it from other
// storage spaces. The root Storage has its storageKey as the empty string. A parent storage space can contain children,
// each with a distinct name. The storageKey of a child is keccak256(parent.storageKey, name). Note that two spaces
// cannot have the same storageKey because that would imply a collision in keccak256.
//
// The contents of all storage spaces are stored in a single, flat key-value store that is implemented as the storage
// of the fictional Ethereum account. The contents of key, within a storage space with storageKey, are stored
// at location keccak256(storageKey, key) in the flat KVS. Two slots, whether in the same or different storage spaces,
// cannot occupy the same location because that would imply a collision in keccak256.

type Storage struct {
	account    common.Address
	db         vm.StateDB
	storageKey []byte
	burner     burn.Burner
	hashCache  *lru.Cache[string, []byte]
}

const StorageReadCost = params.SloadGasEIP2200
const StorageWriteCost = params.SstoreSetGasEIP2200
const StorageWriteZeroCost = params.SstoreResetGasEIP2200

const storageKeyCacheSize = 1024

var storageHashCache = lru.NewCache[string, []byte](storageKeyCacheSize)
var cacheFullLogged atomic.Bool

// NewGeth uses a Geth database to create an evm key-value store
func NewGeth(statedb vm.StateDB, burner burn.Burner) *Storage {
	account := common.HexToAddress("0xA4B05FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF")
	statedb.SetNonce(account, 1) // setting the nonce ensures Geth won't treat ArbOS as empty
	return &Storage{
		account:    account,
		db:         statedb,
		storageKey: []byte{},
		burner:     burner,
		hashCache:  storageHashCache,
	}
}

// NewMemoryBacked uses Geth's memory-backed database to create an evm key-value store
func NewMemoryBacked(burner burn.Burner) *Storage {
	return NewGeth(NewMemoryBackedStateDB(), burner)
}

// NewMemoryBackedStateDB uses Geth's memory-backed database to create a statedb
func NewMemoryBackedStateDB() vm.StateDB {
	raw := rawdb.NewMemoryDatabase()
	db := state.NewDatabase(raw)
	statedb, err := state.New(common.Hash{}, db, nil)
	if err != nil {
		panic("failed to init empty statedb")
	}
	return statedb
}

// We map addresses using "pages" of 256 storage slots. We hash over the page number but not the offset within
// a page, to preserve contiguity within a page. This will reduce cost if/when Ethereum switches to storage
// representations that reward contiguity.
// Because page numbers are 248 bits, this gives us 124-bit security against collision attacks, which is good enough.
func (s *Storage) mapAddress(key common.Hash) common.Hash {
	keyBytes := key.Bytes()
	boundary := common.HashLength - 1
	mapped := make([]byte, 0, common.HashLength)
	mapped = append(mapped, s.cachedKeccak(s.storageKey, keyBytes[:boundary])[:boundary]...)
	mapped = append(mapped, keyBytes[boundary])
	return common.BytesToHash(mapped)
}

func writeCost(value common.Hash) uint64 {
	if value == (common.Hash{}) {
		return StorageWriteZeroCost
	}
	return StorageWriteCost
}

func (s *Storage) Account() common.Address {
	return s.account
}

func (s *Storage) Get(key common.Hash) (common.Hash, error) {
	err := s.burner.Burn(StorageReadCost)
	if err != nil {
		return common.Hash{}, err
	}
	if info := s.burner.TracingInfo(); info != nil {
		info.RecordStorageGet(key)
	}
	return s.db.GetState(s.account, s.mapAddress(key)), nil
}

func (s *Storage) GetStorageSlot(key common.Hash) common.Hash {
	return s.mapAddress(key)
}

func (s *Storage) GetUint64(key common.Hash) (uint64, error) {
	value, err := s.Get(key)
	return value.Big().Uint64(), err
}

func (s *Storage) GetByUint64(key uint64) (common.Hash, error) {
	return s.Get(util.UintToHash(key))
}

func (s *Storage) GetUint64ByUint64(key uint64) (uint64, error) {
	return s.GetUint64(util.UintToHash(key))
}

func (s *Storage) Set(key common.Hash, value common.Hash) error {
	if s.burner.ReadOnly() {
		log.Error("Read-only burner attempted to mutate state", "key", key, "value", value)
		return vm.ErrWriteProtection
	}
	err := s.burner.Burn(writeCost(value))
	if err != nil {
		return err
	}
	if info := s.burner.TracingInfo(); info != nil {
		info.RecordStorageSet(key, value)
	}
	s.db.SetState(s.account, s.mapAddress(key), value)
	return nil
}

func (s *Storage) SetByUint64(key uint64, value common.Hash) error {
	return s.Set(util.UintToHash(key), value)
}

func (s *Storage) SetUint64ByUint64(key uint64, value uint64) error {
	return s.Set(util.UintToHash(key), util.UintToHash(value))
}

func (s *Storage) Clear(key common.Hash) error {
	return s.Set(key, common.Hash{})
}

func (s *Storage) ClearByUint64(key uint64) error {
	return s.Set(util.UintToHash(key), common.Hash{})
}

func (s *Storage) Swap(key common.Hash, newValue common.Hash) (common.Hash, error) {
	oldValue, err := s.Get(key)
	if err != nil {
		return common.Hash{}, err
	}
	return oldValue, s.Set(key, newValue)
}

func (s *Storage) OpenCachedSubStorage(id []byte) *Storage {
	return &Storage{
		account:    s.account,
		db:         s.db,
		storageKey: s.cachedKeccak(s.storageKey, id),
		burner:     s.burner,
		hashCache:  storageHashCache,
	}
}
func (s *Storage) OpenSubStorage(id []byte) *Storage {
	return &Storage{
		account:    s.account,
		db:         s.db,
		storageKey: s.cachedKeccak(s.storageKey, id),
		burner:     s.burner,
		hashCache:  nil,
	}
}

// Returns shallow copy of Storage that won't use storage key hash cache.
// The storage space represented by the returned Storage is kept the same.
func (s *Storage) WithoutCache() *Storage {
	return &Storage{
		account:    s.account,
		db:         s.db,
		storageKey: s.storageKey,
		burner:     s.burner,
		hashCache:  nil,
	}
}

func (s *Storage) SetBytes(b []byte) error {
	err := s.ClearBytes()
	if err != nil {
		return err
	}
	err = s.SetUint64ByUint64(0, uint64(len(b)))
	if err != nil {
		return err
	}
	offset := uint64(1)
	for len(b) >= 32 {
		err = s.SetByUint64(offset, common.BytesToHash(b[:32]))
		if err != nil {
			return err
		}
		b = b[32:]
		offset++
	}
	return s.SetByUint64(offset, common.BytesToHash(b))
}

func (s *Storage) GetBytes() ([]byte, error) {
	bytesLeft, err := s.GetUint64ByUint64(0)
	if err != nil {
		return nil, err
	}
	ret := []byte{}
	offset := uint64(1)
	for bytesLeft >= 32 {
		next, err := s.GetByUint64(offset)
		if err != nil {
			return nil, err
		}
		ret = append(ret, next.Bytes()...)
		bytesLeft -= 32
		offset++
	}
	next, err := s.GetByUint64(offset)
	if err != nil {
		return nil, err
	}
	ret = append(ret, next.Bytes()[32-bytesLeft:]...)
	return ret, nil
}

func (s *Storage) GetBytesSize() (uint64, error) {
	return s.GetUint64ByUint64(0)
}

func (s *Storage) ClearBytes() error {
	bytesLeft, err := s.GetUint64ByUint64(0)
	if err != nil {
		return err
	}
	offset := uint64(1)
	for bytesLeft > 0 {
		err := s.ClearByUint64(offset)
		if err != nil {
			return err
		}
		offset++
		if bytesLeft < 32 {
			bytesLeft = 0
		} else {
			bytesLeft -= 32
		}
	}
	return s.ClearByUint64(0)
}

func (s *Storage) Burner() burn.Burner {
	return s.burner // not public because these should never be changed once set
}

func (s *Storage) Keccak(data ...[]byte) ([]byte, error) {
	byteCount := 0
	for _, part := range data {
		byteCount += len(part)
	}
	cost := 30 + 6*arbmath.WordsForBytes(uint64(byteCount))
	if err := s.burner.Burn(cost); err != nil {
		return nil, err
	}
	return crypto.Keccak256(data...), nil
}

func (s *Storage) KeccakHash(data ...[]byte) (common.Hash, error) {
	bytes, err := s.Keccak(data...)
	return common.BytesToHash(bytes), err
}

// Returns crypto.Keccak256 result for the given data
// If available the result is taken from hash cache
// otherwise crypto.Keccak256 is executed and its result is added to the cache and returned
// note: the method doesn't burn gas, as it's only intended for generating storage subspace keys and mapping slot addresses
// note: returned slice is not thread-safe
func (s *Storage) cachedKeccak(data ...[]byte) []byte {
	if s.hashCache == nil {
		return crypto.Keccak256(data...)
	}
	keyString := string(bytes.Join(data, []byte{}))
	if hash, wasCached := s.hashCache.Get(keyString); wasCached {
		return hash
	}
	hash := crypto.Keccak256(data...)
	evicted := s.hashCache.Add(keyString, hash)
	if evicted && cacheFullLogged.CompareAndSwap(false, true) {
		log.Warn("Hash cache full, we didn't expect that. Some non-static storage keys may fill up the cache.")
	}
	return hash
}

type StorageSlot struct {
	account common.Address
	db      vm.StateDB
	slot    common.Hash
	burner  burn.Burner
}

func (s *Storage) NewSlot(offset uint64) StorageSlot {
	return StorageSlot{s.account, s.db, s.mapAddress(util.UintToHash(offset)), s.burner}
}

func (ss *StorageSlot) Get() (common.Hash, error) {
	err := ss.burner.Burn(StorageReadCost)
	if err != nil {
		return common.Hash{}, err
	}
	if info := ss.burner.TracingInfo(); info != nil {
		info.RecordStorageGet(ss.slot)
	}
	return ss.db.GetState(ss.account, ss.slot), nil
}

func (ss *StorageSlot) Set(value common.Hash) error {
	if ss.burner.ReadOnly() {
		log.Error("Read-only burner attempted to mutate state", "value", value)
		return vm.ErrWriteProtection
	}
	err := ss.burner.Burn(writeCost(value))
	if err != nil {
		return err
	}
	if info := ss.burner.TracingInfo(); info != nil {
		info.RecordStorageSet(ss.slot, value)
	}
	ss.db.SetState(ss.account, ss.slot, value)
	return nil
}

// StorageBackedInt64 is an int64 stored inside the StateDB.
// Implementation note: Conversions between big.Int and common.Hash give weird results
// for negative values, so we cast to uint64 before writing to storage and cast back to int64 after reading.
// Golang casting between uint64 and int64 doesn't change the data, it just reinterprets the same 8 bytes,
// so this is a hacky but reliable way to store an 8-byte int64 in a common.Hash storage slot.
type StorageBackedInt64 struct {
	StorageSlot
}

func (s *Storage) OpenStorageBackedInt64(offset uint64) StorageBackedInt64 {
	return StorageBackedInt64{s.NewSlot(offset)}
}

func (sbu *StorageBackedInt64) Get() (int64, error) {
	raw, err := sbu.StorageSlot.Get()
	if !raw.Big().IsUint64() {
		panic("invalid value found in StorageBackedInt64 storage")
	}
	return int64(raw.Big().Uint64()), err // see implementation note above
}

func (sbu *StorageBackedInt64) Set(value int64) error {
	return sbu.StorageSlot.Set(util.UintToHash(uint64(value))) // see implementation note above
}

// StorageBackedBips represents a number of basis points
type StorageBackedBips struct {
	backing StorageBackedInt64
}

func (s *Storage) OpenStorageBackedBips(offset uint64) StorageBackedBips {
	return StorageBackedBips{StorageBackedInt64{s.NewSlot(offset)}}
}

func (sbu *StorageBackedBips) Get() (arbmath.Bips, error) {
	value, err := sbu.backing.Get()
	return arbmath.Bips(value), err
}

func (sbu *StorageBackedBips) Set(bips arbmath.Bips) error {
	return sbu.backing.Set(int64(bips))
}

type StorageBackedUint64 struct {
	StorageSlot
}

func (s *Storage) OpenStorageBackedUint64(offset uint64) StorageBackedUint64 {
	return StorageBackedUint64{s.NewSlot(offset)}
}

func (sbu *StorageBackedUint64) Get() (uint64, error) {
	raw, err := sbu.StorageSlot.Get()
	if !raw.Big().IsUint64() {
		panic("expected uint64 compatible value in storage")
	}
	return raw.Big().Uint64(), err
}

func (sbu *StorageBackedUint64) Set(value uint64) error {
	bigValue := new(big.Int).SetUint64(value)
	return sbu.StorageSlot.Set(common.BigToHash(bigValue))
}

func (sbu *StorageBackedUint64) Clear() error {
	return sbu.Set(0)
}

func (sbu *StorageBackedUint64) Increment() (uint64, error) {
	old, err := sbu.Get()
	if err != nil {
		return 0, err
	}
	if old+1 < old {
		panic("Overflow in StorageBackedUint64::Increment")
	}
	return old + 1, sbu.Set(old + 1)
}

func (sbu *StorageBackedUint64) Decrement() (uint64, error) {
	old, err := sbu.Get()
	if err != nil {
		return 0, err
	}
	if old == 0 {
		panic("Underflow in StorageBackedUint64::Decrement")
	}
	return old - 1, sbu.Set(old - 1)
}

type MemoryBackedUint64 struct {
	contents uint64
}

func (mbu *MemoryBackedUint64) Get() (uint64, error) {
	return mbu.contents, nil
}

func (mbu *MemoryBackedUint64) Set(val uint64) error {
	mbu.contents = val
	return nil
}

func (mbu *MemoryBackedUint64) Increment() (uint64, error) {
	old := mbu.contents
	if old+1 < old {
		panic("Overflow in MemoryBackedUint64::Increment")
	}
	return old + 1, mbu.Set(old + 1)
}

func (mbu *MemoryBackedUint64) Decrement() (uint64, error) {
	old := mbu.contents
	if old == 0 {
		panic("Underflow in MemoryBackedUint64::Decrement")
	}
	return old - 1, mbu.Set(old - 1)
}

type WrappedUint64 interface {
	Get() (uint64, error)
	Set(uint64) error
	Increment() (uint64, error)
	Decrement() (uint64, error)
}

var twoToThe256 = new(big.Int).Lsh(common.Big1, 256)
var twoToThe256MinusOne = new(big.Int).Sub(twoToThe256, common.Big1)
var twoToThe255 = new(big.Int).Lsh(common.Big1, 255)
var twoToThe255MinusOne = new(big.Int).Sub(twoToThe255, common.Big1)

type StorageBackedBigUint struct {
	StorageSlot
}

func (s *Storage) OpenStorageBackedBigUint(offset uint64) StorageBackedBigUint {
	return StorageBackedBigUint{s.NewSlot(offset)}
}

func (sbbu *StorageBackedBigUint) Get() (*big.Int, error) {
	asHash, err := sbbu.StorageSlot.Get()
	if err != nil {
		return nil, err
	}
	return asHash.Big(), nil
}

// Warning: this will panic if it underflows or overflows with a system burner
// SetSaturatingWithWarning is likely better
func (sbbu *StorageBackedBigUint) SetChecked(val *big.Int) error {
	if val.Sign() < 0 {
		return sbbu.burner.HandleError(fmt.Errorf("underflow in StorageBackedBigUint.Set setting value %v", val))
	}
	if val.BitLen() > 256 {
		return sbbu.burner.HandleError(fmt.Errorf("overflow in StorageBackedBigUint.Set setting value %v", val))
	}
	return sbbu.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

func (sbbu *StorageBackedBigUint) SetSaturatingWithWarning(val *big.Int, name string) error {
	if val.Sign() < 0 {
		log.Warn("ArbOS storage big uint underflowed", "name", name, "value", val)
		val = common.Big0
	} else if val.BitLen() > 256 {
		log.Warn("ArbOS storage big uint overflowed", "name", name, "value", val)
		val = twoToThe256MinusOne
	}
	return sbbu.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

type StorageBackedBigInt struct {
	StorageSlot
}

func (s *Storage) OpenStorageBackedBigInt(offset uint64) StorageBackedBigInt {
	return StorageBackedBigInt{s.NewSlot(offset)}
}

func (sbbi *StorageBackedBigInt) Get() (*big.Int, error) {
	asHash, err := sbbi.StorageSlot.Get()
	if err != nil {
		return nil, err
	}
	asBig := new(big.Int).SetBytes(asHash[:])
	if asBig.Bit(255) != 0 {
		asBig = new(big.Int).Sub(asBig, twoToThe256)
	}
	return asBig, err
}

// Warning: this will panic if it underflows or overflows with a system burner
// SetSaturatingWithWarning is likely better
func (sbbi *StorageBackedBigInt) SetChecked(val *big.Int) error {
	if val.Sign() < 0 {
		val = new(big.Int).Add(val, twoToThe256)
		if val.BitLen() < 256 || val.Sign() <= 0 { // require that it's positive and the top bit is set
			return sbbi.burner.HandleError(fmt.Errorf("underflow in StorageBackedBigInt.Set setting value %v", val))
		}
	} else if val.BitLen() >= 256 {
		return sbbi.burner.HandleError(fmt.Errorf("overflow in StorageBackedBigInt.Set setting value %v", val))
	}
	return sbbi.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

func (sbbi *StorageBackedBigInt) SetSaturatingWithWarning(val *big.Int, name string) error {
	if val.Sign() < 0 {
		origVal := val
		val = new(big.Int).Add(val, twoToThe256)
		if val.BitLen() < 256 || val.Sign() <= 0 { // require that it's positive and the top bit is set
			log.Warn("ArbOS storage big uint underflowed", "name", name, "value", origVal)
			val.Set(twoToThe255)
		}
	} else if val.BitLen() >= 256 {
		log.Warn("ArbOS storage big uint overflowed", "name", name, "value", val)
		val = twoToThe255MinusOne
	}
	return sbbi.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

func (sbbi *StorageBackedBigInt) Set_preVersion7(val *big.Int) error {
	return sbbi.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

func (sbbi *StorageBackedBigInt) SetByUint(val uint64) error {
	return sbbi.StorageSlot.Set(util.UintToHash(val))
}

type StorageBackedAddress struct {
	StorageSlot
}

func (s *Storage) OpenStorageBackedAddress(offset uint64) StorageBackedAddress {
	return StorageBackedAddress{s.NewSlot(offset)}
}

func (sba *StorageBackedAddress) Get() (common.Address, error) {
	value, err := sba.StorageSlot.Get()
	return common.BytesToAddress(value.Bytes()), err
}

func (sba *StorageBackedAddress) Set(val common.Address) error {
	return sba.StorageSlot.Set(util.AddressToHash(val))
}

type StorageBackedAddressOrNil struct {
	StorageSlot
}

var NilAddressRepresentation common.Hash

func init() {
	NilAddressRepresentation = common.BigToHash(new(big.Int).Lsh(big.NewInt(1), 255))
}

func (s *Storage) OpenStorageBackedAddressOrNil(offset uint64) StorageBackedAddressOrNil {
	return StorageBackedAddressOrNil{s.NewSlot(offset)}
}

func (sba *StorageBackedAddressOrNil) Get() (*common.Address, error) {
	asHash, err := sba.StorageSlot.Get()
	if asHash == NilAddressRepresentation || err != nil {
		return nil, err
	} else {
		ret := common.BytesToAddress(asHash.Bytes())
		return &ret, nil
	}
}

func (sba *StorageBackedAddressOrNil) Set(val *common.Address) error {
	if val == nil {
		return sba.StorageSlot.Set(NilAddressRepresentation)
	}
	return sba.StorageSlot.Set(common.BytesToHash(val.Bytes()))
}

type StorageBackedBytes struct {
	Storage
}

func (s *Storage) OpenStorageBackedBytes(id []byte) StorageBackedBytes {
	return StorageBackedBytes{
		*s.OpenSubStorage(id),
	}
}

func (sbb *StorageBackedBytes) Get() ([]byte, error) {
	return sbb.Storage.GetBytes()
}

func (sbb *StorageBackedBytes) Set(val []byte) error {
	return sbb.Storage.SetBytes(val)
}

func (sbb *StorageBackedBytes) Clear() error {
	return sbb.Storage.ClearBytes()
}

func (sbb *StorageBackedBytes) Size() (uint64, error) {
	return sbb.Storage.GetBytesSize()
}

'''
'''--- arbos/storage/storage_test.go ---
package storage

import (
	"bytes"
	"fmt"
	"math/big"
	"math/rand"
	"sync"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func requirePanic(t *testing.T, testCase interface{}, f func()) {
	t.Helper()
	defer func() {
		if recover() == nil {
			t.Fatal("panic expected but function exited successfully for test case", testCase)
		}
	}()
	f()
}

func TestStorageBackedBigInt(t *testing.T) {
	sto := NewMemoryBacked(burn.NewSystemBurner(nil, false))
	sbbi := sto.OpenStorageBackedBigInt(0)
	rawSlot := sto.NewSlot(0)

	twoToThe255 := new(big.Int).Lsh(big.NewInt(1), 255)
	maxUint256 := new(big.Int).Sub(twoToThe255, big.NewInt(1))
	minUint256 := new(big.Int).Neg(twoToThe255)
	for _, in := range []*big.Int{
		big.NewInt(0),
		big.NewInt(1),
		big.NewInt(33),
		big.NewInt(31591083),
		big.NewInt(-1),
		big.NewInt(-33),
		big.NewInt(-31591083),
		maxUint256,
		minUint256,
	} {
		err := sbbi.SetChecked(in)
		if err != nil {
			t.Fatal(err)
		}
		rawVal, err := rawSlot.Get()
		if err != nil {
			t.Fatal(err)
		}
		// Verify that our encoding matches geth's signed complement impl
		expectedRawVal := common.BigToHash(math.U256(new(big.Int).Set(in)))
		if rawVal != expectedRawVal {
			t.Fatal("for input", in, "expected raw value", expectedRawVal, "but got", rawVal)
		}
		gotInverse := math.S256(rawVal.Big())
		if !arbmath.BigEquals(gotInverse, in) {
			t.Fatal("for input", in, "expected raw value", rawVal, "to convert back into input but got", gotInverse)
		}
		out, err := sbbi.Get()
		if err != nil {
			t.Fatal(err)
		}
		if in.Cmp(out) != 0 {
			t.Fatal(in, out, common.BytesToHash(out.Bytes()))
		}

		if in.BitLen() < 200 {
			err = sbbi.Set_preVersion7(in)
			if err != nil {
				t.Fatal(err)
			}
			out, err = sbbi.Get()
			if err != nil {
				t.Fatal(err)
			}
			if new(big.Int).Abs(in).Cmp(out) != 0 {
				t.Fatal(in, out, common.BytesToHash(out.Bytes()))
			}
		}
	}
	for _, in := range []*big.Int{
		new(big.Int).Add(maxUint256, big.NewInt(1)),
		new(big.Int).Sub(minUint256, big.NewInt(1)),
		new(big.Int).Mul(maxUint256, big.NewInt(2)),
		new(big.Int).Mul(minUint256, big.NewInt(2)),
		new(big.Int).Exp(maxUint256, big.NewInt(1025), nil),
		new(big.Int).Exp(minUint256, big.NewInt(1025), nil),
	} {
		requirePanic(t, in, func() {
			_ = sbbi.SetChecked(in)
		})
	}
}

func TestOpenCachedSubStorage(t *testing.T) {
	s := NewMemoryBacked(burn.NewSystemBurner(nil, false))
	var subSpaceIDs [][]byte
	for i := 0; i < 20; i++ {
		subSpaceIDs = append(subSpaceIDs, []byte{byte(rand.Intn(0xff))})
	}
	var expectedKeys [][]byte
	for _, subSpaceID := range subSpaceIDs {
		expectedKeys = append(expectedKeys, crypto.Keccak256(s.storageKey, subSpaceID))
	}
	n := len(subSpaceIDs) * 50
	start := make(chan struct{})
	errs := make(chan error, n)
	var wg sync.WaitGroup
	for i := 0; i < n; i++ {
		j := i % len(subSpaceIDs)
		subSpaceID, expectedKey := subSpaceIDs[j], expectedKeys[j]
		wg.Add(1)
		go func() {
			defer wg.Done()
			<-start
			ss := s.OpenCachedSubStorage(subSpaceID)
			if !bytes.Equal(ss.storageKey, expectedKey) {
				errs <- fmt.Errorf("unexpected storage key, want: %v, have: %v", expectedKey, ss.storageKey)
			}
		}()
	}
	close(start)
	wg.Wait()
	select {
	case err := <-errs:
		t.Fatal(err)
	default:
	}
}

func TestMapAddressCache(t *testing.T) {
	s := NewMemoryBacked(burn.NewSystemBurner(nil, false))
	var keys []common.Hash
	for i := 0; i < 20; i++ {
		keys = append(keys, common.BytesToHash([]byte{byte(rand.Intn(0xff))}))
	}
	var expectedMapped []common.Hash
	for _, key := range keys {
		expectedMapped = append(expectedMapped, s.mapAddress(key))
	}
	n := len(keys) * 50
	start := make(chan struct{})
	errs := make(chan error, n)
	var wg sync.WaitGroup
	for i := 0; i < n; i++ {
		j := i % len(keys)
		key, expected := keys[j], expectedMapped[j]
		wg.Add(1)
		go func() {
			defer wg.Done()
			<-start
			mapped := s.mapAddress(key)
			if !bytes.Equal(mapped.Bytes(), expected.Bytes()) {
				errs <- fmt.Errorf("unexpected storage key, want: %v, have: %v", expected, mapped)
			}
		}()
	}
	close(start)
	wg.Wait()
	if len(errs) > 0 {
		t.Fatal(<-errs)
	}
}

'''
'''--- arbos/tx_processor.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbos

import (
	"errors"
	"fmt"
	"math/big"

	"github.com/offchainlabs/nitro/arbos/l1pricing"

	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/retryables"

	"github.com/offchainlabs/nitro/arbos/arbosState"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/vm"
	glog "github.com/ethereum/go-ethereum/log"
)

var arbosAddress = types.ArbosAddress

const GasEstimationL1PricePadding arbmath.Bips = 11000 // pad estimates by 10%

// A TxProcessor is created and freed for every L2 transaction.
// It tracks state for ArbOS, allowing it infuence in Geth's tx processing.
// Public fields are accessible in precompiles.
type TxProcessor struct {
	msg              *core.Message
	state            *arbosState.ArbosState
	PosterFee        *big.Int // set once in GasChargingHook to track L1 calldata costs
	posterGas        uint64
	computeHoldGas   uint64 // amount of gas temporarily held to prevent compute from exceeding the gas limit
	delayedInbox     bool   // whether this tx was submitted through the delayed inbox
	Callers          []common.Address
	TopTxType        *byte // set once in StartTxHook
	evm              *vm.EVM
	CurrentRetryable *common.Hash
	CurrentRefundTo  *common.Address

	// Caches for the latest L1 block number and hash,
	// for the NUMBER and BLOCKHASH opcodes.
	cachedL1BlockNumber *uint64
	cachedL1BlockHashes map[uint64]common.Hash
}

func NewTxProcessor(evm *vm.EVM, msg *core.Message) *TxProcessor {
	tracingInfo := util.NewTracingInfo(evm, msg.From, arbosAddress, util.TracingBeforeEVM)
	arbosState := arbosState.OpenSystemArbosStateOrPanic(evm.StateDB, tracingInfo, false)
	return &TxProcessor{
		msg:                 msg,
		state:               arbosState,
		PosterFee:           new(big.Int),
		posterGas:           0,
		delayedInbox:        evm.Context.Coinbase != l1pricing.BatchPosterAddress,
		Callers:             []common.Address{},
		TopTxType:           nil,
		evm:                 evm,
		CurrentRetryable:    nil,
		CurrentRefundTo:     nil,
		cachedL1BlockNumber: nil,
		cachedL1BlockHashes: make(map[uint64]common.Hash),
	}
}

func (p *TxProcessor) PushCaller(addr common.Address) {
	p.Callers = append(p.Callers, addr)
}

func (p *TxProcessor) PopCaller() {
	p.Callers = p.Callers[:len(p.Callers)-1]
}

// Attempts to subtract up to `take` from `pool` without going negative.
// Returns the amount subtracted from `pool`.
func takeFunds(pool *big.Int, take *big.Int) *big.Int {
	if take.Sign() < 0 {
		panic("Attempted to take a negative amount of funds")
	}
	if arbmath.BigLessThan(pool, take) {
		oldPool := new(big.Int).Set(pool)
		pool.Set(common.Big0)
		return oldPool
	}
	pool.Sub(pool, take)
	return new(big.Int).Set(take)
}

func (p *TxProcessor) StartTxHook() (endTxNow bool, gasUsed uint64, err error, returnData []byte) {
	// This hook is called before gas charging and will end the state transition if endTxNow is set to true
	// Hence, we must charge for any l2 resources if endTxNow is returned true

	underlyingTx := p.msg.Tx
	if underlyingTx == nil {
		return false, 0, nil, nil
	}

	var tracingInfo *util.TracingInfo
	tipe := underlyingTx.Type()
	p.TopTxType = &tipe
	evm := p.evm

	startTracer := func() func() {
		tracer := evm.Config.Tracer
		if tracer == nil {
			return func() {}
		}
		evm.IncrementDepth() // fake a call
		from := p.msg.From
		tracer.CaptureStart(evm, from, *p.msg.To, false, p.msg.Data, p.msg.GasLimit, p.msg.Value)

		tracingInfo = util.NewTracingInfo(evm, from, *p.msg.To, util.TracingDuringEVM)
		p.state = arbosState.OpenSystemArbosStateOrPanic(evm.StateDB, tracingInfo, false)

		return func() {
			tracer.CaptureEnd(nil, p.state.Burner.Burned(), nil)
			evm.DecrementDepth() // fake the return to the first faked call

			tracingInfo = util.NewTracingInfo(evm, from, *p.msg.To, util.TracingAfterEVM)
			p.state = arbosState.OpenSystemArbosStateOrPanic(evm.StateDB, tracingInfo, false)
		}
	}

	switch tx := underlyingTx.GetInner().(type) {
	case *types.ArbitrumDepositTx:
		from := p.msg.From
		to := p.msg.To
		value := p.msg.Value
		if to == nil {
			return true, 0, errors.New("eth deposit has no To address"), nil
		}
		util.MintBalance(&from, value, evm, util.TracingBeforeEVM, "deposit")
		defer (startTracer())()
		// We intentionally use the variant here that doesn't do tracing,
		// because this transfer is represented as the outer eth transaction.
		// This transfer is necessary because we don't actually invoke the EVM.
		core.Transfer(evm.StateDB, from, *to, value)
		return true, 0, nil, nil
	case *types.ArbitrumInternalTx:
		defer (startTracer())()
		if p.msg.From != arbosAddress {
			return false, 0, errors.New("internal tx not from arbAddress"), nil
		}
		err = ApplyInternalTxUpdate(tx, p.state, evm)
		return true, 0, err, nil
	case *types.ArbitrumSubmitRetryableTx:
		defer (startTracer())()
		statedb := evm.StateDB
		ticketId := underlyingTx.Hash()
		escrow := retryables.RetryableEscrowAddress(ticketId)
		networkFeeAccount, _ := p.state.NetworkFeeAccount()
		from := tx.From
		scenario := util.TracingDuringEVM

		// mint funds with the deposit, then charge fees later
		availableRefund := new(big.Int).Set(tx.DepositValue)
		takeFunds(availableRefund, tx.RetryValue)
		util.MintBalance(&tx.From, tx.DepositValue, evm, scenario, "deposit")

		transfer := func(from, to *common.Address, amount *big.Int) error {
			return util.TransferBalance(from, to, amount, evm, scenario, "during evm execution")
		}

		// check that the user has enough balance to pay for the max submission fee
		balanceAfterMint := evm.StateDB.GetBalance(tx.From)
		if balanceAfterMint.Cmp(tx.MaxSubmissionFee) < 0 {
			err := fmt.Errorf(
				"insufficient funds for max submission fee: address %v have %v want %v",
				tx.From, balanceAfterMint, tx.MaxSubmissionFee,
			)
			return true, 0, err, nil
		}

		submissionFee := retryables.RetryableSubmissionFee(len(tx.RetryData), tx.L1BaseFee)
		if arbmath.BigLessThan(tx.MaxSubmissionFee, submissionFee) {
			// should be impossible as this is checked at L1
			err := fmt.Errorf(
				"max submission fee %v is less than the actual submission fee %v",
				tx.MaxSubmissionFee, submissionFee,
			)
			return true, 0, err, nil
		}

		// collect the submission fee
		if err := transfer(&tx.From, &networkFeeAccount, submissionFee); err != nil {
			// should be impossible as we just checked that they have enough balance for the max submission fee,
			// and we also checked that the max submission fee is at least the actual submission fee
			glog.Error("failed to transfer submissionFee", "err", err)
			return true, 0, err, nil
		}
		withheldSubmissionFee := takeFunds(availableRefund, submissionFee)

		// refund excess submission fee
		submissionFeeRefund := takeFunds(availableRefund, arbmath.BigSub(tx.MaxSubmissionFee, submissionFee))
		if err := transfer(&tx.From, &tx.FeeRefundAddr, submissionFeeRefund); err != nil {
			// should never happen as from's balance should be at least availableRefund at this point
			glog.Error("failed to transfer submissionFeeRefund", "err", err)
		}

		// move the callvalue into escrow
		if callValueErr := transfer(&tx.From, &escrow, tx.RetryValue); callValueErr != nil {
			// The sender doesn't have enough balance to pay for the retryable's callvalue.
			// Since we can't create the retryable, we should refund the submission fee.
			// First, we give the submission fee back to the transaction sender:
			if err := transfer(&networkFeeAccount, &tx.From, submissionFee); err != nil {
				glog.Error("failed to refund submissionFee", "err", err)
			}
			// Then, as limited by availableRefund, we attempt to move the refund to the fee refund address.
			// If the deposit value was lower than the submission fee, only some (or none) of the submission fee may be moved.
			// In that case, any amount up to the deposit value will be refunded to the fee refund address,
			// with the rest remaining in the transaction sender's address (as that's where the funds were pulled from).
			if err := transfer(&tx.From, &tx.FeeRefundAddr, withheldSubmissionFee); err != nil {
				glog.Error("failed to refund withheldSubmissionFee", "err", err)
			}
			return true, 0, callValueErr, nil
		}

		time := evm.Context.Time
		timeout := time + retryables.RetryableLifetimeSeconds

		// we charge for creating the retryable and reaping the next expired one on L1
		retryable, err := p.state.RetryableState().CreateRetryable(
			ticketId,
			timeout,
			tx.From,
			tx.RetryTo,
			tx.RetryValue,
			tx.Beneficiary,
			tx.RetryData,
		)
		p.state.Restrict(err)

		err = EmitTicketCreatedEvent(evm, ticketId)
		if err != nil {
			glog.Error("failed to emit TicketCreated event", "err", err)
		}

		balance := statedb.GetBalance(tx.From)
		effectiveBaseFee := evm.Context.BaseFee
		usergas := p.msg.GasLimit

		if p.msg.TxRunMode != core.MessageCommitMode && p.msg.GasFeeCap.BitLen() == 0 {
			// In gas estimation or eth_call mode, we permit a zero gas fee cap.
			// This matches behavior with normal tx gas estimation and eth_call.
			effectiveBaseFee = common.Big0
		}

		maxGasCost := arbmath.BigMulByUint(tx.GasFeeCap, usergas)
		maxFeePerGasTooLow := arbmath.BigLessThan(tx.GasFeeCap, effectiveBaseFee)
		if arbmath.BigLessThan(balance, maxGasCost) || usergas < params.TxGas || maxFeePerGasTooLow {
			// User either specified too low of a gas fee cap, didn't have enough balance to pay for gas,
			// or the specified gas limit is below the minimum transaction gas cost.
			// Either way, attempt to refund the gas costs, since we're not doing the auto-redeem.
			gasCostRefund := takeFunds(availableRefund, maxGasCost)
			if err := transfer(&tx.From, &tx.FeeRefundAddr, gasCostRefund); err != nil {
				// should never happen as from's balance should be at least availableRefund at this point
				glog.Error("failed to transfer gasCostRefund", "err", err)
			}
			return true, 0, nil, ticketId.Bytes()
		}

		// pay for the retryable's gas and update the pools
		gascost := arbmath.BigMulByUint(effectiveBaseFee, usergas)
		networkCost := gascost
		if p.state.ArbOSVersion() >= 11 {
			infraFeeAccount, err := p.state.InfraFeeAccount()
			p.state.Restrict(err)
			if infraFeeAccount != (common.Address{}) {
				minBaseFee, err := p.state.L2PricingState().MinBaseFeeWei()
				p.state.Restrict(err)
				infraFee := arbmath.BigMin(minBaseFee, effectiveBaseFee)
				infraCost := arbmath.BigMulByUint(infraFee, usergas)
				infraCost = takeFunds(networkCost, infraCost)
				if err := transfer(&tx.From, &infraFeeAccount, infraCost); err != nil {
					glog.Error("failed to transfer gas cost to infrastructure fee account", "err", err)
					return true, 0, nil, ticketId.Bytes()
				}
			}
		}
		if arbmath.BigGreaterThan(networkCost, common.Big0) {
			if err := transfer(&tx.From, &networkFeeAccount, networkCost); err != nil {
				// should be impossible because we just checked the tx.From balance
				glog.Error("failed to transfer gas cost to network fee account", "err", err)
				return true, 0, nil, ticketId.Bytes()
			}
		}

		withheldGasFunds := takeFunds(availableRefund, gascost) // gascost is conceptually charged before the gas price refund
		gasPriceRefund := arbmath.BigMulByUint(arbmath.BigSub(tx.GasFeeCap, effectiveBaseFee), tx.Gas)
		if gasPriceRefund.Sign() < 0 {
			// This should only be possible during gas estimation mode
			gasPriceRefund.SetInt64(0)
		}
		gasPriceRefund = takeFunds(availableRefund, gasPriceRefund)
		if err := transfer(&tx.From, &tx.FeeRefundAddr, gasPriceRefund); err != nil {
			glog.Error("failed to transfer gasPriceRefund", "err", err)
		}
		availableRefund.Add(availableRefund, withheldGasFunds)
		availableRefund.Add(availableRefund, withheldSubmissionFee)

		// emit RedeemScheduled event
		retryTxInner, err := retryable.MakeTx(
			underlyingTx.ChainId(),
			0,
			effectiveBaseFee,
			usergas,
			ticketId,
			tx.FeeRefundAddr,
			availableRefund,
			submissionFee,
		)
		p.state.Restrict(err)

		_, err = retryable.IncrementNumTries()
		p.state.Restrict(err)

		err = EmitReedeemScheduledEvent(
			evm,
			usergas,
			retryTxInner.Nonce,
			ticketId,
			types.NewTx(retryTxInner).Hash(),
			tx.FeeRefundAddr,
			availableRefund,
			submissionFee,
		)
		if err != nil {
			glog.Error("failed to emit RedeemScheduled event", "err", err)
		}

		if tracer := evm.Config.Tracer; tracer != nil {
			redeem, err := util.PackArbRetryableTxRedeem(ticketId)
			if err == nil {
				tracingInfo.MockCall(redeem, usergas, from, types.ArbRetryableTxAddress, common.Big0)
			} else {
				glog.Error("failed to abi-encode auto-redeem", "err", err)
			}
		}

		return true, usergas, nil, ticketId.Bytes()
	case *types.ArbitrumRetryTx:

		// Transfer callvalue from escrow
		escrow := retryables.RetryableEscrowAddress(tx.TicketId)
		scenario := util.TracingBeforeEVM
		if err := util.TransferBalance(&escrow, &tx.From, tx.Value, evm, scenario, "escrow"); err != nil {
			return true, 0, err, nil
		}

		// The redeemer has pre-paid for this tx's gas
		prepaid := arbmath.BigMulByUint(evm.Context.BaseFee, tx.Gas)
		util.MintBalance(&tx.From, prepaid, evm, scenario, "prepaid")
		ticketId := tx.TicketId
		refundTo := tx.RefundTo
		p.CurrentRetryable = &ticketId
		p.CurrentRefundTo = &refundTo
	}
	return false, 0, nil, nil
}

func GetPosterGas(state *arbosState.ArbosState, baseFee *big.Int, runMode core.MessageRunMode, posterCost *big.Int) uint64 {
	if runMode == core.MessageGasEstimationMode {
		// Suggest the amount of gas needed for a given amount of ETH is higher in case of congestion.
		// This will help the user pad the total they'll pay in case the price rises a bit.
		// Note, reducing the poster cost will increase share the network fee gets, not reduce the total.

		minGasPrice, _ := state.L2PricingState().MinBaseFeeWei()

		adjustedPrice := arbmath.BigMulByFrac(baseFee, 7, 8) // assume congestion
		if arbmath.BigLessThan(adjustedPrice, minGasPrice) {
			adjustedPrice = minGasPrice
		}
		baseFee = adjustedPrice

		// Pad the L1 cost in case the L1 gas price rises
		posterCost = arbmath.BigMulByBips(posterCost, GasEstimationL1PricePadding)
	}

	return arbmath.BigToUintSaturating(arbmath.BigDiv(posterCost, baseFee))
}

func (p *TxProcessor) GasChargingHook(gasRemaining *uint64) (common.Address, error) {
	// Because a user pays a 1-dimensional gas price, we must re-express poster L1 calldata costs
	// as if the user was buying an equivalent amount of L2 compute gas. This hook determines what
	// that cost looks like, ensuring the user can pay and saving the result for later reference.

	var gasNeededToStartEVM uint64
	tipReceipient, _ := p.state.NetworkFeeAccount()
	basefee := p.evm.Context.BaseFee

	var poster common.Address
	if p.msg.TxRunMode != core.MessageCommitMode {
		poster = l1pricing.BatchPosterAddress
	} else {
		poster = p.evm.Context.Coinbase
	}

	if p.msg.TxRunMode == core.MessageCommitMode {
		p.msg.SkipL1Charging = false
	}
	if basefee.Sign() > 0 && !p.msg.SkipL1Charging {
		// Since tips go to the network, and not to the poster, we use the basefee.
		// Note, this only determines the amount of gas bought, not the price per gas.

		brotliCompressionLevel, err := p.state.BrotliCompressionLevel()
		if err != nil {
			return common.Address{}, fmt.Errorf("failed to get brotli compression level: %w", err)
		}
		posterCost, calldataUnits := p.state.L1PricingState().PosterDataCost(p.msg, poster, brotliCompressionLevel)
		if calldataUnits > 0 {
			p.state.Restrict(p.state.L1PricingState().AddToUnitsSinceUpdate(calldataUnits))
		}
		p.posterGas = GetPosterGas(p.state, basefee, p.msg.TxRunMode, posterCost)
		p.PosterFee = arbmath.BigMulByUint(basefee, p.posterGas) // round down
		gasNeededToStartEVM = p.posterGas
	}

	if *gasRemaining < gasNeededToStartEVM {
		// the user couldn't pay for call data, so give up
		return tipReceipient, core.ErrIntrinsicGas
	}
	*gasRemaining -= gasNeededToStartEVM

	if p.msg.TxRunMode != core.MessageEthcallMode {
		// If this is a real tx, limit the amount of computed based on the gas pool.
		// We do this by charging extra gas, and then refunding it later.
		gasAvailable, _ := p.state.L2PricingState().PerBlockGasLimit()
		if *gasRemaining > gasAvailable {
			p.computeHoldGas = *gasRemaining - gasAvailable
			*gasRemaining = gasAvailable
		}
	}
	return tipReceipient, nil
}

func (p *TxProcessor) NonrefundableGas() uint64 {
	// EVM-incentivized activity like freeing storage should only refund amounts paid to the network address,
	// which represents the overall burden to node operators. A poster's costs, then, should not be eligible
	// for this refund.
	return p.posterGas
}

func (p *TxProcessor) ForceRefundGas() uint64 {
	return p.computeHoldGas
}

func (p *TxProcessor) EndTxHook(gasLeft uint64, success bool) {

	underlyingTx := p.msg.Tx
	networkFeeAccount, _ := p.state.NetworkFeeAccount()
	scenario := util.TracingAfterEVM

	if gasLeft > p.msg.GasLimit {
		panic("Tx somehow refunds gas after computation")
	}
	gasUsed := p.msg.GasLimit - gasLeft

	if underlyingTx != nil && underlyingTx.Type() == types.ArbitrumRetryTxType {
		inner, _ := underlyingTx.GetInner().(*types.ArbitrumRetryTx)
		effectiveBaseFee := inner.GasFeeCap
		if p.msg.TxRunMode == core.MessageCommitMode && !arbmath.BigEquals(effectiveBaseFee, p.evm.Context.BaseFee) {
			log.Error(
				"ArbitrumRetryTx GasFeeCap doesn't match basefee in commit mode",
				"txHash", underlyingTx.Hash(),
				"gasFeeCap", inner.GasFeeCap,
				"baseFee", p.evm.Context.BaseFee,
			)
			// revert to the old behavior to avoid diverging from older nodes
			effectiveBaseFee = p.evm.Context.BaseFee
		}

		// undo Geth's refund to the From address
		gasRefund := arbmath.BigMulByUint(effectiveBaseFee, gasLeft)
		err := util.BurnBalance(&inner.From, gasRefund, p.evm, scenario, "undoRefund")
		if err != nil {
			log.Error("Uh oh, Geth didn't refund the user", inner.From, gasRefund)
		}

		maxRefund := new(big.Int).Set(inner.MaxRefund)
		refund := func(refundFrom common.Address, amount *big.Int) {
			const errLog = "fee address doesn't have enough funds to give user refund"

			// Refund funds to the fee refund address without overdrafting the L1 deposit.
			toRefundAddr := takeFunds(maxRefund, amount)
			err = util.TransferBalance(&refundFrom, &inner.RefundTo, toRefundAddr, p.evm, scenario, "refund")
			if err != nil {
				// Normally the network fee address should be holding any collected fees.
				// However, in theory, they could've been transferred out during the redeem attempt.
				// If the network fee address doesn't have the necessary balance, log an error and don't give a refund.
				log.Error(errLog, "err", err, "feeAddress", refundFrom)
			}
			// Any extra refund can't be given to the fee refund address if it didn't come from the L1 deposit.
			// Instead, give the refund to the retryable from address.
			err = util.TransferBalance(&refundFrom, &inner.From, arbmath.BigSub(amount, toRefundAddr), p.evm, scenario, "refund")
			if err != nil {
				log.Error(errLog, "err", err, "feeAddress", refundFrom)
			}
		}

		if success {
			// If successful, refund the submission fee.
			refund(networkFeeAccount, inner.SubmissionFeeRefund)
		} else {
			// The submission fee is still taken from the L1 deposit earlier, even if it's not refunded.
			takeFunds(maxRefund, inner.SubmissionFeeRefund)
		}
		// Conceptually, the gas charge is taken from the L1 deposit pool if possible.
		takeFunds(maxRefund, arbmath.BigMulByUint(effectiveBaseFee, gasUsed))
		// Refund any unused gas, without overdrafting the L1 deposit.
		networkRefund := gasRefund
		if p.state.ArbOSVersion() >= 11 {
			infraFeeAccount, err := p.state.InfraFeeAccount()
			p.state.Restrict(err)
			if infraFeeAccount != (common.Address{}) {
				minBaseFee, err := p.state.L2PricingState().MinBaseFeeWei()
				p.state.Restrict(err)
				// TODO MinBaseFeeWei change during RetryTx execution may cause incorrect calculation of the part of the refund that should be taken from infraFeeAccount. Unless the balances of network and infra fee accounts are too low, the amount transferred to refund address should remain correct.
				infraFee := arbmath.BigMin(minBaseFee, effectiveBaseFee)
				infraRefund := arbmath.BigMulByUint(infraFee, gasLeft)
				infraRefund = takeFunds(networkRefund, infraRefund)
				refund(infraFeeAccount, infraRefund)
			}
		}
		refund(networkFeeAccount, networkRefund)

		if success {
			// we don't want to charge for this
			tracingInfo := util.NewTracingInfo(p.evm, arbosAddress, p.msg.From, scenario)
			state := arbosState.OpenSystemArbosStateOrPanic(p.evm.StateDB, tracingInfo, false)
			_, _ = state.RetryableState().DeleteRetryable(inner.TicketId, p.evm, scenario)
		} else {
			// return the Callvalue to escrow
			escrow := retryables.RetryableEscrowAddress(inner.TicketId)
			err := util.TransferBalance(&inner.From, &escrow, inner.Value, p.evm, scenario, "escrow")
			if err != nil {
				// should be impossible because geth credited the inner.Value to inner.From before the transaction
				// and the transaction reverted
				panic(err)
			}
		}
		// we've already credited the network fee account, but we didn't charge the gas pool yet
		p.state.Restrict(p.state.L2PricingState().AddToGasPool(-arbmath.SaturatingCast(gasUsed)))
		return
	}

	basefee := p.evm.Context.BaseFee
	totalCost := arbmath.BigMul(basefee, arbmath.UintToBig(gasUsed)) // total cost = price of gas * gas burnt
	computeCost := arbmath.BigSub(totalCost, p.PosterFee)            // total cost = network's compute + poster's L1 costs
	if computeCost.Sign() < 0 {
		// Uh oh, there's a bug in our charging code.
		// Give all funds to the network account and continue.

		log.Error("total cost < poster cost", "gasUsed", gasUsed, "basefee", basefee, "posterFee", p.PosterFee)
		p.PosterFee = big.NewInt(0)
		computeCost = totalCost
	}

	purpose := "feeCollection"
	if p.state.ArbOSVersion() > 4 {
		infraFeeAccount, err := p.state.InfraFeeAccount()
		p.state.Restrict(err)
		if infraFeeAccount != (common.Address{}) {
			minBaseFee, err := p.state.L2PricingState().MinBaseFeeWei()
			p.state.Restrict(err)
			infraFee := arbmath.BigMin(minBaseFee, basefee)
			computeGas := arbmath.SaturatingUSub(gasUsed, p.posterGas)
			infraComputeCost := arbmath.BigMulByUint(infraFee, computeGas)
			util.MintBalance(&infraFeeAccount, infraComputeCost, p.evm, scenario, purpose)
			computeCost = arbmath.BigSub(computeCost, infraComputeCost)
		}
	}
	if arbmath.BigGreaterThan(computeCost, common.Big0) {
		util.MintBalance(&networkFeeAccount, computeCost, p.evm, scenario, purpose)
	}
	posterFeeDestination := l1pricing.L1PricerFundsPoolAddress
	if p.state.ArbOSVersion() < 2 {
		posterFeeDestination = p.evm.Context.Coinbase
	}
	util.MintBalance(&posterFeeDestination, p.PosterFee, p.evm, scenario, purpose)
	if p.state.ArbOSVersion() >= 10 {
		if _, err := p.state.L1PricingState().AddToL1FeesAvailable(p.PosterFee); err != nil {
			log.Error("failed to update L1FeesAvailable: ", "err", err)
		}
	}

	if p.msg.GasPrice.Sign() > 0 { // in tests, gas price could be 0
		// ArbOS's gas pool is meant to enforce the computational speed-limit.
		// We don't want to remove from the pool the poster's L1 costs (as expressed in L2 gas in this func)
		// Hence, we deduct the previously saved poster L2-gas-equivalent to reveal the compute-only gas

		var computeGas uint64
		if gasUsed > p.posterGas {
			// Don't include posterGas in computeGas as it doesn't represent processing time.
			computeGas = gasUsed - p.posterGas
		} else {
			// Somehow, the core message transition succeeded, but we didn't burn the posterGas.
			// An invariant was violated. To be safe, subtract the entire gas used from the gas pool.
			log.Error("total gas used < poster gas component", "gasUsed", gasUsed, "posterGas", p.posterGas)
			computeGas = gasUsed
		}
		p.state.Restrict(p.state.L2PricingState().AddToGasPool(-arbmath.SaturatingCast(computeGas)))
	}
}

func (p *TxProcessor) ScheduledTxes() types.Transactions {
	scheduled := types.Transactions{}
	time := p.evm.Context.Time
	effectiveBaseFee := p.evm.Context.BaseFee
	chainID := p.evm.ChainConfig().ChainID

	if p.msg.TxRunMode != core.MessageCommitMode && p.msg.GasFeeCap.BitLen() == 0 {
		// In gas estimation or eth_call mode, we permit a zero gas fee cap.
		// This matches behavior with normal tx gas estimation and eth_call.
		effectiveBaseFee = common.Big0
	}

	logs := p.evm.StateDB.GetCurrentTxLogs()
	for _, log := range logs {
		if log.Address != ArbRetryableTxAddress || log.Topics[0] != RedeemScheduledEventID {
			continue
		}
		event := &precompilesgen.ArbRetryableTxRedeemScheduled{}
		err := util.ParseRedeemScheduledLog(event, log)
		if err != nil {
			glog.Error("Failed to parse RedeemScheduled log", "err", err)
			continue
		}
		retryable, err := p.state.RetryableState().OpenRetryable(event.TicketId, time)
		if err != nil || retryable == nil {
			continue
		}
		redeem, _ := retryable.MakeTx(
			chainID,
			event.SequenceNum,
			effectiveBaseFee,
			event.DonatedGas,
			event.TicketId,
			event.GasDonor,
			event.MaxRefund,
			event.SubmissionFeeRefund,
		)
		scheduled = append(scheduled, types.NewTx(redeem))
	}
	return scheduled
}

func (p *TxProcessor) L1BlockNumber(blockCtx vm.BlockContext) (uint64, error) {
	if p.cachedL1BlockNumber != nil {
		return *p.cachedL1BlockNumber, nil
	}
	tracingInfo := util.NewTracingInfo(p.evm, p.msg.From, arbosAddress, util.TracingDuringEVM)
	state, err := arbosState.OpenSystemArbosState(p.evm.StateDB, tracingInfo, false)
	if err != nil {
		return 0, err
	}
	blockNum, err := state.Blockhashes().L1BlockNumber()
	if err != nil {
		return 0, err
	}
	p.cachedL1BlockNumber = &blockNum
	return blockNum, nil
}

func (p *TxProcessor) L1BlockHash(blockCtx vm.BlockContext, l1BlockNumber uint64) (common.Hash, error) {
	hash, cached := p.cachedL1BlockHashes[l1BlockNumber]
	if cached {
		return hash, nil
	}
	tracingInfo := util.NewTracingInfo(p.evm, p.msg.From, arbosAddress, util.TracingDuringEVM)
	state, err := arbosState.OpenSystemArbosState(p.evm.StateDB, tracingInfo, false)
	if err != nil {
		return common.Hash{}, err
	}
	hash, err = state.Blockhashes().BlockHash(l1BlockNumber)
	if err != nil {
		return common.Hash{}, err
	}
	p.cachedL1BlockHashes[l1BlockNumber] = hash
	return hash, nil
}

func (p *TxProcessor) DropTip() bool {
	version := p.state.ArbOSVersion()
	return version != 9 || p.delayedInbox
}

func (p *TxProcessor) GetPaidGasPrice() *big.Int {
	gasPrice := p.evm.GasPrice
	version := p.state.ArbOSVersion()
	if version != 9 {
		gasPrice = p.evm.Context.BaseFee
		if p.msg.TxRunMode != core.MessageCommitMode && p.msg.GasFeeCap.Sign() == 0 {
			gasPrice = common.Big0
		}
	}
	return gasPrice
}

func (p *TxProcessor) GasPriceOp(evm *vm.EVM) *big.Int {
	if p.state.ArbOSVersion() >= 3 {
		return p.GetPaidGasPrice()
	}
	return evm.GasPrice
}

func (p *TxProcessor) FillReceiptInfo(receipt *types.Receipt) {
	receipt.GasUsedForL1 = p.posterGas
}

func (p *TxProcessor) MsgIsNonMutating() bool {
	if p.msg == nil {
		return false
	}
	mode := p.msg.TxRunMode
	return mode == core.MessageGasEstimationMode || mode == core.MessageEthcallMode
}

'''
'''--- arbos/util/retryable_encoding_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"bytes"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/crypto"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/abi/bind/backends"
	"github.com/ethereum/go-ethereum/core"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/testhelpers"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
)

func TestRetryableEncoding(t *testing.T) {
	rand.Seed(time.Now().UnixMilli())
	fakeAddr := testhelpers.RandomAddress()
	key, err := crypto.GenerateKey()
	testhelpers.RequireImpl(t, err)
	auth, err := bind.NewKeyedTransactorWithChainID(key, big.NewInt(1337))
	testhelpers.RequireImpl(t, err)

	alloc := make(core.GenesisAlloc)
	alloc[fakeAddr] = core.GenesisAccount{
		Code:    []byte{0},
		Balance: big.NewInt(0),
	}
	alloc[auth.From] = core.GenesisAccount{
		Balance: big.NewInt(1000000000000000000),
	}
	client := backends.NewSimulatedBackend(alloc, 1000000)

	dest := testhelpers.RandomAddress()
	innerTx := &types.ArbitrumSubmitRetryableTx{
		ChainId:          big.NewInt(654645),
		RequestId:        common.BigToHash(big.NewInt(rand.Int63n(1 << 32))),
		From:             testhelpers.RandomAddress(),
		L1BaseFee:        big.NewInt(876876),
		DepositValue:     big.NewInt(145331),
		GasFeeCap:        big.NewInt(76456),
		Gas:              37655,
		RetryTo:          &dest,
		RetryValue:       big.NewInt(23454),
		Beneficiary:      testhelpers.RandomAddress(),
		MaxSubmissionFee: big.NewInt(567356),
		FeeRefundAddr:    testhelpers.RandomAddress(),
		RetryData:        testhelpers.RandomizeSlice(make([]byte, rand.Int()%512)),
	}

	con, err := precompilesgen.NewArbRetryableTx(fakeAddr, client)
	testhelpers.RequireImpl(t, err)

	var retryTo common.Address
	if innerTx.RetryTo != nil {
		retryTo = *innerTx.RetryTo
	}
	tx, err := con.SubmitRetryable(
		auth,
		innerTx.RequestId,
		innerTx.L1BaseFee,
		innerTx.DepositValue,
		innerTx.RetryValue,
		innerTx.GasFeeCap,
		innerTx.Gas,
		innerTx.MaxSubmissionFee,
		innerTx.FeeRefundAddr,
		innerTx.Beneficiary,
		retryTo,
		innerTx.RetryData,
	)
	testhelpers.RequireImpl(t, err)

	if !bytes.Equal(tx.Data(), types.NewTx(innerTx).Data()) {
		testhelpers.FailImpl(t, "incorrect data encoding")
	}
}

'''
'''--- arbos/util/tracing.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/holiman/uint256"
)

type TracingScenario uint64

const (
	TracingBeforeEVM TracingScenario = iota
	TracingDuringEVM
	TracingAfterEVM
)

type TracingInfo struct {
	Tracer   vm.EVMLogger
	Scenario TracingScenario
	Contract *vm.Contract
	Depth    int
}

// holds an address to satisfy core/vm's ContractRef() interface
type addressHolder struct {
	addr common.Address
}

func (a addressHolder) Address() common.Address {
	return a.addr
}

func NewTracingInfo(evm *vm.EVM, from, to common.Address, scenario TracingScenario) *TracingInfo {
	if evm.Config.Tracer == nil {
		return nil
	}
	return &TracingInfo{
		Tracer:   evm.Config.Tracer,
		Scenario: scenario,
		Contract: vm.NewContract(addressHolder{to}, addressHolder{from}, big.NewInt(0), 0),
		Depth:    evm.Depth(),
	}
}

func (info *TracingInfo) RecordStorageGet(key common.Hash) {
	tracer := info.Tracer
	if info.Scenario == TracingDuringEVM {
		scope := &vm.ScopeContext{
			Memory:   vm.NewMemory(),
			Stack:    TracingStackFromArgs(HashToUint256(key)),
			Contract: info.Contract,
		}
		tracer.CaptureState(0, vm.SLOAD, 0, 0, scope, []byte{}, info.Depth, nil)
	} else {
		tracer.CaptureArbitrumStorageGet(key, info.Depth, info.Scenario == TracingBeforeEVM)
	}
}

func (info *TracingInfo) RecordStorageSet(key, value common.Hash) {
	tracer := info.Tracer
	if info.Scenario == TracingDuringEVM {
		scope := &vm.ScopeContext{
			Memory:   vm.NewMemory(),
			Stack:    TracingStackFromArgs(HashToUint256(key), HashToUint256(value)),
			Contract: info.Contract,
		}
		tracer.CaptureState(0, vm.SSTORE, 0, 0, scope, []byte{}, info.Depth, nil)
	} else {
		tracer.CaptureArbitrumStorageSet(key, value, info.Depth, info.Scenario == TracingBeforeEVM)
	}
}

func (info *TracingInfo) MockCall(input []byte, gas uint64, from, to common.Address, amount *big.Int) {
	tracer := info.Tracer
	depth := info.Depth

	contract := vm.NewContract(addressHolder{to}, addressHolder{from}, amount, gas)

	scope := &vm.ScopeContext{
		Memory: TracingMemoryFromBytes(input),
		Stack: TracingStackFromArgs(
			*uint256.NewInt(gas),                        // gas
			*uint256.NewInt(0).SetBytes(to.Bytes()),     // to address
			*uint256.NewInt(0).SetBytes(amount.Bytes()), // call value
			*uint256.NewInt(0),                          // memory offset
			*uint256.NewInt(uint64(len(input))),         // memory length
			*uint256.NewInt(0),                          // return offset
			*uint256.NewInt(0),                          // return size
		),
		Contract: contract,
	}
	tracer.CaptureState(0, vm.CALL, 0, 0, scope, []byte{}, depth, nil)
	tracer.CaptureEnter(vm.INVALID, from, to, input, 0, amount)

	retScope := &vm.ScopeContext{
		Memory: vm.NewMemory(),
		Stack: TracingStackFromArgs(
			*uint256.NewInt(0), // return offset
			*uint256.NewInt(0), // return size
		),
		Contract: contract,
	}
	tracer.CaptureState(0, vm.RETURN, 0, 0, retScope, []byte{}, depth+1, nil)
	tracer.CaptureExit(nil, 0, nil)

	popScope := &vm.ScopeContext{
		Memory: vm.NewMemory(),
		Stack: TracingStackFromArgs(
			*uint256.NewInt(1), // CALL result success
		),
		Contract: contract,
	}
	tracer.CaptureState(0, vm.POP, 0, 0, popScope, []byte{}, depth, nil)
}

func HashToUint256(hash common.Hash) uint256.Int {
	value := uint256.Int{}
	value.SetBytes(hash.Bytes())
	return value
}

// TracingMemoryFromBytes creates an EVM Memory consisting of the bytes provided
func TracingMemoryFromBytes(input []byte) *vm.Memory {
	memory := vm.NewMemory()
	inputLen := uint64(len(input))
	memory.Resize(inputLen)
	memory.Set(0, inputLen, input)
	return memory
}

// TracingStackFromArgs creates an EVM Stack with the given arguments in canonical order
func TracingStackFromArgs(args ...uint256.Int) *vm.Stack {
	stack := &vm.Stack{}
	for flip := 0; flip < len(args)/2; flip++ { // reverse the order
		flop := len(args) - flip - 1
		args[flip], args[flop] = args[flop], args[flip]
	}
	stack.SetData(args)
	return stack
}

'''
'''--- arbos/util/transfer.go ---
//
// Copyright 2022, Offchain Labs, Inc. All rights reserved.
//

package util

import (
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/arbmath"
)

// TransferBalance represents a balance change occurring aside from a call.
// While most uses will be transfers, setting `from` or `to` to nil will mint or burn funds, respectively.
func TransferBalance(
	from, to *common.Address,
	amount *big.Int,
	evm *vm.EVM,
	scenario TracingScenario,
	purpose string,
) error {
	if amount.Sign() < 0 {
		panic(fmt.Sprintf("Tried to transfer negative amount %v from %v to %v", amount, from, to))
	}
	if from != nil {
		balance := evm.StateDB.GetBalance(*from)
		if arbmath.BigLessThan(balance, amount) {
			return fmt.Errorf("%w: addr %v have %v want %v", vm.ErrInsufficientBalance, *from, balance, amount)
		}
		evm.StateDB.SubBalance(*from, amount)
	}
	if to != nil {
		evm.StateDB.AddBalance(*to, amount)
	}
	if tracer := evm.Config.Tracer; tracer != nil {
		if evm.Depth() != 0 && scenario != TracingDuringEVM {
			// A non-zero depth implies this transfer is occurring inside EVM execution
			log.Error("Tracing scenario mismatch", "scenario", scenario, "depth", evm.Depth())
			return errors.New("tracing scenario mismatch")
		}

		if scenario != TracingDuringEVM {
			tracer.CaptureArbitrumTransfer(evm, from, to, amount, scenario == TracingBeforeEVM, purpose)
			return nil
		}

		if from == nil {
			from = &common.Address{}
		}
		if to == nil {
			to = &common.Address{}
		}

		info := &TracingInfo{
			Tracer:   evm.Config.Tracer,
			Scenario: scenario,
			Contract: vm.NewContract(addressHolder{*to}, addressHolder{*from}, big.NewInt(0), 0),
			Depth:    evm.Depth(),
		}
		info.MockCall([]byte{}, 0, *from, *to, amount)
	}
	return nil
}

// MintBalance mints funds for the user and adds them to their balance
func MintBalance(to *common.Address, amount *big.Int, evm *vm.EVM, scenario TracingScenario, purpose string) {
	err := TransferBalance(nil, to, amount, evm, scenario, purpose)
	if err != nil {
		panic(fmt.Sprintf("impossible error: %v", err))
	}
}

// BurnBalance burns funds from a user's account
func BurnBalance(from *common.Address, amount *big.Int, evm *vm.EVM, scenario TracingScenario, purpose string) error {
	return TransferBalance(from, nil, amount, evm, scenario, purpose)
}

'''
'''--- arbos/util/util.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"encoding/binary"
	"errors"
	"fmt"
	"io"
	"math/big"
	"strings"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

var AddressAliasOffset *big.Int
var InverseAddressAliasOffset *big.Int
var ParseRedeemScheduledLog func(interface{}, *types.Log) error
var ParseL2ToL1TransactionLog func(interface{}, *types.Log) error
var ParseL2ToL1TxLog func(interface{}, *types.Log) error
var PackInternalTxDataStartBlock func(...interface{}) ([]byte, error)
var UnpackInternalTxDataStartBlock func([]byte) (map[string]interface{}, error)
var PackInternalTxDataBatchPostingReport func(...interface{}) ([]byte, error)
var UnpackInternalTxDataBatchPostingReport func([]byte) (map[string]interface{}, error)
var PackArbRetryableTxRedeem func(...interface{}) ([]byte, error)

func init() {
	offset, success := new(big.Int).SetString("0x1111000000000000000000000000000000001111", 0)
	if !success {
		panic("Error initializing AddressAliasOffset")
	}
	AddressAliasOffset = offset
	InverseAddressAliasOffset = arbmath.BigSub(new(big.Int).Lsh(big.NewInt(1), 160), AddressAliasOffset)

	// Create a mechanism for parsing event logs
	logParser := func(source string, name string) func(interface{}, *types.Log) error {
		precompile, err := abi.JSON(strings.NewReader(source))
		if err != nil {
			panic(fmt.Sprintf("failed to parse ABI for %s: %s", name, err))
		}
		inputs := precompile.Events[name].Inputs
		indexed := abi.Arguments{}
		for _, input := range inputs {
			if input.Indexed {
				indexed = append(indexed, input)
			}
		}

		return func(event interface{}, log *types.Log) error {
			unpacked, err := inputs.Unpack(log.Data)
			if err != nil {
				return err
			}
			if err := inputs.Copy(event, unpacked); err != nil {
				return err
			}
			return abi.ParseTopics(event, indexed, log.Topics[1:])
		}
	}

	// Create a mechanism for packing and unpacking calls
	callParser := func(source string, name string) (func(...interface{}) ([]byte, error), func([]byte) (map[string]interface{}, error)) {
		contract, err := abi.JSON(strings.NewReader(source))
		if err != nil {
			panic(fmt.Sprintf("failed to parse ABI for %s: %s", name, err))
		}
		method, ok := contract.Methods[name]
		if !ok {
			panic(fmt.Sprintf("method %v does not exist", name))
		}
		pack := func(args ...interface{}) ([]byte, error) {
			return contract.Pack(name, args...)
		}
		unpack := func(data []byte) (map[string]interface{}, error) {
			if len(data) < 4 {
				return nil, errors.New("data not long enough")
			}
			args := make(map[string]interface{})
			return args, method.Inputs.UnpackIntoMap(args, data[4:])
		}
		return pack, unpack
	}

	ParseRedeemScheduledLog = logParser(precompilesgen.ArbRetryableTxABI, "RedeemScheduled")
	ParseL2ToL1TxLog = logParser(precompilesgen.ArbSysABI, "L2ToL1Tx")
	ParseL2ToL1TransactionLog = logParser(precompilesgen.ArbSysABI, "L2ToL1Transaction")

	acts := precompilesgen.ArbosActsABI
	PackInternalTxDataStartBlock, UnpackInternalTxDataStartBlock = callParser(acts, "startBlock")
	PackInternalTxDataBatchPostingReport, UnpackInternalTxDataBatchPostingReport = callParser(acts, "batchPostingReport")
	PackArbRetryableTxRedeem, _ = callParser(precompilesgen.ArbRetryableTxABI, "redeem")
}

func AddressToHash(address common.Address) common.Hash {
	return common.BytesToHash(address.Bytes())
}

func HashFromReader(rd io.Reader) (common.Hash, error) {
	buf := make([]byte, 32)
	if _, err := io.ReadFull(rd, buf); err != nil {
		return common.Hash{}, err
	}
	return common.BytesToHash(buf), nil
}

func Uint256FromReader(rd io.Reader) (*big.Int, error) {
	asHash, err := HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	return asHash.Big(), nil
}

func HashToWriter(val common.Hash, wr io.Writer) error {
	_, err := wr.Write(val.Bytes())
	return err
}

func AddressFromReader(rd io.Reader) (common.Address, error) {
	buf := make([]byte, 20)
	if _, err := io.ReadFull(rd, buf); err != nil {
		return common.Address{}, err
	}
	return common.BytesToAddress(buf), nil
}

func AddressFrom256FromReader(rd io.Reader) (common.Address, error) {
	h, err := HashFromReader(rd)
	if err != nil {
		return common.Address{}, err
	}
	return common.BytesToAddress(h.Bytes()[12:]), nil
}

func AddressToWriter(val common.Address, wr io.Writer) error {
	_, err := wr.Write(val.Bytes())
	return err
}

func AddressTo256ToWriter(val common.Address, wr io.Writer) error {
	if _, err := wr.Write(make([]byte, 12)); err != nil {
		return err
	}
	return AddressToWriter(val, wr)
}

func Uint64FromReader(rd io.Reader) (uint64, error) {
	buf := make([]byte, 8)
	if _, err := io.ReadFull(rd, buf); err != nil {
		return 0, err
	}
	return binary.BigEndian.Uint64(buf), nil
}

func Uint64ToWriter(val uint64, wr io.Writer) error {
	var buf [8]byte
	binary.BigEndian.PutUint64(buf[:], val)
	_, err := wr.Write(buf[:])
	return err
}

func BytestringFromReader(rd io.Reader, maxBytesToRead uint64) ([]byte, error) {
	size, err := Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}
	if size > maxBytesToRead {
		return nil, errors.New("size too large in ByteStringFromReader")
	}
	buf := make([]byte, size)
	if _, err = io.ReadFull(rd, buf); err != nil {
		return nil, err
	}
	return buf, nil
}

func BytestringToWriter(val []byte, wr io.Writer) error {
	if err := Uint64ToWriter(uint64(len(val)), wr); err != nil {
		return err
	}
	_, err := wr.Write(val)
	return err
}

func IntToHash(val int64) common.Hash {
	return common.BigToHash(big.NewInt(val))
}

func UintToHash(val uint64) common.Hash {
	return common.BigToHash(new(big.Int).SetUint64(val))
}

func HashPlusInt(x common.Hash, y int64) common.Hash {
	return common.BigToHash(new(big.Int).Add(x.Big(), big.NewInt(y))) //BUGBUG: BigToHash(x) converts abs(x) to a Hash
}

func RemapL1Address(l1Addr common.Address) common.Address {
	sumBytes := new(big.Int).Add(new(big.Int).SetBytes(l1Addr.Bytes()), AddressAliasOffset).Bytes()
	if len(sumBytes) > 20 {
		sumBytes = sumBytes[len(sumBytes)-20:]
	}
	return common.BytesToAddress(sumBytes)
}

func InverseRemapL1Address(l1Addr common.Address) common.Address {
	sumBytes := new(big.Int).Add(new(big.Int).SetBytes(l1Addr.Bytes()), InverseAddressAliasOffset).Bytes()
	if len(sumBytes) > 20 {
		sumBytes = sumBytes[len(sumBytes)-20:]
	}
	return common.BytesToAddress(sumBytes)
}

func DoesTxTypeAlias(txType *byte) bool {
	if txType == nil {
		return false
	}
	switch *txType {
	case types.ArbitrumUnsignedTxType:
		fallthrough
	case types.ArbitrumContractTxType:
		fallthrough
	case types.ArbitrumRetryTxType:
		return true
	}
	return false
}

func TxTypeHasPosterCosts(txType byte) bool {
	switch txType {
	case types.ArbitrumUnsignedTxType:
		fallthrough
	case types.ArbitrumContractTxType:
		fallthrough
	case types.ArbitrumRetryTxType:
		fallthrough
	case types.ArbitrumInternalTxType:
		fallthrough
	case types.ArbitrumSubmitRetryableTxType:
		return false
	}
	return true
}

func SafeMapGet[T any](kvs map[string]interface{}, field string) T {
	value, ok := kvs[field]
	if !ok {
		panic(fmt.Sprintf("map does not contain field %v", field))
	}
	cast, ok := value.(T)
	if !ok {
		panic(fmt.Sprintf("field %v is of the wrong type", field))
	}
	return cast
}

'''
'''--- arbstate/das_reader.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbstate

import (
	"bufio"
	"bytes"
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"io"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/das/dastree"
)

type DataAvailabilityReader interface {
	GetByHash(ctx context.Context, hash common.Hash) ([]byte, error)
	ExpirationPolicy(ctx context.Context) (ExpirationPolicy, error)
}

var ErrHashMismatch = errors.New("result does not match expected hash")

// DASMessageHeaderFlag indicates that this data is a certificate for the data availability service,
// which will retrieve the full batch data.
const DASMessageHeaderFlag byte = 0x80

// TreeDASMessageHeaderFlag indicates that this DAS certificate data employs the new merkelization strategy.
// Ignored when DASMessageHeaderFlag is not set.
const TreeDASMessageHeaderFlag byte = 0x08

// L1AuthenticatedMessageHeaderFlag indicates that this message was authenticated by L1. Currently unused.
const L1AuthenticatedMessageHeaderFlag byte = 0x40

// ZeroheavyMessageHeaderFlag indicates that this message is zeroheavy-encoded.
const ZeroheavyMessageHeaderFlag byte = 0x20

// BrotliMessageHeaderByte indicates that the message is brotli-compressed.
const BrotliMessageHeaderByte byte = 0

func IsDASMessageHeaderByte(header byte) bool {
	return (DASMessageHeaderFlag & header) > 0
}

func IsTreeDASMessageHeaderByte(header byte) bool {
	return (TreeDASMessageHeaderFlag & header) > 0
}

func IsZeroheavyEncodedHeaderByte(header byte) bool {
	return (ZeroheavyMessageHeaderFlag & header) > 0
}

func IsBrotliMessageHeaderByte(b uint8) bool {
	return b == BrotliMessageHeaderByte
}

type DataAvailabilityCertificate struct {
	KeysetHash  [32]byte
	DataHash    [32]byte
	Timeout     uint64
	SignersMask uint64
	Sig         blsSignatures.Signature
	Version     uint8
}

func DeserializeDASCertFrom(rd io.Reader) (c *DataAvailabilityCertificate, err error) {
	r := bufio.NewReader(rd)
	c = &DataAvailabilityCertificate{}

	header, err := r.ReadByte()
	if err != nil {
		return nil, err
	}
	if !IsDASMessageHeaderByte(header) {
		return nil, errors.New("tried to deserialize a message that doesn't have the DAS header")
	}

	_, err = io.ReadFull(r, c.KeysetHash[:])
	if err != nil {
		return nil, err
	}

	_, err = io.ReadFull(r, c.DataHash[:])
	if err != nil {
		return nil, err
	}

	var timeoutBuf [8]byte
	_, err = io.ReadFull(r, timeoutBuf[:])
	if err != nil {
		return nil, err
	}
	c.Timeout = binary.BigEndian.Uint64(timeoutBuf[:])

	if IsTreeDASMessageHeaderByte(header) {
		var versionBuf [1]byte
		_, err = io.ReadFull(r, versionBuf[:])
		if err != nil {
			return nil, err
		}
		c.Version = versionBuf[0]
	}

	var signersMaskBuf [8]byte
	_, err = io.ReadFull(r, signersMaskBuf[:])
	if err != nil {
		return nil, err
	}
	c.SignersMask = binary.BigEndian.Uint64(signersMaskBuf[:])

	var blsSignaturesBuf [96]byte
	_, err = io.ReadFull(r, blsSignaturesBuf[:])
	if err != nil {
		return nil, err
	}
	c.Sig, err = blsSignatures.SignatureFromBytes(blsSignaturesBuf[:])
	if err != nil {
		return nil, err
	}

	return c, nil
}

func (c *DataAvailabilityCertificate) SerializeSignableFields() []byte {
	buf := make([]byte, 0, 32+9)
	buf = append(buf, c.DataHash[:]...)

	var intData [8]byte
	binary.BigEndian.PutUint64(intData[:], c.Timeout)
	buf = append(buf, intData[:]...)

	if c.Version != 0 {
		buf = append(buf, c.Version)
	}

	return buf
}

func (c *DataAvailabilityCertificate) RecoverKeyset(
	ctx context.Context,
	da DataAvailabilityReader,
	assumeKeysetValid bool,
) (*DataAvailabilityKeyset, error) {
	keysetBytes, err := da.GetByHash(ctx, c.KeysetHash)
	if err != nil {
		return nil, err
	}
	if !dastree.ValidHash(c.KeysetHash, keysetBytes) {
		return nil, errors.New("keyset hash does not match cert")
	}
	return DeserializeKeyset(bytes.NewReader(keysetBytes), assumeKeysetValid)
}

type DataAvailabilityKeyset struct {
	AssumedHonest uint64
	PubKeys       []blsSignatures.PublicKey
}

func (keyset *DataAvailabilityKeyset) Serialize(wr io.Writer) error {
	if err := util.Uint64ToWriter(keyset.AssumedHonest, wr); err != nil {
		return err
	}
	if err := util.Uint64ToWriter(uint64(len(keyset.PubKeys)), wr); err != nil {
		return err
	}
	for _, pk := range keyset.PubKeys {
		pkBuf := blsSignatures.PublicKeyToBytes(pk)
		buf := []byte{byte(len(pkBuf) / 256), byte(len(pkBuf) % 256)}
		_, err := wr.Write(append(buf, pkBuf...))
		if err != nil {
			return err
		}
	}
	return nil
}

func (keyset *DataAvailabilityKeyset) Hash() (common.Hash, error) {
	wr := bytes.NewBuffer([]byte{})
	if err := keyset.Serialize(wr); err != nil {
		return common.Hash{}, err
	}
	if wr.Len() > dastree.BinSize {
		return common.Hash{}, errors.New("keyset too large")
	}
	return dastree.Hash(wr.Bytes()), nil
}

func DeserializeKeyset(rd io.Reader, assumeKeysetValid bool) (*DataAvailabilityKeyset, error) {
	assumedHonest, err := util.Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}
	numKeys, err := util.Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}
	if numKeys > 64 {
		return nil, errors.New("too many keys in serialized DataAvailabilityKeyset")
	}
	pubkeys := make([]blsSignatures.PublicKey, numKeys)
	buf2 := []byte{0, 0}
	for i := uint64(0); i < numKeys; i++ {
		if _, err := io.ReadFull(rd, buf2); err != nil {
			return nil, err
		}
		buf := make([]byte, int(buf2[0])*256+int(buf2[1]))
		if _, err := io.ReadFull(rd, buf); err != nil {
			return nil, err
		}
		pubkeys[i], err = blsSignatures.PublicKeyFromBytes(buf, assumeKeysetValid)
		if err != nil {
			return nil, err
		}
	}
	return &DataAvailabilityKeyset{
		AssumedHonest: assumedHonest,
		PubKeys:       pubkeys,
	}, nil
}

func (keyset *DataAvailabilityKeyset) VerifySignature(signersMask uint64, data []byte, sig blsSignatures.Signature) error {
	pubkeys := []blsSignatures.PublicKey{}
	numNonSigners := uint64(0)
	for i := 0; i < len(keyset.PubKeys); i++ {
		if (1<<i)&signersMask != 0 {
			pubkeys = append(pubkeys, keyset.PubKeys[i])
		} else {
			numNonSigners++
		}
	}
	if numNonSigners >= keyset.AssumedHonest {
		return errors.New("not enough signers")
	}
	aggregatedPubKey := blsSignatures.AggregatePublicKeys(pubkeys)
	success, err := blsSignatures.VerifySignature(sig, data, aggregatedPubKey)

	if err != nil {
		return err
	}
	if !success {
		return errors.New("bad signature")
	}
	return nil
}

type ExpirationPolicy int64

const (
	KeepForever                ExpirationPolicy = iota // Data is kept forever
	DiscardAfterArchiveTimeout                         // Data is kept till Archive timeout (Archive Timeout is defined by archiving node, assumed to be as long as minimum data timeout)
	DiscardAfterDataTimeout                            // Data is kept till aggregator provided timeout (Aggregator provides a timeout for data while making the put call)
	MixedTimeout                                       // Used for cases with mixed type of timeout policy(Mainly used for aggregators which have data availability services with multiply type of timeout policy)
	DiscardImmediately                                 // Data is never stored (Mainly used for empty/wrapper/placeholder classes)
	// Add more type of expiration policy.
)

func (ep ExpirationPolicy) String() (string, error) {
	switch ep {
	case KeepForever:
		return "KeepForever", nil
	case DiscardAfterArchiveTimeout:
		return "DiscardAfterArchiveTimeout", nil
	case DiscardAfterDataTimeout:
		return "DiscardAfterDataTimeout", nil
	case MixedTimeout:
		return "MixedTimeout", nil
	case DiscardImmediately:
		return "DiscardImmediately", nil
	default:
		return "", errors.New("unknown Expiration Policy")
	}
}

func StringToExpirationPolicy(s string) (ExpirationPolicy, error) {
	switch s {
	case "KeepForever":
		return KeepForever, nil
	case "DiscardAfterArchiveTimeout":
		return DiscardAfterArchiveTimeout, nil
	case "DiscardAfterDataTimeout":
		return DiscardAfterDataTimeout, nil
	case "MixedTimeout":
		return MixedTimeout, nil
	case "DiscardImmediately":
		return DiscardImmediately, nil
	default:
		return -1, fmt.Errorf("invalid Expiration Policy: %s", s)
	}
}

'''
'''--- arbstate/inbox.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbstate

import (
	"bytes"
	"context"
	"encoding/binary"
	"errors"
	"io"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/zeroheavy"
)

type InboxBackend interface {
	PeekSequencerInbox() ([]byte, error)

	GetSequencerInboxPosition() uint64
	AdvanceSequencerInbox()

	GetPositionWithinMessage() uint64
	SetPositionWithinMessage(pos uint64)

	ReadDelayedInbox(seqNum uint64) (*arbostypes.L1IncomingMessage, error)
}

type sequencerMessage struct {
	minTimestamp         uint64
	maxTimestamp         uint64
	minL1Block           uint64
	maxL1Block           uint64
	afterDelayedMessages uint64
	segments             [][]byte
}

const MaxDecompressedLen int = 1024 * 1024 * 16 // 16 MiB
const maxZeroheavyDecompressedLen = 101*MaxDecompressedLen/100 + 64
const MaxSegmentsPerSequencerMessage = 100 * 1024
const MinLifetimeSecondsForDataAvailabilityCert = 7 * 24 * 60 * 60 // one week

func parseSequencerMessage(ctx context.Context, batchNum uint64, data []byte, dasReader DataAvailabilityReader, keysetValidationMode KeysetValidationMode) (*sequencerMessage, error) {
	if len(data) < 40 {
		return nil, errors.New("sequencer message missing L1 header")
	}
	parsedMsg := &sequencerMessage{
		minTimestamp:         binary.BigEndian.Uint64(data[:8]),
		maxTimestamp:         binary.BigEndian.Uint64(data[8:16]),
		minL1Block:           binary.BigEndian.Uint64(data[16:24]),
		maxL1Block:           binary.BigEndian.Uint64(data[24:32]),
		afterDelayedMessages: binary.BigEndian.Uint64(data[32:40]),
		segments:             [][]byte{},
	}
	payload := data[40:]

	if len(payload) > 0 && IsDASMessageHeaderByte(payload[0]) {
		if dasReader == nil {
			log.Error("No DAS Reader configured, but sequencer message found with DAS header")
		} else {
			var err error
			payload, err = RecoverPayloadFromDasBatch(ctx, batchNum, data, dasReader, nil, keysetValidationMode)
			if err != nil {
				return nil, err
			}
			if payload == nil {
				return parsedMsg, nil
			}
		}
	}

	if len(payload) > 0 && IsZeroheavyEncodedHeaderByte(payload[0]) {
		pl, err := io.ReadAll(io.LimitReader(zeroheavy.NewZeroheavyDecoder(bytes.NewReader(payload[1:])), int64(maxZeroheavyDecompressedLen)))
		if err != nil {
			log.Warn("error reading from zeroheavy decoder", err.Error())
			return parsedMsg, nil
		}
		payload = pl
	}

	if len(payload) > 0 && IsBrotliMessageHeaderByte(payload[0]) {
		decompressed, err := arbcompress.Decompress(payload[1:], MaxDecompressedLen)
		if err == nil {
			reader := bytes.NewReader(decompressed)
			stream := rlp.NewStream(reader, uint64(MaxDecompressedLen))
			for {
				var segment []byte
				err := stream.Decode(&segment)
				if err != nil {
					if !errors.Is(err, io.EOF) && !errors.Is(err, io.ErrUnexpectedEOF) {
						log.Warn("error parsing sequencer message segment", "err", err.Error())
					}
					break
				}
				if len(parsedMsg.segments) >= MaxSegmentsPerSequencerMessage {
					log.Warn("too many segments in sequence batch")
					break
				}
				parsedMsg.segments = append(parsedMsg.segments, segment)
			}
		} else {
			log.Warn("sequencer msg decompression failed", "err", err)
		}
	} else {
		length := len(payload)
		if length == 0 {
			log.Warn("empty sequencer message")
		} else {
			log.Warn("unknown sequencer message format", "length", length, "firstByte", payload[0])
		}

	}

	return parsedMsg, nil
}

func RecoverPayloadFromDasBatch(
	ctx context.Context,
	batchNum uint64,
	sequencerMsg []byte,
	dasReader DataAvailabilityReader,
	preimages map[arbutil.PreimageType]map[common.Hash][]byte,
	keysetValidationMode KeysetValidationMode,
) ([]byte, error) {
	var keccakPreimages map[common.Hash][]byte
	if preimages != nil {
		if preimages[arbutil.Keccak256PreimageType] == nil {
			preimages[arbutil.Keccak256PreimageType] = make(map[common.Hash][]byte)
		}
		keccakPreimages = preimages[arbutil.Keccak256PreimageType]
	}
	cert, err := DeserializeDASCertFrom(bytes.NewReader(sequencerMsg[40:]))
	if err != nil {
		log.Error("Failed to deserialize DAS message", "err", err)
		return nil, nil
	}
	version := cert.Version
	recordPreimage := func(key common.Hash, value []byte) {
		keccakPreimages[key] = value
	}

	if version >= 2 {
		log.Error("Your node software is probably out of date", "certificateVersion", version)
		return nil, nil
	}

	getByHash := func(ctx context.Context, hash common.Hash) ([]byte, error) {
		newHash := hash
		if version == 0 {
			newHash = dastree.FlatHashToTreeHash(hash)
		}

		preimage, err := dasReader.GetByHash(ctx, newHash)
		if err != nil && hash != newHash {
			log.Debug("error fetching new style hash, trying old", "new", newHash, "old", hash, "err", err)
			preimage, err = dasReader.GetByHash(ctx, hash)
		}
		if err != nil {
			return nil, err
		}

		switch {
		case version == 0 && crypto.Keccak256Hash(preimage) != hash:
			fallthrough
		case version == 1 && dastree.Hash(preimage) != hash:
			log.Error(
				"preimage mismatch for hash",
				"hash", hash, "err", ErrHashMismatch, "version", version,
			)
			return nil, ErrHashMismatch
		}
		return preimage, nil
	}

	keysetPreimage, err := getByHash(ctx, cert.KeysetHash)
	if err != nil {
		log.Error("Couldn't get keyset", "err", err)
		return nil, err
	}
	if keccakPreimages != nil {
		dastree.RecordHash(recordPreimage, keysetPreimage)
	}

	keyset, err := DeserializeKeyset(bytes.NewReader(keysetPreimage), keysetValidationMode == KeysetDontValidate)
	if err != nil {
		logLevel := log.Error
		if keysetValidationMode == KeysetPanicIfInvalid {
			logLevel = log.Crit
		}
		logLevel("Couldn't deserialize keyset", "err", err, "keysetHash", cert.KeysetHash, "batchNum", batchNum)
		return nil, nil
	}
	err = keyset.VerifySignature(cert.SignersMask, cert.SerializeSignableFields(), cert.Sig)
	if err != nil {
		log.Error("Bad signature on DAS batch", "err", err)
		return nil, nil
	}

	maxTimestamp := binary.BigEndian.Uint64(sequencerMsg[8:16])
	if cert.Timeout < maxTimestamp+MinLifetimeSecondsForDataAvailabilityCert {
		log.Error("Data availability cert expires too soon", "err", "")
		return nil, nil
	}

	dataHash := cert.DataHash
	payload, err := getByHash(ctx, dataHash)
	if err != nil {
		log.Error("Couldn't fetch DAS batch contents", "err", err)
		return nil, err
	}

	if keccakPreimages != nil {
		if version == 0 {
			treeLeaf := dastree.FlatHashToTreeLeaf(dataHash)
			keccakPreimages[dataHash] = payload
			keccakPreimages[crypto.Keccak256Hash(treeLeaf)] = treeLeaf
		} else {
			dastree.RecordHash(recordPreimage, payload)
		}
	}

	return payload, nil
}

type KeysetValidationMode uint8

const KeysetValidate KeysetValidationMode = 0
const KeysetPanicIfInvalid KeysetValidationMode = 1
const KeysetDontValidate KeysetValidationMode = 2

type inboxMultiplexer struct {
	backend                   InboxBackend
	delayedMessagesRead       uint64
	dasReader                 DataAvailabilityReader
	cachedSequencerMessage    *sequencerMessage
	cachedSequencerMessageNum uint64
	cachedSegmentNum          uint64
	cachedSegmentTimestamp    uint64
	cachedSegmentBlockNumber  uint64
	cachedSubMessageNumber    uint64
	keysetValidationMode      KeysetValidationMode
}

func NewInboxMultiplexer(backend InboxBackend, delayedMessagesRead uint64, dasReader DataAvailabilityReader, keysetValidationMode KeysetValidationMode) arbostypes.InboxMultiplexer {
	return &inboxMultiplexer{
		backend:              backend,
		delayedMessagesRead:  delayedMessagesRead,
		dasReader:            dasReader,
		keysetValidationMode: keysetValidationMode,
	}
}

const BatchSegmentKindL2Message uint8 = 0
const BatchSegmentKindL2MessageBrotli uint8 = 1
const BatchSegmentKindDelayedMessages uint8 = 2
const BatchSegmentKindAdvanceTimestamp uint8 = 3
const BatchSegmentKindAdvanceL1BlockNumber uint8 = 4

// Pop returns the message from the top of the sequencer inbox and removes it from the queue.
// Note: this does *not* return parse errors, those are transformed into invalid messages
func (r *inboxMultiplexer) Pop(ctx context.Context) (*arbostypes.MessageWithMetadata, error) {
	if r.cachedSequencerMessage == nil {
		bytes, realErr := r.backend.PeekSequencerInbox()
		if realErr != nil {
			return nil, realErr
		}
		r.cachedSequencerMessageNum = r.backend.GetSequencerInboxPosition()
		var err error
		r.cachedSequencerMessage, err = parseSequencerMessage(ctx, r.cachedSequencerMessageNum, bytes, r.dasReader, r.keysetValidationMode)
		if err != nil {
			return nil, err
		}
	}
	msg, err := r.getNextMsg()
	// advance even if there was an error
	if r.IsCachedSegementLast() {
		r.advanceSequencerMsg()
	} else {
		r.advanceSubMsg()
	}
	// parsing error in getNextMsg
	if msg == nil && err == nil {
		msg = &arbostypes.MessageWithMetadata{
			Message:             arbostypes.InvalidL1Message,
			DelayedMessagesRead: r.delayedMessagesRead,
		}
	}
	return msg, err
}

func (r *inboxMultiplexer) advanceSequencerMsg() {
	if r.cachedSequencerMessage != nil {
		r.delayedMessagesRead = r.cachedSequencerMessage.afterDelayedMessages
	}
	r.backend.SetPositionWithinMessage(0)
	r.backend.AdvanceSequencerInbox()
	r.cachedSequencerMessage = nil
	r.cachedSegmentNum = 0
	r.cachedSegmentTimestamp = 0
	r.cachedSegmentBlockNumber = 0
	r.cachedSubMessageNumber = 0
}

func (r *inboxMultiplexer) advanceSubMsg() {
	prevPos := r.backend.GetPositionWithinMessage()
	r.backend.SetPositionWithinMessage(prevPos + 1)
}

func (r *inboxMultiplexer) IsCachedSegementLast() bool {
	seqMsg := r.cachedSequencerMessage
	// we issue delayed messages until reaching afterDelayedMessages
	if r.delayedMessagesRead < seqMsg.afterDelayedMessages {
		return false
	}
	for segmentNum := int(r.cachedSegmentNum) + 1; segmentNum < len(seqMsg.segments); segmentNum++ {
		segment := seqMsg.segments[segmentNum]
		if len(segment) == 0 {
			continue
		}
		kind := segment[0]
		if kind == BatchSegmentKindL2Message || kind == BatchSegmentKindL2MessageBrotli {
			return false
		}
		if kind == BatchSegmentKindDelayedMessages {
			return false
		}
	}
	return true
}

// Returns a message, the segment number that had this message, and real/backend errors
// parsing errors will be reported to log, return nil msg and nil error
func (r *inboxMultiplexer) getNextMsg() (*arbostypes.MessageWithMetadata, error) {
	targetSubMessage := r.backend.GetPositionWithinMessage()
	seqMsg := r.cachedSequencerMessage
	segmentNum := r.cachedSegmentNum
	timestamp := r.cachedSegmentTimestamp
	blockNumber := r.cachedSegmentBlockNumber
	submessageNumber := r.cachedSubMessageNumber
	var segment []byte
	for {
		if segmentNum >= uint64(len(seqMsg.segments)) {
			break
		}
		segment = seqMsg.segments[int(segmentNum)]
		if len(segment) == 0 {
			segmentNum++
			continue
		}
		segmentKind := segment[0]
		if segmentKind == BatchSegmentKindAdvanceTimestamp || segmentKind == BatchSegmentKindAdvanceL1BlockNumber {
			rd := bytes.NewReader(segment[1:])
			advancing, err := rlp.NewStream(rd, 16).Uint64()
			if err != nil {
				log.Warn("error parsing sequencer advancing segment", "err", err)
				segmentNum++
				continue
			}
			if segmentKind == BatchSegmentKindAdvanceTimestamp {
				timestamp += advancing
			} else if segmentKind == BatchSegmentKindAdvanceL1BlockNumber {
				blockNumber += advancing
			}
			segmentNum++
		} else if submessageNumber < targetSubMessage {
			segmentNum++
			submessageNumber++
		} else {
			break
		}
	}
	r.cachedSegmentNum = segmentNum
	r.cachedSegmentTimestamp = timestamp
	r.cachedSegmentBlockNumber = blockNumber
	r.cachedSubMessageNumber = submessageNumber
	if timestamp < seqMsg.minTimestamp {
		timestamp = seqMsg.minTimestamp
	} else if timestamp > seqMsg.maxTimestamp {
		timestamp = seqMsg.maxTimestamp
	}
	if blockNumber < seqMsg.minL1Block {
		blockNumber = seqMsg.minL1Block
	} else if blockNumber > seqMsg.maxL1Block {
		blockNumber = seqMsg.maxL1Block
	}
	if segmentNum >= uint64(len(seqMsg.segments)) {
		// after end of batch there might be "virtual" delayedMsgSegments
		log.Warn("reading virtual delayed message segment", "delayedMessagesRead", r.delayedMessagesRead, "afterDelayedMessages", seqMsg.afterDelayedMessages)
		segment = []byte{BatchSegmentKindDelayedMessages}
	} else {
		segment = seqMsg.segments[int(segmentNum)]
	}
	if len(segment) == 0 {
		log.Error("empty sequencer message segment", "sequence", r.cachedSegmentNum, "segmentNum", segmentNum)
		return nil, nil
	}
	kind := segment[0]
	segment = segment[1:]
	var msg *arbostypes.MessageWithMetadata
	if kind == BatchSegmentKindL2Message || kind == BatchSegmentKindL2MessageBrotli {

		if kind == BatchSegmentKindL2MessageBrotli {
			decompressed, err := arbcompress.Decompress(segment, arbostypes.MaxL2MessageSize)
			if err != nil {
				log.Info("dropping compressed message", "err", err, "delayedMsg", r.delayedMessagesRead)
				return nil, nil
			}
			segment = decompressed
		}

		msg = &arbostypes.MessageWithMetadata{
			Message: &arbostypes.L1IncomingMessage{
				Header: &arbostypes.L1IncomingMessageHeader{
					Kind:        arbostypes.L1MessageType_L2Message,
					Poster:      l1pricing.BatchPosterAddress,
					BlockNumber: blockNumber,
					Timestamp:   timestamp,
					RequestId:   nil,
					L1BaseFee:   big.NewInt(0),
				},
				L2msg: segment,
			},
			DelayedMessagesRead: r.delayedMessagesRead,
		}
	} else if kind == BatchSegmentKindDelayedMessages {
		if r.delayedMessagesRead >= seqMsg.afterDelayedMessages {
			if segmentNum < uint64(len(seqMsg.segments)) {
				log.Warn(
					"attempt to read past batch delayed message count",
					"delayedMessagesRead", r.delayedMessagesRead,
					"batchAfterDelayedMessages", seqMsg.afterDelayedMessages,
				)
			}
			msg = &arbostypes.MessageWithMetadata{
				Message:             arbostypes.InvalidL1Message,
				DelayedMessagesRead: seqMsg.afterDelayedMessages,
			}
		} else {
			delayed, realErr := r.backend.ReadDelayedInbox(r.delayedMessagesRead)
			if realErr != nil {
				return nil, realErr
			}
			r.delayedMessagesRead += 1
			msg = &arbostypes.MessageWithMetadata{
				Message:             delayed,
				DelayedMessagesRead: r.delayedMessagesRead,
			}
		}
	} else {
		log.Error("bad sequencer message segment kind", "sequence", r.cachedSegmentNum, "segmentNum", segmentNum, "kind", kind)
		return nil, nil
	}
	return msg, nil
}

func (r *inboxMultiplexer) DelayedMessagesRead() uint64 {
	return r.delayedMessagesRead
}

'''
'''--- arbstate/inbox_fuzz_test.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbstate

import (
	"bytes"
	"context"
	"errors"
	"testing"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
)

type multiplexerBackend struct {
	batchSeqNum           uint64
	batch                 []byte
	delayedMessage        []byte
	positionWithinMessage uint64
}

func (b *multiplexerBackend) PeekSequencerInbox() ([]byte, error) {
	if b.batchSeqNum != 0 {
		return nil, errors.New("reading unknown sequencer batch")
	}
	return b.batch, nil
}

func (b *multiplexerBackend) GetSequencerInboxPosition() uint64 {
	return b.batchSeqNum
}

func (b *multiplexerBackend) AdvanceSequencerInbox() {
	b.batchSeqNum++
}

func (b *multiplexerBackend) GetPositionWithinMessage() uint64 {
	return b.positionWithinMessage
}

func (b *multiplexerBackend) SetPositionWithinMessage(pos uint64) {
	b.positionWithinMessage = pos
}

func (b *multiplexerBackend) ReadDelayedInbox(seqNum uint64) (*arbostypes.L1IncomingMessage, error) {
	if seqNum != 0 {
		return nil, errors.New("reading unknown delayed message")
	}
	msg, err := arbostypes.ParseIncomingL1Message(bytes.NewReader(b.delayedMessage), nil)
	if err != nil {
		// The bridge won't generate an invalid L1 message,
		// so here we substitute it with a less invalid one for fuzzing.
		msg = &arbostypes.TestIncomingMessageWithRequestId
	}
	return msg, nil
}

func FuzzInboxMultiplexer(f *testing.F) {
	f.Fuzz(func(t *testing.T, seqMsg []byte, delayedMsg []byte) {
		if len(seqMsg) < 40 {
			return
		}
		backend := &multiplexerBackend{
			batchSeqNum:           0,
			batch:                 seqMsg,
			delayedMessage:        delayedMsg,
			positionWithinMessage: 0,
		}
		multiplexer := NewInboxMultiplexer(backend, 0, nil, KeysetValidate)
		_, err := multiplexer.Pop(context.TODO())
		if err != nil {
			panic(err)
		}
	})
}

'''
'''--- arbutil/block_message_relation.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbutil

type MessageIndex uint64

func BlockNumberToMessageCount(blockNumber uint64, genesisBlockNumber uint64) MessageIndex {
	return MessageIndex(blockNumber + 1 - genesisBlockNumber)
}

// Block number must correspond to a message count, meaning it may not be less than -1
func SignedBlockNumberToMessageCount(blockNumber int64, genesisBlockNumber uint64) MessageIndex {
	return MessageIndex(uint64(blockNumber+1) - genesisBlockNumber)
}

func MessageCountToBlockNumber(messageCount MessageIndex, genesisBlockNumber uint64) int64 {
	return int64(uint64(messageCount)+genesisBlockNumber) - 1
}

'''
'''--- arbutil/correspondingl1blocknumber.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbutil

import (
	"context"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/core/types"
)

func ParentHeaderToL1BlockNumber(header *types.Header) uint64 {
	headerInfo := types.DeserializeHeaderExtraInformation(header)
	if headerInfo.ArbOSFormatVersion > 0 {
		return headerInfo.L1BlockNumber
	}
	return header.Number.Uint64()
}

func CorrespondingL1BlockNumber(ctx context.Context, client L1Interface, parentBlockNumber uint64) (uint64, error) {
	header, err := client.HeaderByNumber(ctx, big.NewInt(int64(parentBlockNumber)))
	if err != nil {
		return 0, fmt.Errorf("error getting L1 block number %d header : %w", parentBlockNumber, err)
	}
	return ParentHeaderToL1BlockNumber(header), nil
}

'''
'''--- arbutil/hash.go ---
package arbutil

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
)

// PaddedKeccak256 pads each argument to 32 bytes, concatenates and returns
// keccak256 hash of the result.
func PaddedKeccak256(args ...[]byte) []byte {
	var data []byte
	for _, arg := range args {
		data = append(data, common.BytesToHash(arg).Bytes()...)
	}
	return crypto.Keccak256(data)
}

// SumBytes sums two byte slices and returns the result.
// If the sum of bytes are over 32 bytes, it return last 32.
func SumBytes(a, b []byte) []byte {
	A := big.NewInt(0).SetBytes(a)
	B := big.NewInt(0).SetBytes(b)
	return common.BytesToHash((A.Add(A, B)).Bytes()).Bytes()
}

'''
'''--- arbutil/hash_test.go ---
package arbutil

import (
	"bytes"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/google/go-cmp/cmp"
)

func TestSlotAddress(t *testing.T) {
	for _, tc := range []struct {
		name string
		args [][]byte
		want []byte
	}{
		{
			name: "isBatchPoster[batchPosterAddr]", // Keccak256(addr, 3)
			args: [][]byte{
				common.FromHex("0xC1b634853Cb333D3aD8663715b08f41A3Aec47cc"), // mainnet batch poster address
				{3},
			},
			want: common.HexToHash("0xa10aa54071443520884ed767b0684edf43acec528b7da83ab38ce60126562660").Bytes(),
		},
		{
			name: "allowedContracts[msg.sender]", // Keccak256(msg.sender, 1)
			args: [][]byte{
				common.FromHex("0x1c479675ad559DC151F6Ec7ed3FbF8ceE79582B6"), // mainnet sequencer address
				{1},
			},
			want: common.HexToHash("0xe85fd79f89ff278fc57d40aecb7947873df9f0beac531c8f71a98f630e1eab62").Bytes(),
		},
		{
			name: "allowedRefundees[refundee]", // Keccak256(msg.sender, 2)
			args: [][]byte{
				common.FromHex("0xC1b634853Cb333D3aD8663715b08f41A3Aec47cc"), // mainnet batch poster address
				{2},
			},
			want: common.HexToHash("0x7686888b19bb7b75e46bb1aa328b65150743f4899443d722f0adf8e252ccda41").Bytes(),
		},
	} {
		t.Run(tc.name, func(t *testing.T) {
			got := PaddedKeccak256(tc.args...)
			if !bytes.Equal(got, tc.want) {
				t.Errorf("slotAddress(%x) = %x, want %x", tc.args, got, tc.want)
			}
		})
	}

}

func TestSumBytes(t *testing.T) {
	for _, tc := range []struct {
		desc       string
		a, b, want []byte
	}{
		{
			desc: "simple case",
			a:    []byte{0x0a, 0x0b},
			b:    []byte{0x03, 0x04},
			want: common.HexToHash("0x0d0f").Bytes(),
		},
		{
			desc: "carry over last byte",
			a:    []byte{0x0a, 0xff},
			b:    []byte{0x01},
			want: common.HexToHash("0x0b00").Bytes(),
		},
		{
			desc: "overflow",
			a:    common.HexToHash("0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff").Bytes(),
			b:    []byte{0x01},
			want: common.HexToHash("0x00").Bytes(),
		},
	} {
		t.Run(tc.desc, func(t *testing.T) {
			got := SumBytes(tc.a, tc.b)
			if diff := cmp.Diff(got, tc.want); diff != "" {
				t.Errorf("SumBytes(%x, %x) = %x want: %x", tc.a, tc.b, got, tc.want)
			}
		})
	}
}

'''
'''--- arbutil/preimage_type.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbutil

type PreimageType uint8

// These values must be kept in sync with `arbitrator/arbutil/src/types.rs`,
// and the if statement in `contracts/src/osp/OneStepProverHostIo.sol` (search for "UNKNOWN_PREIMAGE_TYPE").
const (
	Keccak256PreimageType PreimageType = iota
	Sha2_256PreimageType
)

'''
'''--- arbutil/transaction_data.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbutil

import (
	"context"
	"fmt"

	"github.com/ethereum/go-ethereum/core/types"
)

// GetLogEmitterTxData requires that the tx's data is at least 4 bytes long
func GetLogEmitterTxData(ctx context.Context, client L1Interface, log types.Log) ([]byte, error) {
	tx, err := client.TransactionInBlock(ctx, log.BlockHash, log.TxIndex)
	if err != nil {
		return nil, err
	}
	if tx.Hash() != log.TxHash {
		return nil, fmt.Errorf("L1 client returned unexpected transaction hash %v when looking up block %v transaction %v with expected hash %v", tx.Hash(), log.BlockHash, log.TxIndex, log.TxHash)
	}
	if len(tx.Data()) < 4 {
		return nil, fmt.Errorf("log emitting transaction %v unexpectedly does not have enough data", tx.Hash())
	}
	return tx.Data(), nil
}

'''
'''--- arbutil/wait_for_l1.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbutil

import (
	"context"
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
)

type L1Interface interface {
	bind.ContractBackend
	ethereum.ChainReader
	ethereum.ChainStateReader
	ethereum.TransactionReader
	TransactionSender(ctx context.Context, tx *types.Transaction, block common.Hash, index uint) (common.Address, error)
	BlockNumber(ctx context.Context) (uint64, error)
	PendingCallContract(ctx context.Context, msg ethereum.CallMsg) ([]byte, error)
}

func SendTxAsCall(ctx context.Context, client L1Interface, tx *types.Transaction, from common.Address, blockNum *big.Int, unlimitedGas bool) ([]byte, error) {
	var gas uint64
	if unlimitedGas {
		gas = 0
	} else {
		gas = tx.Gas()
	}
	callMsg := ethereum.CallMsg{
		From:       from,
		To:         tx.To(),
		Gas:        gas,
		GasPrice:   tx.GasPrice(),
		GasFeeCap:  tx.GasFeeCap(),
		GasTipCap:  tx.GasTipCap(),
		Value:      tx.Value(),
		Data:       tx.Data(),
		AccessList: tx.AccessList(),
	}
	return client.CallContract(ctx, callMsg, blockNum)
}

func GetPendingCallBlockNumber(ctx context.Context, client L1Interface) (*big.Int, error) {
	msg := ethereum.CallMsg{
		// Pretend to be a contract deployment to execute EVM code without calling a contract.
		To: nil,
		// Contains the following EVM code, which returns the current block number:
		// NUMBER
		// PUSH1 0
		// MSTORE
		// PUSH1 32
		// PUSH1 0
		// RETURN
		Data: []byte{0x43, 0x60, 0x00, 0x52, 0x60, 0x20, 0x60, 0x00, 0xF3},
	}
	callRes, err := client.PendingCallContract(ctx, msg)
	if err != nil {
		return nil, err
	}
	return new(big.Int).SetBytes(callRes), nil
}

func DetailTxError(ctx context.Context, client L1Interface, tx *types.Transaction, txRes *types.Receipt) error {
	// Re-execute the transaction as a call to get a better error
	if ctx.Err() != nil {
		return ctx.Err()
	}
	if txRes == nil {
		return errors.New("expected receipt")
	}
	if txRes.Status == types.ReceiptStatusSuccessful {
		return nil
	}
	from, err := client.TransactionSender(ctx, tx, txRes.BlockHash, txRes.TransactionIndex)
	if err != nil {
		return fmt.Errorf("TransactionSender got: %w for tx %v", err, tx.Hash())
	}
	_, err = SendTxAsCall(ctx, client, tx, from, txRes.BlockNumber, false)
	if err == nil {
		return fmt.Errorf("tx failed but call succeeded for tx hash %v", tx.Hash())
	}
	_, err = SendTxAsCall(ctx, client, tx, from, txRes.BlockNumber, true)
	if err == nil {
		return fmt.Errorf("%w for tx hash %v", vm.ErrOutOfGas, tx.Hash())
	}
	return fmt.Errorf("SendTxAsCall got: %w for tx hash %v", err, tx.Hash())
}

'''
'''--- blsSignatures/blsSignatures.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package blsSignatures

import (
	cryptorand "crypto/rand"
	"encoding/base64"
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/crypto/bls12381"
)

type PublicKey struct {
	key           *bls12381.PointG2
	validityProof *bls12381.PointG1 // if this is nil, key came from a trusted source
}

type PrivateKey *big.Int

type Signature *bls12381.PointG1

func GeneratePrivKeyString() (string, error) {
	g2 := bls12381.NewG2()
	privKey, err := cryptorand.Int(cryptorand.Reader, g2.Q())
	if err != nil {
		return "", err
	}
	privKeyBytes := PrivateKeyToBytes(privKey)
	encodedPrivKey := make([]byte, base64.StdEncoding.EncodedLen(len(privKeyBytes)))
	base64.StdEncoding.Encode(encodedPrivKey, privKeyBytes)
	return string(encodedPrivKey), nil
}

func GenerateKeys() (PublicKey, PrivateKey, error) {
	g2 := bls12381.NewG2()
	privateKey, err := cryptorand.Int(cryptorand.Reader, g2.Q())
	if err != nil {
		return PublicKey{}, nil, err
	}
	publicKey, err := PublicKeyFromPrivateKey(privateKey)
	return publicKey, privateKey, err
}

func PublicKeyFromPrivateKey(privateKey PrivateKey) (PublicKey, error) {
	pubKey := &bls12381.PointG2{}
	g2 := bls12381.NewG2()
	g2.MulScalar(pubKey, g2.One(), privateKey)
	proof, err := KeyValidityProof(pubKey, privateKey)
	if err != nil {
		return PublicKey{}, err
	}
	publicKey, err := NewPublicKey(pubKey, proof)
	if err != nil {
		return PublicKey{}, err
	}
	return publicKey, nil
}

// KeyValidityProof is the key validity proof mechanism is sufficient to prevent rogue key attacks, if applied to all keys
// that come from untrusted sources. We use the private key to sign the public key, but in the
// signature algorithm we use a tweaked version of the hash-to-curve function so that the result cannot be
// re-used as an ordinary signature.
//
// For a proof that this is sufficient, see Theorem 1 in
// Ristenpart & Yilek, "The Power of Proofs-of-Possession: ..." from EUROCRYPT 2007.
func KeyValidityProof(pubKey *bls12381.PointG2, privateKey PrivateKey) (Signature, error) {
	g2 := bls12381.NewG2()
	return signMessage2(privateKey, g2.ToBytes(pubKey), true)
}

func NewPublicKey(pubKey *bls12381.PointG2, validityProof *bls12381.PointG1) (PublicKey, error) {
	g2 := bls12381.NewG2()
	unverifiedPublicKey := PublicKey{pubKey, validityProof}
	verified, err := verifySignature2(validityProof, g2.ToBytes(pubKey), unverifiedPublicKey, true)
	if err != nil {
		return PublicKey{}, err
	}
	if !verified {
		return PublicKey{}, errors.New("public key validation failed")
	}
	return unverifiedPublicKey, nil
}

func NewTrustedPublicKey(pubKey *bls12381.PointG2) PublicKey {
	return PublicKey{pubKey, nil}
}

func (pubKey PublicKey) ToTrusted() PublicKey {
	if pubKey.validityProof == nil {
		return pubKey
	}
	return NewTrustedPublicKey(pubKey.key)
}

func SignMessage(priv PrivateKey, message []byte) (Signature, error) {
	return signMessage2(priv, message, false)
}

func signMessage2(priv PrivateKey, message []byte, keyValidationMode bool) (Signature, error) {
	pointOnCurve, err := hashToG1Curve(message, keyValidationMode)
	if err != nil {
		return nil, err
	}
	g1 := bls12381.NewG1()
	result := &bls12381.PointG1{}
	g1.MulScalar(result, pointOnCurve, priv)
	return result, nil
}

func VerifySignature(sig Signature, message []byte, publicKey PublicKey) (bool, error) {
	return verifySignature2(sig, message, publicKey, false)
}

func verifySignature2(sig Signature, message []byte, publicKey PublicKey, keyValidationMode bool) (bool, error) {
	pointOnCurve, err := hashToG1Curve(message, keyValidationMode)
	if err != nil {
		return false, err
	}

	engine := bls12381.NewPairingEngine()
	engine.Reset()
	engine.AddPair(pointOnCurve, publicKey.key)
	leftSide := engine.Result()
	engine.AddPair(sig, engine.G2.One())
	rightSide := engine.Result()
	return leftSide.Equal(rightSide), nil
}

func AggregatePublicKeys(pubKeys []PublicKey) PublicKey {
	g2 := bls12381.NewG2()
	ret := g2.Zero()
	for _, pk := range pubKeys {
		g2.Add(ret, ret, pk.key)
	}
	return NewTrustedPublicKey(ret)
}

func AggregateSignatures(sigs []Signature) Signature {
	g1 := bls12381.NewG1()
	ret := g1.Zero()
	for _, s := range sigs {
		g1.Add(ret, ret, s)
	}
	return ret
}

func VerifyAggregatedSignatureSameMessage(sig Signature, message []byte, pubKeys []PublicKey) (bool, error) {
	return VerifySignature(sig, message, AggregatePublicKeys(pubKeys))
}

func VerifyAggregatedSignatureDifferentMessages(sig Signature, messages [][]byte, pubKeys []PublicKey) (bool, error) {

	if len(messages) != len(pubKeys) {
		return false, errors.New("len(messages) does not match (len(pub keys) in verification")
	}
	engine := bls12381.NewPairingEngine()
	engine.Reset()
	for i, msg := range messages {
		pointOnCurve, err := hashToG1Curve(msg, false)
		if err != nil {
			return false, err
		}
		engine.AddPair(pointOnCurve, pubKeys[i].key)
	}
	leftSide := engine.Result()

	engine.Reset()
	engine.AddPair(sig, engine.G2.One())
	rightSide := engine.Result()
	return leftSide.Equal(rightSide), nil
}

// This hashes a message to a [32]byte, then maps the result to the G1 curve using
// the Simplified Shallue-van de Woestijne-Ulas Method, described in Section 6.6.2 of
// https://tools.ietf.org/html/draft-irtf-cfrg-hash-to-curve-06
//
// If keyValidationMode is true, this uses a tweaked version of the padding,
// so that the result will not collide with a result generated in an ordinary signature.
func hashToG1Curve(message []byte, keyValidationMode bool) (*bls12381.PointG1, error) {
	var padding [16]byte
	h := crypto.Keccak256(message)
	if keyValidationMode {
		// modify padding, for domain separation
		padding[0] = 1
	}
	g1 := bls12381.NewG1()
	return g1.MapToCurve(append(padding[:], h...))
}

func PublicKeyToBytes(pub PublicKey) []byte {
	g2 := bls12381.NewG2()
	if pub.validityProof == nil {
		return append([]byte{0}, g2.ToBytes(pub.key)...)
	}
	keyBytes := g2.ToBytes(pub.key)
	sigBytes := SignatureToBytes(pub.validityProof)
	if len(sigBytes) > 255 {
		panic("validity proof too large to serialize")
	}
	return append(append([]byte{byte(len(sigBytes))}, sigBytes...), keyBytes...)
}

func PublicKeyFromBytes(in []byte, trustedSource bool) (PublicKey, error) {
	if len(in) == 0 {
		return PublicKey{}, errors.New("tried to deserialize empty public key")
	}
	g2 := bls12381.NewG2()
	proofLen := int(in[0])
	if proofLen == 0 {
		if !trustedSource {
			return PublicKey{}, errors.New("tried to deserialize unvalidated public key from untrusted source")
		}
		key, err := g2.FromBytes(in[1:])
		if err != nil {
			return PublicKey{}, err
		}
		return NewTrustedPublicKey(key), nil
	} else {
		if len(in) < 1+proofLen {
			return PublicKey{}, errors.New("invalid serialized public key")
		}
		g1 := bls12381.NewG1()
		proofBytes := in[1 : 1+proofLen]
		validityProof, err := g1.FromBytes(proofBytes)
		if err != nil {
			return PublicKey{}, err
		}
		keyBytes := in[1+proofLen:]
		key, err := g2.FromBytes(keyBytes)
		if err != nil {
			return PublicKey{}, err
		}
		if trustedSource {
			// Skip verification of the validity proof
			return PublicKey{key, validityProof}, nil
		}
		return NewPublicKey(key, validityProof)
	}
}

func PrivateKeyToBytes(priv PrivateKey) []byte {
	return ((*big.Int)(priv)).Bytes()
}

func PrivateKeyFromBytes(in []byte) (PrivateKey, error) {
	return new(big.Int).SetBytes(in), nil
}

func SignatureToBytes(sig Signature) []byte {
	g1 := bls12381.NewG1()
	return g1.ToBytes(sig)
}

func SignatureFromBytes(in []byte) (Signature, error) {
	g1 := bls12381.NewG1()
	return g1.FromBytes(in)
}

'''
'''--- blsSignatures/blsSignatures_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package blsSignatures

import (
	"math/rand"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestValidSignature(t *testing.T) {
	pub, priv, err := GenerateKeys()
	Require(t, err)

	message := []byte("The quick brown fox jumped over the lazy dog.")
	sig, err := SignMessage(priv, message)
	Require(t, err)

	verified, err := VerifySignature(sig, message, pub)
	Require(t, err)
	if !verified {
		Fail(t, "valid signature failed to verify")
	}
}

func TestWrongMessageSignature(t *testing.T) {
	pub, priv, err := GenerateKeys()
	Require(t, err)

	message := []byte("The quick brown fox jumped over the lazy dog.")
	sig, err := SignMessage(priv, message)
	Require(t, err)

	verified, err := VerifySignature(sig, append(message, 3), pub)
	Require(t, err)
	if verified {
		Fail(t, "signature check on wrong message didn't fail")
	}
}

func TestWrongKeySignature(t *testing.T) {
	_, priv, err := GenerateKeys()
	Require(t, err)
	pub, _, err := GenerateKeys()
	Require(t, err)

	message := []byte("The quick brown fox jumped over the lazy dog.")
	sig, err := SignMessage(priv, message)
	Require(t, err)

	verified, err := VerifySignature(sig, message, pub)
	Require(t, err)
	if verified {
		Fail(t, "signature check with wrong public key didn't fail")
	}
}

const NumSignaturesToAggregate = 12

func TestSignatureAggregation(t *testing.T) {
	message := []byte("The quick brown fox jumped over the lazy dog.")
	pubKeys := []PublicKey{}
	sigs := []Signature{}
	for i := 0; i < NumSignaturesToAggregate; i++ {
		pub, priv, err := GenerateKeys()
		Require(t, err)
		pubKeys = append(pubKeys, pub)
		sig, err := SignMessage(priv, message)
		Require(t, err)
		sigs = append(sigs, sig)
	}

	verified, err := VerifySignature(AggregateSignatures(sigs), message, AggregatePublicKeys(pubKeys))
	Require(t, err)
	if !verified {
		Fail(t, "First aggregated signature check failed")
	}

	verified, err = VerifyAggregatedSignatureSameMessage(AggregateSignatures(sigs), message, pubKeys)
	Require(t, err)
	if !verified {
		Fail(t, "Second aggregated signature check failed")
	}
}

func TestSignatureAggregationAnyOrder(t *testing.T) {
	message := []byte("The quick brown fox jumped over the lazy dog.")
	pubKeys := []PublicKey{}
	sigs := []Signature{}
	for i := 0; i < NumSignaturesToAggregate; i++ {
		pub, priv, err := GenerateKeys()
		Require(t, err)
		pubKeys = append(pubKeys, pub)
		sig, err := SignMessage(priv, message)
		Require(t, err)
		sigs = append(sigs, sig)
	}

	rand.Seed(time.Now().UnixNano())
	rand.Shuffle(NumSignaturesToAggregate, func(i, j int) { sigs[i], sigs[j] = sigs[j], sigs[i] })
	rand.Shuffle(NumSignaturesToAggregate, func(i, j int) { pubKeys[i], pubKeys[j] = pubKeys[j], pubKeys[i] })

	verified, err := VerifySignature(AggregateSignatures(sigs), message, AggregatePublicKeys(pubKeys))
	Require(t, err)
	if !verified {
		Fail(t, "First aggregated signature check failed")
	}

	rand.Shuffle(NumSignaturesToAggregate, func(i, j int) { sigs[i], sigs[j] = sigs[j], sigs[i] })
	verified, err = VerifyAggregatedSignatureSameMessage(AggregateSignatures(sigs), message, pubKeys)
	Require(t, err)
	if !verified {
		Fail(t, "Second aggregated signature check failed")
	}
}

func TestSignatureAggregationDifferentMessages(t *testing.T) {
	messages := [][]byte{}
	pubKeys := []PublicKey{}
	sigs := []Signature{}

	for i := 0; i < NumSignaturesToAggregate; i++ {
		msg := []byte{byte(i)}
		pubKey, privKey, err := GenerateKeys()
		Require(t, err)
		sig, err := SignMessage(privKey, msg)
		Require(t, err)
		messages = append(messages, msg)
		pubKeys = append(pubKeys, pubKey)
		sigs = append(sigs, sig)
	}

	verified, err := VerifyAggregatedSignatureDifferentMessages(AggregateSignatures(sigs), messages, pubKeys)
	Require(t, err)
	if !verified {
		Fail(t, "First aggregated signature check failed")
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- broadcastclient/broadcastclient.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcastclient

import (
	"context"
	"crypto/tls"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net"
	"net/http"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/gobwas/httphead"
	"github.com/gobwas/ws"
	"github.com/gobwas/ws/wsflate"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

var (
	sourcesConnectedGauge    = metrics.NewRegisteredGauge("arb/feed/sources/connected", nil)
	sourcesDisconnectedGauge = metrics.NewRegisteredGauge("arb/feed/sources/disconnected", nil)
)

type FeedConfig struct {
	Output wsbroadcastserver.BroadcasterConfig `koanf:"output" reload:"hot"`
	Input  Config                              `koanf:"input" reload:"hot"`
}

func (fc *FeedConfig) Validate() error {
	return fc.Output.Validate()
}

func FeedConfigAddOptions(prefix string, f *flag.FlagSet, feedInputEnable bool, feedOutputEnable bool) {
	if feedInputEnable {
		ConfigAddOptions(prefix+".input", f)
	}
	if feedOutputEnable {
		wsbroadcastserver.BroadcasterConfigAddOptions(prefix+".output", f)
	}
}

var FeedConfigDefault = FeedConfig{
	Output: wsbroadcastserver.DefaultBroadcasterConfig,
	Input:  DefaultConfig,
}

type Config struct {
	ReconnectInitialBackoff time.Duration            `koanf:"reconnect-initial-backoff" reload:"hot"`
	ReconnectMaximumBackoff time.Duration            `koanf:"reconnect-maximum-backoff" reload:"hot"`
	RequireChainId          bool                     `koanf:"require-chain-id" reload:"hot"`
	RequireFeedVersion      bool                     `koanf:"require-feed-version" reload:"hot"`
	Timeout                 time.Duration            `koanf:"timeout" reload:"hot"`
	URL                     []string                 `koanf:"url"`
	SecondaryURL            []string                 `koanf:"secondary-url"`
	Verify                  signature.VerifierConfig `koanf:"verify"`
	EnableCompression       bool                     `koanf:"enable-compression" reload:"hot"`
}

func (c *Config) Enable() bool {
	return len(c.URL) > 0 && c.URL[0] != ""
}

type ConfigFetcher func() *Config

func ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Duration(prefix+".reconnect-initial-backoff", DefaultConfig.ReconnectInitialBackoff, "initial duration to wait before reconnect")
	f.Duration(prefix+".reconnect-maximum-backoff", DefaultConfig.ReconnectMaximumBackoff, "maximum duration to wait before reconnect")
	f.Bool(prefix+".require-chain-id", DefaultConfig.RequireChainId, "require chain id to be present on connect")
	f.Bool(prefix+".require-feed-version", DefaultConfig.RequireFeedVersion, "require feed version to be present on connect")
	f.Duration(prefix+".timeout", DefaultConfig.Timeout, "duration to wait before timing out connection to sequencer feed")
	f.StringSlice(prefix+".url", DefaultConfig.URL, "list of primary URLs of sequencer feed source")
	f.StringSlice(prefix+".secondary-url", DefaultConfig.SecondaryURL, "list of secondary URLs of sequencer feed source. Would be started in the order they appear in the list when primary feeds fails")
	signature.FeedVerifierConfigAddOptions(prefix+".verify", f)
	f.Bool(prefix+".enable-compression", DefaultConfig.EnableCompression, "enable per message deflate compression support")
}

var DefaultConfig = Config{
	ReconnectInitialBackoff: time.Second * 1,
	ReconnectMaximumBackoff: time.Second * 64,
	RequireChainId:          false,
	RequireFeedVersion:      false,
	Verify:                  signature.DefultFeedVerifierConfig,
	URL:                     []string{},
	SecondaryURL:            []string{},
	Timeout:                 20 * time.Second,
	EnableCompression:       true,
}

var DefaultTestConfig = Config{
	ReconnectInitialBackoff: 0,
	ReconnectMaximumBackoff: 0,
	RequireChainId:          false,
	RequireFeedVersion:      false,
	Verify:                  signature.DefultFeedVerifierConfig,
	URL:                     []string{""},
	SecondaryURL:            []string{},
	Timeout:                 200 * time.Millisecond,
	EnableCompression:       true,
}

type TransactionStreamerInterface interface {
	AddBroadcastMessages(feedMessages []*broadcaster.BroadcastFeedMessage) error
}

type BroadcastClient struct {
	stopwaiter.StopWaiter

	config       ConfigFetcher
	websocketUrl string
	nextSeqNum   arbutil.MessageIndex
	sigVerifier  *signature.Verifier

	chainId uint64

	// Protects conn and shuttingDown
	connMutex sync.Mutex
	conn      net.Conn

	retryCount int64

	retrying                        bool
	shuttingDown                    bool
	confirmedSequenceNumberListener chan arbutil.MessageIndex
	txStreamer                      TransactionStreamerInterface
	fatalErrChan                    chan error
	adjustCount                     func(int32)
}

var ErrIncorrectFeedServerVersion = errors.New("incorrect feed server version")
var ErrIncorrectChainId = errors.New("incorrect chain id")
var ErrMissingChainId = errors.New("missing chain id")
var ErrMissingFeedServerVersion = errors.New("missing feed server version")

func NewBroadcastClient(
	config ConfigFetcher,
	websocketUrl string,
	chainId uint64,
	currentMessageCount arbutil.MessageIndex,
	txStreamer TransactionStreamerInterface,
	confirmedSequencerNumberListener chan arbutil.MessageIndex,
	fatalErrChan chan error,
	addrVerifier contracts.AddressVerifierInterface,
	adjustCount func(int32),
) (*BroadcastClient, error) {
	sigVerifier, err := signature.NewVerifier(&config().Verify, addrVerifier)
	if err != nil {
		return nil, err
	}
	return &BroadcastClient{
		config:                          config,
		websocketUrl:                    websocketUrl,
		chainId:                         chainId,
		nextSeqNum:                      currentMessageCount,
		txStreamer:                      txStreamer,
		confirmedSequenceNumberListener: confirmedSequencerNumberListener,
		fatalErrChan:                    fatalErrChan,
		sigVerifier:                     sigVerifier,
		adjustCount:                     adjustCount,
	}, err
}

func (bc *BroadcastClient) Start(ctxIn context.Context) {
	bc.StopWaiter.Start(ctxIn, bc)
	if bc.StopWaiter.Stopped() {
		log.Info("broadcast client has already been stopped, not starting")
		return
	}
	bc.LaunchThread(func(ctx context.Context) {
		backoffDuration := bc.config().ReconnectInitialBackoff
		for {
			earlyFrameData, err := bc.connect(ctx, bc.nextSeqNum)
			if errors.Is(err, ErrMissingChainId) ||
				errors.Is(err, ErrIncorrectChainId) ||
				errors.Is(err, ErrMissingFeedServerVersion) ||
				errors.Is(err, ErrIncorrectFeedServerVersion) {
				bc.fatalErrChan <- err
				return
			}
			if err == nil {
				bc.startBackgroundReader(earlyFrameData)
				break
			}
			log.Warn("failed connect to sequencer broadcast, waiting and retrying", "url", bc.websocketUrl, "err", err)
			timer := time.NewTimer(backoffDuration)
			if backoffDuration < bc.config().ReconnectMaximumBackoff {
				backoffDuration *= 2
			}
			select {
			case <-ctx.Done():
				timer.Stop()
				return
			case <-timer.C:
			}
		}
	})
}

func (bc *BroadcastClient) connect(ctx context.Context, nextSeqNum arbutil.MessageIndex) (io.Reader, error) {
	if len(bc.websocketUrl) == 0 {
		// Nothing to do
		return nil, nil
	}

	header := ws.HandshakeHeaderHTTP(http.Header{
		wsbroadcastserver.HTTPHeaderFeedClientVersion:       []string{strconv.Itoa(wsbroadcastserver.FeedClientVersion)},
		wsbroadcastserver.HTTPHeaderRequestedSequenceNumber: []string{strconv.FormatUint(uint64(nextSeqNum), 10)},
	})

	log.Info("connecting to arbitrum inbox message broadcaster", "url", bc.websocketUrl)
	var foundChainId bool
	var foundFeedServerVersion bool
	var chainId uint64
	var feedServerVersion uint64

	config := bc.config()
	var extensions []httphead.Option
	deflateExt := wsflate.DefaultParameters.Option()
	if config.EnableCompression {
		extensions = []httphead.Option{deflateExt}
	}
	timeoutDialer := ws.Dialer{
		Header: header,
		OnHeader: func(key, value []byte) (err error) {
			headerName := string(key)
			headerValue := string(value)
			if headerName == wsbroadcastserver.HTTPHeaderFeedServerVersion {
				foundFeedServerVersion = true
				feedServerVersion, err = strconv.ParseUint(headerValue, 0, 64)
				if err != nil {
					return err
				}
				if feedServerVersion != wsbroadcastserver.FeedServerVersion {
					log.Error(
						"incorrect feed server version",
						"expectedFeedServerVersion",
						wsbroadcastserver.FeedServerVersion,
						"actualFeedServerVersion",
						feedServerVersion,
					)
					return ErrIncorrectFeedServerVersion
				}
			} else if headerName == wsbroadcastserver.HTTPHeaderChainId {
				foundChainId = true
				chainId, err = strconv.ParseUint(headerValue, 0, 64)
				if err != nil {
					return err
				}
				if chainId != bc.chainId {
					log.Error(
						"incorrect chain id when connecting to server feed",
						"expectedChainId",
						bc.chainId,
						"actualChainId",
						chainId,
					)
					return ErrIncorrectChainId
				}
			}
			return nil
		},
		Timeout: 10 * time.Second,
		TLSConfig: &tls.Config{
			MinVersion: tls.VersionTLS12,
		},
		Extensions: extensions,
	}

	if bc.isShuttingDown() {
		return nil, nil
	}

	conn, br, _, err := timeoutDialer.Dial(ctx, bc.websocketUrl)
	if errors.Is(err, ErrIncorrectFeedServerVersion) || errors.Is(err, ErrIncorrectChainId) {
		return nil, err
	}
	if err != nil {
		return nil, fmt.Errorf("broadcast client unable to connect: %w", err)
	}
	if config.RequireChainId && !foundChainId {
		err := conn.Close()
		if err != nil {
			return nil, fmt.Errorf("error closing connection when missing chain id: %w", err)
		}
		return nil, ErrMissingChainId
	}
	if config.RequireFeedVersion && !foundFeedServerVersion {
		err := conn.Close()
		if err != nil {
			return nil, fmt.Errorf("error closing connection when missing feed server version: %w", err)
		}
		return nil, ErrMissingFeedServerVersion
	}

	var earlyFrameData io.Reader
	if br != nil {
		// Depending on how long the client takes to read the response, there may be
		// data after the WebSocket upgrade response in a single read from the socket,
		// ie WebSocket frames sent by the server. If this happens, Dial returns
		// a non-nil bufio.Reader so that data isn't lost. But beware, this buffered
		// reader is still hooked up to the socket; trying to read past what had already
		// been buffered will do a blocking read on the socket, so we have to wrap it
		// in a LimitedReader.
		earlyFrameData = io.LimitReader(br, int64(br.Buffered()))
	}

	bc.connMutex.Lock()
	bc.conn = conn
	bc.connMutex.Unlock()
	log.Info("Feed connected", "feedServerVersion", feedServerVersion, "chainId", chainId, "requestedSeqNum", nextSeqNum)

	return earlyFrameData, nil
}

func (bc *BroadcastClient) startBackgroundReader(earlyFrameData io.Reader) {
	bc.LaunchThread(func(ctx context.Context) {
		connected := false
		sourcesDisconnectedGauge.Inc(1)
		backoffDuration := bc.config().ReconnectInitialBackoff
		flateReader := wsbroadcastserver.NewFlateReader()
		for {
			select {
			case <-ctx.Done():
				return
			default:
			}

			var msg []byte
			var op ws.OpCode
			var err error
			config := bc.config()
			msg, op, err = wsbroadcastserver.ReadData(ctx, bc.conn, earlyFrameData, config.Timeout, ws.StateClientSide, config.EnableCompression, flateReader)
			if err != nil {
				if bc.isShuttingDown() {
					return
				}
				if strings.Contains(err.Error(), "i/o timeout") {
					log.Error("Server connection timed out without receiving data", "url", bc.websocketUrl, "err", err)
				} else if errors.Is(err, io.EOF) || errors.Is(err, io.ErrUnexpectedEOF) {
					log.Warn("readData returned EOF", "url", bc.websocketUrl, "opcode", int(op), "err", err)
				} else {
					log.Error("error calling readData", "url", bc.websocketUrl, "opcode", int(op), "err", err)
				}
				if connected {
					connected = false
					bc.adjustCount(-1)
					sourcesConnectedGauge.Dec(1)
					sourcesDisconnectedGauge.Inc(1)
				}
				_ = bc.conn.Close()
				timer := time.NewTimer(backoffDuration)
				if backoffDuration < bc.config().ReconnectMaximumBackoff {
					backoffDuration *= 2
				}
				select {
				case <-ctx.Done():
					timer.Stop()
					return
				case <-timer.C:
				}
				earlyFrameData = bc.retryConnect(ctx)
				continue
			}
			backoffDuration = bc.config().ReconnectInitialBackoff

			if msg != nil {
				res := broadcaster.BroadcastMessage{}
				err = json.Unmarshal(msg, &res)
				if err != nil {
					log.Error("error unmarshalling message", "msg", msg, "err", err)
					continue
				}

				if !connected {
					connected = true
					sourcesDisconnectedGauge.Dec(1)
					sourcesConnectedGauge.Inc(1)
					bc.adjustCount(1)
				}
				if len(res.Messages) > 0 {
					log.Debug("received batch item", "count", len(res.Messages), "first seq", res.Messages[0].SequenceNumber)
				} else if res.ConfirmedSequenceNumberMessage != nil {
					log.Debug("confirmed sequence number", "seq", res.ConfirmedSequenceNumberMessage.SequenceNumber)
				} else {
					log.Debug("received broadcast with no messages populated", "length", len(msg))
				}
				if res.Version == 1 {
					if len(res.Messages) > 0 {
						for _, message := range res.Messages {
							if message == nil {
								log.Warn("ignoring nil feed message")
								continue
							}

							err := bc.isValidSignature(ctx, message)
							if err != nil {
								log.Error("error validating feed signature", "error", err, "sequence number", message.SequenceNumber)
								bc.fatalErrChan <- fmt.Errorf("error validating feed signature %v: %w", message.SequenceNumber, err)
								continue
							}

							bc.nextSeqNum = message.SequenceNumber + 1
						}
						if err := bc.txStreamer.AddBroadcastMessages(res.Messages); err != nil {
							log.Error("Error adding message from Sequencer Feed", "err", err)
						}
					}
					if res.ConfirmedSequenceNumberMessage != nil && bc.confirmedSequenceNumberListener != nil {
						bc.confirmedSequenceNumberListener <- res.ConfirmedSequenceNumberMessage.SequenceNumber
					}
				}
			}
		}
	})
}

func (bc *BroadcastClient) GetRetryCount() int64 {
	return atomic.LoadInt64(&bc.retryCount)
}

func (bc *BroadcastClient) isShuttingDown() bool {
	bc.connMutex.Lock()
	defer bc.connMutex.Unlock()
	return bc.shuttingDown
}

func (bc *BroadcastClient) retryConnect(ctx context.Context) io.Reader {
	maxWaitDuration := 15 * time.Second
	waitDuration := 500 * time.Millisecond
	bc.retrying = true

	for !bc.isShuttingDown() {
		timer := time.NewTimer(waitDuration)
		select {
		case <-ctx.Done():
			timer.Stop()
			return nil
		case <-timer.C:
		}

		atomic.AddInt64(&bc.retryCount, 1)
		earlyFrameData, err := bc.connect(ctx, bc.nextSeqNum)
		if err == nil {
			bc.retrying = false
			return earlyFrameData
		}

		if waitDuration < maxWaitDuration {
			waitDuration += 500 * time.Millisecond
		}
	}
	return nil
}

func (bc *BroadcastClient) StopAndWait() {
	log.Debug("closing broadcaster client connection")
	bc.StopWaiter.StopAndWait()
	bc.connMutex.Lock()
	defer bc.connMutex.Unlock()

	if !bc.shuttingDown {
		bc.shuttingDown = true
		if bc.conn != nil {
			_ = bc.conn.Close()
		}
	}
}

func (bc *BroadcastClient) isValidSignature(ctx context.Context, message *broadcaster.BroadcastFeedMessage) error {
	if bc.config().Verify.Dangerous.AcceptMissing && bc.sigVerifier == nil {
		// Verifier disabled
		return nil
	}
	hash, err := message.Hash(bc.chainId)
	if err != nil {
		return fmt.Errorf("error getting message hash for sequence number %v: %w", message.SequenceNumber, err)
	}
	return bc.sigVerifier.VerifyHash(ctx, message.Signature, hash)
}

'''
'''--- broadcastclient/broadcastclient_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcastclient

import (
	"context"
	"crypto/ecdsa"
	"errors"
	"fmt"
	"net"
	"net/http"
	"strconv"
	"sync"
	"testing"
	"time"

	"github.com/gobwas/ws"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/util/testhelpers"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

func TestReceiveMessagesWithoutCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, false, false, false, false)
}

func TestReceiveMessagesWithCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, true, true, false, false)
}

func TestReceiveMessagesWithServerOptionalCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, true, true, false, false)
}

func TestReceiveMessagesWithServerOnlyCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, false, true, false, false)
}

func TestReceiveMessagesWithClientOnlyCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, true, false, false, false)
}

func TestReceiveMessagesWithRequiredCompression(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, true, true, true, false)
}

func TestReceiveMessagesWithRequiredCompressionButClientDisabled(t *testing.T) {
	t.Parallel()
	testReceiveMessages(t, false, true, true, true)
}

func testReceiveMessages(t *testing.T, clientCompression bool, serverCompression bool, serverRequire bool, expectNoMessagesReceived bool) {
	t.Helper()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	broadcasterConfig := wsbroadcastserver.DefaultTestBroadcasterConfig
	broadcasterConfig.EnableCompression = serverCompression
	broadcasterConfig.RequireCompression = serverRequire

	messageCount := 1000
	clientCount := 2
	chainId := uint64(9742)

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &broadcasterConfig }, chainId, feedErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	config := DefaultTestConfig
	config.EnableCompression = clientCompression
	var wg sync.WaitGroup
	var expectedCount int
	if expectNoMessagesReceived {
		expectedCount = 0
	} else {
		expectedCount = messageCount
	}
	for i := 0; i < clientCount; i++ {
		startMakeBroadcastClient(ctx, t, config, b.ListenerAddr(), i, expectedCount, chainId, &wg, &sequencerAddr)
	}

	go func() {
		for i := 0; i < messageCount; i++ {
			Require(t, b.BroadcastSingle(arbostypes.TestMessageWithMetadataAndRequestId, arbutil.MessageIndex(i)))
		}
	}()

	wg.Wait()

}

func TestInvalidSignature(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	settings := wsbroadcastserver.DefaultTestBroadcasterConfig

	messageCount := 1
	chainId := uint64(9742)

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	fatalErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &settings }, chainId, fatalErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	badPrivateKey, err := crypto.GenerateKey()
	Require(t, err)
	badPublicKey := badPrivateKey.Public()
	badSequencerAddr := crypto.PubkeyToAddress(*badPublicKey.(*ecdsa.PublicKey))
	config := DefaultTestConfig

	ts := NewDummyTransactionStreamer(chainId, &badSequencerAddr)
	broadcastClient, err := newTestBroadcastClient(
		config,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		nil,
		fatalErrChan,
		&badSequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)

	go func() {
		for i := 0; i < messageCount; i++ {
			Require(t, b.BroadcastSingle(arbostypes.TestMessageWithMetadataAndRequestId, arbutil.MessageIndex(i)))
		}
	}()

	timer := time.NewTimer(2 * time.Second)
	select {
	case err := <-fatalErrChan:
		if errors.Is(err, signature.ErrSignatureNotVerified) {
			t.Log("feed error found as expected")
			return
		}
		t.Errorf("unexpected error occurred: %v", err)
		return
	case <-timer.C:
		t.Error("no feed errors detected")
		return
	case <-ctx.Done():
		timer.Stop()
		return
	}
}

type dummyTransactionStreamer struct {
	messageReceiver chan broadcaster.BroadcastFeedMessage
	chainId         uint64
	sequencerAddr   *common.Address
}

func NewDummyTransactionStreamer(chainId uint64, sequencerAddr *common.Address) *dummyTransactionStreamer {
	return &dummyTransactionStreamer{
		messageReceiver: make(chan broadcaster.BroadcastFeedMessage),
		chainId:         chainId,
		sequencerAddr:   sequencerAddr,
	}
}

func (ts *dummyTransactionStreamer) AddBroadcastMessages(feedMessages []*broadcaster.BroadcastFeedMessage) error {
	for _, feedMessage := range feedMessages {
		ts.messageReceiver <- *feedMessage
	}
	return nil
}

func newTestBroadcastClient(config Config, listenerAddress net.Addr, chainId uint64, currentMessageCount arbutil.MessageIndex, txStreamer TransactionStreamerInterface, confirmedSequenceNumberListener chan arbutil.MessageIndex, feedErrChan chan error, validAddr *common.Address) (*BroadcastClient, error) {
	port := listenerAddress.(*net.TCPAddr).Port
	var av contracts.AddressVerifierInterface
	if validAddr != nil {
		config.Verify.AcceptSequencer = true
		av = contracts.NewMockAddressVerifier(*validAddr)
	} else {
		config.Verify.AcceptSequencer = false
	}
	return NewBroadcastClient(func() *Config { return &config }, fmt.Sprintf("ws://127.0.0.1:%d/", port), chainId, currentMessageCount, txStreamer, confirmedSequenceNumberListener, feedErrChan, av, func(_ int32) {})
}

func startMakeBroadcastClient(ctx context.Context, t *testing.T, clientConfig Config, addr net.Addr, index int, expectedCount int, chainId uint64, wg *sync.WaitGroup, sequencerAddr *common.Address) {
	ts := NewDummyTransactionStreamer(chainId, sequencerAddr)
	feedErrChan := make(chan error, 10)
	broadcastClient, err := newTestBroadcastClient(
		clientConfig,
		addr,
		chainId,
		0,
		ts,
		nil,
		feedErrChan,
		sequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)
	messageCount := 0

	wg.Add(1)

	go func() {
		defer wg.Done()
		defer broadcastClient.StopAndWait()
		var timeout time.Duration
		if expectedCount == 0 {
			timeout = 1 * time.Second
		} else {
			timeout = 60 * time.Second
		}
		for {
			gotMsg := false
			timer := time.NewTimer(timeout)
			select {
			case <-ts.messageReceiver:
				messageCount++
				gotMsg = true
			case <-timer.C:
			case <-ctx.Done():
			case err := <-feedErrChan:
				t.Error(err)
				return
			}
			timer.Stop()
			if (!gotMsg && expectedCount > 0) || (gotMsg && expectedCount == 0) {
				t.Errorf("Client %d expected %d meesages, got %d messages\n", index, expectedCount, messageCount)
				return
			}

			if messageCount == expectedCount {
				return
			}
		}
	}()

}

func TestServerClientDisconnect(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := wsbroadcastserver.DefaultTestBroadcasterConfig
	config.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config }, chainId, feedErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	ts := NewDummyTransactionStreamer(chainId, nil)
	broadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		nil,
		feedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)

	t.Log("broadcasting seq 0 message")
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 0))

	// Wait for client to receive batch to ensure it is connected
	timer := time.NewTimer(5 * time.Second)
	defer timer.Stop()
	select {
	case err := <-feedErrChan:
		t.Errorf("Broadcaster error: %s\n", err.Error())
	case receivedMsg := <-ts.messageReceiver:
		t.Logf("Received Message, Sequence Message: %v\n", receivedMsg)
	case <-timer.C:
		t.Fatal("Client did not receive batch item")
	}

	broadcastClient.StopAndWait()

	disconnectTimer := time.NewTimer(5 * time.Second)
	defer disconnectTimer.Stop()
	for {
		if b.ClientCount() == 0 {
			break
		}

		select {
		case err := <-feedErrChan:
			t.Errorf("Broadcaster error: %s\n", err.Error())
		case <-disconnectTimer.C:
			t.Fatal("Client was not disconnected")
		default:
		}
		time.Sleep(100 * time.Millisecond)
	}
}

func TestBroadcastClientConfirmedMessage(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := wsbroadcastserver.DefaultTestBroadcasterConfig
	config.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config }, chainId, feedErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	confirmedSequenceNumberListener := make(chan arbutil.MessageIndex, 10)
	ts := NewDummyTransactionStreamer(chainId, nil)
	broadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		confirmedSequenceNumberListener,
		feedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)

	t.Log("broadcasting seq 0 message")
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 0))

	// Wait for client to receive batch to ensure it is connected
	timer := time.NewTimer(5 * time.Second)
	defer timer.Stop()
	select {
	case err := <-feedErrChan:
		t.Errorf("Broadcaster error: %s\n", err.Error())
	case receivedMsg := <-ts.messageReceiver:
		t.Logf("Received Message, Sequence Message: %v\n", receivedMsg)
	case <-timer.C:
		t.Fatal("Client did not receive batch item")
	}

	confirmNumber := arbutil.MessageIndex(42)
	b.Confirm(42)

	// Wait for client to receive confirm message
	timer2 := time.NewTimer(5 * time.Second)
	defer timer2.Stop()
	select {
	case err := <-feedErrChan:
		t.Errorf("Broadcaster error: %s", err.Error())
	case confirmed := <-confirmedSequenceNumberListener:
		if confirmed == confirmNumber {
			t.Logf("got confirmed: %v", confirmed)
		} else {
			t.Errorf("Incorrect number confirmed: %v, expected: %v", confirmed, confirmNumber)
		}
	case <-timer2.C:
		t.Fatal("Client did not receive confirm message")
	}

	broadcastClient.StopAndWait()
}
func TestServerIncorrectChainId(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := wsbroadcastserver.DefaultTestBroadcasterConfig
	config.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config }, chainId, feedErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	ts := NewDummyTransactionStreamer(chainId, nil)
	badFeedErrChan := make(chan error, 10)
	badBroadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		b.ListenerAddr(),
		chainId+1,
		0,
		ts,
		nil,
		badFeedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	badBroadcastClient.Start(ctx)
	badTimer := time.NewTimer(5 * time.Second)
	select {
	case err := <-feedErrChan:
		// Got unexpected error
		t.Errorf("Unexpected error %v", err)
		badTimer.Stop()
	case err := <-badFeedErrChan:
		if !errors.Is(err, ErrIncorrectChainId) {
			// Got unexpected error
			t.Errorf("Unexpected error %v", err)
		}
		badTimer.Stop()
	case <-badTimer.C:
		t.Fatal("Client channel did not send error as expected")
	}
}

func TestServerMissingChainId(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	settings := wsbroadcastserver.DefaultTestBroadcasterConfig
	settings.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &settings }, chainId, feedErrChan, dataSigner)

	header := ws.HandshakeHeaderHTTP(http.Header{
		wsbroadcastserver.HTTPHeaderFeedServerVersion: []string{strconv.Itoa(wsbroadcastserver.FeedServerVersion)},
	})

	Require(t, b.Initialize())
	Require(t, b.StartWithHeader(ctx, header))
	defer b.StopAndWait()

	clientConfig := DefaultTestConfig
	clientConfig.RequireChainId = true

	ts := NewDummyTransactionStreamer(chainId, nil)
	badFeedErrChan := make(chan error, 10)
	badBroadcastClient, err := newTestBroadcastClient(
		clientConfig,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		nil,
		badFeedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	badBroadcastClient.Start(ctx)
	badTimer := time.NewTimer(5 * time.Second)
	select {
	case err := <-feedErrChan:
		// Got unexpected error
		t.Errorf("Unexpected error %v", err)
		badTimer.Stop()
	case err := <-badFeedErrChan:
		if !errors.Is(err, ErrMissingChainId) {
			// Got unexpected error
			t.Errorf("Unexpected error %v", err)
		}
		badTimer.Stop()
	case <-badTimer.C:
		t.Fatal("Client channel did not send error as expected")
	}
}

func TestServerIncorrectFeedServerVersion(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	settings := wsbroadcastserver.DefaultTestBroadcasterConfig
	settings.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &settings }, chainId, feedErrChan, dataSigner)

	header := ws.HandshakeHeaderHTTP(http.Header{
		wsbroadcastserver.HTTPHeaderChainId:           []string{strconv.FormatUint(chainId, 10)},
		wsbroadcastserver.HTTPHeaderFeedServerVersion: []string{strconv.Itoa(wsbroadcastserver.FeedServerVersion + 1)},
	})

	Require(t, b.Initialize())
	Require(t, b.StartWithHeader(ctx, header))
	defer b.StopAndWait()

	ts := NewDummyTransactionStreamer(chainId, nil)
	badFeedErrChan := make(chan error, 10)
	badBroadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		nil,
		badFeedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	badBroadcastClient.Start(ctx)
	badTimer := time.NewTimer(5 * time.Second)
	select {
	case err := <-feedErrChan:
		// Got unexpected error
		t.Errorf("Unexpected error %v", err)
		badTimer.Stop()
	case err := <-badFeedErrChan:
		if !errors.Is(err, ErrIncorrectFeedServerVersion) {
			// Got unexpected error
			t.Errorf("Unexpected error %v", err)
		}
		badTimer.Stop()
	case <-badTimer.C:
		t.Fatal("Client channel did not send error as expected")
	}
}

func TestServerMissingFeedServerVersion(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	settings := wsbroadcastserver.DefaultTestBroadcasterConfig
	settings.Ping = 1 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	chainId := uint64(8742)
	feedErrChan := make(chan error, 10)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &settings }, chainId, feedErrChan, dataSigner)

	header := ws.HandshakeHeaderHTTP(http.Header{
		wsbroadcastserver.HTTPHeaderChainId: []string{strconv.FormatUint(chainId, 10)},
	})

	Require(t, b.Initialize())
	Require(t, b.StartWithHeader(ctx, header))
	defer b.StopAndWait()

	clientConfig := DefaultTestConfig
	clientConfig.RequireFeedVersion = true

	ts := NewDummyTransactionStreamer(chainId, nil)
	badFeedErrChan := make(chan error, 10)
	badBroadcastClient, err := newTestBroadcastClient(
		clientConfig,
		b.ListenerAddr(),
		chainId,
		0,
		ts,
		nil,
		badFeedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	badBroadcastClient.Start(ctx)
	badTimer := time.NewTimer(5 * time.Second)
	select {
	case err := <-feedErrChan:
		// Got unexpected error
		t.Errorf("Unexpected error %v", err)
		badTimer.Stop()
	case err := <-badFeedErrChan:
		if !errors.Is(err, ErrMissingFeedServerVersion) {
			// Got unexpected error
			t.Errorf("Unexpected error %v", err)
		}
		badTimer.Stop()
	case <-badTimer.C:
		t.Fatal("Client channel did not send error as expected")
	}
}

func TestBroadcastClientReconnectsOnServerDisconnect(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := wsbroadcastserver.DefaultTestBroadcasterConfig
	config.Ping = 50 * time.Second
	config.ClientTimeout = 150 * time.Second

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	feedErrChan := make(chan error, 10)
	chainId := uint64(8742)
	b1 := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config }, chainId, feedErrChan, dataSigner)

	Require(t, b1.Initialize())
	Require(t, b1.Start(ctx))
	defer b1.StopAndWait()

	broadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		b1.ListenerAddr(),
		chainId,
		0,
		nil,
		nil,
		feedErrChan,
		&sequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)
	defer broadcastClient.StopAndWait()

	// Client set to timeout connection at 200 milliseconds, and server set to send ping every 50 seconds,
	// so at least one timeout/reconnect should happen after 1 seconds
	time.Sleep(1 * time.Second)

	select {
	case err := <-feedErrChan:
		t.Errorf("Broadcaster error: %s\n", err.Error())
	default:
	}

	if broadcastClient.GetRetryCount() <= 0 {
		t.Error("Should have had some retry counts")
	}
}

func TestBroadcasterSendsCachedMessagesOnClientConnect(t *testing.T) {
	t.Parallel()
	/* Uncomment to enable logging
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlTrace)
	log.Root().SetHandler(glogger)
	*/
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	settings := wsbroadcastserver.DefaultTestBroadcasterConfig

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	sequencerAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := signature.DataSignerFromPrivateKey(privateKey)

	feedErrChan := make(chan error, 10)
	chainId := uint64(8744)
	b := broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &settings }, chainId, feedErrChan, dataSigner)

	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 0))
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 1))

	var wg sync.WaitGroup
	for i := 0; i < 2; i++ {
		wg.Add(1)
		connectAndGetCachedMessages(ctx, b.ListenerAddr(), chainId, t, i, feedErrChan, &sequencerAddr, &wg)
	}

	select {
	case err := <-feedErrChan:
		t.Fatalf("Broadcaster error: %s\n", err.Error())
	case <-time.After(10 * time.Millisecond):
	}
	wg.Wait()

	// give the above connections time to reconnect
	time.Sleep(1 * time.Second)

	// Confirmed Accumulator will also broadcast to the clients.
	b.Confirm(0) // remove the first message we generated

	updateTimer := time.NewTimer(2 * time.Second)
	defer updateTimer.Stop()
	for {
		if b.GetCachedMessageCount() == 1 { // should have left the second message
			break
		}

		select {
		case <-updateTimer.C:
			t.Fatal("confirmed accumulator did not get updated")
		case err := <-feedErrChan:
			t.Errorf("Broadcaster error: %s\n", err.Error())
		default:
		}
		time.Sleep(10 * time.Millisecond)
	}

	b.Confirm(1)

	updateTimer2 := time.NewTimer(2 * time.Second)
	defer updateTimer2.Stop()
	for {
		if b.GetCachedMessageCount() == 0 { // should have left the second message
			break
		}

		select {
		case <-updateTimer2.C:
			t.Fatal("cache did not get cleared")
		default:
		}
		time.Sleep(10 * time.Millisecond)
	}
}

func connectAndGetCachedMessages(ctx context.Context, addr net.Addr, chainId uint64, t *testing.T, clientIndex int, feedErrChan chan error, sequencerAddr *common.Address, wg *sync.WaitGroup) {
	ts := NewDummyTransactionStreamer(chainId, nil)
	broadcastClient, err := newTestBroadcastClient(
		DefaultTestConfig,
		addr,
		chainId,
		0,
		ts,
		nil,
		feedErrChan,
		sequencerAddr,
	)
	Require(t, err)
	broadcastClient.Start(ctx)

	go func() {
		defer wg.Done()
		defer broadcastClient.StopAndWait()

		gotMsg := false
		// Wait for client to receive first item
		timer := time.NewTimer(10 * time.Second)
		defer timer.Stop()
		select {
		case receivedMsg := <-ts.messageReceiver:
			t.Logf("client %d received first message: %v\n", clientIndex, receivedMsg)
			gotMsg = true
		case err := <-feedErrChan:
			t.Errorf("client %d feed error: %v\n", clientIndex, err)
		case <-timer.C:
		case <-ctx.Done():
		}
		if !gotMsg {
			t.Errorf("client %d did not receive first batch item\n", clientIndex)
			return
		}

		gotMsg = false
		// Wait for client to receive second item
		timer2 := time.NewTimer(10 * time.Second)
		defer timer2.Stop()
		select {
		case receivedMsg := <-ts.messageReceiver:
			t.Logf("client %d received second message: %v\n", clientIndex, receivedMsg)
			gotMsg = true
		case err := <-feedErrChan:
			t.Errorf("client %d feed error: %v\n", clientIndex, err)
		case <-timer2.C:
		case <-ctx.Done():
		}
		if !gotMsg {
			t.Errorf("client %d did not receive second batch item\n", clientIndex)
		}
	}()
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

'''
'''--- broadcastclients/broadcastclients.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcastclients

import (
	"context"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcastclient"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

const ROUTER_QUEUE_SIZE = 1024
const RECENT_FEED_INITIAL_MAP_SIZE = 1024
const RECENT_FEED_ITEM_TTL = time.Second * 10
const MAX_FEED_INACTIVE_TIME = time.Second * 5
const PRIMARY_FEED_UPTIME = time.Minute * 10

type Router struct {
	stopwaiter.StopWaiter
	messageChan                 chan broadcaster.BroadcastFeedMessage
	confirmedSequenceNumberChan chan arbutil.MessageIndex

	forwardTxStreamer       broadcastclient.TransactionStreamerInterface
	forwardConfirmationChan chan arbutil.MessageIndex
}

func (r *Router) AddBroadcastMessages(feedMessages []*broadcaster.BroadcastFeedMessage) error {
	for _, feedMessage := range feedMessages {
		r.messageChan <- *feedMessage
	}
	return nil
}

type BroadcastClients struct {
	primaryClients   []*broadcastclient.BroadcastClient
	secondaryClients []*broadcastclient.BroadcastClient
	secondaryURL     []string

	primaryRouter   *Router
	secondaryRouter *Router

	// Use atomic access
	connected int32
}

var makeClient func(string, *Router) (*broadcastclient.BroadcastClient, error)

func NewBroadcastClients(
	configFetcher broadcastclient.ConfigFetcher,
	l2ChainId uint64,
	currentMessageCount arbutil.MessageIndex,
	txStreamer broadcastclient.TransactionStreamerInterface,
	confirmedSequenceNumberListener chan arbutil.MessageIndex,
	fatalErrChan chan error,
	addrVerifier contracts.AddressVerifierInterface,
) (*BroadcastClients, error) {
	config := configFetcher()
	if len(config.URL) == 0 && len(config.SecondaryURL) == 0 {
		return nil, nil
	}
	newStandardRouter := func() *Router {
		return &Router{
			messageChan:                 make(chan broadcaster.BroadcastFeedMessage, ROUTER_QUEUE_SIZE),
			confirmedSequenceNumberChan: make(chan arbutil.MessageIndex, ROUTER_QUEUE_SIZE),
			forwardTxStreamer:           txStreamer,
			forwardConfirmationChan:     confirmedSequenceNumberListener,
		}
	}
	clients := BroadcastClients{
		primaryRouter:    newStandardRouter(),
		secondaryRouter:  newStandardRouter(),
		primaryClients:   make([]*broadcastclient.BroadcastClient, 0, len(config.URL)),
		secondaryClients: make([]*broadcastclient.BroadcastClient, 0, len(config.SecondaryURL)),
		secondaryURL:     config.SecondaryURL,
	}
	makeClient = func(url string, router *Router) (*broadcastclient.BroadcastClient, error) {
		return broadcastclient.NewBroadcastClient(
			configFetcher,
			url,
			l2ChainId,
			currentMessageCount,
			router,
			router.confirmedSequenceNumberChan,
			fatalErrChan,
			addrVerifier,
			func(delta int32) { clients.adjustCount(delta) },
		)
	}

	var lastClientErr error
	for _, address := range config.URL {
		client, err := makeClient(address, clients.primaryRouter)
		if err != nil {
			lastClientErr = err
			log.Warn("init broadcast client failed", "address", address)
			continue
		}
		clients.primaryClients = append(clients.primaryClients, client)
	}
	if len(clients.primaryClients) == 0 {
		log.Error("no connected feed on startup, last error: %w", lastClientErr)
		return nil, nil
	}

	return &clients, nil
}

func (bcs *BroadcastClients) adjustCount(delta int32) {
	connected := atomic.AddInt32(&bcs.connected, delta)
	if connected <= 0 {
		log.Error("no connected feed")
	}
}

func (bcs *BroadcastClients) Start(ctx context.Context) {
	bcs.primaryRouter.StopWaiter.Start(ctx, bcs.primaryRouter)
	bcs.secondaryRouter.StopWaiter.Start(ctx, bcs.secondaryRouter)

	for _, client := range bcs.primaryClients {
		client.Start(ctx)
	}

	var lastConfirmed arbutil.MessageIndex
	recentFeedItemsNew := make(map[arbutil.MessageIndex]time.Time, RECENT_FEED_INITIAL_MAP_SIZE)
	recentFeedItemsOld := make(map[arbutil.MessageIndex]time.Time, RECENT_FEED_INITIAL_MAP_SIZE)
	bcs.primaryRouter.LaunchThread(func(ctx context.Context) {
		recentFeedItemsCleanup := time.NewTicker(RECENT_FEED_ITEM_TTL)
		startSecondaryFeedTimer := time.NewTicker(MAX_FEED_INACTIVE_TIME)
		stopSecondaryFeedTimer := time.NewTicker(PRIMARY_FEED_UPTIME)
		primaryFeedIsDownTimer := time.NewTicker(MAX_FEED_INACTIVE_TIME)
		defer recentFeedItemsCleanup.Stop()
		defer startSecondaryFeedTimer.Stop()
		defer stopSecondaryFeedTimer.Stop()
		defer primaryFeedIsDownTimer.Stop()
		for {
			select {
			case <-ctx.Done():
				return

			// Primary feeds
			case msg := <-bcs.primaryRouter.messageChan:
				startSecondaryFeedTimer.Reset(MAX_FEED_INACTIVE_TIME)
				primaryFeedIsDownTimer.Reset(MAX_FEED_INACTIVE_TIME)
				if _, ok := recentFeedItemsNew[msg.SequenceNumber]; ok {
					continue
				}
				if _, ok := recentFeedItemsOld[msg.SequenceNumber]; ok {
					continue
				}
				recentFeedItemsNew[msg.SequenceNumber] = time.Now()
				if err := bcs.primaryRouter.forwardTxStreamer.AddBroadcastMessages([]*broadcaster.BroadcastFeedMessage{&msg}); err != nil {
					log.Error("Error routing message from Primary Sequencer Feeds", "err", err)
				}
			case cs := <-bcs.primaryRouter.confirmedSequenceNumberChan:
				startSecondaryFeedTimer.Reset(MAX_FEED_INACTIVE_TIME)
				primaryFeedIsDownTimer.Reset(MAX_FEED_INACTIVE_TIME)
				if cs == lastConfirmed {
					continue
				}
				lastConfirmed = cs
				if bcs.primaryRouter.forwardConfirmationChan != nil {
					bcs.primaryRouter.forwardConfirmationChan <- cs
				}

			// Secondary Feeds
			case msg := <-bcs.secondaryRouter.messageChan:
				startSecondaryFeedTimer.Reset(MAX_FEED_INACTIVE_TIME)
				if _, ok := recentFeedItemsNew[msg.SequenceNumber]; ok {
					continue
				}
				if _, ok := recentFeedItemsOld[msg.SequenceNumber]; ok {
					continue
				}
				recentFeedItemsNew[msg.SequenceNumber] = time.Now()
				if err := bcs.secondaryRouter.forwardTxStreamer.AddBroadcastMessages([]*broadcaster.BroadcastFeedMessage{&msg}); err != nil {
					log.Error("Error routing message from Secondary Sequencer Feeds", "err", err)
				}
			case cs := <-bcs.secondaryRouter.confirmedSequenceNumberChan:
				startSecondaryFeedTimer.Reset(MAX_FEED_INACTIVE_TIME)
				if cs == lastConfirmed {
					continue
				}
				lastConfirmed = cs
				if bcs.secondaryRouter.forwardConfirmationChan != nil {
					bcs.secondaryRouter.forwardConfirmationChan <- cs
				}

			// Cycle buckets to get rid of old entries
			case <-recentFeedItemsCleanup.C:
				recentFeedItemsOld = recentFeedItemsNew
				recentFeedItemsNew = make(map[arbutil.MessageIndex]time.Time, RECENT_FEED_INITIAL_MAP_SIZE)

			// failed to get messages from both primary and secondary feeds for ~5 seconds, start a new secondary feed
			case <-startSecondaryFeedTimer.C:
				bcs.startSecondaryFeed(ctx)

			// failed to get messages from primary feed for ~5 seconds, reset the timer responsible for stopping a secondary
			case <-primaryFeedIsDownTimer.C:
				stopSecondaryFeedTimer.Reset(PRIMARY_FEED_UPTIME)

			// primary feeds have been up and running for PRIMARY_FEED_UPTIME=10 mins without a failure, stop the recently started secondary feed
			case <-stopSecondaryFeedTimer.C:
				bcs.stopSecondaryFeed()
			}
		}
	})
}

func (bcs *BroadcastClients) startSecondaryFeed(ctx context.Context) {
	pos := len(bcs.secondaryClients)
	if pos < len(bcs.secondaryURL) {
		url := bcs.secondaryURL[pos]
		client, err := makeClient(url, bcs.secondaryRouter)
		if err != nil {
			log.Warn("init broadcast secondary client failed", "address", url)
			bcs.secondaryURL = append(bcs.secondaryURL[:pos], bcs.secondaryURL[pos+1:]...)
			return
		}
		bcs.secondaryClients = append(bcs.secondaryClients, client)
		client.Start(ctx)
		log.Info("secondary feed started", "url", url)
	} else if len(bcs.secondaryURL) > 0 {
		log.Warn("failed to start a new secondary feed all available secondary feeds were started")
	}
}

func (bcs *BroadcastClients) stopSecondaryFeed() {
	pos := len(bcs.secondaryClients)
	if pos > 0 {
		pos -= 1
		bcs.secondaryClients[pos].StopAndWait()
		bcs.secondaryClients = bcs.secondaryClients[:pos]
		log.Info("disconnected secondary feed", "url", bcs.secondaryURL[pos])
	}
}

func (bcs *BroadcastClients) StopAndWait() {
	for _, client := range bcs.primaryClients {
		client.StopAndWait()
	}
	for _, client := range bcs.secondaryClients {
		client.StopAndWait()
	}
}

'''
'''--- broadcaster/broadcaster.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcaster

import (
	"context"
	"net"

	"github.com/gobwas/ws"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

type Broadcaster struct {
	server        *wsbroadcastserver.WSBroadcastServer
	catchupBuffer *SequenceNumberCatchupBuffer
	chainId       uint64
	dataSigner    signature.DataSignerFunc
}

// BroadcastMessage is the base message type for messages to send over the network.
//
// Acts as a variant holding the message types. The type of the message is
// indicated by whichever of the fields is non-empty. The fields holding the message
// types are annotated with omitempty so only the populated message is sent as
// json. The message fields should be pointers or slices and end with
// "Messages" or "Message".
//
// The format is forwards compatible, ie if a json BroadcastMessage is received that
// has fields that are not in the Go struct then deserialization will succeed
// skip the unknown field [1]
//
// References:
// [1] https://pkg.go.dev/encoding/json#Unmarshal
type BroadcastMessage struct {
	Version int `json:"version"`
	// TODO better name than messages since there are different types of messages
	Messages                       []*BroadcastFeedMessage         `json:"messages,omitempty"`
	ConfirmedSequenceNumberMessage *ConfirmedSequenceNumberMessage `json:"confirmedSequenceNumberMessage,omitempty"`
}

type BroadcastFeedMessage struct {
	SequenceNumber arbutil.MessageIndex           `json:"sequenceNumber"`
	Message        arbostypes.MessageWithMetadata `json:"message"`
	Signature      []byte                         `json:"signature"`
}

func (m *BroadcastFeedMessage) Hash(chainId uint64) (common.Hash, error) {
	return m.Message.Hash(m.SequenceNumber, chainId)
}

type ConfirmedSequenceNumberMessage struct {
	SequenceNumber arbutil.MessageIndex `json:"sequenceNumber"`
}

func NewBroadcaster(config wsbroadcastserver.BroadcasterConfigFetcher, chainId uint64, feedErrChan chan error, dataSigner signature.DataSignerFunc) *Broadcaster {
	catchupBuffer := NewSequenceNumberCatchupBuffer(func() bool { return config().LimitCatchup }, func() int { return config().MaxCatchup })
	return &Broadcaster{
		server:        wsbroadcastserver.NewWSBroadcastServer(config, catchupBuffer, chainId, feedErrChan),
		catchupBuffer: catchupBuffer,
		chainId:       chainId,
		dataSigner:    dataSigner,
	}
}

func (b *Broadcaster) NewBroadcastFeedMessage(message arbostypes.MessageWithMetadata, sequenceNumber arbutil.MessageIndex) (*BroadcastFeedMessage, error) {
	var messageSignature []byte
	if b.dataSigner != nil {
		hash, err := message.Hash(sequenceNumber, b.chainId)
		if err != nil {
			return nil, err
		}
		messageSignature, err = b.dataSigner(hash.Bytes())
		if err != nil {
			return nil, err
		}
	}

	return &BroadcastFeedMessage{
		SequenceNumber: sequenceNumber,
		Message:        message,
		Signature:      messageSignature,
	}, nil
}

func (b *Broadcaster) BroadcastSingle(msg arbostypes.MessageWithMetadata, seq arbutil.MessageIndex) error {
	defer func() {
		if r := recover(); r != nil {
			log.Error("recovered error in BroadcastSingle", "recover", r)
		}
	}()
	bfm, err := b.NewBroadcastFeedMessage(msg, seq)
	if err != nil {
		return err
	}

	b.BroadcastSingleFeedMessage(bfm)
	return nil
}

func (b *Broadcaster) BroadcastSingleFeedMessage(bfm *BroadcastFeedMessage) {
	broadcastFeedMessages := make([]*BroadcastFeedMessage, 0, 1)

	broadcastFeedMessages = append(broadcastFeedMessages, bfm)

	b.BroadcastFeedMessages(broadcastFeedMessages)
}

func (b *Broadcaster) BroadcastFeedMessages(messages []*BroadcastFeedMessage) {

	bm := BroadcastMessage{
		Version:  1,
		Messages: messages,
	}

	b.server.Broadcast(bm)
}

func (b *Broadcaster) Confirm(seq arbutil.MessageIndex) {
	log.Debug("confirming sequence number", "sequenceNumber", seq)
	b.server.Broadcast(BroadcastMessage{
		Version:                        1,
		ConfirmedSequenceNumberMessage: &ConfirmedSequenceNumberMessage{seq}})
}

func (b *Broadcaster) ClientCount() int32 {
	return b.server.ClientCount()
}

func (b *Broadcaster) ListenerAddr() net.Addr {
	return b.server.ListenerAddr()
}

func (b *Broadcaster) GetCachedMessageCount() int {
	return b.catchupBuffer.GetMessageCount()
}

func (b *Broadcaster) Initialize() error {
	return b.server.Initialize()
}

func (b *Broadcaster) Start(ctx context.Context) error {
	return b.server.Start(ctx)
}

func (b *Broadcaster) StartWithHeader(ctx context.Context, header ws.HandshakeHeader) error {
	return b.server.StartWithHeader(ctx, header)
}

func (b *Broadcaster) StopAndWait() {
	b.server.StopAndWait()
}

func (b *Broadcaster) Started() bool {
	return b.server.Started()
}

'''
'''--- broadcaster/broadcaster_serialization_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcaster

import (
	"bytes"
	"encoding/json"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
)

func ExampleBroadcastMessage_broadcastfeedmessage() {
	var requestId common.Hash
	msg := BroadcastMessage{
		Version: 1,
		Messages: []*BroadcastFeedMessage{
			{
				SequenceNumber: 12345,
				Message: arbostypes.MessageWithMetadata{
					Message: &arbostypes.L1IncomingMessage{
						Header: &arbostypes.L1IncomingMessageHeader{
							Kind:        0,
							Poster:      [20]byte{},
							BlockNumber: 0,
							Timestamp:   0,
							RequestId:   &requestId,
							L1BaseFee:   big.NewInt(0),
						},
						L2msg: []byte{0xde, 0xad, 0xbe, 0xef},
					},
					DelayedMessagesRead: 3333,
				},
				Signature: nil,
			},
		},
	}
	var buf bytes.Buffer
	encoder := json.NewEncoder(&buf)
	_ = encoder.Encode(msg)
	fmt.Println(buf.String())
	// Output: {"version":1,"messages":[{"sequenceNumber":12345,"message":{"message":{"header":{"kind":0,"sender":"0x0000000000000000000000000000000000000000","blockNumber":0,"timestamp":0,"requestId":"0x0000000000000000000000000000000000000000000000000000000000000000","baseFeeL1":0},"l2Msg":"3q2+7w=="},"delayedMessagesRead":3333},"signature":null}]}
}

func ExampleBroadcastMessage_emptymessage() {
	msg := BroadcastMessage{
		Version: 1,
	}
	var buf bytes.Buffer
	encoder := json.NewEncoder(&buf)
	_ = encoder.Encode(msg)
	fmt.Println(buf.String())
	// Output: {"version":1}
}

func ExampleBroadcastMessage_confirmedseqnum() {
	msg := BroadcastMessage{
		Version: 1,
		ConfirmedSequenceNumberMessage: &ConfirmedSequenceNumberMessage{
			SequenceNumber: 1234,
		},
	}
	var buf bytes.Buffer
	encoder := json.NewEncoder(&buf)
	_ = encoder.Encode(msg)
	fmt.Println(buf.String())
	// Output: {"version":1,"confirmedSequenceNumberMessage":{"sequenceNumber":1234}}
}

'''
'''--- broadcaster/broadcaster_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcaster

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/util/testhelpers"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

type predicate interface {
	Test() bool
	Error() string
}

func waitUntilUpdated(t *testing.T, p predicate) {
	t.Helper()
	updateTimer := time.NewTimer(2 * time.Second)
	defer updateTimer.Stop()
	for {
		if p.Test() {
			break
		}
		select {
		case <-updateTimer.C:
			t.Fatalf("%s", p.Error())
		default:
		}
		time.Sleep(10 * time.Millisecond)
	}
}

type messageCountPredicate struct {
	b              *Broadcaster
	expected       int
	contextMessage string
	was            int
}

func (p *messageCountPredicate) Test() bool {
	p.was = p.b.catchupBuffer.GetMessageCount()
	return p.was == p.expected
}

func (p *messageCountPredicate) Error() string {
	return fmt.Sprintf("Expected %d, was %d: %s", p.expected, p.was, p.contextMessage)
}

func TestBroadcasterMessagesRemovedOnConfirmation(t *testing.T) {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()

	config := wsbroadcastserver.DefaultTestBroadcasterConfig

	chainId := uint64(5555)
	feedErrChan := make(chan error, 10)
	b := NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config }, chainId, feedErrChan, nil)
	Require(t, b.Initialize())
	Require(t, b.Start(ctx))
	defer b.StopAndWait()

	expectMessageCount := func(count int, contextMessage string) predicate {
		return &messageCountPredicate{b, count, contextMessage, 0}
	}

	// Normal broadcasting and confirming
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 1))
	waitUntilUpdated(t, expectMessageCount(1, "after 1 message"))
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 2))
	waitUntilUpdated(t, expectMessageCount(2, "after 2 messages"))
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 3))
	waitUntilUpdated(t, expectMessageCount(3, "after 3 messages"))
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 4))
	waitUntilUpdated(t, expectMessageCount(4, "after 4 messages"))

	b.Confirm(1)
	waitUntilUpdated(t, expectMessageCount(3,
		"after 4 messages, 1 cleared by confirm"))

	b.Confirm(2)
	waitUntilUpdated(t, expectMessageCount(2,
		"after 4 messages, 2 cleared by confirm"))

	b.Confirm(1)
	waitUntilUpdated(t, expectMessageCount(2,
		"nothing changed because confirmed sequence number before cache"))

	b.Confirm(2)
	Require(t, b.BroadcastSingle(arbostypes.EmptyTestMessageWithMetadata, 5))
	waitUntilUpdated(t, expectMessageCount(3,
		"after 5 messages, 2 cleared by confirm"))

	// Confirm not-yet-seen or already confirmed/cleared sequence numbers twice to force clearing cache
	b.Confirm(6)
	waitUntilUpdated(t, expectMessageCount(0,
		"clear all messages after confirmed 1 beyond latest"))
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- broadcaster/sequencenumbercatchupbuffer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package broadcaster

import (
	"errors"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

const (
	// Do not send cache if requested seqnum is older than last cached minus maxRequestedSeqNumOffset
	maxRequestedSeqNumOffset = arbutil.MessageIndex(10_000)
)

var (
	confirmedSequenceNumberGauge = metrics.NewRegisteredGauge("arb/sequencenumber/confirmed", nil)
	cachedMessagesSentHistogram  = metrics.NewRegisteredHistogram("arb/feed/clients/cache/sent", nil, metrics.NewBoundedHistogramSample())
)

type SequenceNumberCatchupBuffer struct {
	messages     []*BroadcastFeedMessage
	messageCount int32
	limitCatchup func() bool
	maxCatchup   func() int
}

func NewSequenceNumberCatchupBuffer(limitCatchup func() bool, maxCatchup func() int) *SequenceNumberCatchupBuffer {
	return &SequenceNumberCatchupBuffer{
		limitCatchup: limitCatchup,
		maxCatchup:   maxCatchup,
	}
}

func (b *SequenceNumberCatchupBuffer) getCacheMessages(requestedSeqNum arbutil.MessageIndex) *BroadcastMessage {
	if len(b.messages) == 0 {
		return nil
	}
	var startingIndex int32
	// Ignore messages older than requested sequence number
	firstCachedSeqNum := b.messages[0].SequenceNumber
	if firstCachedSeqNum < requestedSeqNum {
		lastCachedSeqNum := firstCachedSeqNum + arbutil.MessageIndex(len(b.messages)-1)
		if lastCachedSeqNum < requestedSeqNum {
			// Past end, nothing to return
			return nil
		}
		startingIndex = int32(requestedSeqNum - firstCachedSeqNum)
		if startingIndex >= int32(len(b.messages)) {
			log.Error("unexpected startingIndex", "requestedSeqNum", requestedSeqNum, "firstCachedSeqNum", firstCachedSeqNum, "startingIndex", startingIndex, "lastCachedSeqNum", lastCachedSeqNum, "cacheLength", len(b.messages))
			return nil
		}
		if b.messages[startingIndex].SequenceNumber != requestedSeqNum {
			log.Error("requestedSeqNum not found where expected", "requestedSeqNum", requestedSeqNum, "firstCachedSeqNum", firstCachedSeqNum, "startingIndex", startingIndex, "foundSeqNum", b.messages[startingIndex].SequenceNumber)
			return nil
		}
	} else if b.limitCatchup() && firstCachedSeqNum > maxRequestedSeqNumOffset && requestedSeqNum < (firstCachedSeqNum-maxRequestedSeqNumOffset) {
		// Requested seqnum is too old, don't send any cache
		return nil
	}

	messagesToSend := b.messages[startingIndex:]
	if len(messagesToSend) > 0 {
		bm := BroadcastMessage{
			Version:  1,
			Messages: messagesToSend,
		}

		return &bm
	}

	return nil
}

func (b *SequenceNumberCatchupBuffer) OnRegisterClient(clientConnection *wsbroadcastserver.ClientConnection) (error, int, time.Duration) {
	start := time.Now()
	bm := b.getCacheMessages(clientConnection.RequestedSeqNum())
	var bmCount int
	if bm != nil {
		bmCount = len(bm.Messages)
	}
	if bm != nil {
		// send the newly connected client the requested messages
		err := clientConnection.Write(bm)
		if err != nil {
			log.Error("error sending client cached messages", "error", err, "client", clientConnection.Name, "elapsed", time.Since(start))
			return err, 0, 0
		}
	}

	cachedMessagesSentHistogram.Update(int64(bmCount))

	return nil, bmCount, time.Since(start)
}

// Takes as input an index into the messages array, not a message index
func (b *SequenceNumberCatchupBuffer) pruneBufferToIndex(idx int) {
	b.messages = b.messages[idx:]
	if len(b.messages) > 10 && cap(b.messages) > len(b.messages)*10 {
		// Too much spare capacity, copy to fresh slice to reset memory usage
		b.messages = append([]*BroadcastFeedMessage(nil), b.messages[:len(b.messages)]...)
	}
}

func (b *SequenceNumberCatchupBuffer) deleteConfirmed(confirmedSequenceNumber arbutil.MessageIndex) {
	if len(b.messages) == 0 {
		return
	}

	firstSequenceNumber := b.messages[0].SequenceNumber

	if confirmedSequenceNumber < firstSequenceNumber {
		// Confirmed sequence number is older than cache, so nothing to do
		return
	}

	confirmedIndex := uint64(confirmedSequenceNumber - firstSequenceNumber)

	if confirmedIndex >= uint64(len(b.messages)) {
		log.Error("ConfirmedSequenceNumber is past the end of stored messages", "confirmedSequenceNumber", confirmedSequenceNumber, "firstSequenceNumber", firstSequenceNumber, "cacheLength", len(b.messages))
		b.messages = nil
		return
	}

	if b.messages[confirmedIndex].SequenceNumber != confirmedSequenceNumber {
		// Log instead of returning error here so that the message will be sent to downstream
		// relays to also cause them to be cleared.
		log.Error("Invariant violation: confirmedSequenceNumber is not where expected, clearing buffer", "confirmedSequenceNumber", confirmedSequenceNumber, "firstSequenceNumber", firstSequenceNumber, "cacheLength", len(b.messages), "foundSequenceNumber", b.messages[confirmedIndex].SequenceNumber)
		b.messages = nil
		return
	}

	b.pruneBufferToIndex(int(confirmedIndex) + 1)
}

func (b *SequenceNumberCatchupBuffer) OnDoBroadcast(bmi interface{}) error {
	broadcastMessage, ok := bmi.(BroadcastMessage)
	if !ok {
		msg := "requested to broadcast message of unknown type"
		log.Error(msg)
		return errors.New(msg)
	}
	defer func() { atomic.StoreInt32(&b.messageCount, int32(len(b.messages))) }()

	if confirmMsg := broadcastMessage.ConfirmedSequenceNumberMessage; confirmMsg != nil {
		b.deleteConfirmed(confirmMsg.SequenceNumber)
		confirmedSequenceNumberGauge.Update(int64(confirmMsg.SequenceNumber))
	}

	maxCatchup := b.maxCatchup()
	if maxCatchup == 0 {
		b.messages = nil
		return nil
	}

	for _, newMsg := range broadcastMessage.Messages {
		if len(b.messages) == 0 {
			// Add to empty list
			b.messages = append(b.messages, newMsg)
		} else if expectedSequenceNumber := b.messages[len(b.messages)-1].SequenceNumber + 1; newMsg.SequenceNumber == expectedSequenceNumber {
			// Next sequence number to add to end of list
			b.messages = append(b.messages, newMsg)
		} else if newMsg.SequenceNumber > expectedSequenceNumber {
			log.Warn(
				"Message requested to be broadcast has unexpected sequence number; discarding to seqNum from catchup buffer",
				"seqNum", newMsg.SequenceNumber,
				"expectedSeqNum", expectedSequenceNumber,
			)
			b.messages = nil
			b.messages = append(b.messages, newMsg)
		} else {
			log.Info("Skipping already seen message", "seqNum", newMsg.SequenceNumber)
		}
	}

	if maxCatchup >= 0 && len(b.messages) > maxCatchup {
		b.pruneBufferToIndex(len(b.messages) - maxCatchup)
	}

	return nil

}

func (b *SequenceNumberCatchupBuffer) GetMessageCount() int {
	return int(atomic.LoadInt32(&b.messageCount))
}

'''
'''--- broadcaster/sequencenumbercatchupbuffer_test.go ---
/*
 * Copyright 2020-2021, Offchain Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package broadcaster

import (
	"strings"
	"testing"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestGetEmptyCacheMessages(t *testing.T) {
	buffer := SequenceNumberCatchupBuffer{
		messages:     nil,
		messageCount: 0,
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	// Get everything
	bm := buffer.getCacheMessages(0)
	if bm != nil {
		t.Error("shouldn't have returned anything")
	}
}

func createDummyBroadcastMessages(seqNums []arbutil.MessageIndex) []*BroadcastFeedMessage {
	return createDummyBroadcastMessagesImpl(seqNums, len(seqNums))
}
func createDummyBroadcastMessagesImpl(seqNums []arbutil.MessageIndex, length int) []*BroadcastFeedMessage {
	broadcastMessages := make([]*BroadcastFeedMessage, 0, length)
	for _, seqNum := range seqNums {
		broadcastMessage := &BroadcastFeedMessage{
			SequenceNumber: seqNum,
			Message:        arbostypes.EmptyTestMessageWithMetadata,
		}
		broadcastMessages = append(broadcastMessages, broadcastMessage)
	}

	return broadcastMessages
}

func TestGetCacheMessages(t *testing.T) {
	indexes := []arbutil.MessageIndex{40, 41, 42, 43, 44, 45, 46}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessages(indexes),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	// Get everything
	bm := buffer.getCacheMessages(0)
	if len(bm.Messages) != 7 {
		t.Error("didn't return all messages")
	}

	// Get everything
	bm = buffer.getCacheMessages(1)
	if len(bm.Messages) != 7 {
		t.Error("didn't return all messages")
	}

	// Get everything
	bm = buffer.getCacheMessages(40)
	if len(bm.Messages) != 7 {
		t.Error("didn't return all messages")
	}

	// Get nothing
	bm = buffer.getCacheMessages(100)
	if bm != nil {
		t.Error("should not have returned anything")
	}

	// Test single
	bm = buffer.getCacheMessages(46)
	if bm == nil {
		t.Fatal("nothing returned")
	}
	if len(bm.Messages) != 1 {
		t.Errorf("expected 1 message, got %d messages", len(bm.Messages))
	}
	if bm.Messages[0].SequenceNumber != 46 {
		t.Errorf("expected sequence number 46, got %d", bm.Messages[0].SequenceNumber)
	}

	// Test extremes
	bm = buffer.getCacheMessages(arbutil.MessageIndex(^uint64(0)))
	if bm != nil {
		t.Fatal("should not have returned anything")
	}
}

func TestDeleteConfirmedNil(t *testing.T) {
	buffer := SequenceNumberCatchupBuffer{
		messages:     nil,
		messageCount: 0,
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	buffer.deleteConfirmed(0)
	if len(buffer.messages) != 0 {
		t.Error("nothing should be present")
	}
}

func TestDeleteConfirmInvalidOrder(t *testing.T) {
	indexes := []arbutil.MessageIndex{40, 42}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessages(indexes),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	// Confirm before cache
	buffer.deleteConfirmed(41)
	if len(buffer.messages) != 0 {
		t.Error("cache not in contiguous order should have caused everything to be deleted")
	}
}

func TestDeleteConfirmed(t *testing.T) {
	indexes := []arbutil.MessageIndex{40, 41, 42, 43, 44, 45, 46}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessages(indexes),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	// Confirm older than cache
	buffer.deleteConfirmed(39)
	if len(buffer.messages) != 7 {
		t.Error("nothing should have been deleted")
	}

}
func TestDeleteFreeMem(t *testing.T) {
	indexes := []arbutil.MessageIndex{40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessagesImpl(indexes, len(indexes)*10+1),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	// Confirm older than cache
	buffer.deleteConfirmed(40)
	if cap(buffer.messages) > 20 {
		t.Error("extra memory was not freed, cap: ", cap(buffer.messages))
	}

}

func TestBroadcastBadMessage(t *testing.T) {
	buffer := SequenceNumberCatchupBuffer{
		messages:     nil,
		messageCount: 0,
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	var foo int
	err := buffer.OnDoBroadcast(foo)
	if err == nil {
		t.Error("expected error")
	}
	if !strings.Contains(err.Error(), "unknown type") {
		t.Error("unexpected type")
	}
}

func TestBroadcastPastSeqNum(t *testing.T) {
	indexes := []arbutil.MessageIndex{40}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessagesImpl(indexes, len(indexes)*10+1),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	bm := BroadcastMessage{
		Messages: []*BroadcastFeedMessage{
			{
				SequenceNumber: 39,
			},
		},
	}
	err := buffer.OnDoBroadcast(bm)
	if err != nil {
		t.Error("expected error")
	}

}

func TestBroadcastFutureSeqNum(t *testing.T) {
	indexes := []arbutil.MessageIndex{40}
	buffer := SequenceNumberCatchupBuffer{
		messages:     createDummyBroadcastMessagesImpl(indexes, len(indexes)*10+1),
		messageCount: int32(len(indexes)),
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return -1 },
	}

	bm := BroadcastMessage{
		Messages: []*BroadcastFeedMessage{
			{
				SequenceNumber: 42,
			},
		},
	}
	err := buffer.OnDoBroadcast(bm)
	if err != nil {
		t.Error("expected error")
	}

}

func TestMaxCatchupBufferSize(t *testing.T) {
	limit := 5
	buffer := SequenceNumberCatchupBuffer{
		messages:     nil,
		messageCount: 0,
		limitCatchup: func() bool { return false },
		maxCatchup:   func() int { return limit },
	}

	firstMessage := 10
	for i := firstMessage; i <= 20; i += 2 {
		bm := BroadcastMessage{
			Messages: []*BroadcastFeedMessage{
				{
					SequenceNumber: arbutil.MessageIndex(i),
				},
				{
					SequenceNumber: arbutil.MessageIndex(i + 1),
				},
			},
		}
		err := buffer.OnDoBroadcast(bm)
		Require(t, err)
		haveMessages := buffer.getCacheMessages(0)
		expectedCount := arbmath.MinInt(i+len(bm.Messages)-firstMessage, limit)
		if len(haveMessages.Messages) != expectedCount {
			t.Errorf("after broadcasting messages %v and %v, expected to have %v messages but got %v", i, i+1, expectedCount, len(haveMessages.Messages))
		}
		expectedFirstMessage := arbutil.MessageIndex(arbmath.MaxInt(firstMessage, i+len(bm.Messages)-limit))
		if haveMessages.Messages[0].SequenceNumber != expectedFirstMessage {
			t.Errorf("after broadcasting messages %v and %v, expected the first message to be %v but got %v", i, i+1, expectedFirstMessage, haveMessages.Messages[0].SequenceNumber)
		}
	}
}

'''
'''--- cmd/chaininfo/arbitrum_chain_info.json ---
[
  {
    "chain-name": "arb1",
    "parent-chain-id": 1,
    "parent-chain-is-arbitrum": false,
    "sequencer-url": "https://arb1-sequencer.arbitrum.io/rpc",
    "feed-url": "wss://arb1-feed.arbitrum.io/feed",
    "has-genesis-state": true,
    "chain-config":
    {
      "chainId": 42161,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": false,
        "DataAvailabilityCommittee": false,
        "InitialArbOSVersion": 6,
        "InitialChainOwner": "0xd345e41ae2cb00311956aa7109fc801ae8c81a52",
        "GenesisBlockNum": 0
      }
    },
    "rollup":
    {
      "bridge": "0x8315177ab297ba92a06054ce80a67ed4dbd7ed3a",
      "inbox": "0x4dbd4fc535ac27206064b68ffcf827b0a60bab3f",
      "rollup": "0x5ef0d09d1e6204141b4d37530808ed19f60fba35",
      "sequencer-inbox": "0x1c479675ad559dc151f6ec7ed3fbf8cee79582b6",
      "validator-utils": "0x9e40625f52829cf04bc4839f186d621ee33b0e67",
      "validator-wallet-creator": "0x960953f7c69cd2bc2322db9223a815c680ccc7ea",
      "deployed-at": 15411056
    }
  },
  {
    "chain-name": "nova",
    "parent-chain-id": 1,
    "parent-chain-is-arbitrum": false,
    "sequencer-url": "https://nova.arbitrum.io/rpc",
    "feed-url": "wss://nova-feed.arbitrum.io/feed",
    "das-index-url": "https://nova.arbitrum.io/das-servers",
    "chain-config":
    {
      "chainId": 42170,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": false,
        "DataAvailabilityCommittee": true,
        "InitialArbOSVersion": 1,
        "InitialChainOwner": "0x9c040726f2a657226ed95712245dee84b650a1b5",
        "GenesisBlockNum": 0
      }
    },
    "rollup":
    {
      "bridge": "0xc1ebd02f738644983b6c4b2d440b8e77dde276bd",
      "inbox": "0xc4448b71118c9071bcb9734a0eac55d18a153949",
      "rollup": "0xfb209827c58283535b744575e11953dcc4bead88",
      "sequencer-inbox": "0x211e1c4c7f1bf5351ac850ed10fd68cffcf6c21b",
      "validator-utils": "0x2B081fbaB646D9013f2699BebEf62B7e7d7F0976",
      "validator-wallet-creator": "0xe05465Aab36ba1277dAE36aa27a7B74830e74DE4",
      "deployed-at": 15016829
    }
  },
  {
    "chain-name": "goerli-rollup",
    "parent-chain-id": 5,
    "parent-chain-is-arbitrum": false,
    "sequencer-url": "https://goerli-rollup.arbitrum.io/rpc",
    "feed-url": "wss://goerli-rollup.arbitrum.io/feed",
    "chain-config":
    {
      "chainId": 421613,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": false,
        "DataAvailabilityCommittee": false,
        "InitialArbOSVersion": 2,
        "InitialChainOwner": "0x186b56023d42b2b4e7616589a5c62eef5fca21dd",
        "GenesisBlockNum": 0
      }
    },
    "rollup":
    {
      "bridge": "0xaf4159a80b6cc41ed517db1c453d1ef5c2e4db72",
      "inbox": "0x6bebc4925716945d46f0ec336d5c2564f419682c",
      "rollup": "0x45e5caea8768f42b385a366d3551ad1e0cbfab17",
      "sequencer-inbox": "0x0484a87b144745a2e5b7c359552119b6ea2917a9",
      "validator-utils": "0x344f651fe566a02db939c8657427deb5524ea78e",
      "validator-wallet-creator": "0x53eb4f4524b3b9646d41743054230d3f425397b3",
      "deployed-at": 7217526
    }
  },
  {
    "chain-name": "arb-dev-test",
    "chain-config":
    {
      "chainId": 412346,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": true,
        "DataAvailabilityCommittee": false,
        "InitialArbOSVersion": 11,
        "InitialChainOwner": "0x0000000000000000000000000000000000000000",
        "GenesisBlockNum": 0
      }
    }
  },
  {
    "chain-name": "anytrust-dev-test",
    "chain-config":
    {
      "chainId": 412347,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": true,
        "DataAvailabilityCommittee": true,
        "InitialArbOSVersion": 11,
        "InitialChainOwner": "0x0000000000000000000000000000000000000000",
        "GenesisBlockNum": 0
      }
    }
  },
  {
    "chain-id": 421614,
    "parent-chain-id": 11155111,
    "parent-chain-is-arbitrum": false,
    "chain-name": "sepolia-rollup",
    "sequencer-url": "https://sepolia-rollup-sequencer.arbitrum.io/rpc",
    "feed-url": "wss://sepolia-rollup.arbitrum.io/feed",
    "chain-config":
    {
      "chainId": 421614,
      "homesteadBlock": 0,
      "daoForkBlock": null,
      "daoForkSupport": true,
      "eip150Block": 0,
      "eip150Hash": "0x0000000000000000000000000000000000000000000000000000000000000000",
      "eip155Block": 0,
      "eip158Block": 0,
      "byzantiumBlock": 0,
      "constantinopleBlock": 0,
      "petersburgBlock": 0,
      "istanbulBlock": 0,
      "muirGlacierBlock": 0,
      "berlinBlock": 0,
      "londonBlock": 0,
      "clique":
      {
        "period": 0,
        "epoch": 0
      },
      "arbitrum":
      {
        "EnableArbOS": true,
        "AllowDebugPrecompiles": false,
        "DataAvailabilityCommittee": false,
        "InitialArbOSVersion": 10,
        "InitialChainOwner": "0x71B61c2E250AFa05dFc36304D6c91501bE0965D8",
        "GenesisBlockNum": 0
      }
    },
    "rollup":
    {
      "bridge": "0x38f918D0E9F1b721EDaA41302E399fa1B79333a9",
      "inbox": "0xaAe29B0366299461418F5324a79Afc425BE5ae21",
      "sequencer-inbox": "0x6c97864CE4bEf387dE0b3310A44230f7E3F1be0D",
      "rollup": "0xd80810638dbDF9081b72C1B33c65375e807281C8",
      "validator-utils": "0x1f6860C3cac255fFFa72B7410b1183c3a0D261e0",
      "validator-wallet-creator": "0x894fC71fA0A666352824EC954B401573C861D664",
      "deployed-at": 4139226
    }
  }
]

'''
'''--- cmd/chaininfo/chain_info.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package chaininfo

import (
	_ "embed"
	"encoding/json"
	"fmt"
	"math/big"
	"os"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
)

//go:embed arbitrum_chain_info.json
var DefaultChainInfo []byte

type ChainInfo struct {
	ChainName             string `json:"chain-name"`
	ParentChainId         uint64 `json:"parent-chain-id"`
	ParentChainIsArbitrum *bool  `json:"parent-chain-is-arbitrum"`
	// This is the forwarding target to submit transactions to, called the sequencer URL for clarity
	SequencerUrl    string              `json:"sequencer-url"`
	FeedUrl         string              `json:"feed-url"`
	DasIndexUrl     string              `json:"das-index-url"`
	HasGenesisState bool                `json:"has-genesis-state"`
	ChainConfig     *params.ChainConfig `json:"chain-config"`
	RollupAddresses *RollupAddresses    `json:"rollup"`
}

func GetChainConfig(chainId *big.Int, chainName string, genesisBlockNum uint64, l2ChainInfoFiles []string, l2ChainInfoJson string) (*params.ChainConfig, error) {
	chainInfo, err := ProcessChainInfo(chainId.Uint64(), chainName, l2ChainInfoFiles, l2ChainInfoJson)
	if err != nil {
		return nil, err
	}
	if chainInfo.ChainConfig != nil {
		chainInfo.ChainConfig.ArbitrumChainParams.GenesisBlockNum = genesisBlockNum
		return chainInfo.ChainConfig, nil
	}
	if chainId.Uint64() != 0 {
		return nil, fmt.Errorf("missing chain config for L2 chain ID %v", chainId)
	}
	return nil, fmt.Errorf("missing chain config for L2 chain name %v", chainName)
}

func GetRollupAddressesConfig(chainId uint64, chainName string, l2ChainInfoFiles []string, l2ChainInfoJson string) (RollupAddresses, error) {
	chainInfo, err := ProcessChainInfo(chainId, chainName, l2ChainInfoFiles, l2ChainInfoJson)
	if err != nil {
		return RollupAddresses{}, err
	}
	if chainInfo.RollupAddresses != nil {
		return *chainInfo.RollupAddresses, nil
	}
	if chainId != 0 {
		return RollupAddresses{}, fmt.Errorf("missing rollup addresses for L2 chain ID %v", chainId)
	}
	return RollupAddresses{}, fmt.Errorf("missing rollup addresses for L2 chain name %v", chainName)
}

func ProcessChainInfo(chainId uint64, chainName string, l2ChainInfoFiles []string, l2ChainInfoJson string) (*ChainInfo, error) {
	if l2ChainInfoJson != "" {
		chainInfo, err := findChainInfo(chainId, chainName, []byte(l2ChainInfoJson))
		if err != nil || chainInfo != nil {
			return chainInfo, err
		}
	}
	for _, l2ChainInfoFile := range l2ChainInfoFiles {
		chainsInfoBytes, err := os.ReadFile(l2ChainInfoFile)
		if err != nil {
			return nil, fmt.Errorf("failed to read file %s err %w", l2ChainInfoFile, err)
		}
		chainInfo, err := findChainInfo(chainId, chainName, chainsInfoBytes)
		if err != nil || chainInfo != nil {
			return chainInfo, err
		}
	}

	chainInfo, err := findChainInfo(chainId, chainName, DefaultChainInfo)
	if err != nil || chainInfo != nil {
		return chainInfo, err
	}
	if chainId != 0 {
		return nil, fmt.Errorf("unsupported chain ID %v", chainId)
	}
	return nil, fmt.Errorf("unsupported chain name %v", chainName)
}

func findChainInfo(chainId uint64, chainName string, chainsInfoBytes []byte) (*ChainInfo, error) {
	var chainsInfo []ChainInfo
	err := json.Unmarshal(chainsInfoBytes, &chainsInfo)
	if err != nil {
		return nil, err
	}
	for _, chainInfo := range chainsInfo {
		if (chainId == 0 || chainInfo.ChainConfig.ChainID.Uint64() == chainId) && (chainName == "" || chainInfo.ChainName == chainName) {
			return &chainInfo, nil
		}
	}
	return nil, nil
}

type RollupAddresses struct {
	Bridge                 common.Address `json:"bridge"`
	Inbox                  common.Address `json:"inbox"`
	SequencerInbox         common.Address `json:"sequencer-inbox"`
	Rollup                 common.Address `json:"rollup"`
	NativeToken            common.Address `json:"native-token"`
	UpgradeExecutor        common.Address `json:"upgrade-executor"`
	ValidatorUtils         common.Address `json:"validator-utils"`
	ValidatorWalletCreator common.Address `json:"validator-wallet-creator"`
	DeployedAt             uint64         `json:"deployed-at"`
}

'''
'''--- cmd/conf/chain.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package conf

import (
	"time"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/rpcclient"
	flag "github.com/spf13/pflag"
)

type L1Config struct {
	ID         uint64                   `koanf:"id"`
	Connection rpcclient.ClientConfig   `koanf:"connection" reload:"hot"`
	Wallet     genericconf.WalletConfig `koanf:"wallet"`
}

var L1ConnectionConfigDefault = rpcclient.ClientConfig{
	URL:            "",
	Retries:        2,
	Timeout:        time.Minute,
	ConnectionWait: time.Minute,
}

var L1ConfigDefault = L1Config{
	ID:         0,
	Connection: L1ConnectionConfigDefault,
	Wallet:     DefaultL1WalletConfig,
}

var DefaultL1WalletConfig = genericconf.WalletConfig{
	Pathname:      "wallet",
	Password:      genericconf.WalletConfigDefault.Password,
	PrivateKey:    genericconf.WalletConfigDefault.PrivateKey,
	Account:       genericconf.WalletConfigDefault.Account,
	OnlyCreateKey: genericconf.WalletConfigDefault.OnlyCreateKey,
}

func L1ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".id", L1ConfigDefault.ID, "if set other than 0, will be used to validate database and L1 connection")
	rpcclient.RPCClientAddOptions(prefix+".connection", f, &L1ConfigDefault.Connection)
	genericconf.WalletConfigAddOptions(prefix+".wallet", f, L1ConfigDefault.Wallet.Pathname)
}

func (c *L1Config) ResolveDirectoryNames(chain string) {
	c.Wallet.ResolveDirectoryNames(chain)
}

func (c *L1Config) Validate() error {
	return c.Connection.Validate()
}

type L2Config struct {
	ID                   uint64                   `koanf:"id"`
	Name                 string                   `koanf:"name"`
	InfoFiles            []string                 `koanf:"info-files"`
	InfoJson             string                   `koanf:"info-json"`
	DevWallet            genericconf.WalletConfig `koanf:"dev-wallet"`
	InfoIpfsUrl          string                   `koanf:"info-ipfs-url"`
	InfoIpfsDownloadPath string                   `koanf:"info-ipfs-download-path"`
}

var L2ConfigDefault = L2Config{
	ID:                   0,
	Name:                 "",
	InfoFiles:            []string{}, // Default file used is chaininfo/arbitrum_chain_info.json, stored in DefaultChainInfo in chain_info.go
	InfoJson:             "",
	DevWallet:            genericconf.WalletConfigDefault,
	InfoIpfsUrl:          "",
	InfoIpfsDownloadPath: "/tmp/",
}

func L2ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".id", L2ConfigDefault.ID, "L2 chain ID (determines Arbitrum network)")
	f.String(prefix+".name", L2ConfigDefault.Name, "L2 chain name (determines Arbitrum network)")
	f.StringSlice(prefix+".info-files", L2ConfigDefault.InfoFiles, "L2 chain info json files")
	f.String(prefix+".info-json", L2ConfigDefault.InfoJson, "L2 chain info in json string format")

	// Dev wallet does not exist unless specified
	genericconf.WalletConfigAddOptions(prefix+".dev-wallet", f, "")
	f.String(prefix+".info-ipfs-url", L2ConfigDefault.InfoIpfsUrl, "url to download chain info file")
	f.String(prefix+".info-ipfs-download-path", L2ConfigDefault.InfoIpfsDownloadPath, "path to save temp downloaded file")

}

func (c *L2Config) ResolveDirectoryNames(chain string) {
	c.DevWallet.ResolveDirectoryNames(chain)
}

'''
'''--- cmd/conf/database.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package conf

import (
	"fmt"
	"os"
	"path"
	"path/filepath"

	flag "github.com/spf13/pflag"
)

type PersistentConfig struct {
	GlobalConfig string `koanf:"global-config"`
	Chain        string `koanf:"chain"`
	LogDir       string `koanf:"log-dir"`
	Handles      int    `koanf:"handles"`
	Ancient      string `koanf:"ancient"`
	DBEngine     string `koanf:"db-engine"`
}

var PersistentConfigDefault = PersistentConfig{
	GlobalConfig: ".arbitrum",
	Chain:        "",
	LogDir:       "",
	Handles:      512,
	Ancient:      "",
	DBEngine:     "leveldb",
}

func PersistentConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".global-config", PersistentConfigDefault.GlobalConfig, "directory to store global config")
	f.String(prefix+".chain", PersistentConfigDefault.Chain, "directory to store chain state")
	f.String(prefix+".log-dir", PersistentConfigDefault.LogDir, "directory to store log file")
	f.Int(prefix+".handles", PersistentConfigDefault.Handles, "number of file descriptor handles to use for the database")
	f.String(prefix+".ancient", PersistentConfigDefault.Ancient, "directory of ancient where the chain freezer can be opened")
	f.String(prefix+".db-engine", PersistentConfigDefault.DBEngine, "backing database implementation to use ('leveldb' or 'pebble')")
}

func (c *PersistentConfig) ResolveDirectoryNames() error {
	homeDir, err := os.UserHomeDir()
	if err != nil {
		return fmt.Errorf("unable to read users home directory: %w", err)
	}

	// Make persistent storage directory relative to home directory if not already absolute
	if !filepath.IsAbs(c.GlobalConfig) {
		c.GlobalConfig = path.Join(homeDir, c.GlobalConfig)
	}
	err = os.MkdirAll(c.GlobalConfig, os.ModePerm)
	if err != nil {
		return fmt.Errorf("unable to create global configuration directory: %w", err)
	}

	// Make chain directory relative to persistent storage directory if not already absolute
	if !filepath.IsAbs(c.Chain) {
		c.Chain = path.Join(c.GlobalConfig, c.Chain)
	}
	err = os.MkdirAll(c.Chain, os.ModePerm)
	if err != nil {
		return fmt.Errorf("unable to create chain directory: %w", err)
	}
	if DatabaseInDirectory(c.Chain) {
		return fmt.Errorf("database in --persistent.chain (%s) directory, try specifying parent directory", c.Chain)
	}

	// Make Log directory relative to persistent storage directory if not already absolute
	if !filepath.IsAbs(c.LogDir) {
		c.LogDir = path.Join(c.Chain, c.LogDir)
	}
	if c.LogDir != c.Chain {
		err = os.MkdirAll(c.LogDir, os.ModePerm)
		if err != nil {
			return fmt.Errorf("unable to create Log directory: %w", err)
		}
		if DatabaseInDirectory(c.LogDir) {
			return fmt.Errorf("database in --persistent.log-dir (%s) directory, try specifying parent directory", c.LogDir)
		}
	}
	return nil
}

func DatabaseInDirectory(path string) bool {
	// Consider database present if file `CURRENT` in directory
	_, err := os.Stat(path + "/CURRENT")

	return err == nil
}

func (c *PersistentConfig) Validate() error {
	// we are validating .db-engine here to avoid unintended behaviour as empty string value also has meaning in geth's node.Config.DBEngine
	if c.DBEngine != "leveldb" && c.DBEngine != "pebble" {
		return fmt.Errorf(`invalid .db-engine choice: %q, allowed "leveldb" or "pebble"`, c.DBEngine)
	}
	return nil
}

'''
'''--- cmd/daserver/daserver.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"

	koanfjson "github.com/knadh/koanf/parsers/json"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/metrics/exp"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/headerreader"
)

type DAServerConfig struct {
	EnableRPC         bool                                `koanf:"enable-rpc"`
	RPCAddr           string                              `koanf:"rpc-addr"`
	RPCPort           uint64                              `koanf:"rpc-port"`
	RPCServerTimeouts genericconf.HTTPServerTimeoutConfig `koanf:"rpc-server-timeouts"`

	EnableREST         bool                                `koanf:"enable-rest"`
	RESTAddr           string                              `koanf:"rest-addr"`
	RESTPort           uint64                              `koanf:"rest-port"`
	RESTServerTimeouts genericconf.HTTPServerTimeoutConfig `koanf:"rest-server-timeouts"`

	DataAvailability das.DataAvailabilityConfig `koanf:"data-availability"`

	Conf     genericconf.ConfConfig `koanf:"conf"`
	LogLevel int                    `koanf:"log-level"`
	LogType  string                 `koanf:"log-type"`

	Metrics       bool                            `koanf:"metrics"`
	MetricsServer genericconf.MetricsServerConfig `koanf:"metrics-server"`
	PProf         bool                            `koanf:"pprof"`
	PprofCfg      genericconf.PProf               `koanf:"pprof-cfg"`
}

var DefaultDAServerConfig = DAServerConfig{
	EnableRPC:          false,
	RPCAddr:            "localhost",
	RPCPort:            9876,
	RPCServerTimeouts:  genericconf.HTTPServerTimeoutConfigDefault,
	EnableREST:         false,
	RESTAddr:           "localhost",
	RESTPort:           9877,
	RESTServerTimeouts: genericconf.HTTPServerTimeoutConfigDefault,
	DataAvailability:   das.DefaultDataAvailabilityConfig,
	Conf:               genericconf.ConfConfigDefault,
	LogLevel:           int(log.LvlInfo),
	LogType:            "plaintext",
	Metrics:            false,
	MetricsServer:      genericconf.MetricsServerConfigDefault,
	PProf:              false,
	PprofCfg:           genericconf.PProfDefault,
}

func main() {
	if err := startup(); err != nil {
		log.Error("Error running DAServer", "err", err)
	}
}

func printSampleUsage(progname string) {
	fmt.Printf("\n")
	fmt.Printf("Sample usage:                  %s --help \n", progname)
}

func parseDAServer(args []string) (*DAServerConfig, error) {
	f := flag.NewFlagSet("daserver", flag.ContinueOnError)
	f.Bool("enable-rpc", DefaultDAServerConfig.EnableRPC, "enable the HTTP-RPC server listening on rpc-addr and rpc-port")
	f.String("rpc-addr", DefaultDAServerConfig.RPCAddr, "HTTP-RPC server listening interface")
	f.Uint64("rpc-port", DefaultDAServerConfig.RPCPort, "HTTP-RPC server listening port")
	genericconf.HTTPServerTimeoutConfigAddOptions("rpc-server-timeouts", f)

	f.Bool("enable-rest", DefaultDAServerConfig.EnableREST, "enable the REST server listening on rest-addr and rest-port")
	f.String("rest-addr", DefaultDAServerConfig.RESTAddr, "REST server listening interface")
	f.Uint64("rest-port", DefaultDAServerConfig.RESTPort, "REST server listening port")
	genericconf.HTTPServerTimeoutConfigAddOptions("rest-server-timeouts", f)

	f.Bool("metrics", DefaultDAServerConfig.Metrics, "enable metrics")
	genericconf.MetricsServerAddOptions("metrics-server", f)

	f.Bool("pprof", DefaultDAServerConfig.PProf, "enable pprof")
	genericconf.PProfAddOptions("pprof-cfg", f)

	f.Int("log-level", int(log.LvlInfo), "log level; 1: ERROR, 2: WARN, 3: INFO, 4: DEBUG, 5: TRACE")
	f.String("log-type", DefaultDAServerConfig.LogType, "log type (plaintext or json)")

	das.DataAvailabilityConfigAddDaserverOptions("data-availability", f)
	genericconf.ConfConfigAddOptions("conf", f)

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var serverConfig DAServerConfig
	if err := confighelpers.EndCommonParse(k, &serverConfig); err != nil {
		return nil, err
	}
	if serverConfig.Conf.Dump {
		err = confighelpers.DumpConfig(k, map[string]interface{}{
			"data-availability.key.priv-key": "",
		})
		if err != nil {
			return nil, fmt.Errorf("error removing extra parameters before dump: %w", err)
		}

		c, err := k.Marshal(koanfjson.Parser())
		if err != nil {
			return nil, fmt.Errorf("unable to marshal config file to JSON: %w", err)
		}

		fmt.Println(string(c))
		os.Exit(0)
	}

	return &serverConfig, nil
}

type L1ReaderCloser struct {
	l1Reader *headerreader.HeaderReader
}

func (c *L1ReaderCloser) Close(_ context.Context) error {
	c.l1Reader.StopOnly()
	return nil
}

func (c *L1ReaderCloser) String() string {
	return "l1 reader closer"
}

// Checks metrics and PProf flag, runs them if enabled.
// Note: they are separate so one can enable/disable them as they wish, the only
// requirement is that they can't run on the same address and port.
func startMetrics(cfg *DAServerConfig) error {
	mAddr := fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port)
	pAddr := fmt.Sprintf("%v:%v", cfg.PprofCfg.Addr, cfg.PprofCfg.Port)
	if cfg.Metrics && !metrics.Enabled {
		return fmt.Errorf("metrics must be enabled via command line by adding --metrics, json config has no effect")
	}
	if cfg.Metrics && cfg.PProf && mAddr == pAddr {
		return fmt.Errorf("metrics and pprof cannot be enabled on the same address:port: %s", mAddr)
	}
	if cfg.Metrics {
		go metrics.CollectProcessMetrics(cfg.MetricsServer.UpdateInterval)
		exp.Setup(fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port))
	}
	if cfg.PProf {
		genericconf.StartPprof(pAddr)
	}
	return nil
}

func startup() error {
	// Some different defaults to DAS config in a node.
	das.DefaultDataAvailabilityConfig.Enable = true

	serverConfig, err := parseDAServer(os.Args[1:])
	if err != nil {
		confighelpers.PrintErrorAndExit(err, printSampleUsage)
	}
	if !(serverConfig.EnableRPC || serverConfig.EnableREST) {
		confighelpers.PrintErrorAndExit(errors.New("please specify at least one of --enable-rest or --enable-rpc"), printSampleUsage)
	}

	logFormat, err := genericconf.ParseLogType(serverConfig.LogType)
	if err != nil {
		flag.Usage()
		panic(fmt.Sprintf("Error parsing log type: %v", err))
	}
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, logFormat))
	glogger.Verbosity(log.Lvl(serverConfig.LogLevel))
	log.Root().SetHandler(glogger)

	if err := startMetrics(serverConfig); err != nil {
		return err
	}

	sigint := make(chan os.Signal, 1)
	signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	var l1Reader *headerreader.HeaderReader
	if serverConfig.DataAvailability.ParentChainNodeURL != "" && serverConfig.DataAvailability.ParentChainNodeURL != "none" {
		l1Client, err := das.GetL1Client(ctx, serverConfig.DataAvailability.ParentChainConnectionAttempts, serverConfig.DataAvailability.ParentChainNodeURL)
		if err != nil {
			return err
		}
		arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1Client)
		l1Reader, err = headerreader.New(ctx, l1Client, func() *headerreader.Config { return &headerreader.DefaultConfig }, arbSys) // TODO: config
		if err != nil {
			return err
		}
	}

	var seqInboxAddress *common.Address
	if serverConfig.DataAvailability.SequencerInboxAddress == "none" {
		seqInboxAddress = nil
	} else if len(serverConfig.DataAvailability.SequencerInboxAddress) > 0 {
		seqInboxAddress, err = das.OptionalAddressFromString(serverConfig.DataAvailability.SequencerInboxAddress)
		if err != nil {
			return err
		}
		if seqInboxAddress == nil {
			return errors.New("must provide data-availability.sequencer-inbox-address set to a valid contract address or 'none'")
		}
	} else {
		return errors.New("sequencer-inbox-address must be set to a valid L1 URL and contract address, or 'none'")
	}

	daReader, daWriter, daHealthChecker, dasLifecycleManager, err := das.CreateDAComponentsForDaserver(ctx, &serverConfig.DataAvailability, l1Reader, seqInboxAddress)
	if err != nil {
		return err
	}

	if l1Reader != nil {
		l1Reader.Start(ctx)
		dasLifecycleManager.Register(&L1ReaderCloser{l1Reader})
	}

	vcsRevision, _, vcsTime := confighelpers.GetVersion()
	var rpcServer *http.Server
	if serverConfig.EnableRPC {
		log.Info("Starting HTTP-RPC server", "addr", serverConfig.RPCAddr, "port", serverConfig.RPCPort, "revision", vcsRevision, "vcs.time", vcsTime)

		rpcServer, err = das.StartDASRPCServer(ctx, serverConfig.RPCAddr, serverConfig.RPCPort, serverConfig.RPCServerTimeouts, daReader, daWriter, daHealthChecker)
		if err != nil {
			return err
		}
	}

	var restServer *das.RestfulDasServer
	if serverConfig.EnableREST {
		log.Info("Starting REST server", "addr", serverConfig.RESTAddr, "port", serverConfig.RESTPort, "revision", vcsRevision, "vcs.time", vcsTime)

		restServer, err = das.NewRestfulDasServer(serverConfig.RESTAddr, serverConfig.RESTPort, serverConfig.RESTServerTimeouts, daReader, daHealthChecker)
		if err != nil {
			return err
		}
	}

	<-sigint
	dasLifecycleManager.StopAndWaitUntil(2 * time.Second)

	var err1, err2 error
	if rpcServer != nil {
		err1 = rpcServer.Shutdown(ctx)
	}

	if restServer != nil {
		err2 = restServer.Shutdown()
	}

	if err1 != nil {
		return err1
	}
	return err2
}

'''
'''--- cmd/dataavailability/data_availability_check.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"bytes"
	"context"
	"fmt"
	"math/big"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/metricsutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"

	flag "github.com/spf13/pflag"
)

// Data availability check is done to as to make sure that the data that is being stored by DAS is available at all time.
// This done by taking the latest stored hash and an old stored hash (12 days) and it is checked if these two hashes are
// present across all the DAS provided in the list, if a DAS does not have these hashes an error is thrown.
// This approach does not guarantee 100% data availability, but it's an efficient and easy heuristic for our use case.
//
// This can be used in following manner (not an exhaustive list)
// 1. Continuously call the function by exposing a REST API and create alert if error is returned.
// 2. Call the function in an adhoc manner to check if the provided DAS is live and functioning properly.

const metricBaseOldHash = "arb/das/dataavailability/oldhash/"
const metricBaseNewHash = "arb/das/dataavailability/oldhash/"

type DataAvailabilityCheckConfig struct {
	OnlineUrlList         string        `koanf:"online-url-list"`
	L1NodeURL             string        `koanf:"l1-node-url"`
	L1ConnectionAttempts  int           `koanf:"l1-connection-attempts"`
	SequencerInboxAddress string        `koanf:"sequencer-inbox-address"`
	L1BlocksPerRead       uint64        `koanf:"l1-blocks-per-read"`
	CheckInterval         time.Duration `koanf:"check-interval"`
}

var DefaultDataAvailabilityCheckConfig = DataAvailabilityCheckConfig{
	OnlineUrlList:        "",
	L1ConnectionAttempts: 15,
	L1BlocksPerRead:      100,
	CheckInterval:        5 * time.Minute,
}

type DataAvailabilityCheck struct {
	stopwaiter.StopWaiter
	l1Client       *ethclient.Client
	config         *DataAvailabilityCheckConfig
	inboxAddr      *common.Address
	inboxContract  *bridgegen.SequencerInbox
	urlToReaderMap map[string]arbstate.DataAvailabilityReader
	checkInterval  time.Duration
}

func newDataAvailabilityCheck(ctx context.Context, dataAvailabilityCheckConfig *DataAvailabilityCheckConfig) (*DataAvailabilityCheck, error) {
	l1Client, err := das.GetL1Client(ctx, dataAvailabilityCheckConfig.L1ConnectionAttempts, dataAvailabilityCheckConfig.L1NodeURL)
	if err != nil {
		return nil, err
	}
	seqInboxAddress, err := das.OptionalAddressFromString(dataAvailabilityCheckConfig.SequencerInboxAddress)
	if err != nil {
		return nil, err
	}
	inboxContract, err := bridgegen.NewSequencerInbox(*seqInboxAddress, l1Client)
	if err != nil {
		return nil, err
	}
	onlineUrls, err := das.RestfulServerURLsFromList(ctx, dataAvailabilityCheckConfig.OnlineUrlList)
	if err != nil {
		return nil, err
	}
	urlToReaderMap := make(map[string]arbstate.DataAvailabilityReader, len(onlineUrls))
	for _, url := range onlineUrls {
		reader, err := das.NewRestfulDasClientFromURL(url)
		if err != nil {
			return nil, err
		}
		urlToReaderMap[url] = reader
	}
	return &DataAvailabilityCheck{
		l1Client:       l1Client,
		config:         dataAvailabilityCheckConfig,
		inboxAddr:      seqInboxAddress,
		inboxContract:  inboxContract,
		urlToReaderMap: urlToReaderMap,
		checkInterval:  dataAvailabilityCheckConfig.CheckInterval,
	}, nil
}

func parseDataAvailabilityCheckConfig(args []string) (*DataAvailabilityCheckConfig, error) {
	f := flag.NewFlagSet("dataavailabilitycheck", flag.ContinueOnError)
	f.String("online-url-list", DefaultDataAvailabilityCheckConfig.OnlineUrlList, "a URL to a list of URLs of REST das endpoints that is checked for data availability")
	f.String("l1-node-url", DefaultDataAvailabilityCheckConfig.L1NodeURL, "URL for L1 node")
	f.Int("l1-connection-attempts", DefaultDataAvailabilityCheckConfig.L1ConnectionAttempts, "layer 1 RPC connection attempts (spaced out at least 1 second per attempt, 0 to retry infinitely)")
	f.String("sequencer-inbox-address", DefaultDataAvailabilityCheckConfig.SequencerInboxAddress, "L1 address of SequencerInbox contract")
	f.Uint64("l1-blocks-per-read", DefaultDataAvailabilityCheckConfig.L1BlocksPerRead, "max l1 blocks to read per poll")
	f.Duration("check-interval", DefaultDataAvailabilityCheckConfig.CheckInterval, "interval for running data availability check")
	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var config DataAvailabilityCheckConfig
	if err := confighelpers.EndCommonParse(k, &config); err != nil {
		return nil, err
	}
	return &config, nil
}

func main() {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()
	dataAvailabilityCheckConfig, err := parseDataAvailabilityCheckConfig(os.Args[1:])
	if err != nil {
		panic(err)
	}
	dataAvailabilityCheck, err := newDataAvailabilityCheck(ctx, dataAvailabilityCheckConfig)
	if err != nil {
		panic(err)
	}
	dataAvailabilityCheck.StopWaiter.Start(ctx, dataAvailabilityCheck)
	dataAvailabilityCheck.CallIteratively(dataAvailabilityCheck.start)

	sigint := make(chan os.Signal, 1)
	signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)

	<-sigint
	signal.Stop(sigint)
	close(sigint)
	dataAvailabilityCheck.StopAndWait()
}

func (d *DataAvailabilityCheck) start(ctx context.Context) time.Duration {
	latestHeader, err := d.l1Client.HeaderByNumber(ctx, big.NewInt(rpc.FinalizedBlockNumber.Int64()))
	if err != nil {
		log.Error(err.Error())
		return d.checkInterval
	}
	latestBlockNumber := latestHeader.Number.Uint64()
	oldBlockNumber := latestBlockNumber - 86400 // 12 days old block number

	log.Info("Running new hash data availability check")
	newHashErr := d.checkDataAvailabilityForNewHashInBlockRange(ctx, latestBlockNumber, oldBlockNumber)
	log.Info("Completed new hash data availability check")

	log.Info("Running old hash data availability check")
	oldHashErr := d.checkDataAvailabilityForOldHashInBlockRange(ctx, oldBlockNumber, latestBlockNumber)
	log.Info("Completed old hash data availability check")

	if newHashErr != nil || oldHashErr != nil {
		log.Error(fmt.Sprintf("new hash check: %s, old hash check: %s", newHashErr, oldHashErr))
	}
	return d.checkInterval
}

func (d *DataAvailabilityCheck) checkDataAvailabilityForNewHashInBlockRange(ctx context.Context, latestBlock uint64, oldBlock uint64) error {
	currentBlock := latestBlock
	for currentBlock-d.config.L1BlocksPerRead >= oldBlock {
		query := ethereum.FilterQuery{
			FromBlock: new(big.Int).SetUint64(currentBlock - d.config.L1BlocksPerRead),
			ToBlock:   new(big.Int).SetUint64(currentBlock),
			Addresses: []common.Address{*d.inboxAddr},
			Topics:    [][]common.Hash{{das.BatchDeliveredID}},
		}
		logs, err := d.l1Client.FilterLogs(ctx, query)
		if err != nil {
			return err
		}
		for _, deliveredLog := range logs {
			isDasMessage, err := d.checkDataAvailability(ctx, deliveredLog, metricBaseNewHash)
			if err != nil {
				return err
			}
			if isDasMessage {
				return nil
			}
		}
		currentBlock = currentBlock - d.config.L1BlocksPerRead
	}
	return fmt.Errorf("no das message found between block %d and block %d", latestBlock, oldBlock)
}

func (d *DataAvailabilityCheck) checkDataAvailabilityForOldHashInBlockRange(ctx context.Context, oldBlock uint64, latestBlock uint64) error {
	currentBlock := oldBlock
	for currentBlock+d.config.L1BlocksPerRead <= latestBlock {
		query := ethereum.FilterQuery{
			FromBlock: new(big.Int).SetUint64(currentBlock),
			ToBlock:   new(big.Int).SetUint64(currentBlock + d.config.L1BlocksPerRead),
			Addresses: []common.Address{*d.inboxAddr},
			Topics:    [][]common.Hash{{das.BatchDeliveredID}},
		}
		logs, err := d.l1Client.FilterLogs(ctx, query)
		if err != nil {
			return err
		}
		for _, deliveredLog := range logs {
			isDasMessage, err := d.checkDataAvailability(ctx, deliveredLog, metricBaseOldHash)
			if err != nil {
				return err
			}
			if isDasMessage {
				return nil
			}
		}
		currentBlock = currentBlock + d.config.L1BlocksPerRead
	}
	return fmt.Errorf("no das message found between block %d and block %d", oldBlock, latestBlock)
}

// Trys to find if DAS message is present in the given log and if present
// returns true and validates if the data is available in the storage service.
func (d *DataAvailabilityCheck) checkDataAvailability(ctx context.Context, deliveredLog types.Log, metricBase string) (bool, error) {
	deliveredEvent, err := d.inboxContract.ParseSequencerBatchDelivered(deliveredLog)
	if err != nil {
		return false, err
	}
	data, err := das.FindDASDataFromLog(ctx, d.inboxContract, deliveredEvent, *d.inboxAddr, d.l1Client, deliveredLog)
	if err != nil {
		return false, err
	}
	if data == nil {
		return false, nil
	}
	cert, err := arbstate.DeserializeDASCertFrom(bytes.NewReader(data))
	if err != nil {
		return true, err
	}
	var dataNotFound []string
	for url, reader := range d.urlToReaderMap {
		_, err = reader.GetByHash(ctx, cert.DataHash)
		canonicalUrl := metricsutil.CanonicalizeMetricName(url)
		if err != nil {
			metrics.GetOrRegisterCounter(metricBase+"/"+canonicalUrl+"/failure", nil).Inc(1)
			dataNotFound = append(dataNotFound, url)
			log.Error(fmt.Sprintf("Data with hash: %s not found for: %s\n", common.Hash(cert.DataHash).String(), url))
		} else {
			metrics.GetOrRegisterCounter(metricBase+"/"+canonicalUrl+"/success", nil).Inc(1)
		}
	}
	if len(dataNotFound) > 0 {
		return true, fmt.Errorf("data with hash: %s not found for das:%s", common.Hash(cert.DataHash).String(), dataNotFound)
	}
	return true, nil
}

'''
'''--- cmd/datool/datool.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"bytes"
	"context"
	"crypto/ecdsa"
	"crypto/rand"
	"encoding/base64"
	"errors"
	"fmt"
	"io"
	"os"
	"strings"
	"time"

	koanfjson "github.com/knadh/koanf/parsers/json"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util"

	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/signature"
)

func main() {
	args := os.Args
	if len(args) < 2 {
		panic("Usage: datool [client|keygen|generatehash|dumpkeyset] ...")
	}

	var err error
	switch strings.ToLower(args[1]) {
	case "client":
		err = startClient(args[2:])
	case "keygen":
		err = startKeyGen(args[2:])
	case "generatehash":
		err = generateHash(args[2])
	case "dumpkeyset":
		err = dumpKeyset(args[2:])
	default:
		panic(fmt.Sprintf("Unknown tool '%s' specified, valid tools are 'client', 'keygen', 'generatehash'", args[1]))
	}
	if err != nil {
		panic(err)
	}
}

// datool client ...

func startClient(args []string) error {
	switch strings.ToLower(args[0]) {
	case "rpc":
		switch strings.ToLower(args[1]) {
		case "store":
			return startClientStore(args[2:])
		default:
			return fmt.Errorf("datool client rpc '%s' not supported, valid arguments are 'store'", args[1])

		}
	case "rest":
		switch strings.ToLower(args[1]) {
		case "getbyhash":
			return startRESTClientGetByHash(args[2:])
		default:
			return fmt.Errorf("datool client rest '%s' not supported, valid argument is 'getByHash'", args[1])
		}

	}
	return fmt.Errorf("datool client '%s' not supported, valid arguments are 'rpc' and 'rest'", args[0])

}

// datool client rpc store

type ClientStoreConfig struct {
	URL                   string        `koanf:"url"`
	Message               string        `koanf:"message"`
	RandomMessageSize     int           `koanf:"random-message-size"`
	DASRetentionPeriod    time.Duration `koanf:"das-retention-period"`
	SigningKey            string        `koanf:"signing-key"`
	SigningWallet         string        `koanf:"signing-wallet"`
	SigningWalletPassword string        `koanf:"signing-wallet-password"`
}

func parseClientStoreConfig(args []string) (*ClientStoreConfig, error) {
	f := flag.NewFlagSet("datool client store", flag.ContinueOnError)
	f.String("url", "", "URL of DAS server to connect to")
	f.String("message", "", "message to send")
	f.Int("random-message-size", 0, "send a message of a specified number of random bytes")
	f.String("signing-key", "", "ecdsa private key to sign the message with, treated as a hex string if prefixed with 0x otherise treated as a file; if not specified the message is not signed")
	f.String("signing-wallet", "", "wallet containing ecdsa key to sign the message with")
	f.String("signing-wallet-password", genericconf.PASSWORD_NOT_SET, "password to unlock the wallet, if not specified the user is prompted for the password")
	f.Duration("das-retention-period", 24*time.Hour, "The period which DASes are requested to retain the stored batches.")

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var config ClientStoreConfig
	if err := confighelpers.EndCommonParse(k, &config); err != nil {
		return nil, err
	}
	return &config, nil
}

func startClientStore(args []string) error {
	config, err := parseClientStoreConfig(args)
	if err != nil {
		return err
	}

	client, err := das.NewDASRPCClient(config.URL)
	if err != nil {
		return err
	}

	var dasClient das.DataAvailabilityServiceWriter = client
	if config.SigningKey != "" {
		var privateKey *ecdsa.PrivateKey
		if config.SigningKey[:2] == "0x" {
			privateKey, err = crypto.HexToECDSA(config.SigningKey[2:])
			if err != nil {
				return err
			}
		} else {
			privateKey, err = crypto.LoadECDSA(config.SigningKey)
			if err != nil {
				return err
			}
		}
		signer := signature.DataSignerFromPrivateKey(privateKey)

		dasClient, err = das.NewStoreSigningDAS(dasClient, signer)
		if err != nil {
			return err
		}
	} else if config.SigningWallet != "" {
		walletConf := &genericconf.WalletConfig{
			Pathname:      config.SigningWallet,
			Password:      config.SigningWalletPassword,
			PrivateKey:    "",
			Account:       "",
			OnlyCreateKey: false,
		}
		_, signer, err := util.OpenWallet("datool", walletConf, nil)
		if err != nil {
			return err
		}
		dasClient, err = das.NewStoreSigningDAS(dasClient, signer)
		if err != nil {
			return err
		}
	}

	ctx := context.Background()
	var cert *arbstate.DataAvailabilityCertificate

	if config.RandomMessageSize > 0 {
		message := make([]byte, config.RandomMessageSize)
		_, err = rand.Read(message)
		if err != nil {
			return err
		}
		cert, err = dasClient.Store(ctx, message, uint64(time.Now().Add(config.DASRetentionPeriod).Unix()), []byte{})
	} else if len(config.Message) > 0 {
		cert, err = dasClient.Store(ctx, []byte(config.Message), uint64(time.Now().Add(config.DASRetentionPeriod).Unix()), []byte{})
	} else {
		return errors.New("--message or --random-message-size must be specified")
	}

	if err != nil {
		return err
	}

	serializedCert := das.Serialize(cert)
	fmt.Printf("Hex Encoded Cert: %s\n", hexutil.Encode(serializedCert))
	fmt.Printf("Hex Encoded Data Hash: %s\n", hexutil.Encode(cert.DataHash[:]))

	return nil
}

// datool client rest getbyhash

type RESTClientGetByHashConfig struct {
	URL      string `koanf:"url"`
	DataHash string `koanf:"data-hash"`
}

func parseRESTClientGetByHashConfig(args []string) (*RESTClientGetByHashConfig, error) {
	f := flag.NewFlagSet("datool client retrieve", flag.ContinueOnError)
	f.String("url", "http://localhost:9877", "URL of DAS server to connect to.")
	f.String("data-hash", "", "hash of the message to retrieve, if starts with '0x' it's treated as hex encoded, otherwise base64 encoded")

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var config RESTClientGetByHashConfig
	if err := confighelpers.EndCommonParse(k, &config); err != nil {
		return nil, err
	}
	return &config, nil
}

func startRESTClientGetByHash(args []string) error {
	config, err := parseRESTClientGetByHashConfig(args)
	if err != nil {
		return err
	}

	client, err := das.NewRestfulDasClientFromURL(config.URL)
	if err != nil {
		return err
	}

	var decodedHash []byte
	if strings.HasPrefix(config.DataHash, "0x") {
		decodedHash, err = hexutil.Decode(config.DataHash)
		if err != nil {
			return err
		}
	} else {
		hashDecoder := base64.NewDecoder(base64.StdEncoding, bytes.NewReader([]byte(config.DataHash)))
		decodedHash, err = io.ReadAll(hashDecoder)
		if err != nil {
			return err
		}
	}

	ctx := context.Background()
	message, err := client.GetByHash(ctx, common.BytesToHash(decodedHash))
	if err != nil {
		return err
	}
	fmt.Printf("Message: %s\n", message)
	return nil
}

// das keygen

type KeyGenConfig struct {
	Dir string
	// ECDSA mode.
	ECDSA bool `koanf:"ecdsa"`
	// Wallet mode.
	Wallet bool `koanf:"wallet"`
}

func parseKeyGenConfig(args []string) (*KeyGenConfig, error) {
	f := flag.NewFlagSet("datool keygen", flag.ContinueOnError)
	f.String("dir", "", "the directory to generate the keys in")
	f.Bool("ecdsa", false, "generate an ECDSA keypair instead of BLS")
	f.Bool("wallet", false, "generate the ECDSA keypair in a wallet file")

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var config KeyGenConfig
	if err := confighelpers.EndCommonParse(k, &config); err != nil {
		return nil, err
	}
	return &config, nil
}

func startKeyGen(args []string) error {
	config, err := parseKeyGenConfig(args)
	if err != nil {
		return err
	}

	if !config.ECDSA {
		_, _, err = das.GenerateAndStoreKeys(config.Dir)
		if err != nil {
			return err
		}
		return nil
	} else if !config.Wallet {
		return das.GenerateAndStoreECDSAKeys(config.Dir)
	} else {
		walletConf := &genericconf.WalletConfig{
			Pathname:      config.Dir,
			Password:      genericconf.PASSWORD_NOT_SET, // This causes a prompt for the password
			PrivateKey:    "",
			Account:       "",
			OnlyCreateKey: true,
		}
		_, _, err = util.OpenWallet("datool", walletConf, nil)
		if err != nil && strings.Contains(fmt.Sprint(err), "wallet key created") {
			return nil
		}
		return err
	}
}

func generateHash(message string) error {
	fmt.Printf("Hex Encoded Data Hash: %s\n", hexutil.Encode(dastree.HashBytes([]byte(message))))
	return nil
}

func parseDumpKeyset(args []string) (*DumpKeysetConfig, error) {
	f := flag.NewFlagSet("dump keyset", flag.ContinueOnError)

	das.AggregatorConfigAddOptions("keyset", f)
	genericconf.ConfConfigAddOptions("conf", f)

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var config DumpKeysetConfig
	if err := confighelpers.EndCommonParse(k, &config); err != nil {
		return nil, err
	}

	if config.Conf.Dump {
		c, err := k.Marshal(koanfjson.Parser())
		if err != nil {
			return nil, fmt.Errorf("unable to marshal config file to JSON: %w", err)
		}

		fmt.Println(string(c))
		os.Exit(0)
	}

	if config.Keyset.AssumedHonest == 0 {
		return nil, errors.New("--keyset.assumed-honest must be set")
	}
	if config.Keyset.Backends == "" {
		return nil, errors.New("--keyset.backends must be set")
	}

	return &config, nil
}

// das keygen

type DumpKeysetConfig struct {
	Keyset das.AggregatorConfig   `koanf:"keyset"`
	Conf   genericconf.ConfConfig `koanf:"conf"`
}

func dumpKeyset(args []string) error {
	config, err := parseDumpKeyset(args)
	if err != nil {
		return err
	}

	services, err := das.ParseServices(config.Keyset)
	if err != nil {
		return err
	}

	keysetHash, keysetBytes, err := das.KeysetHashFromServices(services, uint64(config.Keyset.AssumedHonest))
	if err != nil {
		return err
	}

	fmt.Printf("Keyset: %s\n", hexutil.Encode(keysetBytes))
	fmt.Printf("KeysetHash: %s\n", hexutil.Encode(keysetHash[:]))

	return err
}

'''
'''--- cmd/deploy/deploy.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"math/big"
	"os"
	"time"

	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/validator/server_common"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/cmd/util"
	deploycode "github.com/offchainlabs/nitro/deploy"
)

func main() {
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlDebug)
	log.Root().SetHandler(glogger)
	log.Info("deploying rollup")

	ctx := context.Background()

	l1conn := flag.String("l1conn", "", "l1 connection")
	l1keystore := flag.String("l1keystore", "", "l1 private key store")
	deployAccount := flag.String("l1DeployAccount", "", "l1 seq account to use (default is first account in keystore)")
	ownerAddressString := flag.String("ownerAddress", "", "the rollup owner's address")
	sequencerAddressString := flag.String("sequencerAddress", "", "the sequencer's address")
	nativeTokenAddressString := flag.String("nativeTokenAddress", "0x0000000000000000000000000000000000000000", "address of the ERC20 token which is used as native L2 currency")
	maxDataSizeUint := flag.Uint64("maxDataSize", 117964, "maximum data size of a batch or a cross-chain message (default = 90% of Geth's 128KB tx size limit)")
	loserEscrowAddressString := flag.String("loserEscrowAddress", "", "the address which half of challenge loser's funds accumulate at")
	wasmmoduleroot := flag.String("wasmmoduleroot", "", "WASM module root hash")
	wasmrootpath := flag.String("wasmrootpath", "", "path to machine folders")
	l1passphrase := flag.String("l1passphrase", "passphrase", "l1 private key file passphrase")
	l1privatekey := flag.String("l1privatekey", "", "l1 private key")
	outfile := flag.String("l1deployment", "deploy.json", "deployment output json file")
	l1ChainIdUint := flag.Uint64("l1chainid", 1337, "L1 chain ID")
	l2ChainConfig := flag.String("l2chainconfig", "l2_chain_config.json", "L2 chain config json file")
	l2ChainName := flag.String("l2chainname", "", "L2 chain name (will be included in chain info output json file)")
	l2ChainInfo := flag.String("l2chaininfo", "l2_chain_info.json", "L2 chain info output json file")
	authorizevalidators := flag.Uint64("authorizevalidators", 0, "Number of validators to preemptively authorize")
	txTimeout := flag.Duration("txtimeout", 10*time.Minute, "Timeout when waiting for a transaction to be included in a block")
	prod := flag.Bool("prod", false, "Whether to configure the rollup for production or testing")
	flag.Parse()
	l1ChainId := new(big.Int).SetUint64(*l1ChainIdUint)
	maxDataSize := new(big.Int).SetUint64(*maxDataSizeUint)

	if *prod {
		if *wasmmoduleroot == "" {
			panic("must specify wasm module root when launching prod chain")
		}
	}
	if *l2ChainName == "" {
		panic("must specify l2 chain name")
	}

	wallet := genericconf.WalletConfig{
		Pathname:   *l1keystore,
		Account:    *deployAccount,
		Password:   *l1passphrase,
		PrivateKey: *l1privatekey,
	}
	l1TransactionOpts, _, err := util.OpenWallet("l1", &wallet, l1ChainId)
	if err != nil {
		flag.Usage()
		log.Error("error reading keystore")
		panic(err)
	}

	l1client, err := ethclient.Dial(*l1conn)
	if err != nil {
		flag.Usage()
		log.Error("error creating l1client")
		panic(err)
	}

	if !common.IsHexAddress(*sequencerAddressString) && len(*sequencerAddressString) > 0 {
		panic("specified sequencer address is invalid")
	}
	if !common.IsHexAddress(*ownerAddressString) {
		panic("please specify a valid rollup owner address")
	}
	if *prod && !common.IsHexAddress(*loserEscrowAddressString) {
		panic("please specify a valid loser escrow address")
	}

	sequencerAddress := common.HexToAddress(*sequencerAddressString)
	ownerAddress := common.HexToAddress(*ownerAddressString)
	loserEscrowAddress := common.HexToAddress(*loserEscrowAddressString)
	if sequencerAddress != (common.Address{}) && ownerAddress != l1TransactionOpts.From {
		panic("cannot specify sequencer address if owner is not deployer")
	}

	var moduleRoot common.Hash
	if *wasmmoduleroot == "" {
		locator, err := server_common.NewMachineLocator(*wasmrootpath)
		if err != nil {
			panic(err)
		}
		moduleRoot = locator.LatestWasmModuleRoot()
	} else {
		moduleRoot = common.HexToHash(*wasmmoduleroot)
	}
	if moduleRoot == (common.Hash{}) {
		panic("wasmModuleRoot not found")
	}

	headerReaderConfig := headerreader.DefaultConfig
	headerReaderConfig.TxTimeout = *txTimeout

	chainConfigJson, err := os.ReadFile(*l2ChainConfig)
	if err != nil {
		panic(fmt.Errorf("failed to read l2 chain config file: %w", err))
	}
	var chainConfig params.ChainConfig
	err = json.Unmarshal(chainConfigJson, &chainConfig)
	if err != nil {
		panic(fmt.Errorf("failed to deserialize chain config: %w", err))
	}

	arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1client)
	l1Reader, err := headerreader.New(ctx, l1client, func() *headerreader.Config { return &headerReaderConfig }, arbSys)
	if err != nil {
		panic(fmt.Errorf("failed to create header reader: %w", err))
	}
	l1Reader.Start(ctx)
	defer l1Reader.StopAndWait()

	nativeToken := common.HexToAddress(*nativeTokenAddressString)
	deployedAddresses, err := deploycode.DeployOnL1(
		ctx,
		l1Reader,
		l1TransactionOpts,
		sequencerAddress,
		*authorizevalidators,
		arbnode.GenerateRollupConfig(*prod, moduleRoot, ownerAddress, &chainConfig, chainConfigJson, loserEscrowAddress),
		nativeToken,
		maxDataSize,
	)
	if err != nil {
		flag.Usage()
		log.Error("error deploying on l1")
		panic(err)
	}
	deployData, err := json.Marshal(deployedAddresses)
	if err != nil {
		panic(err)
	}
	if err := os.WriteFile(*outfile, deployData, 0600); err != nil {
		panic(err)
	}
	parentChainIsArbitrum := l1Reader.IsParentChainArbitrum()
	chainsInfo := []chaininfo.ChainInfo{
		{
			ChainName:             *l2ChainName,
			ParentChainId:         l1ChainId.Uint64(),
			ParentChainIsArbitrum: &parentChainIsArbitrum,
			ChainConfig:           &chainConfig,
			RollupAddresses:       deployedAddresses,
		},
	}
	chainsInfoJson, err := json.Marshal(chainsInfo)
	if err != nil {
		panic(err)
	}
	if err := os.WriteFile(*l2ChainInfo, chainsInfoJson, 0600); err != nil {
		panic(err)
	}
}

'''
'''--- cmd/genericconf/config.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package genericconf

import (
	"errors"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	flag "github.com/spf13/pflag"
)

type ConfConfig struct {
	Dump           bool          `koanf:"dump"`
	EnvPrefix      string        `koanf:"env-prefix"`
	File           []string      `koanf:"file"`
	S3             S3Config      `koanf:"s3"`
	String         string        `koanf:"string"`
	ReloadInterval time.Duration `koanf:"reload-interval" reload:"hot"`
}

func ConfConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".dump", ConfConfigDefault.Dump, "print out currently active configuration file")
	f.String(prefix+".env-prefix", ConfConfigDefault.EnvPrefix, "environment variables with given prefix will be loaded as configuration values")
	f.StringSlice(prefix+".file", ConfConfigDefault.File, "name of configuration file")
	S3ConfigAddOptions(prefix+".s3", f)
	f.String(prefix+".string", ConfConfigDefault.String, "configuration as JSON string")
	f.Duration(prefix+".reload-interval", ConfConfigDefault.ReloadInterval, "how often to reload configuration (0=disable periodic reloading)")
}

var ConfConfigDefault = ConfConfig{
	Dump:           false,
	EnvPrefix:      "",
	File:           []string{},
	S3:             DefaultS3Config,
	String:         "",
	ReloadInterval: 0,
}

type S3Config struct {
	AccessKey string `koanf:"access-key"`
	Bucket    string `koanf:"bucket"`
	ObjectKey string `koanf:"object-key"`
	Region    string `koanf:"region"`
	SecretKey string `koanf:"secret-key"`
}

func S3ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".access-key", DefaultS3Config.AccessKey, "S3 access key")
	f.String(prefix+".bucket", DefaultS3Config.Bucket, "S3 bucket")
	f.String(prefix+".object-key", DefaultS3Config.ObjectKey, "S3 object key")
	f.String(prefix+".region", DefaultS3Config.Region, "S3 region")
	f.String(prefix+".secret-key", DefaultS3Config.SecretKey, "S3 secret key")
}

var DefaultS3Config = S3Config{
	AccessKey: "",
	Bucket:    "",
	ObjectKey: "",
	Region:    "",
	SecretKey: "",
}

func ParseLogType(logType string) (log.Format, error) {
	if logType == "plaintext" {
		return log.TerminalFormat(false), nil
	} else if logType == "json" {
		return log.JSONFormat(), nil
	}
	return nil, errors.New("invalid log type")
}

type FileLoggingConfig struct {
	Enable     bool   `koanf:"enable"`
	File       string `koanf:"file"`
	MaxSize    int    `koanf:"max-size"`
	MaxAge     int    `koanf:"max-age"`
	MaxBackups int    `koanf:"max-backups"`
	LocalTime  bool   `koanf:"local-time"`
	Compress   bool   `koanf:"compress"`
	BufSize    int    `koanf:"buf-size"`
}

var DefaultFileLoggingConfig = FileLoggingConfig{
	Enable:     true,
	File:       "nitro.log",
	MaxSize:    5,     // 5Mb
	MaxAge:     0,     // don't remove old files based on age
	MaxBackups: 20,    // keep 20 files
	LocalTime:  false, // use UTC time
	Compress:   true,
	BufSize:    512,
}

func FileLoggingConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultFileLoggingConfig.Enable, "enable logging to file")
	f.String(prefix+".file", DefaultFileLoggingConfig.File, "path to log file")
	f.Int(prefix+".max-size", DefaultFileLoggingConfig.MaxSize, "log file size in Mb that will trigger log file rotation (0 = trigger disabled)")
	f.Int(prefix+".max-age", DefaultFileLoggingConfig.MaxAge, "maximum number of days to retain old log files based on the timestamp encoded in their filename (0 = no limit)")
	f.Int(prefix+".max-backups", DefaultFileLoggingConfig.MaxBackups, "maximum number of old log files to retain (0 = no limit)")
	f.Bool(prefix+".local-time", DefaultFileLoggingConfig.LocalTime, "if true: local time will be used in old log filename timestamps")
	f.Bool(prefix+".compress", DefaultFileLoggingConfig.Compress, "enable compression of old log files")
	f.Int(prefix+".buf-size", DefaultFileLoggingConfig.BufSize, "size of intermediate log records buffer")
}

type RpcConfig struct {
	MaxBatchResponseSize int `koanf:"max-batch-response-size"`
	BatchRequestLimit    int `koanf:"batch-request-limit"`
}

var DefaultRpcConfig = RpcConfig{
	MaxBatchResponseSize: 10_000_000, // 10MB
	BatchRequestLimit:    node.DefaultConfig.BatchRequestLimit,
}

func (c *RpcConfig) Apply(stackConf *node.Config) {
	stackConf.BatchResponseMaxSize = c.MaxBatchResponseSize
	stackConf.BatchRequestLimit = c.BatchRequestLimit
}

func RpcConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int(prefix+".max-batch-response-size", DefaultRpcConfig.MaxBatchResponseSize, "the maximum response size for a JSON-RPC request measured in bytes (-1 means no limit)")
	f.Int(prefix+".batch-request-limit", DefaultRpcConfig.BatchRequestLimit, "the maximum number of requests in a batch")
}

'''
'''--- cmd/genericconf/filehandler_test.go ---
package genericconf

import (
	"bytes"
	"encoding/json"
	"errors"
	"io"
	"os"
	"path/filepath"
	"strings"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func pollLogMessagesFromJSONFile(t *testing.T, path string, expected []string) ([]string, error) {
	t.Helper()
	var msgs []string
	var err error
Retry:
	for i := 0; i < 30; i++ {
		time.Sleep(20 * time.Millisecond)
		msgs, err = readLogMessagesFromJSONFile(t, path)
		if err != nil {
			continue
		}
		if len(msgs) == len(expected) {
			for i, m := range msgs {
				if m != expected[i] {
					continue Retry
				}
			}
			return msgs, nil
		}
	}
	return msgs, err
}

func readLogMessagesFromJSONFile(t *testing.T, path string) ([]string, error) {
	t.Helper()
	data, err := os.ReadFile(path)
	if err != nil {
		return []string{}, err
	}
	messages := []string{}
	decoder := json.NewDecoder(bytes.NewBuffer(data))
	var record map[string]interface{}
	for {
		if err = decoder.Decode(&record); err != nil {
			break
		}
		msg, ok := record["msg"]
		if !ok {
			testhelpers.FailImpl(t, "Incorrect record, msg key is missing", "record", record)
		}
		messages = append(messages, msg.(string))
	}
	if errors.Is(err, io.EOF) {
		return messages, nil
	}
	return []string{}, err
}

func testFileHandler(t *testing.T, testCompressed bool) {
	t.Helper()
	testDir := t.TempDir()
	testFileName := "test-file"
	testFile := filepath.Join(testDir, testFileName)
	config := DefaultFileLoggingConfig
	config.MaxSize = 1
	config.Compress = testCompressed
	config.File = testFile
	fileHandler := globalFileHandlerFactory.newHandler(log.JSONFormat(), &config, testFile)
	defer func() { testhelpers.RequireImpl(t, globalFileHandlerFactory.close()) }()
	log.Root().SetHandler(fileHandler)
	expected := []string{"dead", "beef", "ate", "bad", "beef"}
	for _, e := range expected {
		log.Warn(e)
	}
	msgs, err := pollLogMessagesFromJSONFile(t, testFile, expected)
	testhelpers.RequireImpl(t, err)
	if len(msgs) != len(expected) {
		testhelpers.FailImpl(t, "Unexpected number of messages logged to file")
	}
	for i, m := range msgs {
		if m != expected[i] {
			testhelpers.FailImpl(t, "Unexpected message logged to file, have: ", m, " want:", expected[i])
		}
	}
	bigData := make([]byte, 512*1024)
	for i := range bigData {
		bigData[i] = 'x'
	}
	bigString := string(bigData)
	// make sure logs size exceeds 1MB, while keeping log msg < 1MB
	log.Warn(bigString)
	log.Warn(bigString)
	msgs, err = pollLogMessagesFromJSONFile(t, testFile, []string{bigString})
	testhelpers.RequireImpl(t, err)
	if len(msgs) != 1 {
		testhelpers.FailImpl(t, "Unexpected number of messages in the logfile - possible file rotation failure, have: ", len(msgs), " wants: 1")
	}
	if msgs[0] != bigString {
		testhelpers.FailImpl(t, "Unexpected message logged to file, have: ", msgs[0], " want:", bigString)
	}
	var gzFiles int
	var entries []os.DirEntry
	for i := 0; i < 60; i++ {
		time.Sleep(20 * time.Millisecond)
		gzFiles = 0
		var err error
		entries, err = os.ReadDir(testDir)
		testhelpers.RequireImpl(t, err)
		for _, entry := range entries {
			if !strings.HasPrefix(entry.Name(), testFileName) {
				testhelpers.FailImpl(t, "Unexpected file in test dir:", entry.Name())
			}
			if strings.HasSuffix(entry.Name(), ".gz") {
				gzFiles++
			}
		}
		if len(entries) == 2 && (!testCompressed || gzFiles == 1) {
			break
		}
	}
	if testCompressed && gzFiles != 1 {
		testhelpers.FailImpl(t, "Unexpected number of gzip files in test dir:", gzFiles)
	}
	if len(entries) != 2 {
		testhelpers.FailImpl(t, "Unexpected number of files in test dir:", len(entries))
	}
}

func TestFileLoggerWithoutCompression(t *testing.T) {
	testFileHandler(t, false)
}

func TestFileLoggerWithCompression(t *testing.T) {
	testFileHandler(t, true)
}

'''
'''--- cmd/genericconf/getversion17.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build !go1.18

package genericconf

func GetVersion() (string, string) {
	return "development", "development"
}

'''
'''--- cmd/genericconf/getversion18.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build go1.18

package genericconf

import "runtime/debug"

func GetVersion(definedVersion string, definedTime string, definedModified string) (string, string, string) {
	vcsVersion := "development"
	vcsTime := "development"
	vcsModified := "false"
	info, ok := debug.ReadBuildInfo()
	if !ok {
		vcsVersion = "unknown"
	}
	for _, v := range info.Settings {
		if v.Key == "vcs.revision" {
			vcsVersion = v.Value
			if len(vcsVersion) > 7 {
				vcsVersion = vcsVersion[:7]
			}
		} else if v.Key == "vcs.time" {
			vcsTime = v.Value
		} else if v.Key == "vcs.modified" {
			vcsModified = v.Value
		}
	}

	// Defined values override if provided
	if len(definedVersion) > 0 {
		vcsVersion = definedVersion
	}
	if len(definedTime) > 0 {
		vcsTime = definedTime
	}
	if len(definedModified) > 0 {
		vcsModified = definedModified
	}

	if vcsModified == "true" {
		vcsVersion = vcsVersion + "-modified"
	}

	strippedVersion := vcsVersion
	if len(strippedVersion) > 0 && strippedVersion[0] == 'v' {
		strippedVersion = strippedVersion[1:]
	}

	return vcsVersion, strippedVersion, vcsTime
}

'''
'''--- cmd/genericconf/jwt.go ---
package genericconf

import (
	"crypto/rand"
	"errors"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
)

func TryCreatingJWTSecret(filename string) error {
	err := os.MkdirAll(filepath.Dir(filename), 0755)
	if err != nil {
		return fmt.Errorf("couldn't create directory for jwt secret: %w", err)
	}
	f, err := os.OpenFile(filename, os.O_WRONLY|os.O_CREATE|os.O_EXCL, fs.FileMode(0600))
	if errors.Is(err, fs.ErrExist) {
		log.Info("using existing jwt file", "filename", filename)
		return nil
	}
	if err != nil {
		return fmt.Errorf("couldn't create file: %w", err)
	}
	defer func() {
		if err := f.Close(); err != nil {
			log.Error("failed to close file", "err", err)
		}
	}()
	secret := common.Hash{}
	_, err = rand.Read(secret[:])
	if err != nil {
		return fmt.Errorf("couldn't generate secret: %w", err)
	}
	_, err = f.Write([]byte(secret.Hex()))
	if err != nil {
		return fmt.Errorf("couldn't writeto file: %w", err)
	}
	log.Info("created jwt file", "filename", filename)
	return nil
}

'''
'''--- cmd/genericconf/liveconfig.go ---
package genericconf

import (
	"context"
	"os"
	"os/signal"
	"sync"
	"syscall"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

type ConfigConstrain[T any] interface {
	CanReload(T) error
	GetReloadInterval() time.Duration
}

type OnReloadHook[T ConfigConstrain[T]] func(old T, new T) error

func NoopOnReloadHook[T ConfigConstrain[T]](_ T, _ T) error {
	return nil
}

type ConfigParseFunction[T ConfigConstrain[T]] func(context.Context, []string) (T, error)

type LiveConfig[T ConfigConstrain[T]] struct {
	stopwaiter.StopWaiter

	mutex        sync.RWMutex
	args         []string
	config       T
	onReloadHook OnReloadHook[T]
	parse        ConfigParseFunction[T]
}

func (c *LiveConfig[T]) Get() T {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	return c.config
}

func (c *LiveConfig[T]) Set(config T) error {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	if err := c.config.CanReload(config); err != nil {
		return err
	}
	if err := c.onReloadHook(c.config, config); err != nil {
		// TODO(magic) panic? return err? only log the error?
		log.Error("Failed to execute onReloadHook", "err", err)
	}
	c.config = config
	return nil
}

func (c *LiveConfig[T]) Start(ctxIn context.Context) {
	c.StopWaiter.Start(ctxIn, c)

	sigusr1 := make(chan os.Signal, 1)
	signal.Notify(sigusr1, syscall.SIGUSR1)

	c.LaunchThread(func(ctx context.Context) {
		for {
			reloadInterval := c.config.GetReloadInterval()
			if reloadInterval == 0 {
				select {
				case <-ctx.Done():
					return
				case <-sigusr1:
					log.Info("Configuration reload triggered by SIGUSR1.")
				}
			} else {
				timer := time.NewTimer(reloadInterval)
				select {
				case <-ctx.Done():
					timer.Stop()
					return
				case <-sigusr1:
					timer.Stop()
					log.Info("Configuration reload triggered by SIGUSR1.")
				case <-timer.C:
				}
			}
			nodeConfig, err := c.parse(ctx, c.args)
			if err != nil {
				log.Error("error parsing live config", "error", err.Error())
				continue
			}
			err = c.Set(nodeConfig)
			if err != nil {
				log.Error("error updating live config", "error", err.Error())
				continue
			}
		}
	})
}

// SetOnReloadHook is NOT thread-safe and supports setting only one hook
func (c *LiveConfig[T]) SetOnReloadHook(hook OnReloadHook[T]) {
	c.onReloadHook = hook
}

func NewLiveConfig[T ConfigConstrain[T]](args []string, config T, parse ConfigParseFunction[T]) *LiveConfig[T] {
	return &LiveConfig[T]{
		args:         args,
		config:       config,
		onReloadHook: NoopOnReloadHook[T],
		parse:        parse,
	}
}

'''
'''--- cmd/genericconf/logging.go ---
package genericconf

import (
	"context"
	"flag"
	"fmt"
	"os"

	"github.com/ethereum/go-ethereum/log"
	"gopkg.in/natefinch/lumberjack.v2"
)

var globalFileHandlerFactory = fileHandlerFactory{}

type fileHandlerFactory struct {
	writer  *lumberjack.Logger
	records chan *log.Record
	cancel  context.CancelFunc
}

// newHandler is not threadsafe
func (l *fileHandlerFactory) newHandler(logFormat log.Format, config *FileLoggingConfig, filename string) log.Handler {
	l.close()
	l.writer = &lumberjack.Logger{
		Filename:   filename,
		MaxSize:    config.MaxSize,
		MaxBackups: config.MaxBackups,
		MaxAge:     config.MaxAge,
		Compress:   config.Compress,
	}
	// capture copy of the pointer
	writer := l.writer
	// lumberjack.Logger already locks on Write, no need for SyncHandler proxy which is used in StreamHandler
	unsafeStreamHandler := log.LazyHandler(log.FuncHandler(func(r *log.Record) error {
		_, err := writer.Write(logFormat.Format(r))
		return err
	}))
	l.records = make(chan *log.Record, config.BufSize)
	// capture copy
	records := l.records
	var consumerCtx context.Context
	consumerCtx, l.cancel = context.WithCancel(context.Background())
	go func() {
		for {
			select {
			case r := <-records:
				_ = unsafeStreamHandler.Log(r)
			case <-consumerCtx.Done():
				return
			}
		}
	}()
	return log.FuncHandler(func(r *log.Record) error {
		select {
		case records <- r:
			return nil
		default:
			return fmt.Errorf("Buffer overflow, dropping record")
		}
	})
}

// close is not threadsafe
func (l *fileHandlerFactory) close() error {
	if l.cancel != nil {
		l.cancel()
		l.cancel = nil
	}
	if l.writer != nil {
		if err := l.writer.Close(); err != nil {
			return err
		}
		l.writer = nil
	}
	return nil
}

// initLog is not threadsafe
func InitLog(logType string, logLevel log.Lvl, fileLoggingConfig *FileLoggingConfig, pathResolver func(string) string) error {
	logFormat, err := ParseLogType(logType)
	if err != nil {
		flag.Usage()
		return fmt.Errorf("error parsing log type: %w", err)
	}
	var glogger *log.GlogHandler
	// always close previous instance of file logger
	if err := globalFileHandlerFactory.close(); err != nil {
		return fmt.Errorf("failed to close file writer: %w", err)
	}
	if fileLoggingConfig.Enable {
		glogger = log.NewGlogHandler(
			log.MultiHandler(
				log.StreamHandler(os.Stderr, logFormat),
				// on overflow records are dropped silently as MultiHandler ignores errors
				globalFileHandlerFactory.newHandler(logFormat, fileLoggingConfig, pathResolver(fileLoggingConfig.File)),
			))
	} else {
		glogger = log.NewGlogHandler(log.StreamHandler(os.Stderr, logFormat))
	}
	glogger.Verbosity(logLevel)
	log.Root().SetHandler(glogger)
	return nil
}

'''
'''--- cmd/genericconf/pprof.go ---
package genericconf

import (
	"fmt"
	"net/http"

	// Blank import pprof registers its HTTP handlers.
	_ "net/http/pprof" // #nosec G108

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/metrics/exp"
)

func StartPprof(address string) {
	exp.Exp(metrics.DefaultRegistry)
	log.Info("Starting metrics server with pprof", "addr", fmt.Sprintf("http://%s/debug/metrics", address))
	log.Info("Pprof endpoint", "addr", fmt.Sprintf("http://%s/debug/pprof", address))
	go func() {
		if err := http.ListenAndServe(address, http.DefaultServeMux); /* #nosec G114 */ err != nil {
			log.Error("Failure in running pprof server", "err", err)
		}
	}()
}

'''
'''--- cmd/genericconf/server.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package genericconf

import (
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/node"
)

type HTTPConfig struct {
	Addr           string                  `koanf:"addr"`
	Port           int                     `koanf:"port"`
	API            []string                `koanf:"api"`
	RPCPrefix      string                  `koanf:"rpcprefix"`
	CORSDomain     []string                `koanf:"corsdomain"`
	VHosts         []string                `koanf:"vhosts"`
	ServerTimeouts HTTPServerTimeoutConfig `koanf:"server-timeouts"`
}

var HTTPConfigDefault = HTTPConfig{
	Addr:           node.DefaultConfig.HTTPHost,
	Port:           8547,
	API:            append(node.DefaultConfig.HTTPModules, "eth", "arb"),
	RPCPrefix:      node.DefaultConfig.HTTPPathPrefix,
	CORSDomain:     []string{},
	VHosts:         node.DefaultConfig.HTTPVirtualHosts,
	ServerTimeouts: HTTPServerTimeoutConfigDefault,
}

type HTTPServerTimeoutConfig struct {
	ReadTimeout       time.Duration `koanf:"read-timeout"`
	ReadHeaderTimeout time.Duration `koanf:"read-header-timeout"`
	WriteTimeout      time.Duration `koanf:"write-timeout"`
	IdleTimeout       time.Duration `koanf:"idle-timeout"`
}

// HTTPServerTimeoutConfigDefault use geth defaults
var HTTPServerTimeoutConfigDefault = HTTPServerTimeoutConfig{
	ReadTimeout:       30 * time.Second,
	ReadHeaderTimeout: 30 * time.Second,
	WriteTimeout:      30 * time.Second,
	IdleTimeout:       120 * time.Second,
}

func (c HTTPConfig) Apply(stackConf *node.Config) {
	stackConf.HTTPHost = c.Addr
	stackConf.HTTPPort = c.Port
	stackConf.HTTPModules = c.API
	stackConf.HTTPPathPrefix = c.RPCPrefix
	stackConf.HTTPCors = c.CORSDomain
	stackConf.HTTPVirtualHosts = c.VHosts
	stackConf.HTTPTimeouts.ReadTimeout = c.ServerTimeouts.ReadTimeout
	// TODO ReadHeaderTimeout pending on https://github.com/ethereum/go-ethereum/pull/25338
	// stackConf.HTTPTimeouts.ReadHeaderTimeout = c.ServerTimeouts.ReadHeaderTimeout
	stackConf.HTTPTimeouts.WriteTimeout = c.ServerTimeouts.WriteTimeout
	stackConf.HTTPTimeouts.IdleTimeout = c.ServerTimeouts.IdleTimeout
}

func HTTPConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".addr", HTTPConfigDefault.Addr, "HTTP-RPC server listening interface")
	f.Int(prefix+".port", HTTPConfigDefault.Port, "HTTP-RPC server listening port")
	f.StringSlice(prefix+".api", HTTPConfigDefault.API, "APIs offered over the HTTP-RPC interface")
	f.String(prefix+".rpcprefix", HTTPConfigDefault.RPCPrefix, "HTTP path path prefix on which JSON-RPC is served. Use '/' to serve on all paths")
	f.StringSlice(prefix+".corsdomain", HTTPConfigDefault.CORSDomain, "Comma separated list of domains from which to accept cross origin requests (browser enforced)")
	f.StringSlice(prefix+".vhosts", HTTPConfigDefault.VHosts, "Comma separated list of virtual hostnames from which to accept requests (server enforced). Accepts '*' wildcard")
	HTTPServerTimeoutConfigAddOptions(prefix+".server-timeouts", f)
}

func HTTPServerTimeoutConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Duration(prefix+".read-timeout", HTTPServerTimeoutConfigDefault.ReadTimeout, "the maximum duration for reading the entire request (http.Server.ReadTimeout)")
	f.Duration(prefix+".read-header-timeout", HTTPServerTimeoutConfigDefault.ReadHeaderTimeout, "the amount of time allowed to read the request headers (http.Server.ReadHeaderTimeout)")
	f.Duration(prefix+".write-timeout", HTTPServerTimeoutConfigDefault.WriteTimeout, "the maximum duration before timing out writes of the response (http.Server.WriteTimeout)")
	f.Duration(prefix+".idle-timeout", HTTPServerTimeoutConfigDefault.IdleTimeout, "the maximum amount of time to wait for the next request when keep-alives are enabled (http.Server.IdleTimeout)")
}

type WSConfig struct {
	Addr      string   `koanf:"addr"`
	Port      int      `koanf:"port"`
	API       []string `koanf:"api"`
	RPCPrefix string   `koanf:"rpcprefix"`
	Origins   []string `koanf:"origins"`
	ExposeAll bool     `koanf:"expose-all"`
}

var WSConfigDefault = WSConfig{
	Addr:      node.DefaultConfig.WSHost,
	Port:      8548,
	API:       append(node.DefaultConfig.WSModules, "eth", "arb"),
	RPCPrefix: node.DefaultConfig.WSPathPrefix,
	Origins:   []string{},
	ExposeAll: node.DefaultConfig.WSExposeAll,
}

func (c WSConfig) Apply(stackConf *node.Config) {
	stackConf.WSHost = c.Addr
	stackConf.WSPort = c.Port
	stackConf.WSModules = c.API
	stackConf.WSPathPrefix = c.RPCPrefix
	stackConf.WSOrigins = c.Origins
	stackConf.WSExposeAll = c.ExposeAll
}

func WSConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".addr", WSConfigDefault.Addr, "WS-RPC server listening interface")
	f.Int(prefix+".port", WSConfigDefault.Port, "WS-RPC server listening port")
	f.StringSlice(prefix+".api", WSConfigDefault.API, "APIs offered over the WS-RPC interface")
	f.String(prefix+".rpcprefix", WSConfigDefault.RPCPrefix, "WS path path prefix on which JSON-RPC is served. Use '/' to serve on all paths")
	f.StringSlice(prefix+".origins", WSConfigDefault.Origins, "Origins from which to accept websockets requests")
	f.Bool(prefix+".expose-all", WSConfigDefault.ExposeAll, "expose private api via websocket")
}

type IPCConfig struct {
	Path string `koanf:"path"`
}

var IPCConfigDefault = IPCConfig{
	Path: "",
}

func (c *IPCConfig) Apply(stackConf *node.Config) {
	stackConf.IPCPath = c.Path
}

func IPCConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".path", IPCConfigDefault.Path, "Requested location to place the IPC endpoint. An empty path disables IPC.")
}

type GraphQLConfig struct {
	Enable     bool     `koanf:"enable"`
	CORSDomain []string `koanf:"corsdomain"`
	VHosts     []string `koanf:"vhosts"`
}

var GraphQLConfigDefault = GraphQLConfig{
	Enable:     false,
	CORSDomain: []string{},
	VHosts:     node.DefaultConfig.GraphQLVirtualHosts,
}

func (c GraphQLConfig) Apply(stackConf *node.Config) {
	stackConf.GraphQLCors = c.CORSDomain
	stackConf.GraphQLVirtualHosts = c.VHosts
}

func GraphQLConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", GraphQLConfigDefault.Enable, "Enable graphql endpoint on the rpc endpoint")
	f.StringSlice(prefix+".corsdomain", GraphQLConfigDefault.CORSDomain, "Comma separated list of domains from which to accept cross origin requests (browser enforced)")
	f.StringSlice(prefix+".vhosts", GraphQLConfigDefault.VHosts, "Comma separated list of virtual hostnames from which to accept requests (server enforced). Accepts '*' wildcard")
}

type AuthRPCConfig struct {
	Addr      string   `koanf:"addr"`
	Port      int      `koanf:"port"`
	API       []string `koanf:"api"`
	Origins   []string `koanf:"origins"`
	JwtSecret string   `koanf:"jwtsecret"`
}

func (a AuthRPCConfig) Apply(stackConf *node.Config) {
	stackConf.AuthAddr = a.Addr
	stackConf.AuthPort = a.Port
	stackConf.AuthVirtualHosts = []string{} // dont allow http access
	stackConf.JWTSecret = a.JwtSecret
	stackConf.AuthModules = a.API
	stackConf.AuthOrigins = a.Origins
}

var AuthRPCConfigDefault = AuthRPCConfig{
	Addr:      "127.0.0.1",
	Port:      8549,
	API:       []string{"validation"},
	Origins:   []string{"localhost"},
	JwtSecret: "",
}

func AuthRPCConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".addr", AuthRPCConfigDefault.Addr, "AUTH-RPC server listening interface")
	f.String(prefix+".jwtsecret", AuthRPCConfigDefault.JwtSecret, "Path to file holding JWT secret (32B hex)")
	f.Int(prefix+".port", AuthRPCConfigDefault.Port, "AUTH-RPC server listening port")
	f.StringSlice(prefix+".origins", AuthRPCConfigDefault.Origins, "Origins from which to accept AUTH requests")
	f.StringSlice(prefix+".api", AuthRPCConfigDefault.API, "APIs offered over the AUTH-RPC interface")
}

type MetricsServerConfig struct {
	Addr           string        `koanf:"addr"`
	Port           int           `koanf:"port"`
	UpdateInterval time.Duration `koanf:"update-interval"`
}

var MetricsServerConfigDefault = MetricsServerConfig{
	Addr:           "127.0.0.1",
	Port:           6070,
	UpdateInterval: 3 * time.Second,
}

type PProf struct {
	Addr string `koanf:"addr"`
	Port int    `koanf:"port"`
}

var PProfDefault = PProf{
	Addr: "127.0.0.1",
	Port: 6071,
}

func MetricsServerAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".addr", MetricsServerConfigDefault.Addr, "metrics server address")
	f.Int(prefix+".port", MetricsServerConfigDefault.Port, "metrics server port")
	f.Duration(prefix+".update-interval", MetricsServerConfigDefault.UpdateInterval, "metrics server update interval")
}

func PProfAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".addr", PProfDefault.Addr, "pprof server address")
	f.Int(prefix+".port", PProfDefault.Port, "pprof server port")
}

'''
'''--- cmd/genericconf/wallet.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package genericconf

import (
	"path"
	"path/filepath"

	flag "github.com/spf13/pflag"
)

const PASSWORD_NOT_SET = "PASSWORD_NOT_SET"

type WalletConfig struct {
	Pathname      string `koanf:"pathname"`
	Password      string `koanf:"password"`
	PrivateKey    string `koanf:"private-key"`
	Account       string `koanf:"account"`
	OnlyCreateKey bool   `koanf:"only-create-key"`
}

func (w *WalletConfig) Pwd() *string {
	if w.Password == PASSWORD_NOT_SET {
		return nil
	}
	return &w.Password
}

var WalletConfigDefault = WalletConfig{
	Pathname:      "",
	Password:      PASSWORD_NOT_SET,
	PrivateKey:    "",
	Account:       "",
	OnlyCreateKey: false,
}

func WalletConfigAddOptions(prefix string, f *flag.FlagSet, defaultPathname string) {
	f.String(prefix+".pathname", defaultPathname, "pathname for wallet")
	f.String(prefix+".password", WalletConfigDefault.Password, "wallet passphrase")
	f.String(prefix+".private-key", WalletConfigDefault.PrivateKey, "private key for wallet")
	f.String(prefix+".account", WalletConfigDefault.Account, "account to use (default is first account in keystore)")
	f.Bool(prefix+".only-create-key", WalletConfigDefault.OnlyCreateKey, "if true, creates new key then exits")
}

func (w *WalletConfig) ResolveDirectoryNames(chain string) {
	// Make wallet directories relative to chain directory if specified and not already absolute
	if len(w.Pathname) != 0 && !filepath.IsAbs(w.Pathname) {
		w.Pathname = path.Join(chain, w.Pathname)
	}
}

'''
'''--- cmd/ipfshelper/ipfshelper.go ---
package ipfshelper

import (
	"context"
	"fmt"
	"io"
	"math/rand"
	"os"
	"path/filepath"
	"strings"
	"sync"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ipfs/go-libipfs/files"
	coreiface "github.com/ipfs/interface-go-ipfs-core"
	"github.com/ipfs/interface-go-ipfs-core/options"
	"github.com/ipfs/interface-go-ipfs-core/path"
	"github.com/ipfs/kubo/config"
	"github.com/ipfs/kubo/core"
	"github.com/ipfs/kubo/core/coreapi"
	"github.com/ipfs/kubo/core/node/libp2p"
	"github.com/ipfs/kubo/plugin/loader"
	"github.com/ipfs/kubo/repo"
	"github.com/ipfs/kubo/repo/fsrepo"
	"github.com/libp2p/go-libp2p/core/host"
	"github.com/libp2p/go-libp2p/core/peer"
	ma "github.com/multiformats/go-multiaddr"
)

const DefaultIpfsProfiles = ""

type IpfsHelper struct {
	api      coreiface.CoreAPI
	node     *core.IpfsNode
	cfg      *config.Config
	repoPath string
	repo     repo.Repo
}

func (h *IpfsHelper) createRepo(downloadPath string, profiles string) error {
	fileInfo, err := os.Stat(downloadPath)
	if err != nil {
		return fmt.Errorf("failed to stat ipfs repo directory: %w", err)
	}
	if !fileInfo.IsDir() {
		return fmt.Errorf("%s is not a directory", downloadPath)
	}
	h.repoPath = filepath.Join(downloadPath, "ipfs-repo")
	// Create a config with default options and a 2048 bit key
	h.cfg, err = config.Init(io.Discard, 2048)
	if err != nil {
		return err
	}
	if len(profiles) > 0 {
		for _, profile := range strings.Split(profiles, ",") {
			transformer, ok := config.Profiles[profile]
			if !ok {
				return fmt.Errorf("invalid ipfs configuration profile: %s", profile)
			}

			if err := transformer.Transform(h.cfg); err != nil {
				return err
			}
		}
	}
	// Create the repo with the config
	// fsrepo.Init initializes new repo only if it's not initialized yet
	err = fsrepo.Init(h.repoPath, h.cfg)
	if err != nil {
		return fmt.Errorf("failed to init ipfs repo: %w", err)
	}
	h.repo, err = fsrepo.Open(h.repoPath)
	if err != nil {
		return fmt.Errorf("failed to open ipfs repo: %w", err)
	}
	return nil
}

func (h *IpfsHelper) createNode(ctx context.Context, clientOnly bool) error {
	var routing libp2p.RoutingOption
	if clientOnly {
		routing = libp2p.DHTClientOption
	} else {
		routing = libp2p.DHTOption
	}
	nodeOptions := &core.BuildCfg{
		Online:  true,
		Routing: routing,
		Repo:    h.repo,
	}
	var err error
	h.node, err = core.NewNode(ctx, nodeOptions)
	if err != nil {
		return err
	}
	h.api, err = coreapi.NewCoreAPI(h.node)
	return err
}

func (h *IpfsHelper) connectToPeers(ctx context.Context, peers []string) error {
	peerInfos := make(map[peer.ID]*peer.AddrInfo, len(peers))
	for _, addressString := range peers {
		address, err := ma.NewMultiaddr(addressString)
		if err != nil {
			return err
		}
		addressInfo, err := peer.AddrInfoFromP2pAddr(address)
		if err != nil {
			return err
		}
		peerInfo, ok := peerInfos[addressInfo.ID]
		if !ok {
			peerInfo = &peer.AddrInfo{ID: addressInfo.ID}
			peerInfos[peerInfo.ID] = peerInfo
		}
		peerInfo.Addrs = append(peerInfo.Addrs, addressInfo.Addrs...)
	}
	var wg sync.WaitGroup
	wg.Add(len(peerInfos))
	for _, peerInfo := range peerInfos {
		go func(peerInfo *peer.AddrInfo) {
			defer wg.Done()
			err := h.api.Swarm().Connect(ctx, *peerInfo)
			if err != nil {
				log.Warn("failed to connect to peer", "peerId", peerInfo.ID, "err", err)
				return
			}
		}(peerInfo)
	}
	wg.Wait()
	return nil
}

func (h *IpfsHelper) GetPeerHostAddresses() ([]string, error) {
	addresses, err := peer.AddrInfoToP2pAddrs(host.InfoFromHost(h.node.PeerHost))
	if err != nil {
		return []string{}, err
	}
	addressesStrings := make([]string, len(addresses))
	for i, a := range addresses {
		addressesStrings[i] = a.String()
	}
	return addressesStrings, nil
}

func normalizeCidString(cidString string) string {
	if strings.HasPrefix(cidString, "ipfs://") {
		return "/ipfs/" + cidString[7:]
	}
	if strings.HasPrefix(cidString, "ipns://") {
		return "/ipns/" + cidString[7:]
	}
	return cidString
}

func (h *IpfsHelper) DownloadFile(ctx context.Context, cidString string, destinationDir string) (string, error) {
	cidString = normalizeCidString(cidString)
	cidPath := path.New(cidString)
	resolvedPath, err := h.api.ResolvePath(ctx, cidPath)
	if err != nil {
		return "", fmt.Errorf("failed to resolve path: %w", err)
	}
	// first pin the root node, then all its children nodes in random order to improve sharing with peers started at the same time
	if err := h.api.Pin().Add(ctx, resolvedPath, options.Pin.Recursive(false)); err != nil {
		return "", fmt.Errorf("failed to pin root path: %w", err)
	}
	links, err := h.api.Object().Links(ctx, resolvedPath)
	if err != nil {
		return "", fmt.Errorf("failed to get root links: %w", err)
	}
	log.Info("Pinning ipfs subtrees...")
	printProgress := func(done int, all int) {
		if all == 0 {
			all = 1 // avoid division by 0
			done = 1
		}
		fmt.Printf("\033[2K\rPinned %d / %d subtrees (%.2f%%)", done, all, float32(done)/float32(all)*100)
	}
	permutation := rand.Perm(len(links))
	printProgress(0, len(links))
	for i, j := range permutation {
		link := links[j]
		if err := h.api.Pin().Add(ctx, path.IpfsPath(link.Cid), options.Pin.Recursive(true)); err != nil {
			return "", fmt.Errorf("failed to pin child path: %w", err)
		}
		printProgress(i+1, len(links))
	}
	fmt.Printf("\n")
	rootNodeDirectory, err := h.api.Unixfs().Get(ctx, cidPath)
	if err != nil {
		return "", fmt.Errorf("could not get file with CID: %w", err)
	}
	log.Info("Writing file...")
	outputFilePath := filepath.Join(destinationDir, resolvedPath.Cid().String())
	_ = os.Remove(outputFilePath)
	err = files.WriteTo(rootNodeDirectory, outputFilePath)
	if err != nil {
		return "", fmt.Errorf("could not write out the fetched CID: %w", err)
	}
	log.Info("Download done.")
	return outputFilePath, nil
}

func (h *IpfsHelper) AddFile(ctx context.Context, filePath string, includeHidden bool) (path.Resolved, error) {
	fileInfo, err := os.Stat(filePath)
	if err != nil {
		return nil, err
	}
	fileNode, err := files.NewSerialFile(filePath, includeHidden, fileInfo)
	if err != nil {
		return nil, err
	}
	return h.api.Unixfs().Add(ctx, fileNode)
}

func CreateIpfsHelper(ctx context.Context, downloadPath string, clientOnly bool, peerList []string, profiles string) (*IpfsHelper, error) {
	return createIpfsHelperImpl(ctx, downloadPath, clientOnly, peerList, profiles)
}

func (h *IpfsHelper) Close() error {
	return h.node.Close()
}

func setupPlugins() error {
	plugins, err := loader.NewPluginLoader("")
	if err != nil {
		return fmt.Errorf("error loading plugins: %w", err)
	}
	// Load preloaded and external plugins
	if err := plugins.Initialize(); err != nil {
		return fmt.Errorf("error initializing plugins: %w", err)
	}
	if err := plugins.Inject(); err != nil {
		return fmt.Errorf("error initializing plugins: %w", err)
	}
	return nil
}

var loadPluginsOnce sync.Once

func createIpfsHelperImpl(ctx context.Context, downloadPath string, clientOnly bool, peerList []string, profiles string) (*IpfsHelper, error) {
	var onceErr error
	loadPluginsOnce.Do(func() {
		onceErr = setupPlugins()
	})
	if onceErr != nil {
		return nil, onceErr
	}
	client := IpfsHelper{}
	err := client.createRepo(downloadPath, profiles)
	if err != nil {
		return nil, err
	}
	err = client.createNode(ctx, clientOnly)
	if err != nil {
		return nil, err
	}
	err = client.connectToPeers(ctx, peerList)
	if err != nil {
		return nil, err
	}
	return &client, nil
}

func CanBeIpfsPath(pathString string) bool {
	path := path.New(pathString)
	return path.IsValid() == nil ||
		strings.HasPrefix(pathString, "/ipfs/") ||
		strings.HasPrefix(pathString, "/ipld/") ||
		strings.HasPrefix(pathString, "/ipns/") ||
		strings.HasPrefix(pathString, "ipfs://") ||
		strings.HasPrefix(pathString, "ipns://")
}

// TODO break abstraction for now til we figure out what fns are needed
func (h *IpfsHelper) GetAPI() coreiface.CoreAPI {
	return h.api
}

'''
'''--- cmd/ipfshelper/ipfshelper_test.go ---
package ipfshelper

import (
	"bytes"
	"context"
	"math/rand"
	"os"
	"path/filepath"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func getTempFileWithData(t *testing.T, data []byte) string {
	path := filepath.Join(t.TempDir(), "config.json")
	err := os.WriteFile(path, []byte(data), 0600)
	testhelpers.RequireImpl(t, err)
	return path
}

func fileDataEqual(t *testing.T, path string, expected []byte) bool {
	data, err := os.ReadFile(path)
	testhelpers.RequireImpl(t, err)
	return bytes.Equal(data, expected)
}

func TestIpfsHelper(t *testing.T) {
	ctx := context.Background()
	ipfsA, err := createIpfsHelperImpl(ctx, t.TempDir(), false, []string{}, "test")
	testhelpers.RequireImpl(t, err)
	// add a test file to node A
	testData := make([]byte, 1024*1024)
	_, err = rand.Read(testData)
	testhelpers.RequireImpl(t, err)
	testFile := getTempFileWithData(t, testData)
	ipfsTestFilePath, err := ipfsA.AddFile(ctx, testFile, false)
	testhelpers.RequireImpl(t, err)
	testFileCid := ipfsTestFilePath.Cid().String()
	addrsA, err := ipfsA.GetPeerHostAddresses()
	testhelpers.RequireImpl(t, err)
	// create node B connected to node A
	ipfsB, err := createIpfsHelperImpl(ctx, t.TempDir(), false, addrsA, "test")
	testhelpers.RequireImpl(t, err)
	// download the test file with node B
	downloadedFile, err := ipfsB.DownloadFile(ctx, testFileCid, t.TempDir())
	testhelpers.RequireImpl(t, err)
	if !fileDataEqual(t, downloadedFile, testData) {
		testhelpers.FailImpl(t, "Downloaded file does not contain expected data")
	}
	// clean up node A and test downloading the file from yet another node C
	err = ipfsA.Close()
	os.RemoveAll(ipfsA.repoPath)
	testhelpers.RequireImpl(t, err)
	addrsB, err := ipfsB.GetPeerHostAddresses()
	testhelpers.RequireImpl(t, err)
	ipfsC, err := createIpfsHelperImpl(ctx, t.TempDir(), false, addrsB, "test")
	testhelpers.RequireImpl(t, err)
	downloadedFile, err = ipfsC.DownloadFile(ctx, testFileCid, t.TempDir())
	testhelpers.RequireImpl(t, err)
	if !fileDataEqual(t, downloadedFile, testData) {
		testhelpers.FailImpl(t, "Downloaded file does not contain expected data")
	}
	// make sure closing B and C nodes (A already closed) will make it impossible to download the test file from new node D
	ipfsD, err := createIpfsHelperImpl(ctx, t.TempDir(), false, addrsB, "test")
	testhelpers.RequireImpl(t, err)
	err = ipfsB.Close()
	testhelpers.RequireImpl(t, err)
	err = ipfsC.Close()
	testhelpers.RequireImpl(t, err)
	testTimeout := 300 * time.Millisecond
	timeoutCtx, cancel := context.WithTimeout(ctx, testTimeout)
	defer cancel()
	_, err = ipfsD.DownloadFile(timeoutCtx, testFileCid, t.TempDir())
	if err == nil {
		testhelpers.FailImpl(t, "Download attempt did not fail as expected")
	}
	err = ipfsD.Close()
	testhelpers.RequireImpl(t, err)
}

func TestNormalizeCidString(t *testing.T) {
	for _, test := range []struct {
		input    string
		expected string
	}{
		{"ipfs://QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ", "/ipfs/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ"},
		{"ipns://k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8", "/ipns/k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8"},
		{"ipns://docs.ipfs.tech/introduction/", "/ipns/docs.ipfs.tech/introduction/"},
		{"/ipfs/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ", "/ipfs/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ"},
		{"/ipns/k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8", "/ipns/k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8"},
		{"QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ", "QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ"},
	} {
		if res := normalizeCidString(test.input); res != test.expected {
			testhelpers.FailImpl(t, "Failed to normalize cid string, input: ", test.input, " got: ", res, " expected: ", test.expected)
		}
	}
}

func TestCanBeIpfsPath(t *testing.T) {
	correctPaths := []string{
		"QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ",
		"/ipfs/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ",
		"/ipns/k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8",
		"/ipns/docs.ipfs.tech/introduction/",
		"ipfs://QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ",
		"ipns://k51qzi5uqu5dlvj2baxnqndepeb86cbk3ng7n3i46uzyxzyqj2xjonzllnv0v8",
	}
	for _, path := range correctPaths {
		if !CanBeIpfsPath(path) {
			testhelpers.FailImpl(t, "false negative result for path:", path)
		}
	}
	incorrectPaths := []string{"www.ipfs.tech", "https://www.ipfs.tech", "QmIncorrect"}
	for _, path := range incorrectPaths {
		if CanBeIpfsPath(path) {
			testhelpers.FailImpl(t, "false positive result for path:", path)
		}
	}
}

'''
'''--- cmd/nitro-val/config.go ---
package main

import (
	"fmt"
	"reflect"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/p2p"
	"github.com/ethereum/go-ethereum/p2p/nat"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/cmd/conf"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/validator/valnode"
	flag "github.com/spf13/pflag"
)

type ValidationNodeConfig struct {
	Conf          genericconf.ConfConfig          `koanf:"conf" reload:"hot"`
	Validation    valnode.Config                  `koanf:"validation" reload:"hot"`
	LogLevel      int                             `koanf:"log-level" reload:"hot"`
	LogType       string                          `koanf:"log-type" reload:"hot"`
	FileLogging   genericconf.FileLoggingConfig   `koanf:"file-logging" reload:"hot"`
	Persistent    conf.PersistentConfig           `koanf:"persistent"`
	HTTP          genericconf.HTTPConfig          `koanf:"http"`
	WS            genericconf.WSConfig            `koanf:"ws"`
	IPC           genericconf.IPCConfig           `koanf:"ipc"`
	Auth          genericconf.AuthRPCConfig       `koanf:"auth"`
	Metrics       bool                            `koanf:"metrics"`
	MetricsServer genericconf.MetricsServerConfig `koanf:"metrics-server"`
	PProf         bool                            `koanf:"pprof"`
	PprofCfg      genericconf.PProf               `koanf:"pprof-cfg"`
	Workdir       string                          `koanf:"workdir" reload:"hot"`
}

var HTTPConfigDefault = genericconf.HTTPConfig{
	Addr:           "",
	Port:           genericconf.HTTPConfigDefault.Port,
	API:            []string{},
	RPCPrefix:      genericconf.HTTPConfigDefault.RPCPrefix,
	CORSDomain:     genericconf.HTTPConfigDefault.CORSDomain,
	VHosts:         genericconf.HTTPConfigDefault.VHosts,
	ServerTimeouts: genericconf.HTTPConfigDefault.ServerTimeouts,
}

var WSConfigDefault = genericconf.WSConfig{
	Addr:      "",
	Port:      genericconf.WSConfigDefault.Port,
	API:       []string{},
	RPCPrefix: genericconf.WSConfigDefault.RPCPrefix,
	Origins:   genericconf.WSConfigDefault.Origins,
	ExposeAll: genericconf.WSConfigDefault.ExposeAll,
}

var IPCConfigDefault = genericconf.IPCConfig{
	Path: "",
}

var ValidationNodeConfigDefault = ValidationNodeConfig{
	Conf:          genericconf.ConfConfigDefault,
	LogLevel:      int(log.LvlInfo),
	LogType:       "plaintext",
	Persistent:    conf.PersistentConfigDefault,
	HTTP:          HTTPConfigDefault,
	WS:            WSConfigDefault,
	IPC:           IPCConfigDefault,
	Auth:          genericconf.AuthRPCConfigDefault,
	Metrics:       false,
	MetricsServer: genericconf.MetricsServerConfigDefault,
	PProf:         false,
	PprofCfg:      genericconf.PProfDefault,
	Workdir:       "",
}

func ValidationNodeConfigAddOptions(f *flag.FlagSet) {
	genericconf.ConfConfigAddOptions("conf", f)
	valnode.ValidationConfigAddOptions("validation", f)
	f.Int("log-level", ValidationNodeConfigDefault.LogLevel, "log level")
	f.String("log-type", ValidationNodeConfigDefault.LogType, "log type (plaintext or json)")
	genericconf.FileLoggingConfigAddOptions("file-logging", f)
	conf.PersistentConfigAddOptions("persistent", f)
	genericconf.HTTPConfigAddOptions("http", f)
	genericconf.WSConfigAddOptions("ws", f)
	genericconf.IPCConfigAddOptions("ipc", f)
	genericconf.AuthRPCConfigAddOptions("auth", f)
	f.Bool("metrics", ValidationNodeConfigDefault.Metrics, "enable metrics")
	genericconf.MetricsServerAddOptions("metrics-server", f)
	f.Bool("pprof", ValidationNodeConfigDefault.PProf, "enable pprof")
	genericconf.PProfAddOptions("pprof-cfg", f)
	f.String("workdir", ValidationNodeConfigDefault.Workdir, "path used for purpose of resolving relative paths (ia. jwt secret file, log files), if empty then current working directory will be used.")
}

func (c *ValidationNodeConfig) ResolveDirectoryNames() error {
	err := c.Persistent.ResolveDirectoryNames()
	if err != nil {
		return err
	}

	return nil
}

func (c *ValidationNodeConfig) ShallowClone() *ValidationNodeConfig {
	config := &ValidationNodeConfig{}
	*config = *c
	return config
}

func (c *ValidationNodeConfig) CanReload(new *ValidationNodeConfig) error {
	var check func(node, other reflect.Value, path string)
	var err error

	check = func(node, value reflect.Value, path string) {
		if node.Kind() != reflect.Struct {
			return
		}

		for i := 0; i < node.NumField(); i++ {
			fieldTy := node.Type().Field(i)
			if !fieldTy.IsExported() {
				continue
			}
			hot := fieldTy.Tag.Get("reload") == "hot"
			dot := path + "." + fieldTy.Name

			first := node.Field(i).Interface()
			other := value.Field(i).Interface()

			if !hot && !reflect.DeepEqual(first, other) {
				err = fmt.Errorf("illegal change to %v%v%v", colors.Red, dot, colors.Clear)
			} else {
				check(node.Field(i), value.Field(i), dot)
			}
		}
	}

	check(reflect.ValueOf(c).Elem(), reflect.ValueOf(new).Elem(), "config")
	return err
}

func (c *ValidationNodeConfig) GetReloadInterval() time.Duration {
	return c.Conf.ReloadInterval
}

func (c *ValidationNodeConfig) Validate() error {
	// TODO
	return nil
}

var DefaultValidationNodeStackConfig = node.Config{
	DataDir:             node.DefaultDataDir(),
	HTTPPort:            node.DefaultHTTPPort,
	AuthAddr:            node.DefaultAuthHost,
	AuthPort:            node.DefaultAuthPort,
	AuthVirtualHosts:    node.DefaultAuthVhosts,
	HTTPModules:         []string{""},
	HTTPVirtualHosts:    []string{"localhost"},
	HTTPTimeouts:        rpc.DefaultHTTPTimeouts,
	WSPort:              node.DefaultWSPort,
	WSModules:           []string{"validation"},
	GraphQLVirtualHosts: []string{"localhost"},
	P2P: p2p.Config{
		ListenAddr: ":30303",
		MaxPeers:   50,
		NAT:        nat.Any(),
	},
}

'''
'''--- cmd/nitro-val/nitro_val.go ---
package main

import (
	"context"
	"fmt"
	_ "net/http/pprof" // #nosec G108
	"os"
	"os/signal"
	"path/filepath"
	"syscall"

	flag "github.com/spf13/pflag"

	_ "github.com/ethereum/go-ethereum/eth/tracers/js"
	_ "github.com/ethereum/go-ethereum/eth/tracers/native"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/metrics/exp"
	"github.com/ethereum/go-ethereum/node"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	_ "github.com/offchainlabs/nitro/nodeInterface"
	"github.com/offchainlabs/nitro/validator/valnode"
)

func printSampleUsage(name string) {
	fmt.Printf("Sample usage: %s --help \n", name)
}

func main() {
	os.Exit(mainImpl())
}

// Checks metrics and PProf flag, runs them if enabled.
// Note: they are separate so one can enable/disable them as they wish, the only
// requirement is that they can't run on the same address and port.
func startMetrics(cfg *ValidationNodeConfig) error {
	mAddr := fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port)
	pAddr := fmt.Sprintf("%v:%v", cfg.PprofCfg.Addr, cfg.PprofCfg.Port)
	if cfg.Metrics && !metrics.Enabled {
		return fmt.Errorf("metrics must be enabled via command line by adding --metrics, json config has no effect")
	}
	if cfg.Metrics && cfg.PProf && mAddr == pAddr {
		return fmt.Errorf("metrics and pprof cannot be enabled on the same address:port: %s", mAddr)
	}
	if cfg.Metrics {
		go metrics.CollectProcessMetrics(cfg.MetricsServer.UpdateInterval)
		exp.Setup(fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port))
	}
	if cfg.PProf {
		genericconf.StartPprof(pAddr)
	}
	return nil
}

// Returns the exit code
func mainImpl() int {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()

	args := os.Args[1:]
	nodeConfig, err := ParseNode(ctx, args)
	if err != nil {
		confighelpers.PrintErrorAndExit(err, printSampleUsage)
	}
	stackConf := DefaultValidationNodeStackConfig
	stackConf.DataDir = "" // ephemeral
	nodeConfig.HTTP.Apply(&stackConf)
	nodeConfig.WS.Apply(&stackConf)
	nodeConfig.Auth.Apply(&stackConf)
	nodeConfig.IPC.Apply(&stackConf)
	stackConf.P2P.ListenAddr = ""
	stackConf.P2P.NoDial = true
	stackConf.P2P.NoDiscovery = true
	vcsRevision, strippedRevision, vcsTime := confighelpers.GetVersion()
	stackConf.Version = strippedRevision

	pathResolver := func(workdir string) func(string) string {
		if workdir == "" {
			workdir, err = os.Getwd()
			if err != nil {
				log.Warn("Failed to get workdir", "err", err)
			}
		}
		return func(path string) string {
			if filepath.IsAbs(path) {
				return path
			}
			return filepath.Join(workdir, path)
		}
	}

	err = genericconf.InitLog(nodeConfig.LogType, log.Lvl(nodeConfig.LogLevel), &nodeConfig.FileLogging, pathResolver(nodeConfig.Persistent.LogDir))
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error initializing logging: %v\n", err)
		return 1
	}
	if stackConf.JWTSecret == "" && stackConf.AuthAddr != "" {
		filename := pathResolver(nodeConfig.Persistent.GlobalConfig)("jwtsecret")
		if err := genericconf.TryCreatingJWTSecret(filename); err != nil {
			log.Error("Failed to prepare jwt secret file", "err", err)
			return 1
		}
		stackConf.JWTSecret = filename
	}

	log.Info("Running Arbitrum nitro validation node", "revision", vcsRevision, "vcs.time", vcsTime)

	liveNodeConfig := genericconf.NewLiveConfig[*ValidationNodeConfig](args, nodeConfig, ParseNode)
	liveNodeConfig.SetOnReloadHook(func(oldCfg *ValidationNodeConfig, newCfg *ValidationNodeConfig) error {

		return genericconf.InitLog(newCfg.LogType, log.Lvl(newCfg.LogLevel), &newCfg.FileLogging, pathResolver(nodeConfig.Persistent.LogDir))
	})

	valnode.EnsureValidationExposedViaAuthRPC(&stackConf)

	stack, err := node.New(&stackConf)
	if err != nil {
		flag.Usage()
		log.Crit("failed to initialize geth stack", "err", err)
	}

	if err := startMetrics(nodeConfig); err != nil {
		log.Error("Error starting metrics", "error", err)
		return 1
	}

	fatalErrChan := make(chan error, 10)

	valNode, err := valnode.CreateValidationNode(
		func() *valnode.Config { return &liveNodeConfig.Get().Validation },
		stack,
		fatalErrChan,
	)
	if err != nil {
		log.Error("couldn't init validation node", "err", err)
		return 1
	}

	err = valNode.Start(ctx)
	if err != nil {
		log.Error("error starting validator node", "err", err)
		return 1
	}
	err = stack.Start()
	if err != nil {
		fatalErrChan <- fmt.Errorf("error starting stack: %w", err)
	}
	defer stack.Close()

	liveNodeConfig.Start(ctx)
	defer liveNodeConfig.StopAndWait()

	sigint := make(chan os.Signal, 1)
	signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)

	exitCode := 0
	select {
	case err := <-fatalErrChan:
		log.Error("shutting down due to fatal error", "err", err)
		defer log.Error("shut down due to fatal error", "err", err)
		exitCode = 1
	case <-sigint:
		log.Info("shutting down because of sigint")
	}

	// cause future ctrl+c's to panic
	close(sigint)

	return exitCode
}

func ParseNode(ctx context.Context, args []string) (*ValidationNodeConfig, error) {
	f := flag.NewFlagSet("", flag.ContinueOnError)

	ValidationNodeConfigAddOptions(f)

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	err = confighelpers.ApplyOverrides(f, k)
	if err != nil {
		return nil, err
	}

	var nodeConfig ValidationNodeConfig
	if err := confighelpers.EndCommonParse(k, &nodeConfig); err != nil {
		return nil, err
	}

	// Don't print wallet passwords
	if nodeConfig.Conf.Dump {
		err = confighelpers.DumpConfig(k, map[string]interface{}{
			"l1.wallet.password":        "",
			"l1.wallet.private-key":     "",
			"l2.dev-wallet.password":    "",
			"l2.dev-wallet.private-key": "",
		})
		if err != nil {
			return nil, err
		}
	}

	// Don't pass around wallet contents with normal configuration

	err = nodeConfig.Validate()
	if err != nil {
		return nil, err
	}
	return &nodeConfig, nil
}

'''
'''--- cmd/nitro/config_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"reflect"
	"strings"
	"syscall"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"

	"github.com/r3labs/diff/v3"
	flag "github.com/spf13/pflag"
)

func TestEmptyCliConfig(t *testing.T) {
	f := flag.NewFlagSet("", flag.ContinueOnError)
	NodeConfigAddOptions(f)
	k, err := confighelpers.BeginCommonParse(f, []string{})
	Require(t, err)
	var emptyCliNodeConfig NodeConfig
	err = confighelpers.EndCommonParse(k, &emptyCliNodeConfig)
	Require(t, err)
	if !reflect.DeepEqual(emptyCliNodeConfig, NodeConfigDefault) {
		changelog, err := diff.Diff(emptyCliNodeConfig, NodeConfigDefault)
		Require(t, err)
		Fail(t, "empty cli config differs from expected default", changelog)
	}
}

func TestSeqConfig(t *testing.T) {
	args := strings.Split("--persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --chain.id 421613 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.sequencer --execution.sequencer.enable --node.feed.output.enable --node.feed.output.port 9642", " ")
	_, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)
}

func TestUnsafeStakerConfig(t *testing.T) {
	args := strings.Split("--persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --chain.id 421613 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.staker.enable --node.staker.strategy MakeNodes --node.staker.staker-interval 10s --execution.forwarding-target null --node.staker.dangerous.without-block-validator", " ")
	_, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)
}

func TestValidatorConfig(t *testing.T) {
	args := strings.Split("--persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --chain.id 421613 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.staker.enable --node.staker.strategy MakeNodes --node.staker.staker-interval 10s --execution.forwarding-target null", " ")
	_, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)
}

func TestAggregatorConfig(t *testing.T) {
	args := strings.Split("--persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --chain.id 421613 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.sequencer --execution.sequencer.enable --node.feed.output.enable --node.feed.output.port 9642 --node.data-availability.enable --node.data-availability.rpc-aggregator.backends {[\"url\":\"http://localhost:8547\",\"pubkey\":\"abc==\",\"signerMask\":0x1]}", " ")
	_, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)
}

func TestReloads(t *testing.T) {
	var check func(node reflect.Value, cold bool, path string)
	check = func(node reflect.Value, cold bool, path string) {
		if node.Kind() != reflect.Struct {
			return
		}

		for i := 0; i < node.NumField(); i++ {
			hot := node.Type().Field(i).Tag.Get("reload") == "hot"
			dot := path + "." + node.Type().Field(i).Name
			if hot && cold {
				t.Fatalf(fmt.Sprintf(
					"Option %v%v%v is reloadable but %v%v%v is not",
					colors.Red, dot, colors.Clear,
					colors.Red, path, colors.Clear,
				))
			}
			if hot {
				colors.PrintBlue(dot)
			}
			check(node.Field(i), !hot, dot)
		}
	}

	config := NodeConfigDefault
	update := NodeConfigDefault
	update.Node.BatchPoster.MaxSize++

	check(reflect.ValueOf(config), false, "config")
	Require(t, config.CanReload(&config))
	Require(t, config.CanReload(&update))

	testUnsafe := func() {
		t.Helper()
		if config.CanReload(&update) == nil {
			Fail(t, "failed to detect unsafe reload")
		}
		update = NodeConfigDefault
	}

	// check that non-reloadable fields fail assignment
	update.Metrics = !update.Metrics
	testUnsafe()
	update.ParentChain.ID++
	testUnsafe()
	update.Node.Staker.Enable = !update.Node.Staker.Enable
	testUnsafe()
}

func TestLiveNodeConfig(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// create a config file
	configFile := filepath.Join(t.TempDir(), "config.json")
	jsonConfig := "{\"chain\":{\"id\":421613}}"
	Require(t, WriteToConfigFile(configFile, jsonConfig))

	args := strings.Split("--file-logging.enable=false --persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.sequencer --execution.sequencer.enable --node.feed.output.enable --node.feed.output.port 9642", " ")
	args = append(args, []string{"--conf.file", configFile}...)
	config, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)

	liveConfig := genericconf.NewLiveConfig[*NodeConfig](args, config, func(ctx context.Context, args []string) (*NodeConfig, error) {
		nodeConfig, _, _, err := ParseNode(ctx, args)
		return nodeConfig, err
	})

	// check updating the config
	update := config.ShallowClone()
	expected := config.ShallowClone()
	update.Node.BatchPoster.MaxSize += 100
	expected.Node.BatchPoster.MaxSize += 100
	Require(t, liveConfig.Set(update))
	if !reflect.DeepEqual(liveConfig.Get(), expected) {
		Fail(t, "failed to set config")
	}

	// check that an invalid reload gets rejected
	update = config.ShallowClone()
	update.ParentChain.ID++
	if liveConfig.Set(update) == nil {
		Fail(t, "failed to reject invalid update")
	}
	if !reflect.DeepEqual(liveConfig.Get(), expected) {
		Fail(t, "config should not change if its update fails")
	}

	// starting the LiveConfig after testing LiveConfig.set to avoid race condition in the test
	liveConfig.Start(ctx)

	// reload config
	expected = config.ShallowClone()
	Require(t, syscall.Kill(syscall.Getpid(), syscall.SIGUSR1))
	if !PollLiveConfigUntilEqual(liveConfig, expected) {
		Fail(t, "live config differs from expected")
	}

	// check that reloading the config again doesn't change anything
	Require(t, syscall.Kill(syscall.Getpid(), syscall.SIGUSR1))
	time.Sleep(80 * time.Millisecond)
	if !reflect.DeepEqual(liveConfig.Get(), expected) {
		Fail(t, "live config differs from expected")
	}

	// change the config file
	expected = config.ShallowClone()
	expected.Node.BatchPoster.MaxSize += 100
	jsonConfig = fmt.Sprintf("{\"node\":{\"batch-poster\":{\"max-size\":\"%d\"}}, \"chain\":{\"id\":421613}}", expected.Node.BatchPoster.MaxSize)
	Require(t, WriteToConfigFile(configFile, jsonConfig))

	// trigger LiveConfig reload
	Require(t, syscall.Kill(syscall.Getpid(), syscall.SIGUSR1))

	if !PollLiveConfigUntilEqual(liveConfig, expected) {
		Fail(t, "failed to update config", config.Node.BatchPoster.MaxSize, update.Node.BatchPoster.MaxSize)
	}

	// change chain.id in the config file (currently non-reloadable)
	jsonConfig = fmt.Sprintf("{\"node\":{\"batch-poster\":{\"max-size\":\"%d\"}}, \"chain\":{\"id\":421703}}", expected.Node.BatchPoster.MaxSize)
	Require(t, WriteToConfigFile(configFile, jsonConfig))

	// trigger LiveConfig reload
	Require(t, syscall.Kill(syscall.Getpid(), syscall.SIGUSR1))

	if PollLiveConfigUntilNotEqual(liveConfig, expected) {
		Fail(t, "failed to reject invalid update")
	}
}

func TestPeriodicReloadOfLiveNodeConfig(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// create config file with ReloadInterval = 20 ms
	configFile := filepath.Join(t.TempDir(), "config.json")
	jsonConfig := "{\"conf\":{\"reload-interval\":\"20ms\"}}"
	Require(t, WriteToConfigFile(configFile, jsonConfig))

	args := strings.Split("--persistent.chain /tmp/data --init.dev-init --node.parent-chain-reader.enable=false --parent-chain.id 5 --chain.id 421613 --parent-chain.wallet.pathname /l1keystore --parent-chain.wallet.password passphrase --http.addr 0.0.0.0 --ws.addr 0.0.0.0 --node.sequencer --execution.sequencer.enable --node.feed.output.enable --node.feed.output.port 9642", " ")
	args = append(args, []string{"--conf.file", configFile}...)
	config, _, _, err := ParseNode(context.Background(), args)
	Require(t, err)

	liveConfig := genericconf.NewLiveConfig[*NodeConfig](args, config, func(ctx context.Context, args []string) (*NodeConfig, error) {
		nodeConfig, _, _, err := ParseNode(ctx, args)
		return nodeConfig, err
	})
	liveConfig.Start(ctx)

	// test if periodic reload works
	expected := config.ShallowClone()
	expected.Conf.ReloadInterval = 0
	jsonConfig = "{\"conf\":{\"reload-interval\":\"0\"}}"
	Require(t, WriteToConfigFile(configFile, jsonConfig))
	start := time.Now()
	if !PollLiveConfigUntilEqual(liveConfig, expected) {
		Fail(t, fmt.Sprintf("failed to update config after %d ms, while reload interval is %s", time.Since(start).Milliseconds(), config.Conf.ReloadInterval))
	}

	// test if previous config successfully disabled periodic reload
	expected = config.ShallowClone()
	expected.Conf.ReloadInterval = 10 * time.Millisecond
	jsonConfig = "{\"conf\":{\"reload-interval\":\"10ms\"}}"
	Require(t, WriteToConfigFile(configFile, jsonConfig))
	time.Sleep(80 * time.Millisecond)
	if reflect.DeepEqual(liveConfig.Get(), expected) {
		Fail(t, "failed to disable periodic reload")
	}
}

func WriteToConfigFile(path string, jsonConfig string) error {
	return os.WriteFile(path, []byte(jsonConfig), 0600)
}

func PollLiveConfigUntilEqual(liveConfig *genericconf.LiveConfig[*NodeConfig], expected *NodeConfig) bool {
	return PollLiveConfig(liveConfig, expected, true)
}
func PollLiveConfigUntilNotEqual(liveConfig *genericconf.LiveConfig[*NodeConfig], expected *NodeConfig) bool {
	return PollLiveConfig(liveConfig, expected, false)
}

func PollLiveConfig(liveConfig *genericconf.LiveConfig[*NodeConfig], expected *NodeConfig, equal bool) bool {
	for i := 0; i < 16; i++ {
		if reflect.DeepEqual(liveConfig.Get(), expected) == equal {
			return true
		}
		time.Sleep(10 * time.Millisecond)
	}
	return false
}

func Require(t *testing.T, err error, text ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, text...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- cmd/nitro/init.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"math/big"
	"os"
	"reflect"
	"regexp"
	"runtime"
	"strings"
	"sync"
	"time"

	"github.com/offchainlabs/nitro/cmd/util"

	"github.com/cavaliergopher/grab/v3"
	extract "github.com/codeclysm/extract/v3"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state/pruner"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/cmd/ipfshelper"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/statetransfer"
	"github.com/spf13/pflag"
)

type InitConfig struct {
	Force           bool          `koanf:"force"`
	Url             string        `koanf:"url"`
	DownloadPath    string        `koanf:"download-path"`
	DownloadPoll    time.Duration `koanf:"download-poll"`
	DevInit         bool          `koanf:"dev-init"`
	DevInitAddress  string        `koanf:"dev-init-address"`
	DevInitBlockNum uint64        `koanf:"dev-init-blocknum"`
	Empty           bool          `koanf:"empty"`
	AccountsPerSync uint          `koanf:"accounts-per-sync"`
	ImportFile      string        `koanf:"import-file"`
	ThenQuit        bool          `koanf:"then-quit"`
	Prune           string        `koanf:"prune"`
	PruneBloomSize  uint64        `koanf:"prune-bloom-size"`
	ResetToMessage  int64         `koanf:"reset-to-message"`
}

var InitConfigDefault = InitConfig{
	Force:           false,
	Url:             "",
	DownloadPath:    "/tmp/",
	DownloadPoll:    time.Minute,
	DevInit:         false,
	DevInitAddress:  "",
	DevInitBlockNum: 0,
	ImportFile:      "",
	AccountsPerSync: 100000,
	ThenQuit:        false,
	Prune:           "",
	PruneBloomSize:  2048,
	ResetToMessage:  -1,
}

func InitConfigAddOptions(prefix string, f *pflag.FlagSet) {
	f.Bool(prefix+".force", InitConfigDefault.Force, "if true: in case database exists init code will be reexecuted and genesis block compared to database")
	f.String(prefix+".url", InitConfigDefault.Url, "url to download initializtion data - will poll if download fails")
	f.String(prefix+".download-path", InitConfigDefault.DownloadPath, "path to save temp downloaded file")
	f.Duration(prefix+".download-poll", InitConfigDefault.DownloadPoll, "how long to wait between polling attempts")
	f.Bool(prefix+".dev-init", InitConfigDefault.DevInit, "init with dev data (1 account with balance) instead of file import")
	f.String(prefix+".dev-init-address", InitConfigDefault.DevInitAddress, "Address of dev-account. Leave empty to use the dev-wallet.")
	f.Uint64(prefix+".dev-init-blocknum", InitConfigDefault.DevInitBlockNum, "Number of preinit blocks. Must exist in ancient database.")
	f.Bool(prefix+".empty", InitConfigDefault.Empty, "init with empty state")
	f.Bool(prefix+".then-quit", InitConfigDefault.ThenQuit, "quit after init is done")
	f.String(prefix+".import-file", InitConfigDefault.ImportFile, "path for json data to import")
	f.Uint(prefix+".accounts-per-sync", InitConfigDefault.AccountsPerSync, "during init - sync database every X accounts. Lower value for low-memory systems. 0 disables.")
	f.String(prefix+".prune", InitConfigDefault.Prune, "pruning for a given use: \"full\" for full nodes serving RPC requests, or \"validator\" for validators")
	f.Uint64(prefix+".prune-bloom-size", InitConfigDefault.PruneBloomSize, "the amount of memory in megabytes to use for the pruning bloom filter (higher values prune better)")
	f.Int64(prefix+".reset-to-message", InitConfigDefault.ResetToMessage, "forces a reset to an old message height. Also set max-reorg-resequence-depth=0 to force re-reading messages")
}

func downloadInit(ctx context.Context, initConfig *InitConfig) (string, error) {
	if initConfig.Url == "" {
		return "", nil
	}
	if strings.HasPrefix(initConfig.Url, "file:") {
		return initConfig.Url[5:], nil
	}
	if ipfshelper.CanBeIpfsPath(initConfig.Url) {
		ipfsNode, err := ipfshelper.CreateIpfsHelper(ctx, initConfig.DownloadPath, false, []string{}, ipfshelper.DefaultIpfsProfiles)
		if err != nil {
			return "", err
		}
		log.Info("Downloading initial database via IPFS", "url", initConfig.Url)
		initFile, downloadErr := ipfsNode.DownloadFile(ctx, initConfig.Url, initConfig.DownloadPath)
		closeErr := ipfsNode.Close()
		if downloadErr != nil {
			if closeErr != nil {
				log.Error("Failed to close IPFS node after download error", "err", closeErr)
			}
			return "", fmt.Errorf("Failed to download file from IPFS: %w", downloadErr)
		}
		if closeErr != nil {
			return "", fmt.Errorf("Failed to close IPFS node: %w", err)
		}
		return initFile, nil
	}
	grabclient := grab.NewClient()
	log.Info("Downloading initial database", "url", initConfig.Url)
	fmt.Println()
	printTicker := time.NewTicker(time.Second)
	defer printTicker.Stop()
	attempt := 0
	for {
		attempt++
		req, err := grab.NewRequest(initConfig.DownloadPath, initConfig.Url)
		if err != nil {
			panic(err)
		}
		resp := grabclient.Do(req.WithContext(ctx))
		firstPrintTime := time.Now().Add(time.Second * 2)
	updateLoop:
		for {
			select {
			case <-printTicker.C:
				if time.Now().After(firstPrintTime) {
					bps := resp.BytesPerSecond()
					if bps == 0 {
						bps = 1 // avoid division by zero
					}
					done := resp.BytesComplete()
					total := resp.Size()
					timeRemaining := time.Second * (time.Duration(total-done) / time.Duration(bps))
					timeRemaining = timeRemaining.Truncate(time.Millisecond * 10)
					fmt.Printf("\033[2K\r  transferred %v / %v bytes (%.2f%%) [%.2fMbps, %s remaining]",
						done,
						total,
						resp.Progress()*100,
						bps*8/1000000,
						timeRemaining.String())
				}
			case <-resp.Done:
				if err := resp.Err(); err != nil {
					fmt.Printf("\n  attempt %d failed: %v\n", attempt, err)
					break updateLoop
				}
				fmt.Printf("\n")
				log.Info("Download done", "filename", resp.Filename, "duration", resp.Duration())
				fmt.Println()
				return resp.Filename, nil
			case <-ctx.Done():
				return "", ctx.Err()
			}
		}
		select {
		case <-ctx.Done():
			return "", ctx.Err()
		case <-time.After(initConfig.DownloadPoll):
		}
	}
}

func validateBlockChain(blockChain *core.BlockChain, chainConfig *params.ChainConfig) error {
	statedb, err := blockChain.State()
	if err != nil {
		return err
	}
	currentArbosState, err := arbosState.OpenSystemArbosState(statedb, nil, true)
	if err != nil {
		return err
	}
	chainId, err := currentArbosState.ChainId()
	if err != nil {
		return err
	}
	if chainId.Cmp(chainConfig.ChainID) != 0 {
		return fmt.Errorf("attempted to launch node with chain ID %v on ArbOS state with chain ID %v", chainConfig.ChainID, chainId)
	}
	oldSerializedConfig, err := currentArbosState.ChainConfig()
	if err != nil {
		return fmt.Errorf("failed to get old chain config from ArbOS state: %w", err)
	}
	if len(oldSerializedConfig) != 0 {
		var oldConfig params.ChainConfig
		err = json.Unmarshal(oldSerializedConfig, &oldConfig)
		if err != nil {
			return fmt.Errorf("failed to deserialize old chain config: %w", err)
		}
		currentBlock := blockChain.CurrentBlock()
		if currentBlock == nil {
			return errors.New("failed to get current block")
		}
		if err := oldConfig.CheckCompatible(chainConfig, currentBlock.Number.Uint64(), currentBlock.Time); err != nil {
			return fmt.Errorf("invalid chain config, not compatible with previous: %w", err)
		}
	}

	return nil
}

type importantRoots struct {
	chainDb ethdb.Database
	roots   []common.Hash
	heights []uint64
}

// The minimum block distance between two important roots
const minRootDistance = 2000

// Marks a header as important, and records its root and height.
// If overwrite is true, it'll remove any future roots and replace them with this header.
// If overwrite is false, it'll ignore this header if it has future roots.
func (r *importantRoots) addHeader(header *types.Header, overwrite bool) error {
	targetBlockNum := header.Number.Uint64()
	for {
		if header == nil || header.Root == (common.Hash{}) {
			log.Error("missing state of pruning target", "blockNum", targetBlockNum)
			return nil
		}
		exists, err := r.chainDb.Has(header.Root.Bytes())
		if err != nil {
			return err
		}
		if exists {
			break
		}
		num := header.Number.Uint64()
		if num%3000 == 0 {
			log.Info("looking for old block with state to keep", "current", num, "target", targetBlockNum)
		}
		// An underflow is fine here because it'll just return nil due to not found
		header = rawdb.ReadHeader(r.chainDb, header.ParentHash, num-1)
	}
	height := header.Number.Uint64()
	for len(r.heights) > 0 && r.heights[len(r.heights)-1] > height {
		if !overwrite {
			return nil
		}
		r.roots = r.roots[:len(r.roots)-1]
		r.heights = r.heights[:len(r.heights)-1]
	}
	if len(r.heights) > 0 && r.heights[len(r.heights)-1]+minRootDistance > height {
		return nil
	}
	r.roots = append(r.roots, header.Root)
	r.heights = append(r.heights, height)
	return nil
}

var hashListRegex = regexp.MustCompile("^(0x)?[0-9a-fA-F]{64}(,(0x)?[0-9a-fA-F]{64})*$")

// Finds important roots to retain while proving
func findImportantRoots(ctx context.Context, chainDb ethdb.Database, stack *node.Node, nodeConfig *NodeConfig, cacheConfig *core.CacheConfig, l1Client arbutil.L1Interface, rollupAddrs chaininfo.RollupAddresses) ([]common.Hash, error) {
	initConfig := &nodeConfig.Init
	chainConfig := gethexec.TryReadStoredChainConfig(chainDb)
	if chainConfig == nil {
		return nil, errors.New("database doesn't have a chain config (was this node initialized?)")
	}
	arbDb, err := stack.OpenDatabase("arbitrumdata", 0, 0, "", true)
	if err != nil {
		return nil, err
	}
	defer func() {
		err := arbDb.Close()
		if err != nil {
			log.Warn("failed to close arbitrum database after finding pruning targets", "err", err)
		}
	}()
	roots := importantRoots{
		chainDb: chainDb,
	}
	genesisNum := chainConfig.ArbitrumChainParams.GenesisBlockNum
	genesisHash := rawdb.ReadCanonicalHash(chainDb, genesisNum)
	genesisHeader := rawdb.ReadHeader(chainDb, genesisHash, genesisNum)
	if genesisHeader == nil {
		return nil, errors.New("missing L2 genesis block header")
	}
	err = roots.addHeader(genesisHeader, false)
	if err != nil {
		return nil, err
	}
	if initConfig.Prune == "validator" {
		if l1Client == nil || reflect.ValueOf(l1Client).IsNil() {
			return nil, errors.New("an L1 connection is required for validator pruning")
		}
		callOpts := bind.CallOpts{
			Context:     ctx,
			BlockNumber: big.NewInt(int64(rpc.FinalizedBlockNumber)),
		}
		rollup, err := staker.NewRollupWatcher(rollupAddrs.Rollup, l1Client, callOpts)
		if err != nil {
			return nil, err
		}
		latestConfirmedNum, err := rollup.LatestConfirmed(&callOpts)
		if err != nil {
			return nil, err
		}
		latestConfirmedNode, err := rollup.LookupNode(ctx, latestConfirmedNum)
		if err != nil {
			return nil, err
		}
		confirmedHash := latestConfirmedNode.Assertion.AfterState.GlobalState.BlockHash
		confirmedNumber := rawdb.ReadHeaderNumber(chainDb, confirmedHash)
		var confirmedHeader *types.Header
		if confirmedNumber != nil {
			confirmedHeader = rawdb.ReadHeader(chainDb, confirmedHash, *confirmedNumber)
		}
		if confirmedHeader != nil {
			err = roots.addHeader(confirmedHeader, false)
			if err != nil {
				return nil, err
			}
		} else {
			log.Warn("missing latest confirmed block", "hash", confirmedHash)
		}

		validatorDb := rawdb.NewTable(arbDb, storage.BlockValidatorPrefix)
		lastValidated, err := staker.ReadLastValidatedInfo(validatorDb)
		if err != nil {
			return nil, err
		}
		if lastValidated != nil {
			var lastValidatedHeader *types.Header
			headerNum := rawdb.ReadHeaderNumber(chainDb, lastValidated.GlobalState.BlockHash)
			if headerNum != nil {
				lastValidatedHeader = rawdb.ReadHeader(chainDb, lastValidated.GlobalState.BlockHash, *headerNum)
			}
			if lastValidatedHeader != nil {
				err = roots.addHeader(lastValidatedHeader, false)
				if err != nil {
					return nil, err
				}
			} else {
				log.Warn("missing latest validated block", "hash", lastValidated.GlobalState.BlockHash)
			}
		}
	} else if initConfig.Prune == "full" {
		if nodeConfig.Node.ValidatorRequired() {
			return nil, errors.New("refusing to prune to full-node level when validator is enabled (you should prune in validator mode)")
		}
	} else if hashListRegex.MatchString(initConfig.Prune) {
		parts := strings.Split(initConfig.Prune, ",")
		roots := []common.Hash{genesisHeader.Root}
		for _, part := range parts {
			root := common.HexToHash(part)
			if root == genesisHeader.Root {
				// This was already included in the builtin list
				continue
			}
			roots = append(roots, root)
		}
		return roots, nil
	} else {
		return nil, fmt.Errorf("unknown pruning mode: \"%v\"", initConfig.Prune)
	}
	if l1Client != nil {
		// Find the latest finalized block and add it as a pruning target
		l1Block, err := l1Client.BlockByNumber(ctx, big.NewInt(int64(rpc.FinalizedBlockNumber)))
		if err != nil {
			return nil, fmt.Errorf("failed to get finalized block: %w", err)
		}
		l1BlockNum := l1Block.NumberU64()
		tracker, err := arbnode.NewInboxTracker(arbDb, nil, nil)
		if err != nil {
			return nil, err
		}
		batch, err := tracker.GetBatchCount()
		if err != nil {
			return nil, err
		}
		for {
			if ctx.Err() != nil {
				return nil, ctx.Err()
			}
			if batch == 0 {
				// No batch has been finalized
				break
			}
			batch -= 1
			meta, err := tracker.GetBatchMetadata(batch)
			if err != nil {
				return nil, err
			}
			if meta.ParentChainBlock <= l1BlockNum {
				signedBlockNum := arbutil.MessageCountToBlockNumber(meta.MessageCount, genesisNum)
				blockNum := uint64(signedBlockNum)
				l2Hash := rawdb.ReadCanonicalHash(chainDb, blockNum)
				l2Header := rawdb.ReadHeader(chainDb, l2Hash, blockNum)
				if l2Header == nil {
					log.Warn("latest finalized L2 block is unknown", "blockNum", signedBlockNum)
					break
				}
				err = roots.addHeader(l2Header, false)
				if err != nil {
					return nil, err
				}
				break
			}
		}
	}
	roots.roots = append(roots.roots, common.Hash{}) // the latest snapshot
	log.Info("found pruning target blocks", "heights", roots.heights, "roots", roots.roots)
	return roots.roots, nil
}

func pruneChainDb(ctx context.Context, chainDb ethdb.Database, stack *node.Node, nodeConfig *NodeConfig, cacheConfig *core.CacheConfig, l1Client arbutil.L1Interface, rollupAddrs chaininfo.RollupAddresses) error {
	config := &nodeConfig.Init
	if config.Prune == "" {
		return pruner.RecoverPruning(stack.InstanceDir(), chainDb)
	}
	root, err := findImportantRoots(ctx, chainDb, stack, nodeConfig, cacheConfig, l1Client, rollupAddrs)
	if err != nil {
		return fmt.Errorf("failed to find root to retain for pruning: %w", err)
	}

	pruner, err := pruner.NewPruner(chainDb, pruner.Config{Datadir: stack.InstanceDir(), BloomSize: config.PruneBloomSize})
	if err != nil {
		return err
	}
	return pruner.Prune(root)
}

func openInitializeChainDb(ctx context.Context, stack *node.Node, config *NodeConfig, chainId *big.Int, cacheConfig *core.CacheConfig, l1Client arbutil.L1Interface, rollupAddrs chaininfo.RollupAddresses) (ethdb.Database, *core.BlockChain, error) {
	if !config.Init.Force {
		if readOnlyDb, err := stack.OpenDatabaseWithFreezer("l2chaindata", 0, 0, "", "", true); err == nil {
			if chainConfig := gethexec.TryReadStoredChainConfig(readOnlyDb); chainConfig != nil {
				readOnlyDb.Close()
				chainDb, err := stack.OpenDatabaseWithFreezer("l2chaindata", config.Execution.Caching.DatabaseCache, config.Persistent.Handles, config.Persistent.Ancient, "", false)
				if err != nil {
					return chainDb, nil, err
				}
				err = pruneChainDb(ctx, chainDb, stack, config, cacheConfig, l1Client, rollupAddrs)
				if err != nil {
					return chainDb, nil, fmt.Errorf("error pruning: %w", err)
				}
				l2BlockChain, err := gethexec.GetBlockChain(chainDb, cacheConfig, chainConfig, config.Execution.TxLookupLimit)
				if err != nil {
					return chainDb, nil, err
				}
				err = validateBlockChain(l2BlockChain, chainConfig)
				if err != nil {
					return chainDb, l2BlockChain, err
				}
				return chainDb, l2BlockChain, nil
			}
			readOnlyDb.Close()
		}
	}

	initFile, err := downloadInit(ctx, &config.Init)
	if err != nil {
		return nil, nil, err
	}

	if initFile != "" {
		reader, err := os.Open(initFile)
		if err != nil {
			return nil, nil, fmt.Errorf("couln't open init '%v' archive: %w", initFile, err)
		}
		stat, err := reader.Stat()
		if err != nil {
			return nil, nil, err
		}
		log.Info("extracting downloaded init archive", "size", fmt.Sprintf("%dMB", stat.Size()/1024/1024))
		err = extract.Archive(context.Background(), reader, stack.InstanceDir(), nil)
		if err != nil {
			return nil, nil, fmt.Errorf("couln't extract init archive '%v' err:%w", initFile, err)
		}
	}

	var initDataReader statetransfer.InitDataReader = nil

	chainDb, err := stack.OpenDatabaseWithFreezer("l2chaindata", config.Execution.Caching.DatabaseCache, config.Persistent.Handles, config.Persistent.Ancient, "", false)
	if err != nil {
		return chainDb, nil, err
	}

	if config.Init.ImportFile != "" {
		initDataReader, err = statetransfer.NewJsonInitDataReader(config.Init.ImportFile)
		if err != nil {
			return chainDb, nil, fmt.Errorf("error reading import file: %w", err)
		}
	}
	if config.Init.Empty {
		if initDataReader != nil {
			return chainDb, nil, errors.New("multiple init methods supplied")
		}
		initData := statetransfer.ArbosInitializationInfo{
			NextBlockNumber: 0,
		}
		initDataReader = statetransfer.NewMemoryInitDataReader(&initData)
	}
	if config.Init.DevInit {
		if initDataReader != nil {
			return chainDb, nil, errors.New("multiple init methods supplied")
		}
		initData := statetransfer.ArbosInitializationInfo{
			NextBlockNumber: config.Init.DevInitBlockNum,
			Accounts: []statetransfer.AccountInitializationInfo{
				{
					Addr:       common.HexToAddress(config.Init.DevInitAddress),
					EthBalance: new(big.Int).Mul(big.NewInt(params.Ether), big.NewInt(1000)),
					Nonce:      0,
				},
			},
		}
		initDataReader = statetransfer.NewMemoryInitDataReader(&initData)
	}

	var chainConfig *params.ChainConfig

	var l2BlockChain *core.BlockChain
	txIndexWg := sync.WaitGroup{}
	if initDataReader == nil {
		chainConfig = gethexec.TryReadStoredChainConfig(chainDb)
		if chainConfig == nil {
			return chainDb, nil, errors.New("no --init.* mode supplied and chain data not in expected directory")
		}
		l2BlockChain, err = gethexec.GetBlockChain(chainDb, cacheConfig, chainConfig, config.Execution.TxLookupLimit)
		if err != nil {
			return chainDb, nil, err
		}
		genesisBlockNr := chainConfig.ArbitrumChainParams.GenesisBlockNum
		genesisBlock := l2BlockChain.GetBlockByNumber(genesisBlockNr)
		if genesisBlock != nil {
			log.Info("loaded genesis block from database", "number", genesisBlockNr, "hash", genesisBlock.Hash())
		} else {
			// The node will probably die later, but might as well not kill it here?
			log.Error("database missing genesis block", "number", genesisBlockNr)
		}
		testUpdateTxIndex(chainDb, chainConfig, &txIndexWg)
	} else {
		genesisBlockNr, err := initDataReader.GetNextBlockNumber()
		if err != nil {
			return chainDb, nil, err
		}
		combinedL2ChainInfoFiles := config.Chain.InfoFiles
		if config.Chain.InfoIpfsUrl != "" {
			l2ChainInfoIpfsFile, err := util.GetL2ChainInfoIpfsFile(ctx, config.Chain.InfoIpfsUrl, config.Chain.InfoIpfsDownloadPath)
			if err != nil {
				log.Error("error getting l2 chain info file from ipfs", "err", err)
			}
			combinedL2ChainInfoFiles = append(combinedL2ChainInfoFiles, l2ChainInfoIpfsFile)
		}
		chainConfig, err = chaininfo.GetChainConfig(new(big.Int).SetUint64(config.Chain.ID), config.Chain.Name, genesisBlockNr, combinedL2ChainInfoFiles, config.Chain.InfoJson)
		if err != nil {
			return chainDb, nil, err
		}
		testUpdateTxIndex(chainDb, chainConfig, &txIndexWg)
		ancients, err := chainDb.Ancients()
		if err != nil {
			return chainDb, nil, err
		}
		if ancients < genesisBlockNr {
			return chainDb, nil, fmt.Errorf("%v pre-init blocks required, but only %v found", genesisBlockNr, ancients)
		}
		if ancients > genesisBlockNr {
			storedGenHash := rawdb.ReadCanonicalHash(chainDb, genesisBlockNr)
			storedGenBlock := rawdb.ReadBlock(chainDb, storedGenHash, genesisBlockNr)
			if storedGenBlock.Header().Root == (common.Hash{}) {
				return chainDb, nil, fmt.Errorf("attempting to init genesis block %x, but this block is in database with no state root", genesisBlockNr)
			}
			log.Warn("Re-creating genesis though it seems to exist in database", "blockNr", genesisBlockNr)
		}
		log.Info("Initializing", "ancients", ancients, "genesisBlockNr", genesisBlockNr)
		if config.Init.ThenQuit {
			cacheConfig.SnapshotWait = true
		}
		var parsedInitMessage *arbostypes.ParsedInitMessage
		if config.Node.ParentChainReader.Enable {
			delayedBridge, err := arbnode.NewDelayedBridge(l1Client, rollupAddrs.Bridge, rollupAddrs.DeployedAt)
			if err != nil {
				return chainDb, nil, fmt.Errorf("failed creating delayed bridge while attempting to get serialized chain config from init message: %w", err)
			}
			deployedAt := new(big.Int).SetUint64(rollupAddrs.DeployedAt)
			delayedMessages, err := delayedBridge.LookupMessagesInRange(ctx, deployedAt, deployedAt, nil)
			if err != nil {
				return chainDb, nil, fmt.Errorf("failed getting delayed messages while attempting to get serialized chain config from init message: %w", err)
			}
			var initMessage *arbostypes.L1IncomingMessage
			for _, msg := range delayedMessages {
				if msg.Message.Header.Kind == arbostypes.L1MessageType_Initialize {
					initMessage = msg.Message
					break
				}
			}
			if initMessage == nil {
				return chainDb, nil, fmt.Errorf("failed to get init message while attempting to get serialized chain config")
			}
			parsedInitMessage, err = initMessage.ParseInitMessage()
			if err != nil {
				return chainDb, nil, err
			}
			if parsedInitMessage.ChainId.Cmp(chainId) != 0 {
				return chainDb, nil, fmt.Errorf("expected L2 chain ID %v but read L2 chain ID %v from init message in L1 inbox", chainId, parsedInitMessage.ChainId)
			}
			if parsedInitMessage.ChainConfig != nil {
				if err := parsedInitMessage.ChainConfig.CheckCompatible(chainConfig, chainConfig.ArbitrumChainParams.GenesisBlockNum, 0); err != nil {
					return chainDb, nil, fmt.Errorf("incompatible chain config read from init message in L1 inbox: %w", err)
				}
			}
			log.Info("Read serialized chain config from init message", "json", string(parsedInitMessage.SerializedChainConfig))
		} else {
			serializedChainConfig, err := json.Marshal(chainConfig)
			if err != nil {
				return chainDb, nil, err
			}
			parsedInitMessage = &arbostypes.ParsedInitMessage{
				ChainId:               chainConfig.ChainID,
				InitialL1BaseFee:      arbostypes.DefaultInitialL1BaseFee,
				ChainConfig:           chainConfig,
				SerializedChainConfig: serializedChainConfig,
			}
			log.Warn("Created fake init message as L1Reader is disabled and serialized chain config from init message is not available", "json", string(serializedChainConfig))
		}

		l2BlockChain, err = gethexec.WriteOrTestBlockChain(chainDb, cacheConfig, initDataReader, chainConfig, parsedInitMessage, config.Execution.TxLookupLimit, config.Init.AccountsPerSync)
		if err != nil {
			return chainDb, nil, err
		}
	}
	txIndexWg.Wait()
	err = chainDb.Sync()
	if err != nil {
		return chainDb, l2BlockChain, err
	}

	err = pruneChainDb(ctx, chainDb, stack, config, cacheConfig, l1Client, rollupAddrs)
	if err != nil {
		return chainDb, nil, fmt.Errorf("error pruning: %w", err)
	}

	err = validateBlockChain(l2BlockChain, chainConfig)
	if err != nil {
		return chainDb, l2BlockChain, err
	}

	return chainDb, l2BlockChain, nil
}

func testTxIndexUpdated(chainDb ethdb.Database, lastBlock uint64) bool {
	var transactions types.Transactions
	blockHash := rawdb.ReadCanonicalHash(chainDb, lastBlock)
	reReadNumber := rawdb.ReadHeaderNumber(chainDb, blockHash)
	if reReadNumber == nil {
		return false
	}
	for ; ; lastBlock-- {
		blockHash := rawdb.ReadCanonicalHash(chainDb, lastBlock)
		block := rawdb.ReadBlock(chainDb, blockHash, lastBlock)
		transactions = block.Transactions()
		if len(transactions) == 0 {
			if lastBlock == 0 {
				return true
			}
			continue
		}
		entry := rawdb.ReadTxLookupEntry(chainDb, transactions[len(transactions)-1].Hash())
		return entry != nil
	}
}

func testUpdateTxIndex(chainDb ethdb.Database, chainConfig *params.ChainConfig, globalWg *sync.WaitGroup) {
	lastBlock := chainConfig.ArbitrumChainParams.GenesisBlockNum
	if lastBlock == 0 {
		// no Tx, no need to update index
		return
	}

	lastBlock -= 1
	if testTxIndexUpdated(chainDb, lastBlock) {
		return
	}

	var localWg sync.WaitGroup
	threads := runtime.NumCPU()
	var failedTxIndiciesMutex sync.Mutex
	failedTxIndicies := make(map[common.Hash]uint64)
	for thread := 0; thread < threads; thread++ {
		thread := thread
		localWg.Add(1)
		go func() {
			batch := chainDb.NewBatch()
			for blockNum := uint64(thread); blockNum <= lastBlock; blockNum += uint64(threads) {
				blockHash := rawdb.ReadCanonicalHash(chainDb, blockNum)
				block := rawdb.ReadBlock(chainDb, blockHash, blockNum)
				receipts := rawdb.ReadRawReceipts(chainDb, blockHash, blockNum)
				for i, receipt := range receipts {
					// receipt.TxHash isn't populated as we used ReadRawReceipts
					txHash := block.Transactions()[i].Hash()
					if receipt.Status != 0 || receipt.GasUsed != 0 {
						rawdb.WriteTxLookupEntries(batch, blockNum, []common.Hash{txHash})
					} else {
						failedTxIndiciesMutex.Lock()
						prev, exists := failedTxIndicies[txHash]
						if !exists || prev < blockNum {
							failedTxIndicies[txHash] = blockNum
						}
						failedTxIndiciesMutex.Unlock()
					}
				}
				rawdb.WriteHeaderNumber(batch, block.Header().Hash(), blockNum)
				if blockNum%1_000_000 == 0 {
					log.Info("writing tx lookup entries", "block", blockNum)
				}
				if batch.ValueSize() >= ethdb.IdealBatchSize {
					err := batch.Write()
					if err != nil {
						panic(err)
					}
					batch.Reset()
				}
			}
			err := batch.Write()
			if err != nil {
				panic(err)
			}
			localWg.Done()
		}()
	}

	globalWg.Add(1)
	go func() {
		localWg.Wait()
		batch := chainDb.NewBatch()
		for txHash, blockNum := range failedTxIndicies {
			if rawdb.ReadTxLookupEntry(chainDb, txHash) == nil {
				rawdb.WriteTxLookupEntries(batch, blockNum, []common.Hash{txHash})
			}
			if batch.ValueSize() >= ethdb.IdealBatchSize {
				err := batch.Write()
				if err != nil {
					panic(err)
				}
				batch.Reset()
			}
		}
		err := batch.Write()
		if err != nil {
			panic(err)
		}
		log.Info("Tx lookup entries written")
		globalWg.Done()
	}()
}

'''
'''--- cmd/nitro/nitro.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"crypto/ecdsa"
	"errors"
	"fmt"
	"io"
	"math/big"
	"os"
	"os/signal"
	"path/filepath"
	"reflect"
	"strings"
	"syscall"
	"time"

	"github.com/cockroachdb/pebble"
	"github.com/knadh/koanf"
	"github.com/knadh/koanf/providers/confmap"
	flag "github.com/spf13/pflag"
	"github.com/syndtr/goleveldb/leveldb"

	"github.com/ethereum/go-ethereum/accounts"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/keystore"
	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	_ "github.com/ethereum/go-ethereum/eth/tracers/js"
	_ "github.com/ethereum/go-ethereum/eth/tracers/native"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/graphql"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/metrics/exp"
	"github.com/ethereum/go-ethereum/node"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbnode/resourcemanager"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/cmd/conf"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/execution/gethexec"
	_ "github.com/offchainlabs/nitro/nodeInterface"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/staker/validatorwallet"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/rpcclient"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/validator/valnode"
)

func printSampleUsage(name string) {
	fmt.Printf("Sample usage: %s [OPTIONS] \n\n", name)
	fmt.Printf("Options:\n")
	fmt.Printf("  --help\n")
	fmt.Printf("  --dev: Start a default L2-only dev chain\n")
}

func addUnlockWallet(accountManager *accounts.Manager, walletConf *genericconf.WalletConfig) (common.Address, error) {
	var devAddr common.Address

	var devPrivKey *ecdsa.PrivateKey
	var err error
	if walletConf.PrivateKey != "" {
		devPrivKey, err = crypto.HexToECDSA(walletConf.PrivateKey)
		if err != nil {
			return common.Address{}, err
		}

		devAddr = crypto.PubkeyToAddress(devPrivKey.PublicKey)

		log.Info("Dev node funded private key", "priv", walletConf.PrivateKey)
		log.Info("Funded public address", "addr", devAddr)
	}

	if walletConf.Pathname != "" {
		myKeystore := keystore.NewKeyStore(walletConf.Pathname, keystore.StandardScryptN, keystore.StandardScryptP)
		accountManager.AddBackend(myKeystore)
		var account accounts.Account
		if myKeystore.HasAddress(devAddr) {
			account.Address = devAddr
			account, err = myKeystore.Find(account)
		} else if walletConf.Account != "" && myKeystore.HasAddress(common.HexToAddress(walletConf.Account)) {
			account.Address = common.HexToAddress(walletConf.Account)
			account, err = myKeystore.Find(account)
		} else {
			if walletConf.Pwd() == nil {
				return common.Address{}, errors.New("l2 password not set")
			}
			if devPrivKey == nil {
				return common.Address{}, errors.New("l2 private key not set")
			}
			account, err = myKeystore.ImportECDSA(devPrivKey, *walletConf.Pwd())
		}
		if err != nil {
			return common.Address{}, err
		}
		if walletConf.Pwd() == nil {
			return common.Address{}, errors.New("l2 password not set")
		}
		err = myKeystore.Unlock(account, *walletConf.Pwd())
		if err != nil {
			return common.Address{}, err
		}
	}
	return devAddr, nil
}

func closeDb(db io.Closer, name string) {
	if db != nil {
		err := db.Close()
		// unfortunately the freezer db means we can't just use errors.Is
		if err != nil && !strings.Contains(err.Error(), leveldb.ErrClosed.Error()) && !strings.Contains(err.Error(), pebble.ErrClosed.Error()) {
			log.Warn("failed to close database on shutdown", "db", name, "err", err)
		}
	}
}

func main() {
	os.Exit(mainImpl())
}

// Checks metrics and PProf flag, runs them if enabled.
// Note: they are separate so one can enable/disable them as they wish, the only
// requirement is that they can't run on the same address and port.
func startMetrics(cfg *NodeConfig) error {
	mAddr := fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port)
	pAddr := fmt.Sprintf("%v:%v", cfg.PprofCfg.Addr, cfg.PprofCfg.Port)
	if cfg.Metrics && !metrics.Enabled {
		return fmt.Errorf("metrics must be enabled via command line by adding --metrics, json config has no effect")
	}
	if cfg.Metrics && cfg.PProf && mAddr == pAddr {
		return fmt.Errorf("metrics and pprof cannot be enabled on the same address:port: %s", mAddr)
	}
	if cfg.Metrics {
		go metrics.CollectProcessMetrics(cfg.MetricsServer.UpdateInterval)
		exp.Setup(fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port))
	}
	if cfg.PProf {
		genericconf.StartPprof(pAddr)
	}
	return nil
}

// Returns the exit code
func mainImpl() int {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()

	args := os.Args[1:]
	nodeConfig, l1Wallet, l2DevWallet, err := ParseNode(ctx, args)
	if err != nil {
		confighelpers.PrintErrorAndExit(err, printSampleUsage)
	}
	stackConf := node.DefaultConfig
	stackConf.DataDir = nodeConfig.Persistent.Chain
	stackConf.DBEngine = nodeConfig.Persistent.DBEngine
	nodeConfig.Rpc.Apply(&stackConf)
	nodeConfig.HTTP.Apply(&stackConf)
	nodeConfig.WS.Apply(&stackConf)
	nodeConfig.Auth.Apply(&stackConf)
	nodeConfig.IPC.Apply(&stackConf)
	nodeConfig.GraphQL.Apply(&stackConf)
	if nodeConfig.WS.ExposeAll {
		stackConf.WSModules = append(stackConf.WSModules, "personal")
	}
	stackConf.P2P.ListenAddr = ""
	stackConf.P2P.NoDial = true
	stackConf.P2P.NoDiscovery = true
	vcsRevision, strippedRevision, vcsTime := confighelpers.GetVersion()
	stackConf.Version = strippedRevision

	pathResolver := func(workdir string) func(string) string {
		if workdir == "" {
			workdir, err = os.Getwd()
			if err != nil {
				log.Warn("Failed to get workdir", "err", err)
			}
		}
		return func(path string) string {
			if filepath.IsAbs(path) {
				return path
			}
			return filepath.Join(workdir, path)
		}
	}

	if stackConf.JWTSecret == "" && stackConf.AuthAddr != "" {
		filename := pathResolver(nodeConfig.Persistent.GlobalConfig)("jwtsecret")
		if err := genericconf.TryCreatingJWTSecret(filename); err != nil {
			log.Error("Failed to prepare jwt secret file", "err", err)
			return 1
		}
		stackConf.JWTSecret = filename
	}
	err = genericconf.InitLog(nodeConfig.LogType, log.Lvl(nodeConfig.LogLevel), &nodeConfig.FileLogging, pathResolver(nodeConfig.Persistent.LogDir))
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error initializing logging: %v\n", err)
		return 1
	}

	log.Info("Running Arbitrum nitro node", "revision", vcsRevision, "vcs.time", vcsTime)

	if nodeConfig.Node.Dangerous.NoL1Listener {
		nodeConfig.Node.ParentChainReader.Enable = false
		nodeConfig.Node.BatchPoster.Enable = false
		nodeConfig.Node.DelayedSequencer.Enable = false
	} else {
		nodeConfig.Node.ParentChainReader.Enable = true
	}

	if nodeConfig.Execution.Sequencer.Enable && nodeConfig.Node.ParentChainReader.Enable && nodeConfig.Node.InboxReader.HardReorg {
		flag.Usage()
		log.Crit("hard reorgs cannot safely be enabled with sequencer mode enabled")
	}
	if nodeConfig.Execution.Sequencer.Enable != nodeConfig.Node.Sequencer {
		log.Error("consensus and execution must agree if sequencing is enabled or not", "Execution.Sequencer.Enable", nodeConfig.Execution.Sequencer.Enable, "Node.Sequencer", nodeConfig.Node.Sequencer)
	}

	var l1TransactionOpts *bind.TransactOpts
	var dataSigner signature.DataSignerFunc
	var l1TransactionOptsValidator *bind.TransactOpts
	var l1TransactionOptsBatchPoster *bind.TransactOpts
	sequencerNeedsKey := (nodeConfig.Node.Sequencer && !nodeConfig.Node.Feed.Output.DisableSigning) || nodeConfig.Node.BatchPoster.Enable
	validatorNeedsKey := nodeConfig.Node.Staker.OnlyCreateWalletContract || nodeConfig.Node.Staker.Enable && !strings.EqualFold(nodeConfig.Node.Staker.Strategy, "watchtower")

	l1Wallet.ResolveDirectoryNames(nodeConfig.Persistent.Chain)
	defaultL1WalletConfig := conf.DefaultL1WalletConfig
	defaultL1WalletConfig.ResolveDirectoryNames(nodeConfig.Persistent.Chain)

	nodeConfig.Node.Staker.ParentChainWallet.ResolveDirectoryNames(nodeConfig.Persistent.Chain)
	defaultValidatorL1WalletConfig := staker.DefaultValidatorL1WalletConfig
	defaultValidatorL1WalletConfig.ResolveDirectoryNames(nodeConfig.Persistent.Chain)

	nodeConfig.Node.BatchPoster.ParentChainWallet.ResolveDirectoryNames(nodeConfig.Persistent.Chain)
	defaultBatchPosterL1WalletConfig := arbnode.DefaultBatchPosterL1WalletConfig
	defaultBatchPosterL1WalletConfig.ResolveDirectoryNames(nodeConfig.Persistent.Chain)

	if nodeConfig.Node.Staker.ParentChainWallet == defaultValidatorL1WalletConfig && nodeConfig.Node.BatchPoster.ParentChainWallet == defaultBatchPosterL1WalletConfig {
		if sequencerNeedsKey || validatorNeedsKey || l1Wallet.OnlyCreateKey {
			l1TransactionOpts, dataSigner, err = util.OpenWallet("l1", l1Wallet, new(big.Int).SetUint64(nodeConfig.ParentChain.ID))
			if err != nil {
				flag.Usage()
				log.Crit("error opening parent chain wallet", "path", l1Wallet.Pathname, "account", l1Wallet.Account, "err", err)
			}
			if l1Wallet.OnlyCreateKey {
				return 0
			}
			l1TransactionOptsBatchPoster = l1TransactionOpts
			l1TransactionOptsValidator = l1TransactionOpts
		}
	} else {
		if *l1Wallet != defaultL1WalletConfig {
			log.Crit("--parent-chain.wallet cannot be set if either --node.staker.l1-wallet or --node.batch-poster.l1-wallet are set")
		}
		if sequencerNeedsKey || nodeConfig.Node.BatchPoster.ParentChainWallet.OnlyCreateKey {
			l1TransactionOptsBatchPoster, dataSigner, err = util.OpenWallet("l1-batch-poster", &nodeConfig.Node.BatchPoster.ParentChainWallet, new(big.Int).SetUint64(nodeConfig.ParentChain.ID))
			if err != nil {
				flag.Usage()
				log.Crit("error opening Batch poster parent chain wallet", "path", nodeConfig.Node.BatchPoster.ParentChainWallet.Pathname, "account", nodeConfig.Node.BatchPoster.ParentChainWallet.Account, "err", err)
			}
			if nodeConfig.Node.BatchPoster.ParentChainWallet.OnlyCreateKey {
				return 0
			}
		}
		if validatorNeedsKey || nodeConfig.Node.Staker.ParentChainWallet.OnlyCreateKey {
			l1TransactionOptsValidator, _, err = util.OpenWallet("l1-validator", &nodeConfig.Node.Staker.ParentChainWallet, new(big.Int).SetUint64(nodeConfig.ParentChain.ID))
			if err != nil {
				flag.Usage()
				log.Crit("error opening Validator parent chain wallet", "path", nodeConfig.Node.Staker.ParentChainWallet.Pathname, "account", nodeConfig.Node.Staker.ParentChainWallet.Account, "err", err)
			}
			if nodeConfig.Node.Staker.ParentChainWallet.OnlyCreateKey {
				return 0
			}
		}
	}

	combinedL2ChainInfoFile := nodeConfig.Chain.InfoFiles
	if nodeConfig.Chain.InfoIpfsUrl != "" {
		l2ChainInfoIpfsFile, err := util.GetL2ChainInfoIpfsFile(ctx, nodeConfig.Chain.InfoIpfsUrl, nodeConfig.Chain.InfoIpfsDownloadPath)
		if err != nil {
			log.Error("error getting chain info file from ipfs", "err", err)
		}
		combinedL2ChainInfoFile = append(combinedL2ChainInfoFile, l2ChainInfoIpfsFile)
	}

	if nodeConfig.Node.Staker.Enable {
		if !nodeConfig.Node.ParentChainReader.Enable {
			flag.Usage()
			log.Crit("validator must have the parent chain reader enabled")
		}
		strategy, err := nodeConfig.Node.Staker.ParseStrategy()
		if err != nil {
			log.Crit("couldn't parse staker strategy", "err", err)
		}
		if strategy != staker.WatchtowerStrategy && !nodeConfig.Node.Staker.Dangerous.WithoutBlockValidator {
			nodeConfig.Node.BlockValidator.Enable = true
		}
	}

	if nodeConfig.Execution.RPC.MaxRecreateStateDepth == arbitrum.UninitializedMaxRecreateStateDepth {
		if nodeConfig.Execution.Caching.Archive {
			nodeConfig.Execution.RPC.MaxRecreateStateDepth = arbitrum.DefaultArchiveNodeMaxRecreateStateDepth
		} else {
			nodeConfig.Execution.RPC.MaxRecreateStateDepth = arbitrum.DefaultNonArchiveNodeMaxRecreateStateDepth
		}
	}
	liveNodeConfig := genericconf.NewLiveConfig[*NodeConfig](args, nodeConfig, func(ctx context.Context, args []string) (*NodeConfig, error) {
		nodeConfig, _, _, err := ParseNode(ctx, args)
		return nodeConfig, err
	})

	var rollupAddrs chaininfo.RollupAddresses
	var l1Client *ethclient.Client
	if nodeConfig.Node.ParentChainReader.Enable {
		confFetcher := func() *rpcclient.ClientConfig { return &liveNodeConfig.Get().ParentChain.Connection }
		rpcClient := rpcclient.NewRpcClient(confFetcher, nil)
		err := rpcClient.Start(ctx)
		if err != nil {
			log.Crit("couldn't connect to L1", "err", err)
		}
		l1Client = ethclient.NewClient(rpcClient)
		l1ChainId, err := l1Client.ChainID(ctx)
		if err != nil {
			log.Crit("couldn't read L1 chainid", "err", err)
		}
		if l1ChainId.Uint64() != nodeConfig.ParentChain.ID {
			log.Crit("L1 chainID doesn't fit config", "found", l1ChainId.Uint64(), "expected", nodeConfig.ParentChain.ID)
		}

		log.Info("connected to l1 chain", "l1url", nodeConfig.ParentChain.Connection.URL, "l1chainid", nodeConfig.ParentChain.ID)

		rollupAddrs, err = chaininfo.GetRollupAddressesConfig(nodeConfig.Chain.ID, nodeConfig.Chain.Name, combinedL2ChainInfoFile, nodeConfig.Chain.InfoJson)
		if err != nil {
			log.Crit("error getting rollup addresses", "err", err)
		}
	}

	if nodeConfig.Node.Staker.OnlyCreateWalletContract {
		if !nodeConfig.Node.Staker.UseSmartContractWallet {
			flag.Usage()
			log.Crit("--node.validator.only-create-wallet-contract requires --node.validator.use-smart-contract-wallet")
		}
		arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1Client)
		l1Reader, err := headerreader.New(ctx, l1Client, func() *headerreader.Config { return &liveNodeConfig.Get().Node.ParentChainReader }, arbSys)
		if err != nil {
			log.Crit("failed to get L1 headerreader", "error", err)
		}

		// Just create validator smart wallet if needed then exit
		deployInfo, err := chaininfo.GetRollupAddressesConfig(nodeConfig.Chain.ID, nodeConfig.Chain.Name, combinedL2ChainInfoFile, nodeConfig.Chain.InfoJson)
		if err != nil {
			log.Crit("error getting rollup addresses config", "err", err)
		}
		addr, err := validatorwallet.GetValidatorWalletContract(ctx, deployInfo.ValidatorWalletCreator, int64(deployInfo.DeployedAt), l1TransactionOptsValidator, l1Reader, true)
		if err != nil {
			log.Crit("error creating validator wallet contract", "error", err, "address", l1TransactionOptsValidator.From.Hex())
		}
		fmt.Printf("Created validator smart contract wallet at %s, remove --node.validator.only-create-wallet-contract and restart\n", addr.String())
		return 0
	}

	if nodeConfig.Execution.Caching.Archive && nodeConfig.Execution.TxLookupLimit != 0 {
		log.Info("retaining ability to lookup full transaction history as archive mode is enabled")
		nodeConfig.Execution.TxLookupLimit = 0
	}

	if err := resourcemanager.Init(&nodeConfig.Node.ResourceMgmt); err != nil {
		flag.Usage()
		log.Crit("Failed to start resource management module", "err", err)
	}

	var sameProcessValidationNodeEnabled bool
	if nodeConfig.Node.BlockValidator.Enable && (nodeConfig.Node.BlockValidator.ValidationServer.URL == "self" || nodeConfig.Node.BlockValidator.ValidationServer.URL == "self-auth") {
		sameProcessValidationNodeEnabled = true
		valnode.EnsureValidationExposedViaAuthRPC(&stackConf)
	}
	stack, err := node.New(&stackConf)
	if err != nil {
		flag.Usage()
		log.Crit("failed to initialize geth stack", "err", err)
	}
	{
		devAddr, err := addUnlockWallet(stack.AccountManager(), l2DevWallet)
		if err != nil {
			flag.Usage()
			log.Crit("error opening L2 dev wallet", "err", err)
		}
		if devAddr != (common.Address{}) {
			nodeConfig.Init.DevInitAddress = devAddr.String()
		}
	}

	if err := startMetrics(nodeConfig); err != nil {
		log.Error("Error starting metrics", "error", err)
		return 1
	}

	var deferFuncs []func()
	defer func() {
		for i := range deferFuncs {
			deferFuncs[i]()
		}
	}()

	chainDb, l2BlockChain, err := openInitializeChainDb(ctx, stack, nodeConfig, new(big.Int).SetUint64(nodeConfig.Chain.ID), gethexec.DefaultCacheConfigFor(stack, &nodeConfig.Execution.Caching), l1Client, rollupAddrs)
	if l2BlockChain != nil {
		deferFuncs = append(deferFuncs, func() { l2BlockChain.Stop() })
	}
	deferFuncs = append(deferFuncs, func() { closeDb(chainDb, "chainDb") })
	if err != nil {
		flag.Usage()
		log.Error("error initializing database", "err", err)
		return 1
	}

	arbDb, err := stack.OpenDatabase("arbitrumdata", 0, 0, "", false)
	deferFuncs = append(deferFuncs, func() { closeDb(arbDb, "arbDb") })
	if err != nil {
		log.Error("failed to open database", "err", err)
		return 1
	}

	if nodeConfig.Init.ThenQuit && nodeConfig.Init.ResetToMessage < 0 {
		return 0
	}

	if l2BlockChain.Config().ArbitrumChainParams.DataAvailabilityCommittee && !nodeConfig.Node.DataAvailability.Enable {
		flag.Usage()
		log.Error("a data availability service must be configured for this chain (see the --node.data-availability family of options)")
		return 1
	}

	fatalErrChan := make(chan error, 10)

	var valNode *valnode.ValidationNode
	if sameProcessValidationNodeEnabled {
		valNode, err = valnode.CreateValidationNode(
			func() *valnode.Config { return &liveNodeConfig.Get().Validation },
			stack,
			fatalErrChan,
		)
		if err != nil {
			valNode = nil
			log.Warn("couldn't init validation node", "err", err)
		}
	}

	execNode, err := gethexec.CreateExecutionNode(
		ctx,
		stack,
		chainDb,
		l2BlockChain,
		l1Client,
		func() *gethexec.Config { return &liveNodeConfig.Get().Execution },
	)
	if err != nil {
		log.Error("failed to create execution node", "err", err)
		return 1
	}

	currentNode, err := arbnode.CreateNode(
		ctx,
		stack,
		execNode,
		arbDb,
		&NodeConfigFetcher{liveNodeConfig},
		l2BlockChain.Config(),
		l1Client,
		&rollupAddrs,
		l1TransactionOptsValidator,
		l1TransactionOptsBatchPoster,
		dataSigner,
		fatalErrChan,
	)
	if err != nil {
		log.Error("failed to create node", "err", err)
		return 1
	}
	liveNodeConfig.SetOnReloadHook(func(oldCfg *NodeConfig, newCfg *NodeConfig) error {
		if err := genericconf.InitLog(newCfg.LogType, log.Lvl(newCfg.LogLevel), &newCfg.FileLogging, pathResolver(nodeConfig.Persistent.LogDir)); err != nil {
			return fmt.Errorf("failed to re-init logging: %w", err)
		}
		return currentNode.OnConfigReload(&oldCfg.Node, &newCfg.Node)
	})

	if nodeConfig.Node.Dangerous.NoL1Listener && nodeConfig.Init.DevInit {
		// If we don't have any messages, we're not connected to the L1, and we're using a dev init,
		// we should create our own fake init message.
		count, err := currentNode.TxStreamer.GetMessageCount()
		if err != nil {
			log.Warn("Getmessagecount failed. Assuming new database", "err", err)
			count = 0
		}
		if count == 0 {
			err = currentNode.TxStreamer.AddFakeInitMessage()
			if err != nil {
				panic(err)
			}
		}
	}
	gqlConf := nodeConfig.GraphQL
	if gqlConf.Enable {
		if err := graphql.New(stack, execNode.Backend.APIBackend(), execNode.FilterSystem, gqlConf.CORSDomain, gqlConf.VHosts); err != nil {
			log.Error("failed to register the GraphQL service", "err", err)
			return 1
		}
	}

	if valNode != nil {
		err = valNode.Start(ctx)
		if err != nil {
			fatalErrChan <- fmt.Errorf("error starting validator node: %w", err)
		} else {
			log.Info("validation node started")
		}
	}
	if err == nil {
		err = currentNode.Start(ctx)
		if err != nil {
			fatalErrChan <- fmt.Errorf("error starting node: %w", err)
		}
		// remove previous deferFuncs, StopAndWait closes database and blockchain.
		deferFuncs = []func(){func() { currentNode.StopAndWait() }}
	}

	sigint := make(chan os.Signal, 1)
	signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)

	exitCode := 0

	if err == nil && nodeConfig.Init.ResetToMessage > 0 {
		err = currentNode.TxStreamer.ReorgTo(arbutil.MessageIndex(nodeConfig.Init.ResetToMessage))
		if err != nil {
			fatalErrChan <- fmt.Errorf("error reseting message: %w", err)
			exitCode = 1
		}
		if nodeConfig.Init.ThenQuit {
			close(sigint)

			return exitCode
		}
	}

	select {
	case err := <-fatalErrChan:
		log.Error("shutting down due to fatal error", "err", err)
		defer log.Error("shut down due to fatal error", "err", err)
		exitCode = 1
	case <-sigint:
		log.Info("shutting down because of sigint")
	}

	// cause future ctrl+c's to panic
	close(sigint)

	return exitCode
}

type NodeConfig struct {
	Conf          genericconf.ConfConfig          `koanf:"conf" reload:"hot"`
	Node          arbnode.Config                  `koanf:"node" reload:"hot"`
	Execution     gethexec.Config                 `koanf:"execution" reload:"hot"`
	Validation    valnode.Config                  `koanf:"validation" reload:"hot"`
	ParentChain   conf.L1Config                   `koanf:"parent-chain" reload:"hot"`
	Chain         conf.L2Config                   `koanf:"chain"`
	LogLevel      int                             `koanf:"log-level" reload:"hot"`
	LogType       string                          `koanf:"log-type" reload:"hot"`
	FileLogging   genericconf.FileLoggingConfig   `koanf:"file-logging" reload:"hot"`
	Persistent    conf.PersistentConfig           `koanf:"persistent"`
	HTTP          genericconf.HTTPConfig          `koanf:"http"`
	WS            genericconf.WSConfig            `koanf:"ws"`
	IPC           genericconf.IPCConfig           `koanf:"ipc"`
	Auth          genericconf.AuthRPCConfig       `koanf:"auth"`
	GraphQL       genericconf.GraphQLConfig       `koanf:"graphql"`
	Metrics       bool                            `koanf:"metrics"`
	MetricsServer genericconf.MetricsServerConfig `koanf:"metrics-server"`
	PProf         bool                            `koanf:"pprof"`
	PprofCfg      genericconf.PProf               `koanf:"pprof-cfg"`
	Init          InitConfig                      `koanf:"init"`
	Rpc           genericconf.RpcConfig           `koanf:"rpc"`
}

var NodeConfigDefault = NodeConfig{
	Conf:          genericconf.ConfConfigDefault,
	Node:          arbnode.ConfigDefault,
	Execution:     gethexec.ConfigDefault,
	Validation:    valnode.DefaultValidationConfig,
	ParentChain:   conf.L1ConfigDefault,
	Chain:         conf.L2ConfigDefault,
	LogLevel:      int(log.LvlInfo),
	LogType:       "plaintext",
	FileLogging:   genericconf.DefaultFileLoggingConfig,
	Persistent:    conf.PersistentConfigDefault,
	HTTP:          genericconf.HTTPConfigDefault,
	WS:            genericconf.WSConfigDefault,
	IPC:           genericconf.IPCConfigDefault,
	Auth:          genericconf.AuthRPCConfigDefault,
	GraphQL:       genericconf.GraphQLConfigDefault,
	Metrics:       false,
	MetricsServer: genericconf.MetricsServerConfigDefault,
	Init:          InitConfigDefault,
	Rpc:           genericconf.DefaultRpcConfig,
	PProf:         false,
	PprofCfg:      genericconf.PProfDefault,
}

func NodeConfigAddOptions(f *flag.FlagSet) {
	genericconf.ConfConfigAddOptions("conf", f)
	arbnode.ConfigAddOptions("node", f, true, true)
	gethexec.ConfigAddOptions("execution", f)
	valnode.ValidationConfigAddOptions("validation", f)
	conf.L1ConfigAddOptions("parent-chain", f)
	conf.L2ConfigAddOptions("chain", f)
	f.Int("log-level", NodeConfigDefault.LogLevel, "log level")
	f.String("log-type", NodeConfigDefault.LogType, "log type (plaintext or json)")
	genericconf.FileLoggingConfigAddOptions("file-logging", f)
	conf.PersistentConfigAddOptions("persistent", f)
	genericconf.HTTPConfigAddOptions("http", f)
	genericconf.WSConfigAddOptions("ws", f)
	genericconf.IPCConfigAddOptions("ipc", f)
	genericconf.AuthRPCConfigAddOptions("auth", f)
	genericconf.GraphQLConfigAddOptions("graphql", f)
	f.Bool("metrics", NodeConfigDefault.Metrics, "enable metrics")
	genericconf.MetricsServerAddOptions("metrics-server", f)
	f.Bool("pprof", NodeConfigDefault.PProf, "enable pprof")
	genericconf.PProfAddOptions("pprof-cfg", f)

	InitConfigAddOptions("init", f)
	genericconf.RpcConfigAddOptions("rpc", f)
}

func (c *NodeConfig) ResolveDirectoryNames() error {
	err := c.Persistent.ResolveDirectoryNames()
	if err != nil {
		return err
	}
	c.ParentChain.ResolveDirectoryNames(c.Persistent.Chain)
	c.Chain.ResolveDirectoryNames(c.Persistent.Chain)

	return nil
}

func (c *NodeConfig) ShallowClone() *NodeConfig {
	config := &NodeConfig{}
	*config = *c
	return config
}

func (c *NodeConfig) CanReload(new *NodeConfig) error {
	var check func(node, other reflect.Value, path string)
	var err error

	check = func(node, value reflect.Value, path string) {
		if node.Kind() != reflect.Struct {
			return
		}

		for i := 0; i < node.NumField(); i++ {
			fieldTy := node.Type().Field(i)
			if !fieldTy.IsExported() {
				continue
			}
			hot := fieldTy.Tag.Get("reload") == "hot"
			dot := path + "." + fieldTy.Name

			first := node.Field(i).Interface()
			other := value.Field(i).Interface()

			if !hot && !reflect.DeepEqual(first, other) {
				err = fmt.Errorf("illegal change to %v%v%v", colors.Red, dot, colors.Clear)
			} else {
				check(node.Field(i), value.Field(i), dot)
			}
		}
	}

	check(reflect.ValueOf(c).Elem(), reflect.ValueOf(new).Elem(), "config")
	return err
}

func (c *NodeConfig) Validate() error {
	if err := c.ParentChain.Validate(); err != nil {
		return err
	}
	if err := c.Node.Validate(); err != nil {
		return err
	}
	return c.Persistent.Validate()
}

func (c *NodeConfig) GetReloadInterval() time.Duration {
	return c.Conf.ReloadInterval
}

func ParseNode(ctx context.Context, args []string) (*NodeConfig, *genericconf.WalletConfig, *genericconf.WalletConfig, error) {
	f := flag.NewFlagSet("", flag.ContinueOnError)

	NodeConfigAddOptions(f)

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, nil, nil, err
	}

	l2ChainId := k.Int64("chain.id")
	l2ChainName := k.String("chain.name")
	l2ChainInfoIpfsUrl := k.String("chain.info-ipfs-url")
	l2ChainInfoIpfsDownloadPath := k.String("chain.info-ipfs-download-path")
	if l2ChainId == 0 && l2ChainName == "" {
		return nil, nil, nil, errors.New("must specify --chain.id or --chain.name to choose rollup")
	}
	l2ChainInfoFiles := k.Strings("chain.info-files")
	l2ChainInfoJson := k.String("chain.info-json")
	chainFound, err := applyChainParameters(ctx, k, uint64(l2ChainId), l2ChainName, l2ChainInfoFiles, l2ChainInfoJson, l2ChainInfoIpfsUrl, l2ChainInfoIpfsDownloadPath)
	if err != nil {
		return nil, nil, nil, err
	}

	err = confighelpers.ApplyOverrides(f, k)
	if err != nil {
		return nil, nil, nil, err
	}

	var nodeConfig NodeConfig
	if err := confighelpers.EndCommonParse(k, &nodeConfig); err != nil {
		return nil, nil, nil, err
	}

	// Don't print wallet passwords
	if nodeConfig.Conf.Dump {
		err = confighelpers.DumpConfig(k, map[string]interface{}{
			"parent-chain.wallet.password":    "",
			"parent-chain.wallet.private-key": "",
			"chain.dev-wallet.password":       "",
			"chain.dev-wallet.private-key":    "",
		})
		if err != nil {
			return nil, nil, nil, err
		}
	}

	if nodeConfig.Persistent.Chain == "" {
		if !chainFound {
			// If persistent-chain not defined, user not creating custom chain
			if l2ChainId != 0 {
				return nil, nil, nil, fmt.Errorf("Unknown chain id: %d, L2ChainInfoFiles: %v.  update chain id, modify --chain.info-files or provide --persistent.chain\n", l2ChainId, l2ChainInfoFiles)
			}
			return nil, nil, nil, fmt.Errorf("Unknown chain name: %s, L2ChainInfoFiles: %v.  update chain name, modify --chain.info-files or provide --persistent.chain\n", l2ChainName, l2ChainInfoFiles)
		}
		return nil, nil, nil, errors.New("--persistent.chain not specified")
	}

	err = nodeConfig.ResolveDirectoryNames()
	if err != nil {
		return nil, nil, nil, err
	}

	// Don't pass around wallet contents with normal configuration
	l1Wallet := nodeConfig.ParentChain.Wallet
	l2DevWallet := nodeConfig.Chain.DevWallet
	nodeConfig.ParentChain.Wallet = genericconf.WalletConfigDefault
	nodeConfig.Chain.DevWallet = genericconf.WalletConfigDefault

	if nodeConfig.Execution.Caching.Archive {
		nodeConfig.Node.MessagePruner.Enable = false
	}
	err = nodeConfig.Validate()
	if err != nil {
		return nil, nil, nil, err
	}
	return &nodeConfig, &l1Wallet, &l2DevWallet, nil
}

func applyChainParameters(ctx context.Context, k *koanf.Koanf, chainId uint64, chainName string, l2ChainInfoFiles []string, l2ChainInfoJson string, l2ChainInfoIpfsUrl string, l2ChainInfoIpfsDownloadPath string) (bool, error) {
	combinedL2ChainInfoFiles := l2ChainInfoFiles
	if l2ChainInfoIpfsUrl != "" {
		l2ChainInfoIpfsFile, err := util.GetL2ChainInfoIpfsFile(ctx, l2ChainInfoIpfsUrl, l2ChainInfoIpfsDownloadPath)
		if err != nil {
			log.Error("error getting l2 chain info file from ipfs", "err", err)
		}
		combinedL2ChainInfoFiles = append(combinedL2ChainInfoFiles, l2ChainInfoIpfsFile)
	}
	chainInfo, err := chaininfo.ProcessChainInfo(chainId, chainName, combinedL2ChainInfoFiles, l2ChainInfoJson)
	if err != nil {
		return false, err
	}
	var parentChainIsArbitrum bool
	if chainInfo.ParentChainIsArbitrum != nil {
		parentChainIsArbitrum = *chainInfo.ParentChainIsArbitrum
	} else {
		log.Warn("Chain information parentChainIsArbitrum field missing, in the future this will be required", "chainId", chainInfo.ChainConfig.ChainID, "parentChainId", chainInfo.ParentChainId)
		_, err := chaininfo.ProcessChainInfo(chainInfo.ParentChainId, "", combinedL2ChainInfoFiles, "")
		if err == nil {
			parentChainIsArbitrum = true
		}
	}
	chainDefaults := map[string]interface{}{
		"persistent.chain": chainInfo.ChainName,
		"chain.id":         chainInfo.ChainConfig.ChainID.Uint64(),
		"parent-chain.id":  chainInfo.ParentChainId,
	}
	if chainInfo.SequencerUrl != "" {
		chainDefaults["execution.forwarding-target"] = chainInfo.SequencerUrl
	}
	if chainInfo.FeedUrl != "" {
		chainDefaults["node.feed.input.url"] = chainInfo.FeedUrl
	}
	if chainInfo.DasIndexUrl != "" {
		chainDefaults["node.data-availability.enable"] = true
		chainDefaults["node.data-availability.rest-aggregator.enable"] = true
		chainDefaults["node.data-availability.rest-aggregator.online-url-list"] = chainInfo.DasIndexUrl
	}
	if !chainInfo.HasGenesisState {
		chainDefaults["init.empty"] = true
	}
	if parentChainIsArbitrum {
		l2MaxTxSize := gethexec.DefaultSequencerConfig.MaxTxDataSize
		bufferSpace := 5000
		if l2MaxTxSize < bufferSpace*2 {
			return false, fmt.Errorf("not enough room in parent chain max tx size %v for bufferSpace %v * 2", l2MaxTxSize, bufferSpace)
		}
		safeBatchSize := l2MaxTxSize - bufferSpace
		chainDefaults["node.batch-poster.max-size"] = safeBatchSize
		chainDefaults["node.sequencer.max-tx-data-size"] = safeBatchSize - bufferSpace
	}
	err = k.Load(confmap.Provider(chainDefaults, "."), nil)
	if err != nil {
		return false, err
	}
	return true, nil
}

type NodeConfigFetcher struct {
	*genericconf.LiveConfig[*NodeConfig]
}

func (f *NodeConfigFetcher) Get() *arbnode.Config {
	return &f.LiveConfig.Get().Node
}

'''
'''--- cmd/relay/config_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"strings"
	"testing"

	"github.com/offchainlabs/nitro/relay"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestRelayConfig(t *testing.T) {
	args := strings.Split("--node.feed.output.port 9652 --node.feed.input.url ws://sequencer:9642/feed", " ")
	_, err := relay.ParseRelay(context.Background(), args)
	testhelpers.RequireImpl(t, err)
}

'''
'''--- cmd/relay/relay.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"os/signal"
	"syscall"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/metrics/exp"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/relay"
)

func init() {
	http.DefaultServeMux = http.NewServeMux()
}

func main() {
	if err := startup(); err != nil {
		log.Error("Error running relay", "err", err)
	}
}

func printSampleUsage(progname string) {
	fmt.Printf("\n")
	fmt.Printf("Sample usage:                  %s --node.feed.input.url=<L1 RPC> --chain.id=<L2 chain id> \n", progname)
}

// Checks metrics and PProf flag, runs them if enabled.
// Note: they are separate so one can enable/disable them as they wish, the only
// requirement is that they can't run on the same address and port.
func startMetrics(cfg *relay.Config) error {
	mAddr := fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port)
	pAddr := fmt.Sprintf("%v:%v", cfg.PprofCfg.Addr, cfg.PprofCfg.Port)
	if cfg.Metrics && !metrics.Enabled {
		return fmt.Errorf("metrics must be enabled via command line by adding --metrics, json config has no effect")
	}
	if cfg.Metrics && cfg.PProf && mAddr == pAddr {
		return fmt.Errorf("metrics and pprof cannot be enabled on the same address:port: %s", mAddr)
	}
	if cfg.Metrics {
		go metrics.CollectProcessMetrics(cfg.MetricsServer.UpdateInterval)
		exp.Setup(fmt.Sprintf("%v:%v", cfg.MetricsServer.Addr, cfg.MetricsServer.Port))
	}
	if cfg.PProf {
		genericconf.StartPprof(pAddr)
	}
	return nil
}

func startup() error {
	ctx := context.Background()

	relayConfig, err := relay.ParseRelay(ctx, os.Args[1:])
	if err != nil || len(relayConfig.Node.Feed.Input.URL) == 0 || relayConfig.Node.Feed.Input.URL[0] == "" || relayConfig.Chain.ID == 0 {
		confighelpers.PrintErrorAndExit(err, printSampleUsage)
	}

	logFormat, err := genericconf.ParseLogType(relayConfig.LogType)
	if err != nil {
		flag.Usage()
		panic(fmt.Sprintf("Error parsing log type: %v", err))
	}
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, logFormat))
	glogger.Verbosity(log.Lvl(relayConfig.LogLevel))
	log.Root().SetHandler(glogger)

	vcsRevision, _, vcsTime := confighelpers.GetVersion()
	log.Info("Running Arbitrum nitro relay", "revision", vcsRevision, "vcs.time", vcsTime)

	defer log.Info("Cleanly shutting down relay")

	sigint := make(chan os.Signal, 1)
	signal.Notify(sigint, os.Interrupt, syscall.SIGTERM)

	// Start up an arbitrum sequencer relay
	feedErrChan := make(chan error, 10)
	newRelay, err := relay.NewRelay(relayConfig, feedErrChan)
	if err != nil {
		return err
	}

	if err := startMetrics(relayConfig); err != nil {
		return err
	}

	if err := newRelay.Start(ctx); err != nil {
		return err
	}

	select {
	case <-sigint:
		log.Info("shutting down because of sigint")
	case err := <-feedErrChan:
		log.Error("error connecting, exiting", "err", err)
	}

	// cause future ctrl+c's to panic
	close(sigint)

	newRelay.StopAndWait()
	return nil
}

'''
'''--- cmd/replay/db.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"bytes"
	"encoding/hex"
	"errors"
	"fmt"

	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/wavmio"
)

type PreimageDb struct{}

func (db PreimageDb) Has(key []byte) (bool, error) {
	if len(key) != 32 {
		return false, nil
	}
	return false, errors.New("preimage DB doesn't support Has")
}

func (db PreimageDb) Get(key []byte) ([]byte, error) {
	var hash [32]byte
	copy(hash[:], key)
	if len(key) == 32 {
		copy(hash[:], key)
	} else if len(key) == len(rawdb.CodePrefix)+32 && bytes.HasPrefix(key, rawdb.CodePrefix) {
		// Retrieving code
		copy(hash[:], key[len(rawdb.CodePrefix):])
	} else {
		return nil, fmt.Errorf("preimage DB attempted to access non-hash key %v", hex.EncodeToString(key))
	}
	return wavmio.ResolveTypedPreimage(arbutil.Keccak256PreimageType, hash)
}

func (db PreimageDb) Put(key []byte, value []byte) error {
	return errors.New("preimage DB doesn't support Put")
}

func (db PreimageDb) Delete(key []byte) error {
	return errors.New("preimage DB doesn't support Delete")
}

func (db PreimageDb) NewBatch() ethdb.Batch {
	return NopBatcher{db}
}

func (db PreimageDb) NewBatchWithSize(size int) ethdb.Batch {
	return NopBatcher{db}
}

func (db PreimageDb) NewIterator(prefix []byte, start []byte) ethdb.Iterator {
	return ErrorIterator{}
}

func (db PreimageDb) Stat(property string) (string, error) {
	return "", errors.New("preimage DB doesn't support Stat")
}

func (db PreimageDb) Compact(start []byte, limit []byte) error {
	return nil
}

func (db PreimageDb) NewSnapshot() (ethdb.Snapshot, error) {
	// This is fine as PreimageDb doesn't support mutation
	return db, nil
}

func (db PreimageDb) Close() error {
	return nil
}

func (db PreimageDb) Release() {
}

type NopBatcher struct {
	ethdb.KeyValueStore
}

func (b NopBatcher) ValueSize() int {
	return 0
}

func (b NopBatcher) Write() error {
	return nil
}

func (b NopBatcher) Reset() {}

func (b NopBatcher) Replay(w ethdb.KeyValueWriter) error {
	return nil
}

type ErrorIterator struct{}

func (i ErrorIterator) Next() bool {
	return false
}

func (i ErrorIterator) Error() error {
	return errors.New("preimage DB doesn't support iterators")
}

func (i ErrorIterator) Key() []byte {
	return []byte{}
}

func (i ErrorIterator) Value() []byte {
	return []byte{}
}

func (i ErrorIterator) Release() {}

'''
'''--- cmd/replay/main.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"bytes"
	"context"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"os"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/consensus"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/gethhook"
	"github.com/offchainlabs/nitro/wavmio"
)

func getBlockHeaderByHash(hash common.Hash) *types.Header {
	enc, err := wavmio.ResolveTypedPreimage(arbutil.Keccak256PreimageType, hash)
	if err != nil {
		panic(fmt.Errorf("Error resolving preimage: %w", err))
	}
	header := &types.Header{}
	err = rlp.DecodeBytes(enc, &header)
	if err != nil {
		panic(fmt.Errorf("Error parsing resolved block header: %w", err))
	}
	return header
}

type WavmChainContext struct{}

func (c WavmChainContext) Engine() consensus.Engine {
	return arbos.Engine{}
}

func (c WavmChainContext) GetHeader(hash common.Hash, num uint64) *types.Header {
	header := getBlockHeaderByHash(hash)
	if !header.Number.IsUint64() || header.Number.Uint64() != num {
		panic(fmt.Sprintf("Retrieved wrong block number for header hash %v -- requested %v but got %v", hash, num, header.Number.String()))
	}
	return header
}

type WavmInbox struct{}

func (i WavmInbox) PeekSequencerInbox() ([]byte, error) {
	pos := wavmio.GetInboxPosition()
	res := wavmio.ReadInboxMessage(pos)
	log.Info("PeekSequencerInbox", "pos", pos, "res[:8]", res[:8])
	return res, nil
}

func (i WavmInbox) GetSequencerInboxPosition() uint64 {
	pos := wavmio.GetInboxPosition()
	log.Info("GetSequencerInboxPosition", "pos", pos)
	return pos
}

func (i WavmInbox) AdvanceSequencerInbox() {
	log.Info("AdvanceSequencerInbox")
	wavmio.AdvanceInboxMessage()
}

func (i WavmInbox) GetPositionWithinMessage() uint64 {
	pos := wavmio.GetPositionWithinMessage()
	log.Info("GetPositionWithinMessage", "pos", pos)
	return pos
}

func (i WavmInbox) SetPositionWithinMessage(pos uint64) {
	log.Info("SetPositionWithinMessage", "pos", pos)
	wavmio.SetPositionWithinMessage(pos)
}

func (i WavmInbox) ReadDelayedInbox(seqNum uint64) (*arbostypes.L1IncomingMessage, error) {
	log.Info("ReadDelayedMsg", "seqNum", seqNum)
	data := wavmio.ReadDelayedInboxMessage(seqNum)
	return arbostypes.ParseIncomingL1Message(bytes.NewReader(data), func(batchNum uint64) ([]byte, error) {
		return wavmio.ReadInboxMessage(batchNum), nil
	})
}

type PreimageDASReader struct {
}

func (dasReader *PreimageDASReader) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	oracle := func(hash common.Hash) ([]byte, error) {
		return wavmio.ResolveTypedPreimage(arbutil.Keccak256PreimageType, hash)
	}
	return dastree.Content(hash, oracle)
}

func (dasReader *PreimageDASReader) HealthCheck(ctx context.Context) error {
	return nil
}

func (dasReader *PreimageDASReader) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.DiscardImmediately, nil
}

// To generate:
// key, _ := crypto.HexToECDSA("0000000000000000000000000000000000000000000000000000000000000001")
// sig, _ := crypto.Sign(make([]byte, 32), key)
// println(hex.EncodeToString(sig))
const sampleSignature = "a0b37f8fba683cc68f6574cd43b39f0343a50008bf6ccea9d13231d9e7e2e1e411edc8d307254296264aebfc3dc76cd8b668373a072fd64665b50000e9fcce5201"

// We call this early to populate the secp256k1 ecc basepoint cache in the cached early machine state.
// That means we don't need to re-compute it for every block.
func populateEcdsaCaches() {
	signature, err := hex.DecodeString(sampleSignature)
	if err != nil {
		log.Warn("failed to decode sample signature to populate ECDSA cache", "err", err)
		return
	}
	_, err = crypto.Ecrecover(make([]byte, 32), signature)
	if err != nil {
		log.Warn("failed to recover signature to populate ECDSA cache", "err", err)
		return
	}
}

func main() {
	wavmio.StubInit()
	gethhook.RequireHookedGeth()

	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlError)
	log.Root().SetHandler(glogger)

	populateEcdsaCaches()

	raw := rawdb.NewDatabase(PreimageDb{})
	db := state.NewDatabase(raw)

	lastBlockHash := wavmio.GetLastBlockHash()

	var lastBlockHeader *types.Header
	var lastBlockStateRoot common.Hash
	if lastBlockHash != (common.Hash{}) {
		lastBlockHeader = getBlockHeaderByHash(lastBlockHash)
		lastBlockStateRoot = lastBlockHeader.Root
	}

	log.Info("Initial State", "lastBlockHash", lastBlockHash, "lastBlockStateRoot", lastBlockStateRoot)
	statedb, err := state.NewDeterministic(lastBlockStateRoot, db)
	if err != nil {
		panic(fmt.Sprintf("Error opening state db: %v", err.Error()))
	}

	readMessage := func(dasEnabled bool) *arbostypes.MessageWithMetadata {
		var delayedMessagesRead uint64
		if lastBlockHeader != nil {
			delayedMessagesRead = lastBlockHeader.Nonce.Uint64()
		}
		var dasReader arbstate.DataAvailabilityReader
		if dasEnabled {
			dasReader = &PreimageDASReader{}
		}
		backend := WavmInbox{}
		var keysetValidationMode = arbstate.KeysetPanicIfInvalid
		if backend.GetPositionWithinMessage() > 0 {
			keysetValidationMode = arbstate.KeysetDontValidate
		}
		inboxMultiplexer := arbstate.NewInboxMultiplexer(backend, delayedMessagesRead, dasReader, keysetValidationMode)
		ctx := context.Background()
		message, err := inboxMultiplexer.Pop(ctx)
		if err != nil {
			panic(fmt.Sprintf("Error reading from inbox multiplexer: %v", err.Error()))
		}

		return message
	}

	var newBlock *types.Block
	if lastBlockStateRoot != (common.Hash{}) {
		// ArbOS has already been initialized.
		// Load the chain config and then produce a block normally.

		initialArbosState, err := arbosState.OpenSystemArbosState(statedb, nil, true)
		if err != nil {
			panic(fmt.Sprintf("Error opening initial ArbOS state: %v", err.Error()))
		}
		chainId, err := initialArbosState.ChainId()
		if err != nil {
			panic(fmt.Sprintf("Error getting chain ID from initial ArbOS state: %v", err.Error()))
		}
		genesisBlockNum, err := initialArbosState.GenesisBlockNum()
		if err != nil {
			panic(fmt.Sprintf("Error getting genesis block number from initial ArbOS state: %v", err.Error()))
		}
		chainConfigJson, err := initialArbosState.ChainConfig()
		if err != nil {
			panic(fmt.Sprintf("Error getting chain config from initial ArbOS state: %v", err.Error()))
		}
		var chainConfig *params.ChainConfig
		if len(chainConfigJson) > 0 {
			chainConfig = &params.ChainConfig{}
			err = json.Unmarshal(chainConfigJson, chainConfig)
			if err != nil {
				panic(fmt.Sprintf("Error parsing chain config: %v", err.Error()))
			}
			if chainConfig.ChainID.Cmp(chainId) != 0 {
				panic(fmt.Sprintf("Error: chain id mismatch, chainID: %v, chainConfig.ChainID: %v", chainId, chainConfig.ChainID))
			}
			if chainConfig.ArbitrumChainParams.GenesisBlockNum != genesisBlockNum {
				panic(fmt.Sprintf("Error: genesis block number mismatch, genesisBlockNum: %v, chainConfig.ArbitrumParams.GenesisBlockNum: %v", genesisBlockNum, chainConfig.ArbitrumChainParams.GenesisBlockNum))
			}
		} else {
			log.Info("Falling back to hardcoded chain config.")
			chainConfig, err = chaininfo.GetChainConfig(chainId, "", genesisBlockNum, []string{}, "")
			if err != nil {
				panic(err)
			}
		}

		message := readMessage(chainConfig.ArbitrumChainParams.DataAvailabilityCommittee)

		chainContext := WavmChainContext{}
		batchFetcher := func(batchNum uint64) ([]byte, error) {
			return wavmio.ReadInboxMessage(batchNum), nil
		}
		newBlock, _, err = arbos.ProduceBlock(message.Message, message.DelayedMessagesRead, lastBlockHeader, statedb, chainContext, chainConfig, batchFetcher)
		if err != nil {
			panic(err)
		}

	} else {
		// Initialize ArbOS with this init message and create the genesis block.

		message := readMessage(false)

		initMessage, err := message.Message.ParseInitMessage()
		if err != nil {
			panic(err)
		}
		chainConfig := initMessage.ChainConfig
		if chainConfig == nil {
			log.Info("No chain config in the init message. Falling back to hardcoded chain config.")
			chainConfig, err = chaininfo.GetChainConfig(initMessage.ChainId, "", 0, []string{}, "")
			if err != nil {
				panic(err)
			}
		}

		_, err = arbosState.InitializeArbosState(statedb, burn.NewSystemBurner(nil, false), chainConfig, initMessage)
		if err != nil {
			panic(fmt.Sprintf("Error initializing ArbOS: %v", err.Error()))
		}

		newBlock = arbosState.MakeGenesisBlock(common.Hash{}, 0, 0, statedb.IntermediateRoot(true), chainConfig)

	}

	newBlockHash := newBlock.Hash()

	log.Info("Final State", "newBlockHash", newBlockHash, "StateRoot", newBlock.Root())

	extraInfo := types.DeserializeHeaderExtraInformation(newBlock.Header())
	if extraInfo.ArbOSFormatVersion == 0 {
		panic(fmt.Sprintf("Error deserializing header extra info: %+v", newBlock.Header()))
	}
	wavmio.SetLastBlockHash(newBlockHash)
	wavmio.SetSendRoot(extraInfo.SendRoot)

	wavmio.StubFinal()
}

'''
'''--- cmd/seq-coordinator-invalidate/seq-coordinator-invalidate.go ---
//
// Copyright 2021-2022, Offchain Labs, Inc. All rights reserved.
//

package main

import (
	"context"
	"fmt"
	"os"
	"strconv"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/redisutil"
)

func main() {
	if len(os.Args) != 4 {
		fmt.Fprintf(os.Stderr, "Usage: seq-coordinator-invalidate [redis url] [signing key] [msg index]\n")
		os.Exit(1)
	}
	redisUrl := os.Args[1]
	signingKey := os.Args[2]
	msgIndex, err := strconv.ParseUint(os.Args[3], 10, 64)
	if err != nil {
		panic("Failed to parse msg index: " + err.Error())
	}
	redisClient, err := redisutil.RedisClientFromURL(redisUrl)
	if err != nil {
		panic(err)
	}
	if redisClient == nil {
		panic("redis url not defined")
	}
	err = arbnode.StandaloneSeqCoordinatorInvalidateMsgIndex(context.Background(), redisClient, signingKey, arbutil.MessageIndex(msgIndex))
	if err != nil {
		panic(err)
	}
}

'''
'''--- cmd/seq-coordinator-manager/rediscoordinator/redis_coordinator.go ---
package rediscoordinator

import (
	"context"
	"errors"
	"strings"

	"github.com/go-redis/redis/v8"
	"github.com/offchainlabs/nitro/util/redisutil"
)

// RedisCoordinator builds upon RedisCoordinator of redisutil with additional functionality
type RedisCoordinator struct {
	*redisutil.RedisCoordinator
}

// UpdatePriorities updates the priority list of sequencers
func (rc *RedisCoordinator) UpdatePriorities(ctx context.Context, priorities []string) error {
	prioritiesString := strings.Join(priorities, ",")
	err := rc.Client.Set(ctx, redisutil.PRIORITIES_KEY, prioritiesString, 0).Err()
	if err != nil {
		if errors.Is(err, redis.Nil) {
			err = errors.New("sequencer priorities unset")
		}
		return err
	}
	return nil
}

'''
'''--- cmd/seq-coordinator-manager/seq-coordinator-manager.go ---
package main

import (
	"context"
	"fmt"
	"os"
	"strconv"

	"github.com/enescakir/emoji"
	"github.com/ethereum/go-ethereum/log"
	"github.com/gdamore/tcell/v2"
	"github.com/offchainlabs/nitro/cmd/seq-coordinator-manager/rediscoordinator"
	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/rivo/tview"
)

// Tview
var pages = tview.NewPages()
var app = tview.NewApplication()

// Lists
var prioritySeqList = tview.NewList().ShowSecondaryText(false)
var nonPrioritySeqList = tview.NewList().ShowSecondaryText(false)

// Forms
var addSeqForm = tview.NewForm()
var priorityForm = tview.NewForm()
var nonPriorityForm = tview.NewForm()

// Sequencer coordinator management UI data store
type manager struct {
	redisCoordinator *rediscoordinator.RedisCoordinator
	prioritiesSet    map[string]bool
	livelinessSet    map[string]bool
	priorityList     []string
	nonPriorityList  []string
}

func main() {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()

	args := os.Args[1:]
	if len(args) != 1 {
		fmt.Fprintf(os.Stderr, "Usage: seq-coordinator-manager [redis-url]\n")
		os.Exit(1)
	}
	redisURL := args[0]
	redisutilCoordinator, err := redisutil.NewRedisCoordinator(redisURL)
	if err != nil {
		panic(err)
	}

	seqManager := &manager{
		redisCoordinator: &rediscoordinator.RedisCoordinator{
			RedisCoordinator: redisutilCoordinator,
		},
		prioritiesSet: make(map[string]bool),
		livelinessSet: make(map[string]bool),
	}

	seqManager.refreshAllLists(ctx)
	seqManager.populateLists(ctx)

	prioritySeqList.SetSelectedFunc(func(index int, name string, second_name string, shortcut rune) {
		nonPriorityForm.Clear(true)

		n := len(seqManager.priorityList)
		priorities := make([]string, n)
		for i := 0; i < n; i++ {
			priorities[i] = strconv.Itoa(i)
		}

		target := index
		priorityForm.Clear(true)
		priorityForm.AddDropDown("Change priority to ->", priorities, index, func(priority string, selection int) {
			target = selection
		})
		priorityForm.AddButton("Update", func() {
			if target != index {
				seqManager.updatePriorityList(ctx, index, target)
			}
			priorityForm.Clear(true)
			seqManager.populateLists(ctx)
			pages.SwitchToPage("Menu")
			app.SetFocus(prioritySeqList)
		})
		priorityForm.AddButton("Cancel", func() {
			priorityForm.Clear(true)
			pages.SwitchToPage("Menu")
			app.SetFocus(prioritySeqList)
		})
		priorityForm.AddButton("Remove", func() {
			url := seqManager.priorityList[index]
			delete(seqManager.prioritiesSet, url)
			seqManager.updatePriorityList(ctx, index, 0)
			seqManager.priorityList = seqManager.priorityList[1:]

			priorityForm.Clear(true)
			seqManager.populateLists(ctx)
			pages.SwitchToPage("Menu")
			app.SetFocus(prioritySeqList)
		})
		priorityForm.SetFocus(0)
		app.SetFocus(priorityForm)
	})

	nonPrioritySeqList.SetSelectedFunc(func(index int, name string, second_name string, shortcut rune) {
		priorityForm.Clear(true)

		n := len(seqManager.priorityList)
		priorities := make([]string, n+1)
		for i := 0; i < n+1; i++ {
			priorities[i] = strconv.Itoa(i)
		}

		target := index
		nonPriorityForm.Clear(true)
		nonPriorityForm.AddDropDown("Set priority to ->", priorities, index, func(priority string, selection int) {
			target = selection
		})
		nonPriorityForm.AddButton("Update", func() {
			key := seqManager.nonPriorityList[index]
			seqManager.priorityList = append(seqManager.priorityList, key)
			seqManager.prioritiesSet[key] = true

			index = len(seqManager.priorityList) - 1
			seqManager.updatePriorityList(ctx, index, target)

			nonPriorityForm.Clear(true)
			seqManager.populateLists(ctx)
			pages.SwitchToPage("Menu")
			if len(seqManager.nonPriorityList) > 0 {
				app.SetFocus(nonPrioritySeqList)
			} else {
				app.SetFocus(prioritySeqList)
			}
		})
		nonPriorityForm.AddButton("Cancel", func() {
			nonPriorityForm.Clear(true)
			pages.SwitchToPage("Menu")
			app.SetFocus(nonPrioritySeqList)
		})
		nonPriorityForm.SetFocus(0)
		app.SetFocus(nonPriorityForm)
	})

	// UI design
	flex := tview.NewFlex()
	priorityHeading := tview.NewTextView().
		SetTextColor(tcell.ColorYellow).
		SetText("-----Priority List-----")
	nonPriorityHeading := tview.NewTextView().
		SetTextColor(tcell.ColorYellow).
		SetText("-----Not in priority list but online-----")
	instructions := tview.NewTextView().
		SetTextColor(tcell.ColorYellow).
		SetText("(r) to refresh\n(s) to save all changes\n(c) to switch between lists\n(a) to add sequencer\n(q) to quit\n(tab) to navigate")

	flex.SetDirection(tview.FlexRow).
		AddItem(priorityHeading, 0, 1, false).
		AddItem(tview.NewFlex().
			AddItem(prioritySeqList, 0, 2, true).
			AddItem(priorityForm, 0, 3, true), 0, 12, true).
		AddItem(nonPriorityHeading, 0, 1, false).
		AddItem(tview.NewFlex().
			AddItem(nonPrioritySeqList, 0, 2, true).
			AddItem(nonPriorityForm, 0, 3, true), 0, 12, true).
		AddItem(instructions, 0, 3, false).SetBorder(true)

	flex.SetInputCapture(func(event *tcell.EventKey) *tcell.EventKey {
		if event.Rune() == 114 {
			seqManager.refreshAllLists(ctx)
			priorityForm.Clear(true)
			nonPriorityForm.Clear(true)
			seqManager.populateLists(ctx)
			pages.SwitchToPage("Menu")
			app.SetFocus(prioritySeqList)
		} else if event.Rune() == 115 {
			seqManager.pushUpdates(ctx)
			priorityForm.Clear(true)
			nonPriorityForm.Clear(true)
			seqManager.populateLists(ctx)
			pages.SwitchToPage("Menu")
			app.SetFocus(prioritySeqList)
		} else if event.Rune() == 97 {
			addSeqForm.Clear(true)
			seqManager.addSeqPriorityForm(ctx)
			pages.SwitchToPage("Add Sequencer")
		} else if event.Rune() == 99 {
			if prioritySeqList.HasFocus() || priorityForm.HasFocus() {
				priorityForm.Clear(true)
				app.SetFocus(nonPrioritySeqList)
			} else {
				nonPriorityForm.Clear(true)
				app.SetFocus(prioritySeqList)
			}
		} else if event.Rune() == 113 {
			app.Stop()
		}
		return event
	})

	pages.AddPage("Menu", flex, true, true)
	pages.AddPage("Add Sequencer", addSeqForm, true, false)

	if err := app.SetRoot(pages, true).EnableMouse(true).Run(); err != nil {
		panic(err)
	}
}

// updatePriorityList updates the list by changing the position of seq present at `index` to target
func (sm *manager) updatePriorityList(ctx context.Context, index int, target int) {
	for i := index - 1; i >= target; i-- {
		sm.priorityList[i], sm.priorityList[i+1] = sm.priorityList[i+1], sm.priorityList[i]
	}
	for i := index + 1; i <= target; i++ {
		sm.priorityList[i], sm.priorityList[i-1] = sm.priorityList[i-1], sm.priorityList[i]
	}

	urlList := []string{}
	for url := range sm.livelinessSet {
		if _, ok := sm.prioritiesSet[url]; !ok {
			urlList = append(urlList, url)
		}
	}
	sm.nonPriorityList = urlList
}

// populateLists populates seq's in priority list and seq's that are online but not in priority
func (sm *manager) populateLists(ctx context.Context) {
	prioritySeqList.Clear()
	chosen, err := sm.redisCoordinator.CurrentChosenSequencer(ctx)
	if err != nil {
		panic(err)
	}
	for index, seqURL := range sm.priorityList {
		sec := ""
		if seqURL == chosen {
			sec = fmt.Sprintf(" %vchosen", emoji.LeftArrow)
		}
		status := fmt.Sprintf("(%d) %v ", index, emoji.RedCircle)
		if _, ok := sm.livelinessSet[seqURL]; ok {
			status = fmt.Sprintf("(%d) %v ", index, emoji.GreenCircle)
		}
		prioritySeqList.AddItem(status+seqURL+sec, "", rune(0), nil).SetSecondaryTextColor(tcell.ColorPurple)
	}

	nonPrioritySeqList.Clear()
	status := fmt.Sprintf("(-) %v ", emoji.GreenCircle)
	for _, seqURL := range sm.nonPriorityList {
		nonPrioritySeqList.AddItem(status+seqURL, "", rune(0), nil)
	}
}

// addSeqPriorityForm returns a form with fields to add a new sequencer to priority list
func (sm *manager) addSeqPriorityForm(ctx context.Context) *tview.Form {
	URL := ""
	addSeqForm.AddInputField("Sequencer URL", "", 0, nil, func(url string) {
		URL = url
	})
	addSeqForm.AddButton("Cancel", func() {
		priorityForm.Clear(true)
		sm.populateLists(ctx)
		pages.SwitchToPage("Menu")
	})
	addSeqForm.AddButton("Add", func() {
		// check if url is valid, i.e it doesnt already exist in the priority list
		if _, ok := sm.prioritiesSet[URL]; !ok && URL != "" {
			sm.prioritiesSet[URL] = true
			sm.priorityList = append(sm.priorityList, URL)
		}
		sm.populateLists(ctx)
		pages.SwitchToPage("Menu")
	})
	return addSeqForm
}

// pushUpdates pushes the local changes to the redis server
func (sm *manager) pushUpdates(ctx context.Context) {
	err := sm.redisCoordinator.UpdatePriorities(ctx, sm.priorityList)
	if err != nil {
		log.Warn("Failed to push local changes to the priority list")
	}
	sm.refreshAllLists(ctx)
}

// refreshAllLists gets the current status of all the lists displayed in the UI
func (sm *manager) refreshAllLists(ctx context.Context) {
	priorityList, err := sm.redisCoordinator.GetPriorities(ctx)
	if err != nil {
		panic(err)
	}
	sm.priorityList = priorityList
	sm.prioritiesSet = getMapfromlist(priorityList)

	livelinessList, err := sm.redisCoordinator.GetLiveliness(ctx)
	if err != nil {
		panic(err)
	}
	sm.livelinessSet = getMapfromlist(livelinessList)

	urlList := []string{}
	for url := range sm.livelinessSet {
		if _, ok := sm.prioritiesSet[url]; !ok {
			urlList = append(urlList, url)
		}
	}
	sm.nonPriorityList = urlList
}

func getMapfromlist(list []string) map[string]bool {
	mapping := make(map[string]bool)
	for _, url := range list {
		mapping[url] = true
	}
	return mapping
}

'''
'''--- cmd/util/chaininfoutil.go ---
package util

import (
	"context"
	"fmt"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/cmd/ipfshelper"
)

func GetL2ChainInfoIpfsFile(ctx context.Context, l2ChainInfoIpfsUrl string, l2ChainInfoIpfsDownloadPath string) (string, error) {
	ipfsNode, err := ipfshelper.CreateIpfsHelper(ctx, l2ChainInfoIpfsDownloadPath, false, []string{}, ipfshelper.DefaultIpfsProfiles)
	if err != nil {
		return "", err
	}
	log.Info("Downloading l2 info file via IPFS", "url", l2ChainInfoIpfsDownloadPath)
	l2ChainInfoFile, downloadErr := ipfsNode.DownloadFile(ctx, l2ChainInfoIpfsUrl, l2ChainInfoIpfsDownloadPath)
	closeErr := ipfsNode.Close()
	if downloadErr != nil {
		if closeErr != nil {
			log.Error("Failed to close IPFS node after download error", "err", closeErr)
		}
		return "", fmt.Errorf("failed to download file from IPFS: %w", downloadErr)
	}
	if closeErr != nil {
		return "", fmt.Errorf("failed to close IPFS node: %w", err)
	}
	return l2ChainInfoFile, nil
}

'''
'''--- cmd/util/confighelpers/configuration.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package confighelpers

import (
	"errors"
	"fmt"
	"os"
	"strings"

	"github.com/knadh/koanf"
	"github.com/knadh/koanf/parsers/json"
	koanfjson "github.com/knadh/koanf/parsers/json"
	"github.com/knadh/koanf/providers/confmap"
	"github.com/knadh/koanf/providers/env"
	"github.com/knadh/koanf/providers/file"
	"github.com/knadh/koanf/providers/posflag"
	"github.com/knadh/koanf/providers/rawbytes"
	"github.com/knadh/koanf/providers/s3"
	"github.com/mitchellh/mapstructure"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/cmd/genericconf"
)

var (
	version  = ""
	datetime = ""
	modified = ""
)

func ApplyOverrides(f *flag.FlagSet, k *koanf.Koanf) error {
	// Apply command line options and environment variables
	if err := applyOverrideOverrides(f, k); err != nil {
		return err
	}

	// Load configuration file from S3 if setup
	if len(k.String("conf.s3.secret-key")) != 0 {
		if err := loadS3Variables(k); err != nil {
			return fmt.Errorf("error loading S3 settings: %w", err)
		}

		if err := applyOverrideOverrides(f, k); err != nil {
			return err
		}
	}

	// Local config file overrides S3 config file
	configFiles := k.Strings("conf.file")
	for _, configFile := range configFiles {
		if len(configFile) > 0 {
			if err := k.Load(file.Provider(configFile), json.Parser()); err != nil {
				return fmt.Errorf("error loading local config file: %w", err)
			}

			if err := applyOverrideOverrides(f, k); err != nil {
				return err
			}
		}
	}

	return nil
}

// applyOverrideOverrides for configuration values that need to be re-applied for each configuration item applied
func applyOverrideOverrides(f *flag.FlagSet, k *koanf.Koanf) error {
	// Command line overrides config file or config string
	if err := k.Load(posflag.Provider(f, ".", k), nil); err != nil {
		return fmt.Errorf("error loading command line config: %w", err)
	}

	// Config string overrides any config file
	configString := k.String("conf.string")
	if len(configString) > 0 {
		if err := k.Load(rawbytes.Provider([]byte(configString)), json.Parser()); err != nil {
			return fmt.Errorf("error loading config string config: %w", err)
		}

		// Command line overrides config file or config string
		if err := k.Load(posflag.Provider(f, ".", k), nil); err != nil {
			return fmt.Errorf("error loading command line config: %w", err)
		}
	}

	// Environment variables overrides config files or command line options
	if err := loadEnvironmentVariables(k); err != nil {
		return fmt.Errorf("error loading environment variables: %w", err)
	}

	return nil
}

func loadEnvironmentVariables(k *koanf.Koanf) error {
	envPrefix := k.String("conf.env-prefix")
	if len(envPrefix) != 0 {
		return k.Load(env.Provider(envPrefix+"_", ".", func(s string) string {
			// FOO__BAR -> foo-bar to handle dash in config names
			s = strings.ReplaceAll(strings.ToLower(
				strings.TrimPrefix(s, envPrefix+"_")), "__", "-")
			return strings.ReplaceAll(s, "_", ".")
		}), nil)
	}

	return nil
}

func loadS3Variables(k *koanf.Koanf) error {
	return k.Load(s3.Provider(s3.Config{
		AccessKey: k.String("conf.s3.access-key"),
		SecretKey: k.String("conf.s3.secret-key"),
		Region:    k.String("conf.s3.region"),
		Bucket:    k.String("conf.s3.bucket"),
		ObjectKey: k.String("conf.s3.object-key"),
	}), nil)
}

var ErrVersion = errors.New("configuration: version requested")

func GetVersion() (string, string, string) {
	return genericconf.GetVersion(version, datetime, modified)
}

func PrintErrorAndExit(err error, usage func(string)) {
	vcsRevision, _, vcsTime := GetVersion()
	fmt.Printf("Version: %v, time: %v\n", vcsRevision, vcsTime)
	if err != nil && errors.Is(err, ErrVersion) {
		// Already printed version, just exit
		os.Exit(0)
	}
	usage(os.Args[0])
	if err != nil && !errors.Is(err, flag.ErrHelp) {
		fmt.Printf("\nFatal configuration error: %s\n", err.Error())
		os.Exit(1)
	} else {
		os.Exit(0)
	}
}

func devFlagArgs() []string {
	args := []string{
		"--init.dev-init",
		"--init.dev-init-address", "0x3f1Eae7D46d88F08fc2F8ed27FCb2AB183EB2d0E",
		"--node.dangerous.no-l1-listener",
		"--node.parent-chain-reader.enable=false",
		"--parent-chain.id=1337",
		"--chain.id=412346",
		"--persistent.chain", "/tmp/dev-test",
		"--node.sequencer",
		"--node.dangerous.no-sequencer-coordinator",
		"--node.staker.enable=false",
		"--init.empty=false",
		"--http.port", "8547",
		"--http.addr", "127.0.0.1",
	}
	return args
}

func BeginCommonParse(f *flag.FlagSet, args []string) (*koanf.Koanf, error) {
	for _, arg := range args {
		if arg == "--version" || arg == "-v" {
			return nil, ErrVersion
		} else if arg == "--dev" {
			args = devFlagArgs()
			break
		}
	}
	if err := f.Parse(args); err != nil {
		return nil, err
	}

	if f.NArg() != 0 {
		// Unexpected number of parameters
		return nil, fmt.Errorf("unexpected parameter: %s", f.Arg(0))
	}

	var k = koanf.New(".")

	// Initial application of command line parameters and environment variables so other methods can be applied
	if err := ApplyOverrides(f, k); err != nil {
		return nil, err
	}

	return k, nil
}

func EndCommonParse(k *koanf.Koanf, config interface{}) error {
	decoderConfig := mapstructure.DecoderConfig{
		ErrorUnused: true,

		// Default values
		DecodeHook: mapstructure.ComposeDecodeHookFunc(
			mapstructure.StringToTimeDurationHookFunc()),
		Metadata:         nil,
		Result:           config,
		WeaklyTypedInput: true,
	}
	err := k.UnmarshalWithConf("", config, koanf.UnmarshalConf{DecoderConfig: &decoderConfig})
	if err != nil {
		return err
	}

	return nil
}

func DumpConfig(k *koanf.Koanf, extraOverrideFields map[string]interface{}) error {
	overrideFields := map[string]interface{}{"conf.dump": false}

	// Don't keep printing configuration file
	for k, v := range extraOverrideFields {
		overrideFields[k] = v
	}

	err := k.Load(confmap.Provider(overrideFields, "."), nil)
	if err != nil {
		return fmt.Errorf("error removing extra parameters before dump: %w", err)
	}

	c, err := k.Marshal(koanfjson.Parser())
	if err != nil {
		return fmt.Errorf("unable to marshal config file to JSON: %w", err)
	}

	fmt.Println(string(c))
	os.Exit(0)
	return fmt.Errorf("Unreachable")
}

'''
'''--- cmd/util/keystore.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"fmt"
	"math/big"
	"strings"
	"syscall"

	"golang.org/x/term"

	"github.com/ethereum/go-ethereum/accounts"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/keystore"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/signature"
)

func OpenWallet(description string, walletConfig *genericconf.WalletConfig, chainId *big.Int) (*bind.TransactOpts, signature.DataSignerFunc, error) {
	if walletConfig.PrivateKey != "" {
		privateKey, err := crypto.HexToECDSA(walletConfig.PrivateKey)
		if err != nil {
			return nil, nil, err
		}
		var txOpts *bind.TransactOpts
		if chainId != nil {
			txOpts, err = bind.NewKeyedTransactorWithChainID(privateKey, chainId)
			if err != nil {
				return nil, nil, err
			}
		}
		signer := func(data []byte) ([]byte, error) {
			return crypto.Sign(data, privateKey)
		}

		return txOpts, signer, nil
	}

	ks := keystore.NewKeyStore(
		walletConfig.Pathname,
		keystore.StandardScryptN,
		keystore.StandardScryptP,
	)

	account, err := openKeystore(ks, description, walletConfig, readPass)
	if err != nil {
		return nil, nil, err
	}
	if walletConfig.OnlyCreateKey {
		log.Info(fmt.Sprintf("Wallet key created with address %s, backup wallet (%s) and remove --%s.wallet.only-create-key to run normally", account.Address.Hex(), walletConfig.Pathname, description))
		return nil, nil, nil
	}

	var txOpts *bind.TransactOpts
	if chainId != nil {
		txOpts, err = bind.NewKeyStoreTransactorWithChainID(ks, *account, chainId)
		if err != nil {
			return nil, nil, err
		}
	}
	signer := func(data []byte) ([]byte, error) {
		return ks.SignHash(*account, data)
	}

	return txOpts, signer, nil
}

func openKeystore(ks *keystore.KeyStore, description string, walletConfig *genericconf.WalletConfig, getPassword func() (string, error)) (*accounts.Account, error) {
	creatingNew := len(ks.Accounts()) == 0
	if creatingNew && !walletConfig.OnlyCreateKey {
		return nil, fmt.Errorf("no wallet exists, re-run with --%s.wallet.only-create-key to create a wallet", description)
	}
	if !creatingNew && walletConfig.OnlyCreateKey {
		return nil, fmt.Errorf("wallet key already created, backup key (%s) and remove --%s.wallet.only-create-key to run normally", walletConfig.Pathname, description)
	}
	passOpt := walletConfig.Pwd()
	var password string
	if passOpt != nil {
		password = *passOpt
	} else {
		if creatingNew {
			fmt.Print("Enter new account password: ")
		} else {
			fmt.Print("Enter account password: ")
		}
		var err error
		password, err = getPassword()
		if err != nil {
			return nil, err
		}
	}

	if creatingNew {
		a, err := ks.NewAccount(password)
		return &a, err
	}

	var account accounts.Account
	if walletConfig.Account == "" {
		if len(ks.Accounts()) > 1 {
			names := make([]string, 0, len(ks.Accounts()))
			for _, acct := range ks.Accounts() {
				names = append(names, acct.Address.Hex())
			}
			return nil, fmt.Errorf("too many existing accounts, choose one: %s", strings.Join(names, ","))
		}
		account = ks.Accounts()[0]
	} else {
		address := common.HexToAddress(walletConfig.Account)
		var emptyAddress common.Address
		if address == emptyAddress {
			return nil, fmt.Errorf("supplied address is invalid: %s", walletConfig.Account)
		}
		var err error
		account, err = ks.Find(accounts.Account{Address: address})
		if err != nil {
			return nil, err
		}
	}

	if err := ks.Unlock(account, password); err != nil {
		return nil, err
	}
	return &account, nil
}

func readPass() (string, error) {
	bytePassword, err := term.ReadPassword(syscall.Stdin)
	if err != nil {
		return "", err
	}
	passphrase := string(bytePassword)
	passphrase = strings.TrimSpace(passphrase)
	return passphrase, nil
}

'''
'''--- cmd/util/keystore_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"fmt"
	"strings"
	"testing"

	"github.com/ethereum/go-ethereum/accounts"
	"github.com/ethereum/go-ethereum/accounts/keystore"

	"github.com/offchainlabs/nitro/cmd/genericconf"
)

func openTestKeystore(description string, walletConfig *genericconf.WalletConfig, getPassword func() (string, error)) (*keystore.KeyStore, *accounts.Account, error) {
	ks := keystore.NewKeyStore(
		walletConfig.Pathname,
		keystore.LightScryptN,
		keystore.LightScryptP,
	)
	acc, err := openKeystore(ks, description, walletConfig, getPassword)
	return ks, acc, err
}

func createWallet(t *testing.T, pathname string) {
	t.Helper()
	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = pathname
	walletConf.OnlyCreateKey = true
	walletConf.Password = "foo"

	testPassCalled := false
	testPass := func() (string, error) {
		testPassCalled = true
		return "", nil
	}

	if _, _, err := openTestKeystore("test", &walletConf, testPass); err != nil {
		t.Fatalf("openTestKeystore() unexpected error: %v", err)
	}
	if testPassCalled {
		t.Error("password prompted for when it should not have been")
	}
}

func TestNewKeystoreNoCreate(t *testing.T) {
	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = t.TempDir()
	walletConf.OnlyCreateKey = false

	_, _, err := openTestKeystore("test", &walletConf, readPass)
	if err == nil {
		t.Fatalf("should have failed")
	}
	noWalletError := "no wallet exists"
	if !strings.Contains(err.Error(), noWalletError) {
		t.Fatalf("incorrect failure: %v, should have been %s", err, noWalletError)
	}
}

func TestExistingKeystoreNoCreate(t *testing.T) {
	pathname := t.TempDir()

	// Create dummy wallet
	createWallet(t, pathname)

	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = pathname
	walletConf.OnlyCreateKey = true
	walletConf.Password = "foo"

	testPassCalled := false
	testPass := func() (string, error) {
		testPassCalled = true
		return "", nil
	}

	_, _, err := openTestKeystore("test", &walletConf, testPass)
	if err == nil {
		t.Fatalf("should have failed")
	}
	keyAlreadyCreatedError := "wallet key already created"
	if !strings.Contains(err.Error(), keyAlreadyCreatedError) {
		t.Fatalf("incorrect failure: %v, should have been %s", err, keyAlreadyCreatedError)
	}
	if testPassCalled {
		t.Error("password prompted for when it should not have been")
	}
}

func TestNewKeystoreNewPasswordConfig(t *testing.T) {
	createWallet(t, t.TempDir())
}

func TestNewKeystorePromptPasswordTerminal(t *testing.T) {
	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = t.TempDir()
	walletConf.OnlyCreateKey = true
	password := "foo"

	testPassCalled := false
	getPass := func() (string, error) {
		testPassCalled = true
		return password, nil
	}

	if _, _, err := openTestKeystore("test", &walletConf, getPass); err != nil {
		t.Fatalf("openTestKeystore() unexpected error: %v", err)
	}
	if !testPassCalled {
		t.Error("password not prompted for")
	}

	// Unit test doesn't like unflushed output
	fmt.Printf("\n")
}

func TestExistingKeystorePromptPasswordTerminal(t *testing.T) {
	pathname := t.TempDir()

	// Create dummy wallet
	createWallet(t, pathname)

	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = pathname
	walletConf.OnlyCreateKey = false
	password := "foo"

	testPassCalled := false
	testPass := func() (string, error) {
		testPassCalled = true
		return password, nil
	}

	_, _, err := openTestKeystore("test", &walletConf, testPass)
	if err != nil {
		t.Fatalf("should not have have failed")
	}
	if !testPassCalled {
		t.Error("password not prompted for")
	}

	// Unit test doesn't like unflushed output
	fmt.Printf("\n")
}

func TestExistingKeystoreAccountName(t *testing.T) {
	walletConf := genericconf.WalletConfigDefault
	walletConf.Pathname = t.TempDir()
	walletConf.OnlyCreateKey = true
	password := "foo"

	testPassCalled := false
	testPass := func() (string, error) {
		testPassCalled = true
		return password, nil
	}

	if _, _, err := openTestKeystore("test", &walletConf, testPass); err != nil {
		t.Fatalf("openTestKeystore() unexpected error: %v", err)
	}
	if !testPassCalled {
		t.Error("password not prompted for")
	}

	// Get new account
	walletConf.OnlyCreateKey = false
	_, account, err := openTestKeystore("test", &walletConf, testPass)
	if err != nil {
		t.Fatalf("open failed: %v", err)
	}
	if account == nil {
		t.Fatal("account missing")
	}

	// Request account by name
	walletConf.Account = account.Address.Hex()
	_, account2, err := openTestKeystore("test", &walletConf, testPass)
	if err != nil {
		t.Fatalf("open failed: %v", err)
	}
	if !strings.EqualFold(account2.Address.Hex(), walletConf.Account) {
		t.Fatalf("requested account %s doesn't match returned account %s", walletConf.Account, account2.Address.Hex())
	}

	// Test getting key with invalid address
	walletConf.Account = "junk"
	_, _, err = openTestKeystore("test", &walletConf, testPass)
	if err == nil {
		t.Fatal("should have failed")
	}
	invalidAddressError := "address is invalid"
	keyCreatedError := "wallet key created"
	if !strings.Contains(err.Error(), invalidAddressError) {
		t.Fatalf("incorrect failure: %v, should have been %s", err, keyCreatedError)
	}

	// Test getting key with incorrect address
	walletConf.Account = "0x85d31caC32F0ECd3a978f31d040528B9A219F1C7"
	_, _, err = openTestKeystore("test", &walletConf, testPass)
	if err == nil {
		t.Fatal("should have failed")
	}
	incorrectAddressError := "no key for given address"
	if !strings.Contains(err.Error(), incorrectAddressError) {
		t.Fatalf("incorrect failure: %v, should have been %s", err, keyCreatedError)
	}

	// Unit test doesn't like unflushed output
	fmt.Printf("\n")
}

'''
'''--- codecov.yml ---
coverage:
  status:
    project:
      default: false
    patch: false
github_checks:
  annotations: false
comment:
  layout: "diff"
  behavior: once
  after_n_builds: 2

'''
'''--- das/aggregator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"math/bits"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/pretty"
)

type AggregatorConfig struct {
	Enable        bool   `koanf:"enable"`
	AssumedHonest int    `koanf:"assumed-honest"`
	Backends      string `koanf:"backends"`
}

var DefaultAggregatorConfig = AggregatorConfig{
	AssumedHonest: 0,
	Backends:      "",
}

var BatchToDasFailed = errors.New("unable to batch to DAS")

func AggregatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultAggregatorConfig.Enable, "enable storage/retrieval of sequencer batch data from a list of RPC endpoints; this should only be used by the batch poster and not in combination with other DAS storage types")
	f.Int(prefix+".assumed-honest", DefaultAggregatorConfig.AssumedHonest, "Number of assumed honest backends (H). If there are N backends, K=N+1-H valid responses are required to consider an Store request to be successful.")
	f.String(prefix+".backends", DefaultAggregatorConfig.Backends, "JSON RPC backend configuration")
}

type Aggregator struct {
	config         AggregatorConfig
	services       []ServiceDetails
	requestTimeout time.Duration

	// calculated fields
	requiredServicesForStore       int
	maxAllowedServiceStoreFailures int
	keysetHash                     [32]byte
	keysetBytes                    []byte
	addrVerifier                   *contracts.AddressVerifier
}

type ServiceDetails struct {
	service     DataAvailabilityServiceWriter
	pubKey      blsSignatures.PublicKey
	signersMask uint64
	metricName  string
}

func (s *ServiceDetails) String() string {
	return fmt.Sprintf("ServiceDetails{service: %v, signersMask %d}", s.service, s.signersMask)
}

func NewServiceDetails(service DataAvailabilityServiceWriter, pubKey blsSignatures.PublicKey, signersMask uint64, metricName string) (*ServiceDetails, error) {
	if bits.OnesCount64(signersMask) != 1 {
		return nil, fmt.Errorf("tried to configure backend DAS %v with invalid signersMask %X", service, signersMask)
	}
	return &ServiceDetails{
		service:     service,
		pubKey:      pubKey,
		signersMask: signersMask,
		metricName:  metricName,
	}, nil
}

func NewAggregator(ctx context.Context, config DataAvailabilityConfig, services []ServiceDetails) (*Aggregator, error) {
	if config.ParentChainNodeURL == "none" {
		return NewAggregatorWithSeqInboxCaller(config, services, nil)
	}
	l1client, err := GetL1Client(ctx, config.ParentChainConnectionAttempts, config.ParentChainNodeURL)
	if err != nil {
		return nil, err
	}
	seqInboxAddress, err := OptionalAddressFromString(config.SequencerInboxAddress)
	if err != nil {
		return nil, err
	}
	if seqInboxAddress == nil {
		return NewAggregatorWithSeqInboxCaller(config, services, nil)
	}
	return NewAggregatorWithL1Info(config, services, l1client, *seqInboxAddress)
}

func NewAggregatorWithL1Info(
	config DataAvailabilityConfig,
	services []ServiceDetails,
	l1client arbutil.L1Interface,
	seqInboxAddress common.Address,
) (*Aggregator, error) {
	seqInboxCaller, err := bridgegen.NewSequencerInboxCaller(seqInboxAddress, l1client)
	if err != nil {
		return nil, err
	}
	return NewAggregatorWithSeqInboxCaller(config, services, seqInboxCaller)
}

func NewAggregatorWithSeqInboxCaller(
	config DataAvailabilityConfig,
	services []ServiceDetails,
	seqInboxCaller *bridgegen.SequencerInboxCaller,
) (*Aggregator, error) {

	keysetHash, keysetBytes, err := KeysetHashFromServices(services, uint64(config.RPCAggregator.AssumedHonest))
	if err != nil {
		return nil, err
	}

	var addrVerifier *contracts.AddressVerifier
	if seqInboxCaller != nil {
		addrVerifier = contracts.NewAddressVerifier(seqInboxCaller)
	}

	return &Aggregator{
		config:                         config.RPCAggregator,
		services:                       services,
		requestTimeout:                 config.RequestTimeout,
		requiredServicesForStore:       len(services) + 1 - config.RPCAggregator.AssumedHonest,
		maxAllowedServiceStoreFailures: config.RPCAggregator.AssumedHonest - 1,
		keysetHash:                     keysetHash,
		keysetBytes:                    keysetBytes,
		addrVerifier:                   addrVerifier,
	}, nil
}

type storeResponse struct {
	details ServiceDetails
	sig     blsSignatures.Signature
	err     error
}

// Store calls Store on each backend DAS in parallel and collects responses.
// If there were at least K responses then it aggregates the signatures and
// signersMasks from each DAS together into the DataAvailabilityCertificate
// then Store returns immediately. If there were any backend Store subroutines
// that were still running when Aggregator.Store returns, they are allowed to
// continue running until the context is canceled (eg via TimeoutWrapper),
// with their results discarded.
//
// If Store gets enough errors that K successes is impossible, then it stops early
// and returns an error.
//
// If Store gets not enough successful responses by the time its context is canceled
// (eg via TimeoutWrapper) then it also returns an error.
//
// If Sequencer Inbox contract details are provided when a das.Aggregator is
// constructed, calls to Store(...) will try to verify the passed-in data's signature
// is from the batch poster. If the contract details are not provided, then the
// signature is not checked, which is useful for testing.
func (a *Aggregator) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	log.Trace("das.Aggregator.Store", "message", pretty.FirstFewBytes(message), "timeout", time.Unix(int64(timeout), 0), "sig", pretty.FirstFewBytes(sig))
	if a.addrVerifier != nil {
		actualSigner, err := DasRecoverSigner(message, timeout, sig)
		if err != nil {
			return nil, err
		}
		isBatchPosterOrSequencer, err := a.addrVerifier.IsBatchPosterOrSequencer(ctx, actualSigner)
		if err != nil {
			return nil, err
		}
		if !isBatchPosterOrSequencer {
			return nil, errors.New("store request not properly signed")
		}
	}

	responses := make(chan storeResponse, len(a.services))

	expectedHash := dastree.Hash(message)
	for _, d := range a.services {
		go func(ctx context.Context, d ServiceDetails) {
			storeCtx, cancel := context.WithTimeout(ctx, a.requestTimeout)
			const metricBase string = "arb/das/rpc/aggregator/store"
			var metricWithServiceName = metricBase + "/" + d.metricName
			defer cancel()
			incFailureMetric := func() {
				metrics.GetOrRegisterCounter(metricWithServiceName+"/error/total", nil).Inc(1)
				metrics.GetOrRegisterCounter(metricBase+"/error/all/total", nil).Inc(1)
			}

			cert, err := d.service.Store(storeCtx, message, timeout, sig)
			if err != nil {
				incFailureMetric()
				if errors.Is(err, context.DeadlineExceeded) {
					metrics.GetOrRegisterCounter(metricWithServiceName+"/error/timeout/total", nil).Inc(1)
				} else {
					metrics.GetOrRegisterCounter(metricWithServiceName+"/error/client/total", nil).Inc(1)
				}
				responses <- storeResponse{d, nil, err}
				return
			}

			verified, err := blsSignatures.VerifySignature(
				cert.Sig, cert.SerializeSignableFields(), d.pubKey,
			)
			if err != nil {
				incFailureMetric()
				metrics.GetOrRegisterCounter(metricWithServiceName+"/error/bad_response/total", nil).Inc(1)
				responses <- storeResponse{d, nil, err}
				return
			}
			if !verified {
				incFailureMetric()
				metrics.GetOrRegisterCounter(metricWithServiceName+"/error/bad_response/total", nil).Inc(1)
				responses <- storeResponse{d, nil, errors.New("signature verification failed")}
				return
			}

			// SignersMask from backend DAS is ignored.

			if cert.DataHash != expectedHash {
				incFailureMetric()
				metrics.GetOrRegisterCounter(metricWithServiceName+"/error/bad_response/total", nil).Inc(1)
				responses <- storeResponse{d, nil, errors.New("hash verification failed")}
				return
			}
			if cert.Timeout != timeout {
				incFailureMetric()
				metrics.GetOrRegisterCounter(metricWithServiceName+"/error/bad_response/total", nil).Inc(1)
				responses <- storeResponse{d, nil, fmt.Errorf("timeout was %d, expected %d", cert.Timeout, timeout)}
				return
			}

			metrics.GetOrRegisterCounter(metricWithServiceName+"/success/total", nil).Inc(1)
			metrics.GetOrRegisterCounter(metricBase+"/success/all/total", nil).Inc(1)
			responses <- storeResponse{d, cert.Sig, nil}
		}(ctx, d)
	}

	var aggCert arbstate.DataAvailabilityCertificate

	type certDetails struct {
		pubKeys        []blsSignatures.PublicKey
		sigs           []blsSignatures.Signature
		aggSignersMask uint64
		err            error
	}

	// Collect responses from backends.
	certDetailsChan := make(chan certDetails)
	go func() {
		var pubKeys []blsSignatures.PublicKey
		var sigs []blsSignatures.Signature
		var aggSignersMask uint64
		var storeFailures, successfullyStoredCount int
		var returned bool
		for i := 0; i < len(a.services); i++ {

			select {
			case <-ctx.Done():
				break
			case r := <-responses:
				if r.err != nil {
					storeFailures++
					log.Warn("das.Aggregator: Error from backend", "backend", r.details.service, "signerMask", r.details.signersMask, "err", r.err)
				} else {
					pubKeys = append(pubKeys, r.details.pubKey)
					sigs = append(sigs, r.sig)
					aggSignersMask |= r.details.signersMask

					successfullyStoredCount++
				}
			}

			// As soon as enough responses are returned, pass the response to
			// certDetailsChan, so the Store function can return, but also continue
			// running until all responses are received (or the context is canceled)
			// in order to produce accurate logs/metrics.
			if !returned {
				if successfullyStoredCount >= a.requiredServicesForStore {
					cd := certDetails{}
					cd.pubKeys = append(cd.pubKeys, pubKeys...)
					cd.sigs = append(cd.sigs, sigs...)
					cd.aggSignersMask = aggSignersMask
					certDetailsChan <- cd
					returned = true
					if a.maxAllowedServiceStoreFailures > 0 && // Ignore the case where AssumedHonest = 1, probably a testnet
						storeFailures+1 > a.maxAllowedServiceStoreFailures {
						log.Error("das.Aggregator: storing the batch data succeeded to enough DAS commitee members to generate the Data Availability Cert, but if one more had failed then the cert would not have been able to be generated. Look for preceding logs with \"Error from backend\"")
					}
				} else if storeFailures > a.maxAllowedServiceStoreFailures {
					cd := certDetails{}
					cd.err = fmt.Errorf("aggregator failed to store message to at least %d out of %d DASes (assuming %d are honest). %w", a.requiredServicesForStore, len(a.services), a.config.AssumedHonest, BatchToDasFailed)
					certDetailsChan <- cd
					returned = true
				}
			}

		}
	}()

	cd := <-certDetailsChan

	if cd.err != nil {
		return nil, cd.err
	}

	aggCert.Sig = blsSignatures.AggregateSignatures(cd.sigs)
	aggPubKey := blsSignatures.AggregatePublicKeys(cd.pubKeys)
	aggCert.SignersMask = cd.aggSignersMask

	aggCert.DataHash = expectedHash
	aggCert.Timeout = timeout
	aggCert.KeysetHash = a.keysetHash
	aggCert.Version = 1

	verified, err := blsSignatures.VerifySignature(aggCert.Sig, aggCert.SerializeSignableFields(), aggPubKey)
	if err != nil {
		//nolint:errorlint
		return nil, fmt.Errorf("%s. %w", err.Error(), BatchToDasFailed)
	}
	if !verified {
		return nil, fmt.Errorf("failed aggregate signature check. %w", BatchToDasFailed)
	}
	return &aggCert, nil
}

func (a *Aggregator) String() string {
	var b bytes.Buffer
	b.WriteString("das.Aggregator{")
	first := true
	for _, d := range a.services {
		if !first {
			b.WriteString(",")
		}
		b.WriteString(fmt.Sprintf("signersMask(aggregator):%d,", d.signersMask))
		b.WriteString(d.service.String())
	}
	b.WriteString("}")
	return b.String()
}

'''
'''--- das/aggregator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"math/rand"
	"os"
	"strconv"
	"sync"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/blsSignatures"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
)

func TestDAS_BasicAggregationLocal(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	numBackendDAS := 10
	var backends []ServiceDetails
	var storageServices []StorageService
	for i := 0; i < numBackendDAS; i++ {
		privKey, err := blsSignatures.GeneratePrivKeyString()
		Require(t, err)

		config := DataAvailabilityConfig{
			Enable: true,
			Key: KeyConfig{
				PrivKey: privKey,
			},
			ParentChainNodeURL: "none",
		}

		storageServices = append(storageServices, NewMemoryBackedStorageService(ctx))
		das, err := NewSignAfterStoreDASWriter(ctx, config, storageServices[i])
		Require(t, err)
		signerMask := uint64(1 << i)
		details, err := NewServiceDetails(das, *das.pubKey, signerMask, "service"+strconv.Itoa(i))
		Require(t, err)
		backends = append(backends, *details)
	}

	aggregator, err := NewAggregator(ctx, DataAvailabilityConfig{RPCAggregator: AggregatorConfig{AssumedHonest: 1}, ParentChainNodeURL: "none"}, backends)
	Require(t, err)

	rawMsg := []byte("It's time for you to see the fnords.")
	cert, err := aggregator.Store(ctx, rawMsg, 0, []byte{})
	Require(t, err, "Error storing message")

	for _, storageService := range storageServices {
		messageRetrieved, err := storageService.GetByHash(ctx, cert.DataHash)
		Require(t, err, "Failed to retrieve message")
		if !bytes.Equal(rawMsg, messageRetrieved) {
			Fail(t, "Retrieved message is not the same as stored one.")
		}
	}
}

type failureType int

const (
	success failureType = iota
	immediateError
	tooSlow
	dataCorruption
)

type failureInjector interface {
	shouldFail() failureType
}

type randomBagOfFailures struct {
	t        *testing.T
	failures []failureType
	mutex    sync.Mutex
}

func newRandomBagOfFailures(t *testing.T, nSuccess, nFailures int, highestFailureType failureType) *randomBagOfFailures {
	var failures []failureType
	for i := 0; i < nSuccess; i++ {
		failures = append(failures, success)
	}

	for i := 0; i < nFailures; i++ {
		failures = append(failures, failureType(rand.Int()%int(highestFailureType)+1))
	}

	rand.Shuffle(len(failures), func(i, j int) { failures[i], failures[j] = failures[j], failures[i] })

	log.Trace("Injected failures", "failures", failures)

	return &randomBagOfFailures{
		t:        t,
		failures: failures,
	}
}

func (b *randomBagOfFailures) shouldFail() failureType {
	b.mutex.Lock()
	defer b.mutex.Unlock()
	if len(b.failures) == 0 {
		Fail(b.t, "shouldFail called more times than expected")
	}

	toReturn := b.failures[0]
	b.failures = b.failures[1:]
	return toReturn
}

type WrapStore struct {
	t        *testing.T
	injector failureInjector
	DataAvailabilityServiceWriter
}

func (w *WrapStore) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	switch w.injector.shouldFail() {
	case success:
		return w.DataAvailabilityServiceWriter.Store(ctx, message, timeout, sig)
	case immediateError:
		return nil, errors.New("expected Store failure")
	case tooSlow:
		<-ctx.Done()
		return nil, ctx.Err()
	case dataCorruption:
		cert, err := w.DataAvailabilityServiceWriter.Store(ctx, message, timeout, sig)
		if err != nil {
			return nil, err
		}
		cert.DataHash[0] = ^cert.DataHash[0]
		return cert, nil
	}
	Fail(w.t)
	return nil, nil
}

func max(a, b int) int {
	if a > b {
		return a
	}
	return b
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func enableLogging() {
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlTrace)
	log.Root().SetHandler(glogger)
}

func testConfigurableStorageFailures(t *testing.T, shouldFailAggregation bool) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	numBackendDAS := (rand.Int() % 20) + 1
	assumedHonest := (rand.Int() % numBackendDAS) + 1
	var nFailures int
	if shouldFailAggregation {
		nFailures = max(assumedHonest, rand.Int()%(numBackendDAS+1))
	} else {
		nFailures = min(assumedHonest-1, rand.Int()%(numBackendDAS+1))
	}
	nSuccesses := numBackendDAS - nFailures
	log.Trace(fmt.Sprintf("Testing aggregator with K:%d with K=N+1-H, N:%d, H:%d, and %d successes", numBackendDAS+1-assumedHonest, numBackendDAS, assumedHonest, nSuccesses))

	injectedFailures := newRandomBagOfFailures(t, nSuccesses, nFailures, dataCorruption)
	var backends []ServiceDetails
	var storageServices []StorageService
	for i := 0; i < numBackendDAS; i++ {
		privKey, err := blsSignatures.GeneratePrivKeyString()
		Require(t, err)

		config := DataAvailabilityConfig{
			Enable: true,
			Key: KeyConfig{
				PrivKey: privKey,
			},
			ParentChainNodeURL: "none",
		}

		storageServices = append(storageServices, NewMemoryBackedStorageService(ctx))
		das, err := NewSignAfterStoreDASWriter(ctx, config, storageServices[i])
		Require(t, err)
		signerMask := uint64(1 << i)
		details, err := NewServiceDetails(&WrapStore{t, injectedFailures, das}, *das.pubKey, signerMask, "service"+strconv.Itoa(i))
		Require(t, err)
		backends = append(backends, *details)
	}

	aggregator, err := NewAggregator(
		ctx,
		DataAvailabilityConfig{
			RPCAggregator:      AggregatorConfig{AssumedHonest: assumedHonest},
			ParentChainNodeURL: "none",
			RequestTimeout:     time.Millisecond * 2000,
		}, backends)
	Require(t, err)

	rawMsg := []byte("It's time for you to see the fnords.")
	cert, err := aggregator.Store(ctx, rawMsg, 0, []byte{})
	if !shouldFailAggregation {
		Require(t, err, "Error storing message")
	} else {
		if err == nil {
			Fail(t, "Expected error from too many failed DASes.")
		}
		return
	}

	// Wait for all stores that would succeed to succeed.
	time.Sleep(time.Millisecond * 2000)
	retrievalFailures := 0
	for _, storageService := range storageServices {
		messageRetrieved, err := storageService.GetByHash(ctx, cert.DataHash)
		if err != nil {
			retrievalFailures++
		} else if !bytes.Equal(rawMsg, messageRetrieved) {
			retrievalFailures++
		}
	}
	if retrievalFailures > nFailures {
		Fail(t, fmt.Sprintf("retrievalFailures(%d) > nFailures(%d)", retrievalFailures, nFailures))
	}
}

func initTest(t *testing.T) int {
	t.Parallel()
	seed := time.Now().UnixNano()
	seedStr := os.Getenv("SEED")
	if len(seedStr) > 0 {
		var err error
		intSeed, err := strconv.Atoi(seedStr)
		Require(t, err, "Failed to parse string")
		seed = int64(intSeed)
	}
	rand.Seed(seed)

	runsStr := os.Getenv("RUNS")
	runs := 2 ^ 32
	if len(runsStr) > 0 {
		var err error
		runs, err = strconv.Atoi(runsStr)
		Require(t, err, "Failed to parse string")
	}

	loggingStr := os.Getenv("LOGGING")
	if len(loggingStr) > 0 {
		enableLogging()
	}

	log.Trace(fmt.Sprintf("Running test with seed %d", seed))

	return runs
}

func TestDAS_LessThanHStorageFailures(t *testing.T) {
	runs := initTest(t)

	for i := 0; i < min(runs, 20); i++ {
		t.Run(fmt.Sprintf("%d", i), func(t *testing.T) {
			t.Parallel()
			testConfigurableStorageFailures(t, false)
		})
	}
}

func TestDAS_AtLeastHStorageFailures(t *testing.T) {
	runs := initTest(t)
	for i := 0; i < min(runs, 10); i++ {
		t.Run(fmt.Sprintf("%d", i), func(t *testing.T) {
			t.Parallel()
			testConfigurableStorageFailures(t, true)
		})
	}
}

'''
'''--- das/bigcache_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"time"

	"github.com/allegro/bigcache"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
)

type BigCacheConfig struct {
	// TODO add other config information like HardMaxCacheSize
	Enable             bool          `koanf:"enable"`
	Expiration         time.Duration `koanf:"expiration"`
	MaxEntriesInWindow int
}

var DefaultBigCacheConfig = BigCacheConfig{
	Expiration: time.Hour,
}

var TestBigCacheConfig = BigCacheConfig{
	Enable:             true,
	Expiration:         time.Hour,
	MaxEntriesInWindow: 1000,
}

func BigCacheConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultBigCacheConfig.Enable, "Enable local in-memory caching of sequencer batch data")
	f.Duration(prefix+".expiration", DefaultBigCacheConfig.Expiration, "Expiration time for in-memory cached sequencer batches")
}

type BigCacheStorageService struct {
	baseStorageService StorageService
	bigCacheConfig     BigCacheConfig
	bigCache           *bigcache.BigCache
}

func NewBigCacheStorageService(bigCacheConfig BigCacheConfig, baseStorageService StorageService) (StorageService, error) {
	conf := bigcache.DefaultConfig(bigCacheConfig.Expiration)
	if bigCacheConfig.MaxEntriesInWindow > 0 {
		conf.MaxEntriesInWindow = bigCacheConfig.MaxEntriesInWindow
	}
	bigCache, err := bigcache.NewBigCache(conf)
	if err != nil {
		return nil, err
	}
	return &BigCacheStorageService{
		baseStorageService: baseStorageService,
		bigCacheConfig:     bigCacheConfig,
		bigCache:           bigCache,
	}, nil
}

func (bcs *BigCacheStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.BigCacheStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", bcs)

	ret, err := bcs.bigCache.Get(string(key.Bytes()))
	if err != nil {
		ret, err = bcs.baseStorageService.GetByHash(ctx, key)
		if err != nil {
			return nil, err
		}

		err = bcs.bigCache.Set(string(key.Bytes()), ret)
		if err != nil {
			return nil, err
		}
		return ret, err
	}

	return ret, err
}

func (bcs *BigCacheStorageService) Put(ctx context.Context, value []byte, timeout uint64) error {
	logPut("das.BigCacheStorageService.Put", value, timeout, bcs)
	err := bcs.baseStorageService.Put(ctx, value, timeout)
	if err != nil {
		return err
	}
	return bcs.bigCache.Set(string(dastree.HashBytes(value)), value)
}

func (bcs *BigCacheStorageService) Sync(ctx context.Context) error {
	return bcs.baseStorageService.Sync(ctx)
}

func (bcs *BigCacheStorageService) Close(ctx context.Context) error {
	err := bcs.bigCache.Close()
	if err != nil {
		return err
	}
	return bcs.baseStorageService.Close(ctx)
}

func (bcs *BigCacheStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return bcs.baseStorageService.ExpirationPolicy(ctx)
}

func (bcs *BigCacheStorageService) String() string {
	return fmt.Sprintf("BigCacheStorageService(%+v)", bcs.bigCacheConfig)
}

func (bcs *BigCacheStorageService) HealthCheck(ctx context.Context) error {
	return bcs.baseStorageService.HealthCheck(ctx)
}

'''
'''--- das/bigcache_storage_service_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"testing"
	"time"

	"github.com/allegro/bigcache"
	"github.com/offchainlabs/nitro/das/dastree"
)

func TestBigCacheStorageService(t *testing.T) {
	ctx := context.Background()
	timeout := uint64(time.Now().Add(time.Hour).Unix())
	baseStorageService := NewMemoryBackedStorageService(ctx)
	bigCache, err := bigcache.NewBigCache(bigcache.DefaultConfig(TestBigCacheConfig.Expiration))
	Require(t, err)
	bigCacheService := &BigCacheStorageService{
		baseStorageService: baseStorageService,
		bigCacheConfig:     TestBigCacheConfig,
		bigCache:           bigCache,
	}
	Require(t, err)

	val1 := []byte("The first value")
	val1CorrectKey := dastree.Hash(val1)
	val1IncorrectKey := dastree.Hash(append(val1, 0))

	_, err = bigCacheService.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}

	err = bigCacheService.Put(ctx, val1, timeout)
	Require(t, err)

	_, err = bigCacheService.GetByHash(ctx, val1IncorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err := bigCacheService.GetByHash(ctx, val1CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}

	// For Case where the value is present in the base storage but not present in the cache.
	val2 := []byte("The Second value")
	val2CorrectKey := dastree.Hash(val2)
	val2IncorrectKey := dastree.Hash(append(val2, 0))

	err = baseStorageService.Put(ctx, val2, timeout)
	Require(t, err)

	_, err = bigCacheService.GetByHash(ctx, val2IncorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err = bigCacheService.GetByHash(ctx, val2CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val2) {
		t.Fatal(val, val2)
	}

	// For Case where the value is present in the cache storage but not present in the base.
	emptyBaseStorageService := NewMemoryBackedStorageService(ctx)
	bigCacheServiceWithEmptyBaseStorage := &BigCacheStorageService{
		baseStorageService: emptyBaseStorageService,
		bigCacheConfig:     TestBigCacheConfig,
		bigCache:           bigCache,
	}
	val, err = bigCacheServiceWithEmptyBaseStorage.GetByHash(ctx, val1CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}

	// Closes the base storage properly.
	err = bigCacheService.Close(ctx)
	Require(t, err)
	_, err = baseStorageService.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrClosed) {
		t.Fatal(err)
	}
}

'''
'''--- das/chain_fetch_das.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"sync"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/util/pretty"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

type syncedKeysetCache struct {
	cache map[[32]byte][]byte
	sync.RWMutex
}

func (c *syncedKeysetCache) get(key [32]byte) ([]byte, bool) {
	c.RLock()
	defer c.RUnlock()
	res, ok := c.cache[key]
	return res, ok
}

func (c *syncedKeysetCache) put(key [32]byte, value []byte) {
	c.Lock()
	defer c.Unlock()
	c.cache[key] = value
}

type ChainFetchReader struct {
	arbstate.DataAvailabilityReader
	seqInboxCaller   *bridgegen.SequencerInboxCaller
	seqInboxFilterer *bridgegen.SequencerInboxFilterer
	keysetCache      syncedKeysetCache
}

func NewChainFetchReader(inner arbstate.DataAvailabilityReader, l1client arbutil.L1Interface, seqInboxAddr common.Address) (*ChainFetchReader, error) {
	seqInbox, err := bridgegen.NewSequencerInbox(seqInboxAddr, l1client)
	if err != nil {
		return nil, err
	}

	return NewChainFetchReaderWithSeqInbox(inner, seqInbox)
}

func NewChainFetchReaderWithSeqInbox(inner arbstate.DataAvailabilityReader, seqInbox *bridgegen.SequencerInbox) (*ChainFetchReader, error) {
	return &ChainFetchReader{
		DataAvailabilityReader: inner,
		seqInboxCaller:         &seqInbox.SequencerInboxCaller,
		seqInboxFilterer:       &seqInbox.SequencerInboxFilterer,
		keysetCache:            syncedKeysetCache{cache: make(map[[32]byte][]byte)},
	}, nil
}

func (c *ChainFetchReader) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	log.Trace("das.ChainFetchReader.GetByHash", "hash", pretty.PrettyHash(hash))
	return chainFetchGetByHash(ctx, c.DataAvailabilityReader, &c.keysetCache, c.seqInboxCaller, c.seqInboxFilterer, hash)
}
func (c *ChainFetchReader) String() string {
	return "ChainFetchReader"
}

func chainFetchGetByHash(
	ctx context.Context,
	daReader arbstate.DataAvailabilityReader,
	cache *syncedKeysetCache,
	seqInboxCaller *bridgegen.SequencerInboxCaller,
	seqInboxFilterer *bridgegen.SequencerInboxFilterer,
	hash common.Hash,
) ([]byte, error) {
	// try to fetch from the cache
	res, ok := cache.get(hash)
	if ok {
		return res, nil
	}

	// try to fetch from the inner DAS
	innerRes, err := daReader.GetByHash(ctx, hash)
	if err == nil && dastree.ValidHash(hash, innerRes) {
		return innerRes, nil
	}

	// try to fetch from the L1 chain
	blockNumBig, err := seqInboxCaller.GetKeysetCreationBlock(&bind.CallOpts{Context: ctx}, hash)
	if err != nil {
		return nil, err
	}
	if !blockNumBig.IsUint64() {
		return nil, errors.New("block number too large")
	}
	blockNum := blockNumBig.Uint64()
	blockNumPlus1 := blockNum + 1

	filterOpts := &bind.FilterOpts{
		Start:   blockNum,
		End:     &blockNumPlus1,
		Context: ctx,
	}
	iter, err := seqInboxFilterer.FilterSetValidKeyset(filterOpts, [][32]byte{hash})
	if err != nil {
		return nil, err
	}
	for iter.Next() {
		if dastree.ValidHash(hash, iter.Event.KeysetBytes) {
			cache.put(hash, iter.Event.KeysetBytes)
			return iter.Event.KeysetBytes, nil
		}
	}
	if iter.Error() != nil {
		return nil, iter.Error()
	}

	return nil, ErrNotFound
}

'''
'''--- das/das.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
)

type DataAvailabilityServiceWriter interface {
	// Store requests that the message be stored until timeout (UTC time in unix epoch seconds).
	Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error)
	fmt.Stringer
}

type DataAvailabilityServiceReader interface {
	arbstate.DataAvailabilityReader
	fmt.Stringer
}

type DataAvailabilityServiceHealthChecker interface {
	HealthCheck(ctx context.Context) error
}

type DataAvailabilityConfig struct {
	Enable bool `koanf:"enable"`

	RequestTimeout time.Duration `koanf:"request-timeout"`

	LocalCache BigCacheConfig `koanf:"local-cache"`
	RedisCache RedisConfig    `koanf:"redis-cache"`

	LocalDBStorage     LocalDBStorageConfig     `koanf:"local-db-storage"`
	LocalFileStorage   LocalFileStorageConfig   `koanf:"local-file-storage"`
	S3Storage          S3StorageServiceConfig   `koanf:"s3-storage"`
	IpfsStorage        IpfsStorageServiceConfig `koanf:"ipfs-storage"`
	RegularSyncStorage RegularSyncStorageConfig `koanf:"regular-sync-storage"`

	Key KeyConfig `koanf:"key"`

	RPCAggregator  AggregatorConfig              `koanf:"rpc-aggregator"`
	RestAggregator RestfulClientAggregatorConfig `koanf:"rest-aggregator"`
	NEARAggregator NearAggregatorConfig          `koanf:"near-aggregator"`

	ParentChainNodeURL              string `koanf:"parent-chain-node-url"`
	ParentChainConnectionAttempts   int    `koanf:"parent-chain-connection-attempts"`
	SequencerInboxAddress           string `koanf:"sequencer-inbox-address"`
	ExtraSignatureCheckingPublicKey string `koanf:"extra-signature-checking-public-key"`

	PanicOnError             bool `koanf:"panic-on-error"`
	DisableSignatureChecking bool `koanf:"disable-signature-checking"`
}

var DefaultDataAvailabilityConfig = DataAvailabilityConfig{
	RequestTimeout:                5 * time.Second,
	Enable:                        false,
	RestAggregator:                DefaultRestfulClientAggregatorConfig,
	ParentChainConnectionAttempts: 15,
	PanicOnError:                  false,
	IpfsStorage:                   DefaultIpfsStorageServiceConfig,
}

func OptionalAddressFromString(s string) (*common.Address, error) {
	if s == "none" {
		return nil, nil
	}
	if s == "" {
		return nil, errors.New("must provide address for signer or specify 'none'")
	}
	if !common.IsHexAddress(s) {
		return nil, fmt.Errorf("invalid address for signer: %v", s)
	}
	addr := common.HexToAddress(s)
	return &addr, nil
}

func DataAvailabilityConfigAddNodeOptions(prefix string, f *flag.FlagSet) {
	dataAvailabilityConfigAddOptions(prefix, f, roleNode)
}

func DataAvailabilityConfigAddDaserverOptions(prefix string, f *flag.FlagSet) {
	dataAvailabilityConfigAddOptions(prefix, f, roleDaserver)
}

type role int

const (
	roleNode role = iota
	roleDaserver
)

func dataAvailabilityConfigAddOptions(prefix string, f *flag.FlagSet, r role) {
	f.Bool(prefix+".enable", DefaultDataAvailabilityConfig.Enable, "enable Anytrust Data Availability mode")
	f.Bool(prefix+".panic-on-error", DefaultDataAvailabilityConfig.PanicOnError, "whether the Data Availability Service should fail immediately on errors (not recommended)")

	if r == roleDaserver {
		f.Bool(prefix+".disable-signature-checking", DefaultDataAvailabilityConfig.DisableSignatureChecking, "disables signature checking on Data Availability Store requests (DANGEROUS, FOR TESTING ONLY)")

		// Cache options
		BigCacheConfigAddOptions(prefix+".local-cache", f)
		RedisConfigAddOptions(prefix+".redis-cache", f)

		// Storage options
		LocalDBStorageConfigAddOptions(prefix+".local-db-storage", f)
		LocalFileStorageConfigAddOptions(prefix+".local-file-storage", f)
		S3ConfigAddOptions(prefix+".s3-storage", f)
		RegularSyncStorageConfigAddOptions(prefix+".regular-sync-storage", f)

		// Key config for storage
		KeyConfigAddOptions(prefix+".key", f)

		f.String(prefix+".extra-signature-checking-public-key", DefaultDataAvailabilityConfig.ExtraSignatureCheckingPublicKey, "public key to use to validate Data Availability Store requests in addition to the Sequencer's public key determined using sequencer-inbox-address, can be a file or the hex-encoded public key beginning with 0x; useful for testing")
	}
	if r == roleNode {
		// These are only for batch poster
		AggregatorConfigAddOptions(prefix+".rpc-aggregator", f)
		f.Duration(prefix+".request-timeout", DefaultDataAvailabilityConfig.RequestTimeout, "Data Availability Service timeout duration for Store requests")
	}

	// Both the Nitro node and daserver can use these options.
	IpfsStorageServiceConfigAddOptions(prefix+".ipfs-storage", f)
	RestfulClientAggregatorConfigAddOptions(prefix+".rest-aggregator", f)
	NearAggregatorConfigAddOptions(prefix+".near-aggregator", f)

	f.String(prefix+".parent-chain-node-url", DefaultDataAvailabilityConfig.ParentChainNodeURL, "URL for parent chain node, only used in standalone daserver; when running as part of a node that node's L1 configuration is used")
	f.Int(prefix+".parent-chain-connection-attempts", DefaultDataAvailabilityConfig.ParentChainConnectionAttempts, "parent chain RPC connection attempts (spaced out at least 1 second per attempt, 0 to retry infinitely), only used in standalone daserver; when running as part of a node that node's parent chain configuration is used")
	f.String(prefix+".sequencer-inbox-address", DefaultDataAvailabilityConfig.SequencerInboxAddress, "parent chain address of SequencerInbox contract")
}

func Serialize(c *arbstate.DataAvailabilityCertificate) []byte {

	flags := arbstate.DASMessageHeaderFlag
	if c.Version != 0 {
		flags |= arbstate.TreeDASMessageHeaderFlag
	}

	buf := make([]byte, 0)
	buf = append(buf, flags)
	buf = append(buf, c.KeysetHash[:]...)
	buf = append(buf, c.SerializeSignableFields()...)

	var intData [8]byte
	binary.BigEndian.PutUint64(intData[:], c.SignersMask)
	buf = append(buf, intData[:]...)

	return append(buf, blsSignatures.SignatureToBytes(c.Sig)...)
}

func GetL1Client(ctx context.Context, maxConnectionAttempts int, l1URL string) (*ethclient.Client, error) {
	if maxConnectionAttempts <= 0 {
		maxConnectionAttempts = math.MaxInt
	}
	var l1Client *ethclient.Client
	var err error
	for i := 1; i <= maxConnectionAttempts; i++ {
		l1Client, err = ethclient.DialContext(ctx, l1URL)
		if err == nil {
			return l1Client, nil
		}
		log.Warn("error connecting to L1 from DAS", "l1URL", l1URL, "err", err)

		timer := time.NewTimer(time.Second * 1)
		select {
		case <-ctx.Done():
			timer.Stop()
			return nil, errors.New("aborting startup")
		case <-timer.C:
		}
	}
	return nil, err
}

'''
'''--- das/dasRpcClient.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/log"

	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/util/pretty"
)

type DASRPCClient struct { // implements DataAvailabilityService
	clnt *rpc.Client
	url  string
}

func NewDASRPCClient(target string) (*DASRPCClient, error) {
	clnt, err := rpc.Dial(target)
	if err != nil {
		return nil, err
	}
	return &DASRPCClient{
		clnt: clnt,
		url:  target,
	}, nil
}

func (c *DASRPCClient) Store(ctx context.Context, message []byte, timeout uint64, reqSig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	log.Trace("das.DASRPCClient.Store(...)", "message", pretty.FirstFewBytes(message), "timeout", time.Unix(int64(timeout), 0), "sig", pretty.FirstFewBytes(reqSig), "this", *c)
	var ret StoreResult
	if err := c.clnt.CallContext(ctx, &ret, "das_store", hexutil.Bytes(message), hexutil.Uint64(timeout), hexutil.Bytes(reqSig)); err != nil {
		return nil, err
	}
	respSig, err := blsSignatures.SignatureFromBytes(ret.Sig)
	if err != nil {
		return nil, err
	}
	return &arbstate.DataAvailabilityCertificate{
		DataHash:    common.BytesToHash(ret.DataHash),
		Timeout:     uint64(ret.Timeout),
		SignersMask: uint64(ret.SignersMask),
		Sig:         respSig,
		KeysetHash:  common.BytesToHash(ret.KeysetHash),
		Version:     byte(ret.Version),
	}, nil
}

func (c *DASRPCClient) String() string {
	return fmt.Sprintf("DASRPCClient{url:%s}", c.url)
}

func (c *DASRPCClient) HealthCheck(ctx context.Context) error {
	return c.clnt.CallContext(ctx, nil, "das_healthCheck")
}

func (c *DASRPCClient) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	var res string
	err := c.clnt.CallContext(ctx, &res, "das_expirationPolicy")
	if err != nil {
		return -1, err
	}
	return arbstate.StringToExpirationPolicy(res)
}

'''
'''--- das/dasRpcServer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"net"
	"net/http"
	"time"

	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/pretty"
)

var (
	rpcStoreRequestGauge      = metrics.NewRegisteredGauge("arb/das/rpc/store/requests", nil)
	rpcStoreSuccessGauge      = metrics.NewRegisteredGauge("arb/das/rpc/store/success", nil)
	rpcStoreFailureGauge      = metrics.NewRegisteredGauge("arb/das/rpc/store/failure", nil)
	rpcStoreStoredBytesGauge  = metrics.NewRegisteredGauge("arb/das/rpc/store/bytes", nil)
	rpcStoreDurationHistogram = metrics.NewRegisteredHistogram("arb/das/rpc/store/duration", nil, metrics.NewBoundedHistogramSample())
)

type DASRPCServer struct {
	daReader        DataAvailabilityServiceReader
	daWriter        DataAvailabilityServiceWriter
	daHealthChecker DataAvailabilityServiceHealthChecker
}

func StartDASRPCServer(ctx context.Context, addr string, portNum uint64, rpcServerTimeouts genericconf.HTTPServerTimeoutConfig, daReader DataAvailabilityServiceReader, daWriter DataAvailabilityServiceWriter, daHealthChecker DataAvailabilityServiceHealthChecker) (*http.Server, error) {
	listener, err := net.Listen("tcp", fmt.Sprintf("%s:%d", addr, portNum))
	if err != nil {
		return nil, err
	}
	return StartDASRPCServerOnListener(ctx, listener, rpcServerTimeouts, daReader, daWriter, daHealthChecker)
}

func StartDASRPCServerOnListener(ctx context.Context, listener net.Listener, rpcServerTimeouts genericconf.HTTPServerTimeoutConfig, daReader DataAvailabilityServiceReader, daWriter DataAvailabilityServiceWriter, daHealthChecker DataAvailabilityServiceHealthChecker) (*http.Server, error) {
	rpcServer := rpc.NewServer()
	err := rpcServer.RegisterName("das", &DASRPCServer{
		daReader:        daReader,
		daWriter:        daWriter,
		daHealthChecker: daHealthChecker,
	})
	if err != nil {
		return nil, err
	}

	srv := &http.Server{
		Handler:           rpcServer,
		ReadTimeout:       rpcServerTimeouts.ReadTimeout,
		ReadHeaderTimeout: rpcServerTimeouts.ReadHeaderTimeout,
		WriteTimeout:      rpcServerTimeouts.WriteTimeout,
		IdleTimeout:       rpcServerTimeouts.IdleTimeout,
	}

	go func() {
		err := srv.Serve(listener)
		if err != nil {
			return
		}
	}()
	go func() {
		<-ctx.Done()
		_ = srv.Shutdown(context.Background())
	}()
	return srv, nil
}

type StoreResult struct {
	DataHash    hexutil.Bytes  `json:"dataHash,omitempty"`
	Timeout     hexutil.Uint64 `json:"timeout,omitempty"`
	SignersMask hexutil.Uint64 `json:"signersMask,omitempty"`
	KeysetHash  hexutil.Bytes  `json:"keysetHash,omitempty"`
	Sig         hexutil.Bytes  `json:"sig,omitempty"`
	Version     hexutil.Uint64 `json:"version,omitempty"`
}

func (serv *DASRPCServer) Store(ctx context.Context, message hexutil.Bytes, timeout hexutil.Uint64, sig hexutil.Bytes) (*StoreResult, error) {
	log.Trace("dasRpc.DASRPCServer.Store", "message", pretty.FirstFewBytes(message), "message length", len(message), "timeout", time.Unix(int64(timeout), 0), "sig", pretty.FirstFewBytes(sig), "this", serv)
	rpcStoreRequestGauge.Inc(1)
	start := time.Now()
	success := false
	defer func() {
		if success {
			rpcStoreSuccessGauge.Inc(1)
		} else {
			rpcStoreFailureGauge.Inc(1)
		}
		rpcStoreDurationHistogram.Update(time.Since(start).Nanoseconds())
	}()

	cert, err := serv.daWriter.Store(ctx, message, uint64(timeout), sig)
	if err != nil {
		return nil, err
	}
	rpcStoreStoredBytesGauge.Inc(int64(len(message)))
	success = true
	return &StoreResult{
		KeysetHash:  cert.KeysetHash[:],
		DataHash:    cert.DataHash[:],
		Timeout:     hexutil.Uint64(cert.Timeout),
		SignersMask: hexutil.Uint64(cert.SignersMask),
		Sig:         blsSignatures.SignatureToBytes(cert.Sig),
		Version:     hexutil.Uint64(cert.Version),
	}, nil
}

func (serv *DASRPCServer) HealthCheck(ctx context.Context) error {
	return serv.daHealthChecker.HealthCheck(ctx)
}

func (serv *DASRPCServer) ExpirationPolicy(ctx context.Context) (string, error) {
	expirationPolicy, err := serv.daReader.ExpirationPolicy(ctx)
	if err != nil {
		return "", err
	}
	return expirationPolicy.String()
}

'''
'''--- das/das_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func testDASStoreRetrieveMultipleInstances(t *testing.T, storageType string) {
	firstCtx, firstCancel := context.WithCancel(context.Background())

	dbPath := t.TempDir()
	_, _, err := GenerateAndStoreKeys(dbPath)
	Require(t, err)

	enableFileStorage, enableDbStorage := false, false
	switch storageType {
	case "db":
		enableDbStorage = true
	case "files":
		enableFileStorage = true
	default:
		Fail(t, "unknown storage type")
	}

	config := DataAvailabilityConfig{
		Enable: true,
		Key: KeyConfig{
			KeyDir: dbPath,
		},
		LocalFileStorage: LocalFileStorageConfig{
			Enable:  enableFileStorage,
			DataDir: dbPath,
		},
		LocalDBStorage: LocalDBStorageConfig{
			Enable:  enableDbStorage,
			DataDir: dbPath,
		},
		ParentChainNodeURL: "none",
	}

	var syncFromStorageServicesFirst []*IterableStorageService
	var syncToStorageServicesFirst []StorageService
	storageService, lifecycleManager, err := CreatePersistentStorageService(firstCtx, &config, &syncFromStorageServicesFirst, &syncToStorageServicesFirst)
	Require(t, err)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	daWriter, err := NewSignAfterStoreDASWriter(firstCtx, config, storageService)
	Require(t, err, "no das")
	var daReader DataAvailabilityServiceReader = storageService

	timeout := uint64(time.Now().Add(time.Hour * 24).Unix())
	messageSaved := []byte("hello world")
	cert, err := daWriter.Store(firstCtx, messageSaved, timeout, []byte{})
	Require(t, err, "Error storing message")
	if cert.Timeout != timeout {
		Fail(t, fmt.Sprintf("Expected timeout of %d in cert, was %d", timeout, cert.Timeout))
	}

	messageRetrieved, err := daReader.GetByHash(firstCtx, cert.DataHash)
	Require(t, err, "Failed to retrieve message")
	if !bytes.Equal(messageSaved, messageRetrieved) {
		Fail(t, "Retrieved message is not the same as stored one.")
	}

	firstCancel()
	time.Sleep(500 * time.Millisecond)

	// 2nd das instance can read keys from disk
	secondCtx, secondCancel := context.WithCancel(context.Background())
	defer secondCancel()

	var syncFromStorageServicesSecond []*IterableStorageService
	var syncToStorageServicesSecond []StorageService
	storageService2, lifecycleManager, err := CreatePersistentStorageService(secondCtx, &config, &syncFromStorageServicesSecond, &syncToStorageServicesSecond)
	Require(t, err)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	var daReader2 DataAvailabilityServiceReader = storageService2

	messageRetrieved2, err := daReader2.GetByHash(secondCtx, cert.DataHash)
	Require(t, err, "Failed to retrieve message")
	if !bytes.Equal(messageSaved, messageRetrieved2) {
		Fail(t, "Retrieved message is not the same as stored one.")
	}

	messageRetrieved2, err = daReader2.GetByHash(secondCtx, cert.DataHash)
	Require(t, err, "Failed to getByHash message")
	if !bytes.Equal(messageSaved, messageRetrieved2) {
		Fail(t, "Retrieved message is not the same as stored one.")
	}
}

func TestDASStoreRetrieveMultipleInstancesFiles(t *testing.T) {
	testDASStoreRetrieveMultipleInstances(t, "files")
}

func TestDASStoreRetrieveMultipleInstancesDB(t *testing.T) {
	testDASStoreRetrieveMultipleInstances(t, "db")
}

func testDASMissingMessage(t *testing.T, storageType string) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	dbPath := t.TempDir()
	_, _, err := GenerateAndStoreKeys(dbPath)
	Require(t, err)

	enableFileStorage, enableDbStorage := false, false
	switch storageType {
	case "db":
		enableDbStorage = true
	case "files":
		enableFileStorage = true
	default:
		Fail(t, "unknown storage type")
	}

	config := DataAvailabilityConfig{
		Enable: true,
		Key: KeyConfig{
			KeyDir: dbPath,
		},
		LocalFileStorage: LocalFileStorageConfig{
			Enable:  enableFileStorage,
			DataDir: dbPath,
		},
		LocalDBStorage: LocalDBStorageConfig{
			Enable:  enableDbStorage,
			DataDir: dbPath,
		},
		ParentChainNodeURL: "none",
	}

	var syncFromStorageServices []*IterableStorageService
	var syncToStorageServices []StorageService
	storageService, lifecycleManager, err := CreatePersistentStorageService(ctx, &config, &syncFromStorageServices, &syncToStorageServices)
	Require(t, err)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	daWriter, err := NewSignAfterStoreDASWriter(ctx, config, storageService)
	Require(t, err, "no das")
	var daReader DataAvailabilityServiceReader = storageService

	messageSaved := []byte("hello world")
	timeout := uint64(time.Now().Add(time.Hour * 24).Unix())
	cert, err := daWriter.Store(ctx, messageSaved, timeout, []byte{})
	Require(t, err, "Error storing message")
	if cert.Timeout != timeout {
		Fail(t, fmt.Sprintf("Expected timeout of %d in cert, was %d", timeout, cert.Timeout))
	}

	// Change the hash to look up
	cert.DataHash[0] += 1

	_, err = daReader.GetByHash(ctx, cert.DataHash)
	if err == nil {
		Fail(t, "Expected an error when retrieving message that is not in the store.")
	}

	_, err = daReader.GetByHash(ctx, cert.DataHash)
	if err == nil {
		Fail(t, "Expected an error when getting by hash a message that is not in the store.")
	}
}

func TestDASMissingMessageFiles(t *testing.T) {
	testDASMissingMessage(t, "files")
}

func TestDASMissingMessageDB(t *testing.T) {
	testDASMissingMessage(t, "db")
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- das/dastree/dastree.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package dastree

import (
	"encoding/binary"
	"fmt"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/util/arbmath"
)

const BinSize = 64 * 1024 // 64 kB
const NodeByte = byte(0xff)
const LeafByte = byte(0xfe)

type bytes32 = common.Hash

type node struct {
	hash bytes32
	size uint32
}

// RecordHash chunks the preimage into 64kB bins and generates a recursive hash tree,
// calling the caller-supplied record function for each hash/preimage pair created in
// building the tree structure.
func RecordHash(record func(bytes32, []byte), preimage ...[]byte) bytes32 {
	// Algorithm
	//  1. split the preimage into 64kB bins and double hash them to produce the tree's leaves
	//  2. repeatedly hash pairs and their combined length, bubbling up any odd-one's out, to form the root
	//  3. invert the first bit of the root hash
	//
	//            r'        <=>  invert(H(0xff, H(0xff, 0, 1, L(0:1)), 2, L(0:2)))    step 4
	//            |
	//            r         <=>  H(0xff, H(0xff, 0, 1, L(0:1)), 2, L(0:2))            step 3
	//           / \
	//          *   2       <=>  H(0xff, 0, 1, L(0:1)), 2                             step 2
	//         / \
	//        0   1         <=>  0, 1, 2                                              step 1
	//
	//      0   1   2       <=>  leaf n = H(0xfe, H(bin n))                           step 0
	//
	//  Where H is keccak and L is the length
	//  Intermediate hashes like '*' from above may be recorded via the `record` closure
	//

	keccord := func(value []byte) bytes32 {
		hash := crypto.Keccak256Hash(value)
		record(hash, value)
		return hash
	}
	prepend := func(before byte, slice []byte) []byte {
		return append([]byte{before}, slice...)
	}

	unrolled := arbmath.ConcatByteSlices(preimage...)
	if len(unrolled) == 0 {
		return arbmath.FlipBit(keccord(prepend(LeafByte, keccord([]byte{}).Bytes())), 0)
	}

	length := uint32(len(unrolled))
	leaves := []node{}
	for bin := uint32(0); bin < length; bin += BinSize {
		end := arbmath.MinInt(bin+BinSize, length)
		hash := keccord(prepend(LeafByte, keccord(unrolled[bin:end]).Bytes()))
		leaves = append(leaves, node{hash, end - bin})
	}

	layer := leaves
	for len(layer) > 1 {
		prior := len(layer)
		after := prior/2 + prior%2
		paired := make([]node, after)
		for i := 0; i < prior-1; i += 2 {
			firstHash := layer[i].hash.Bytes()
			otherHash := layer[i+1].hash.Bytes()
			sizeUnder := layer[i].size + layer[i+1].size
			dataUnder := arbmath.ConcatByteSlices(firstHash, otherHash, arbmath.Uint32ToBytes(sizeUnder))
			parent := node{
				keccord(prepend(NodeByte, dataUnder)),
				sizeUnder,
			}
			paired[i/2] = parent
		}
		if prior%2 == 1 {
			paired[after-1] = layer[prior-1]
		}
		layer = paired
	}
	return arbmath.FlipBit(layer[0].hash, 0)
}

func Hash(preimage ...[]byte) bytes32 {
	// Merkelizes without recording anything. All but the validator's DAS will call this
	return RecordHash(func(bytes32, []byte) {}, preimage...)
}

func HashBytes(preimage ...[]byte) []byte {
	return Hash(preimage...).Bytes()
}

func FlatHashToTreeHash(flat bytes32) bytes32 {
	// Forms a degenerate dastree that's just a single leaf
	// note: the inner preimage may be larger than the 64 kB standard
	return arbmath.FlipBit(crypto.Keccak256Hash(FlatHashToTreeLeaf(flat)), 0)
}

func FlatHashToTreeLeaf(flat bytes32) []byte {
	// Prepends a flat hash with a leaf byte to emulate a leaf's nesting
	return append([]byte{LeafByte}, flat.Bytes()...)
}

func ValidHash(hash bytes32, preimage []byte) bool {
	if hash == Hash(preimage) {
		return true
	}
	if len(preimage) > 0 {
		kind := preimage[0]
		return kind != NodeByte && kind != LeafByte && hash == crypto.Keccak256Hash(preimage)
	}
	return false
}

// Reverses hashes to reveal the full preimage under the root using the preimage oracle.
// This function also checks that the size-data is consistent and that the hash is canonical.
//
// Notes
//  1. Because we accept degenerate dastrees, we can't check that single-leaf trees are canonical.
//  2. For any canonical dastree, there exists a degenerate single-leaf equivalent that we accept.
//  3. We also accept old-style flat hashes
//  4. Only the committee can produce trees unwrapped by this function
//  5. When the replay binary calls this, the oracle function must be infallible.
func Content(root bytes32, oracle func(bytes32) ([]byte, error)) ([]byte, error) {

	unpeal := func(hash bytes32) (byte, []byte, error) {
		data, err := oracle(hash)
		if err != nil {
			return 0, nil, err
		}
		size := len(data)
		if size == 0 {
			return 0, nil, fmt.Errorf("invalid node %v", hash)
		}
		kind := data[0]
		if (kind == LeafByte && size != 33) || (kind == NodeByte && size != 69) {
			return 0, nil, fmt.Errorf("invalid node for hash %v: %v", hash, data)
		}
		return kind, data[1:], nil
	}

	start := arbmath.FlipBit(root, 0)
	total := uint32(0)
	kind, upper, err := unpeal(start)
	if err != nil {
		return nil, err
	}
	switch kind {
	case LeafByte:
		return oracle(common.BytesToHash(upper))
	case NodeByte:
		total = binary.BigEndian.Uint32(upper[64:])
	default:
		return nil, fmt.Errorf("unexpected root preimage of kind %v: %v", kind, upper)
	}

	leaves := []node{}
	stack := []node{{hash: start, size: total}}

	for len(stack) > 0 {
		place := stack[len(stack)-1]
		stack = stack[:len(stack)-1]
		kind, data, err := unpeal(place.hash)
		if err != nil {
			return nil, err
		}

		switch kind {
		case LeafByte:
			leaf := node{
				hash: common.BytesToHash(data),
				size: place.size,
			}
			leaves = append(leaves, leaf)
		case NodeByte:
			count := binary.BigEndian.Uint32(data[64:])
			power := uint32(arbmath.NextOrCurrentPowerOf2(uint64(count)))

			if place.size != count {
				return nil, fmt.Errorf("invalid size data: %v vs %v for %v", count, place.size, data)
			}

			prior := node{
				hash: common.BytesToHash(data[:32]),
				size: power / 2,
			}
			after := node{
				hash: common.BytesToHash(data[32:64]),
				size: count - power/2,
			}

			// we want to expand leftward so we reverse their order
			stack = append(stack, after, prior)
		default:
			return nil, fmt.Errorf("failed to resolve preimage %v %v", place.hash, data)
		}
	}

	preimage := []byte{}
	for i, leaf := range leaves { // TODO We can parallelize leaf fetching in future.
		bin, err := oracle(leaf.hash)
		if err != nil {
			return nil, err
		}
		if len(bin) != int(leaf.size) {
			return nil, fmt.Errorf("leaf %v has an incorrectly sized bin: %v vs %v", i, len(bin), leaf.size)
		}
		preimage = append(preimage, bin...)
	}

	// Check the hash matches. Given the size data this should never fail but we'll check anyway
	if Hash(preimage) != root {
		return nil, fmt.Errorf("preimage not canonically hashed")
	}
	return preimage, nil
}

'''
'''--- das/dastree/dastree_test.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package dastree

import (
	"bytes"
	"math/rand"
	"testing"

	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/pretty"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestDASTree(t *testing.T) {
	store := make(map[bytes32][]byte)
	tests := [][]byte{
		{}, {0x32}, crypto.Keccak256(),
		make([]byte, BinSize), make([]byte, BinSize+1), make([]byte, 4*BinSize),
	}
	for i := 0; i < 64; i++ {
		large := make([]byte, rand.Intn(12*BinSize))
		tests = append(tests, large)
	}

	record := func(key bytes32, value []byte) {
		colors.PrintGrey("storing ", key, " ", pretty.PrettyBytes(value))
		store[key] = value
		if crypto.Keccak256Hash(value) != key {
			Fail(t, "key not the hash of value")
		}
	}
	oracle := func(key bytes32) ([]byte, error) {
		preimage, ok := store[key]
		if !ok {
			Fail(t, "no preimage for key", key)
		}
		if crypto.Keccak256Hash(preimage) != key {
			Fail(t, "key not the hash of preimage")
		}
		colors.PrintBlue("loading ", key, " ", pretty.PrettyBytes(preimage))
		return preimage, nil
	}

	hashes := map[bytes32][]byte{}
	for _, test := range tests {
		hash := RecordHash(record, test)
		hashes[hash] = test
	}

	for key, value := range hashes {
		colors.PrintMint("testing ", key)
		preimage, err := Content(key, oracle)
		Require(t, err, key)

		if !bytes.Equal(preimage, value) || !ValidHash(key, preimage) {
			Fail(t, "incorrect preimage", pretty.FirstFewBytes(preimage), pretty.FirstFewBytes(value))
		}
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- das/db_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"time"

	badger "github.com/dgraph-io/badger/v3"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

type LocalDBStorageConfig struct {
	Enable                 bool   `koanf:"enable"`
	DataDir                string `koanf:"data-dir"`
	DiscardAfterTimeout    bool   `koanf:"discard-after-timeout"`
	SyncFromStorageService bool   `koanf:"sync-from-storage-service"`
	SyncToStorageService   bool   `koanf:"sync-to-storage-service"`
}

var DefaultLocalDBStorageConfig = LocalDBStorageConfig{}

func LocalDBStorageConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultLocalDBStorageConfig.Enable, "enable storage/retrieval of sequencer batch data from a database on the local filesystem")
	f.String(prefix+".data-dir", DefaultLocalDBStorageConfig.DataDir, "directory in which to store the database")
	f.Bool(prefix+".discard-after-timeout", DefaultLocalDBStorageConfig.DiscardAfterTimeout, "discard data after its expiry timeout")
	f.Bool(prefix+".sync-from-storage-service", DefaultLocalDBStorageConfig.SyncFromStorageService, "enable db storage to be used as a source for regular sync storage")
	f.Bool(prefix+".sync-to-storage-service", DefaultLocalDBStorageConfig.SyncToStorageService, "enable db storage to be used as a sink for regular sync storage")
}

type DBStorageService struct {
	db                  *badger.DB
	discardAfterTimeout bool
	dirPath             string
	stopWaiter          stopwaiter.StopWaiterSafe
}

func NewDBStorageService(ctx context.Context, dirPath string, discardAfterTimeout bool) (StorageService, error) {
	db, err := badger.Open(badger.DefaultOptions(dirPath))
	if err != nil {
		return nil, err
	}

	ret := &DBStorageService{
		db:                  db,
		discardAfterTimeout: discardAfterTimeout,
		dirPath:             dirPath,
	}
	if err := ret.stopWaiter.Start(ctx, ret); err != nil {
		return nil, err
	}
	err = ret.stopWaiter.LaunchThreadSafe(func(myCtx context.Context) {
		ticker := time.NewTicker(5 * time.Minute)
		defer ticker.Stop()
		defer func() {
			if err := ret.db.Close(); err != nil {
				log.Error("Failed to close DB", "err", err)
			}
		}()
		for {
			select {
			case <-ticker.C:
				for db.RunValueLogGC(0.7) == nil {
					select {
					case <-myCtx.Done():
						return
					default:
					}
				}
			case <-myCtx.Done():
				return
			}
		}
	})
	if err != nil {
		return nil, err
	}

	return ret, nil
}

func (dbs *DBStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.DBStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", dbs)

	var ret []byte
	err := dbs.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get(key.Bytes())
		if err != nil {
			return err
		}
		return item.Value(func(val []byte) error {
			ret = append([]byte{}, val...)
			return nil
		})
	})
	if errors.Is(err, badger.ErrKeyNotFound) {
		return ret, ErrNotFound
	}
	return ret, err
}

func (dbs *DBStorageService) Put(ctx context.Context, data []byte, timeout uint64) error {
	logPut("das.DBStorageService.Put", data, timeout, dbs)

	return dbs.db.Update(func(txn *badger.Txn) error {
		e := badger.NewEntry(dastree.HashBytes(data), data)
		if dbs.discardAfterTimeout {
			e = e.WithTTL(time.Until(time.Unix(int64(timeout), 0)))
		}
		return txn.SetEntry(e)
	})
}

func (dbs *DBStorageService) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	return dbs.db.Update(func(txn *badger.Txn) error {
		e := badger.NewEntry(key.Bytes(), value)
		return txn.SetEntry(e)
	})
}

func (dbs *DBStorageService) Sync(ctx context.Context) error {
	return dbs.db.Sync()
}

func (dbs *DBStorageService) Close(ctx context.Context) error {
	return dbs.stopWaiter.StopAndWait()
}

func (dbs *DBStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	if dbs.discardAfterTimeout {
		return arbstate.DiscardAfterDataTimeout, nil
	}
	return arbstate.KeepForever, nil
}

func (dbs *DBStorageService) String() string {
	return "BadgerDB(" + dbs.dirPath + ")"
}

func (dbs *DBStorageService) HealthCheck(ctx context.Context) error {
	testData := []byte("Test-Data")
	err := dbs.Put(ctx, testData, uint64(time.Now().Add(time.Minute).Unix()))
	if err != nil {
		return err
	}
	res, err := dbs.GetByHash(ctx, dastree.Hash(testData))
	if err != nil {
		return err
	}
	if !bytes.Equal(res, testData) {
		return errors.New("invalid GetByHash result")
	}
	return nil
}

'''
'''--- das/extra_signature_checker_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/hex"
	"errors"
	"io/ioutil"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/util/signature"
)

type StubSignatureCheckDAS struct {
	keyDir string
}

func (s *StubSignatureCheckDAS) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	pubkeyEncoded, err := ioutil.ReadFile(s.keyDir + "/ecdsa.pub")
	if err != nil {
		return nil, err
	}
	pubkey, err := hex.DecodeString(string(pubkeyEncoded))
	if err != nil {
		return nil, err
	}

	verified := crypto.VerifySignature(pubkey, dasStoreHash(message, timeout), sig[:64])
	if !verified {
		return nil, errors.New("signature verification failed")
	}
	return nil, nil
}

func (s *StubSignatureCheckDAS) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.KeepForever, nil
}

func (s *StubSignatureCheckDAS) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	return []byte{}, nil
}

func (s *StubSignatureCheckDAS) HealthCheck(ctx context.Context) error {
	return nil
}

func (s *StubSignatureCheckDAS) String() string {
	return "StubSignatureCheckDAS"
}

func TestExtraSignatureCheck(t *testing.T) {
	keyDir := t.TempDir()
	err := GenerateAndStoreECDSAKeys(keyDir)
	Require(t, err)

	privateKey, err := crypto.LoadECDSA(keyDir + "/ecdsa")
	Require(t, err)
	signer := signature.DataSignerFromPrivateKey(privateKey)

	var da DataAvailabilityServiceWriter = &StubSignatureCheckDAS{keyDir}
	da, err = NewStoreSigningDAS(da, signer)
	Require(t, err)

	_, err = da.Store(context.Background(), []byte("Hello world"), 1234, []byte{})
	Require(t, err)
}

func TestSimpleSignatureCheck(t *testing.T) {
	keyDir := t.TempDir()
	err := GenerateAndStoreECDSAKeys(keyDir)
	Require(t, err)
	privateKey, err := crypto.LoadECDSA(keyDir + "/ecdsa")
	Require(t, err)

	data := []byte("Hello World")
	dataHash := crypto.Keccak256(data)
	sig, err := crypto.Sign(dataHash, privateKey)
	Require(t, err)

	pubkeyEncoded, err := ioutil.ReadFile(keyDir + "/ecdsa.pub")
	Require(t, err)

	pubkey, err := hex.DecodeString(string(pubkeyEncoded))
	Require(t, err)

	verified := crypto.VerifySignature(pubkey, dataHash, sig[:64])
	if !verified {
		Fail(t, "Signature not verified")
	}
}

func TestEvenSimplerSignatureCheck(t *testing.T) {
	privateKey, err := crypto.GenerateKey()
	Require(t, err)

	data := []byte("Hello World")
	dataHash := crypto.Keccak256(data)
	sig, err := crypto.Sign(dataHash, privateKey)
	Require(t, err)

	pubkey, err := crypto.SigToPub(dataHash, sig)
	Require(t, err)
	if !bytes.Equal(crypto.FromECDSAPub(pubkey), crypto.FromECDSAPub(&privateKey.PublicKey)) {
		Fail(t, "Derived pubkey doesn't match pubkey")
	}

	verified := crypto.VerifySignature(crypto.FromECDSAPub(&privateKey.PublicKey), dataHash, sig[:64])
	if !verified {
		Fail(t, "Signature not verified")
	}
}

'''
'''--- das/factory.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"fmt"
	"math"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/signature"
)

// CreatePersistentStorageService creates any storage services that persist to files, database, cloud storage,
// and group them together into a RedundantStorage instance if there is more than one.
func CreatePersistentStorageService(
	ctx context.Context,
	config *DataAvailabilityConfig,
	syncFromStorageServices *[]*IterableStorageService,
	syncToStorageServices *[]StorageService,
) (StorageService, *LifecycleManager, error) {
	storageServices := make([]StorageService, 0, 10)
	var lifecycleManager LifecycleManager
	if config.LocalDBStorage.Enable {
		s, err := NewDBStorageService(ctx, config.LocalDBStorage.DataDir, config.LocalDBStorage.DiscardAfterTimeout)
		if err != nil {
			return nil, nil, err
		}
		if config.LocalDBStorage.SyncFromStorageService {
			iterableStorageService := NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(s))
			*syncFromStorageServices = append(*syncFromStorageServices, iterableStorageService)
			s = iterableStorageService
		}
		if config.LocalDBStorage.SyncToStorageService {
			*syncToStorageServices = append(*syncToStorageServices, s)
		}
		lifecycleManager.Register(s)
		storageServices = append(storageServices, s)
	}

	if config.LocalFileStorage.Enable {
		s, err := NewLocalFileStorageService(config.LocalFileStorage.DataDir)
		if err != nil {
			return nil, nil, err
		}
		if config.LocalFileStorage.SyncFromStorageService {
			iterableStorageService := NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(s))
			*syncFromStorageServices = append(*syncFromStorageServices, iterableStorageService)
			s = iterableStorageService
		}
		if config.LocalFileStorage.SyncToStorageService {
			*syncToStorageServices = append(*syncToStorageServices, s)
		}
		lifecycleManager.Register(s)
		storageServices = append(storageServices, s)
	}

	if config.S3Storage.Enable {
		s, err := NewS3StorageService(config.S3Storage)
		if err != nil {
			return nil, nil, err
		}
		lifecycleManager.Register(s)
		if config.S3Storage.SyncFromStorageService {
			iterableStorageService := NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(s))
			*syncFromStorageServices = append(*syncFromStorageServices, iterableStorageService)
			s = iterableStorageService
		}
		if config.S3Storage.SyncToStorageService {
			*syncToStorageServices = append(*syncToStorageServices, s)
		}
		storageServices = append(storageServices, s)
	}

	if config.IpfsStorage.Enable {
		s, err := NewIpfsStorageService(ctx, config.IpfsStorage)
		if err != nil {
			return nil, nil, err
		}
		lifecycleManager.Register(s)
		storageServices = append(storageServices, s)
	}

	if len(storageServices) > 1 {
		s, err := NewRedundantStorageService(ctx, storageServices)
		if err != nil {
			return nil, nil, err
		}
		lifecycleManager.Register(s)
		return s, &lifecycleManager, nil
	}
	if len(storageServices) == 1 {
		return storageServices[0], &lifecycleManager, nil
	}
	return nil, &lifecycleManager, nil
}

func WrapStorageWithCache(
	ctx context.Context,
	config *DataAvailabilityConfig,
	storageService StorageService,
	syncFromStorageServices *[]*IterableStorageService,
	syncToStorageServices *[]StorageService,
	lifecycleManager *LifecycleManager) (StorageService, error) {
	if storageService == nil {
		return nil, nil
	}

	// Enable caches, Redis and (local) BigCache. Local is the outermost, so it will be tried first.
	var err error
	if config.RedisCache.Enable {
		storageService, err = NewRedisStorageService(config.RedisCache, storageService)
		lifecycleManager.Register(storageService)
		if err != nil {
			return nil, err
		}
		if config.RedisCache.SyncFromStorageService {
			iterableStorageService := NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(storageService))
			*syncFromStorageServices = append(*syncFromStorageServices, iterableStorageService)
			storageService = iterableStorageService
		}
		if config.RedisCache.SyncToStorageService {
			*syncToStorageServices = append(*syncToStorageServices, storageService)
		}
	}
	if config.LocalCache.Enable {
		storageService, err = NewBigCacheStorageService(config.LocalCache, storageService)
		lifecycleManager.Register(storageService)
		if err != nil {
			return nil, err
		}
	}
	return storageService, nil
}

func CreateNearDAS(
	ctx context.Context,
	config *DataAvailabilityConfig,
	lifecycleManager *LifecycleManager,
) (DataAvailabilityServiceWriter, DataAvailabilityServiceReader, *NearService, error) {
	if !config.NEARAggregator.Enable {
		return nil, nil, nil, errors.New("--node.data-availability.near-aggregator.enable must be set")
	}

	log.Info("Initialising near service")
	nearSvc, err := NewNearService(*config)
	if err != nil {
		log.Error("initialising near service", "error", err)
		return nil, nil, nil, err
	}
	var nearWriter DataAvailabilityServiceWriter = nearSvc
	// log.Info("initialising near aggregator")
	//nearAggr, err = NewNearAggregator(ctx, *config, nearSvc)
	// if err != nil {
	// 	return nil, nil, err
	// }

	var nearReader DataAvailabilityServiceReader = nearSvc
	log.Info("initialising Chain Fetch Reader")
	// FIXME: lifecycleManager.Register(nearr)
	if err != nil {
		return nil, nil, nil, err
	}
	return nearWriter, nearReader, nearSvc, nil
}

func CreateBatchPosterDAS(
	ctx context.Context,
	config *DataAvailabilityConfig,
	dataSigner signature.DataSignerFunc,
	l1Reader arbutil.L1Interface,
	sequencerInboxAddr common.Address,
) (DataAvailabilityServiceWriter, DataAvailabilityServiceReader, *LifecycleManager, error) {
	if !config.Enable {
		return nil, nil, nil, nil
	}

	// Check config requirements
	if !config.RPCAggregator.Enable || !config.RestAggregator.Enable {
		return nil, nil, nil, errors.New("--node.data-availability.rpc-aggregator.enable and rest-aggregator.enable must be set when running a Batch Poster in AnyTrust mode")
	}

	if config.IpfsStorage.Enable {
		return nil, nil, nil, errors.New("--node.data-availability.ipfs-storage.enable may not be set when running a Nitro AnyTrust node in Batch Poster mode")
	}
	// Done checking config requirements

	var daWriter DataAvailabilityServiceWriter
	daWriter, err := NewRPCAggregator(ctx, *config)
	if err != nil {
		return nil, nil, nil, err
	}
	if dataSigner != nil {
		// In some tests the batch poster does not sign Store requests
		daWriter, err = NewStoreSigningDAS(daWriter, dataSigner)
		if err != nil {
			return nil, nil, nil, err
		}
	}

	restAgg, err := NewRestfulClientAggregator(ctx, &config.RestAggregator)
	if err != nil {
		return nil, nil, nil, err
	}
	restAgg.Start(ctx)
	var lifecycleManager LifecycleManager
	lifecycleManager.Register(restAgg)
	var daReader DataAvailabilityServiceReader = restAgg
	daReader, err = NewChainFetchReader(daReader, l1Reader, sequencerInboxAddr)
	if err != nil {
		return nil, nil, nil, err
	}

	return daWriter, daReader, &lifecycleManager, nil
}

func CreateDAComponentsForDaserver(
	ctx context.Context,
	config *DataAvailabilityConfig,
	l1Reader *headerreader.HeaderReader,
	seqInboxAddress *common.Address,
) (DataAvailabilityServiceReader, DataAvailabilityServiceWriter, DataAvailabilityServiceHealthChecker, *LifecycleManager, error) {
	if !config.Enable {
		return nil, nil, nil, nil, nil
	}

	// Check config requirements
	if !config.LocalDBStorage.Enable &&
		!config.LocalFileStorage.Enable &&
		!config.S3Storage.Enable &&
		!config.IpfsStorage.Enable {
		return nil, nil, nil, nil, errors.New("At least one of --data-availability.(local-db-storage|local-file-storage|s3-storage|ipfs-storage) must be enabled.")
	}
	// Done checking config requirements

	var syncFromStorageServices []*IterableStorageService
	var syncToStorageServices []StorageService
	storageService, dasLifecycleManager, err := CreatePersistentStorageService(ctx, config, &syncFromStorageServices, &syncToStorageServices)
	if err != nil {
		return nil, nil, nil, nil, err
	}

	storageService, err = WrapStorageWithCache(ctx, config, storageService, &syncFromStorageServices, &syncToStorageServices, dasLifecycleManager)
	if err != nil {
		return nil, nil, nil, nil, err
	}

	var daWriter DataAvailabilityServiceWriter
	var daReader DataAvailabilityServiceReader = storageService
	var daHealthChecker DataAvailabilityServiceHealthChecker = storageService

	if config.NEARAggregator.Enable {
		log.Info("Enabling NEAR aggregator")
		w, r, svc, err := CreateNearDAS(ctx, config, dasLifecycleManager)
		if err != nil {
			return nil, nil, nil, nil, err
		}
		if config.NEARAggregator.StorageConfig.Enable {
			nearStorageService, err := NewNearStorageService(r, *svc, config.NEARAggregator.StorageConfig.DataDir)
			if err != nil {
				return nil, nil, nil, nil, err
			}
			daReader = nearStorageService
			syncConf := config.NEARAggregator.StorageConfig.SyncToStorage
			var retentionPeriodSeconds uint64
			if uint64(syncConf.RetentionPeriod) == math.MaxUint64 {
				retentionPeriodSeconds = math.MaxUint64
			} else {
				retentionPeriodSeconds = uint64(syncConf.RetentionPeriod.Seconds())
			}

			if config.NEARAggregator.StorageConfig.SyncToStorage.Eager {
				if l1Reader == nil || seqInboxAddress == nil {
					return nil, nil, nil, nil, errors.New("l1-node-url and sequencer-inbox-address must be specified along with sync-to-storage.eager")
				}
				storageService, err = NewSyncingFallbackStorageService(
					ctx,
					nearStorageService,
					storageService,
					storageService,
					l1Reader,
					*seqInboxAddress,
					&syncConf)
				if err != nil {
					return nil, nil, nil, nil, err
				}
			} else {
				storageService = NewFallbackStorageService(nearStorageService, storageService, storageService,
					retentionPeriodSeconds, syncConf.IgnoreWriteErrors, true)
			}

			dasLifecycleManager.Register(storageService)
			if err != nil {
				return nil, nil, nil, nil, err
			}
		} else {
			daWriter = w
			daReader = r
		}
	} else {
		// The REST aggregator is used as the fallback if requested data is not present
		// in the storage service.
		if config.RestAggregator.Enable {
			restAgg, err := NewRestfulClientAggregator(ctx, &config.RestAggregator)
			if err != nil {
				return nil, nil, nil, nil, err
			}
			restAgg.Start(ctx)
			dasLifecycleManager.Register(restAgg)

			syncConf := &config.RestAggregator.SyncToStorage
			var retentionPeriodSeconds uint64
			if uint64(syncConf.RetentionPeriod) == math.MaxUint64 {
				retentionPeriodSeconds = math.MaxUint64
			} else {
				retentionPeriodSeconds = uint64(syncConf.RetentionPeriod.Seconds())
			}

			if syncConf.Eager {
				if l1Reader == nil || seqInboxAddress == nil {
					return nil, nil, nil, nil, errors.New("l1-node-url and sequencer-inbox-address must be specified along with sync-to-storage.eager")
				}
				storageService, err = NewSyncingFallbackStorageService(
					ctx,
					storageService,
					restAgg,
					restAgg,
					l1Reader,
					*seqInboxAddress,
					syncConf)
				dasLifecycleManager.Register(storageService)
				if err != nil {
					return nil, nil, nil, nil, err
				}
			} else {
				storageService = NewFallbackStorageService(storageService, restAgg, restAgg,
					retentionPeriodSeconds, syncConf.IgnoreWriteErrors, true)
				dasLifecycleManager.Register(storageService)
			}

		}
	}

	if config.Key.KeyDir != "" || config.Key.PrivKey != "" {
		var seqInboxCaller *bridgegen.SequencerInboxCaller
		if seqInboxAddress != nil {
			seqInbox, err := bridgegen.NewSequencerInbox(*seqInboxAddress, (*l1Reader).Client())
			if err != nil {
				return nil, nil, nil, nil, err
			}

			seqInboxCaller = &seqInbox.SequencerInboxCaller
		}
		if config.DisableSignatureChecking {
			seqInboxCaller = nil
		}

		privKey, err := config.Key.BLSPrivKey()
		if err != nil {
			return nil, nil, nil, nil, err
		}

		daWriter, err = NewSignAfterStoreDASWriterWithSeqInboxCaller(
			privKey,
			seqInboxCaller,
			storageService,
			config.ExtraSignatureCheckingPublicKey,
		)
		if err != nil {
			return nil, nil, nil, nil, err
		}
	}
	log.Info("Da reader/writer", "reader", daReader, "writer", daWriter)

	if config.RegularSyncStorage.Enable && len(syncFromStorageServices) != 0 && len(syncToStorageServices) != 0 {
		regularlySyncStorage := NewRegularlySyncStorage(syncFromStorageServices, syncToStorageServices, config.RegularSyncStorage)
		regularlySyncStorage.Start(ctx)
	}

	if seqInboxAddress != nil {
		daReader, err = NewChainFetchReader(daReader, (*l1Reader).Client(), *seqInboxAddress)
		if err != nil {
			return nil, nil, nil, nil, err
		}
	}

	return daReader, daWriter, daHealthChecker, dasLifecycleManager, nil
}

func CreateDAReaderForNode(
	ctx context.Context,
	config *DataAvailabilityConfig,
	l1Reader *headerreader.HeaderReader,
	seqInboxAddress *common.Address,
) (DataAvailabilityServiceReader, *LifecycleManager, error) {
	if !config.Enable {
		return nil, nil, nil
	}

	// Check config requirements
	if config.RPCAggregator.Enable {
		return nil, nil, errors.New("node.data-availability.rpc-aggregator is only for Batch Poster mode")
	}

	if !config.NEARAggregator.Enable {
		if !config.RestAggregator.Enable && !config.IpfsStorage.Enable {
			return nil, nil, fmt.Errorf("--node.data-availability.enable was set but neither of --node.data-availability.(rest-aggregator|ipfs-storage) were enabled. When running a Nitro Anytrust node in non-Batch Poster mode, some way to get the batch data is required.")
		}
	}

	if config.RestAggregator.SyncToStorage.Eager {
		return nil, nil, errors.New("--node.data-availability.rest-aggregator.sync-to-storage.eager can't be used with a Nitro node, only lazy syncing can be used.")
	}
	// Done checking config requirements

	storageService, dasLifecycleManager, err := CreatePersistentStorageService(ctx, config, nil, nil)
	if err != nil {
		return nil, nil, err
	}

	var daReader DataAvailabilityServiceReader
	if config.NEARAggregator.Enable {
		log.Info("Initialising near service")
		_, r, _, err := CreateNearDAS(ctx, config, dasLifecycleManager)
		if err != nil {
			return nil, nil, err
		}

		if storageService != nil {
			syncConf := &config.RestAggregator.SyncToStorage
			var retentionPeriodSeconds uint64
			if uint64(syncConf.RetentionPeriod) == math.MaxUint64 {
				retentionPeriodSeconds = math.MaxUint64
			} else {
				retentionPeriodSeconds = uint64(syncConf.RetentionPeriod.Seconds())
			}

			// This falls back to REST and updates the local IPFS repo if the data is found.
			nearSvc, err := NewNearService(*config)
			if err != nil {
				log.Error("initialising near service", "error", err)
				return nil, nil, err
			}
			// TODO: hack fix
			storageService = NewFallbackStorageService(storageService, r, nearSvc,
				retentionPeriodSeconds, syncConf.IgnoreWriteErrors, true)
			dasLifecycleManager.Register(storageService)

			daReader = storageService
		} else {
			daReader = r
		}
	} else if config.RestAggregator.Enable {
		var restAgg *SimpleDASReaderAggregator
		restAgg, err = NewRestfulClientAggregator(ctx, &config.RestAggregator)
		if err != nil {
			return nil, nil, err
		}
		restAgg.Start(ctx)
		dasLifecycleManager.Register(restAgg)

		if storageService != nil {
			syncConf := &config.RestAggregator.SyncToStorage
			var retentionPeriodSeconds uint64
			if uint64(syncConf.RetentionPeriod) == math.MaxUint64 {
				retentionPeriodSeconds = math.MaxUint64
			} else {
				retentionPeriodSeconds = uint64(syncConf.RetentionPeriod.Seconds())
			}

			// This falls back to REST and updates the local IPFS repo if the data is found.
			storageService = NewFallbackStorageService(storageService, restAgg, restAgg,
				retentionPeriodSeconds, syncConf.IgnoreWriteErrors, true)
			dasLifecycleManager.Register(storageService)

			daReader = storageService
		} else {
			daReader = restAgg
		}
	}

	if seqInboxAddress != nil {
		seqInbox, err := bridgegen.NewSequencerInbox(*seqInboxAddress, (*l1Reader).Client())
		if err != nil {
			return nil, nil, err
		}
		daReader, err = NewChainFetchReaderWithSeqInbox(daReader, seqInbox)
		if err != nil {
			return nil, nil, err
		}
	}

	return daReader, dasLifecycleManager, nil
}

'''
'''--- das/fallback_storage_service.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/pretty"
)

type FallbackStorageService struct {
	StorageService
	backup                     arbstate.DataAvailabilityReader
	backupHealthChecker        DataAvailabilityServiceHealthChecker
	backupRetentionSeconds     uint64
	ignoreRetentionWriteErrors bool
	preventRecursiveGets       bool
	currentlyFetching          map[[32]byte]bool
	currentlyFetchingMutex     sync.RWMutex
}

// NewFallbackStorageService is a StorageService that relies on a "primary" StorageService and a "backup". Puts go to the primary.
// GetByHashes are tried first in the primary. If they aren't found in the primary, the backup is tried, and
// a successful GetByHash result from the backup is Put into the primary.
func NewFallbackStorageService(
	primary StorageService,
	backup arbstate.DataAvailabilityReader,
	backupHealthChecker DataAvailabilityServiceHealthChecker,
	backupRetentionSeconds uint64, // how long to retain data that we copy in from the backup (MaxUint64 means forever)
	ignoreRetentionWriteErrors bool, // if true, don't return error if write of retention data to primary fails
	preventRecursiveGets bool, // if true, return NotFound on simultaneous calls to Gets that miss in primary (prevents infinite recursion)
) *FallbackStorageService {
	return &FallbackStorageService{
		primary,
		backup,
		backupHealthChecker,
		backupRetentionSeconds,
		ignoreRetentionWriteErrors,
		preventRecursiveGets,
		make(map[[32]byte]bool),
		sync.RWMutex{},
	}
}

func (f *FallbackStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.FallbackStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", f)
	if f.preventRecursiveGets {
		f.currentlyFetchingMutex.RLock()
		if f.currentlyFetching[key] {
			// This is a recursive call, so return not-found
			f.currentlyFetchingMutex.RUnlock()
			return nil, ErrNotFound
		}
		f.currentlyFetchingMutex.RUnlock()
	}

	data, err := f.StorageService.GetByHash(ctx, key)
	if err != nil {
		doDelete := false
		if f.preventRecursiveGets {
			f.currentlyFetchingMutex.Lock()
			if !f.currentlyFetching[key] {
				f.currentlyFetching[key] = true
				doDelete = true
			}
			f.currentlyFetchingMutex.Unlock()
		}
		log.Trace("das.FallbackStorageService.GetByHash trying fallback")
		data, err = f.backup.GetByHash(ctx, key)
		if doDelete {
			f.currentlyFetchingMutex.Lock()
			delete(f.currentlyFetching, key)
			f.currentlyFetchingMutex.Unlock()
		}
		if err != nil {
			return nil, err
		}
		if dastree.ValidHash(key, data) {
			putErr := f.StorageService.Put(
				ctx, data, arbmath.SaturatingUAdd(uint64(time.Now().Unix()), f.backupRetentionSeconds),
			)
			if putErr != nil && !f.ignoreRetentionWriteErrors {
				return nil, err
			}
		}
	}
	return data, err
}

func (f *FallbackStorageService) String() string {
	return "FallbackStorageService(stoargeService:" + f.StorageService.String() + ")"
}

func (f *FallbackStorageService) HealthCheck(ctx context.Context) error {
	err := f.StorageService.HealthCheck(ctx)
	if err != nil {
		return err
	}
	return f.backupHealthChecker.HealthCheck(ctx)
}

'''
'''--- das/fallback_storage_service_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"testing"

	"github.com/ethereum/go-ethereum/common/math"
	"github.com/offchainlabs/nitro/das/dastree"
)

func TestFallbackStorageService(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	val1 := []byte("First value")
	hash1 := dastree.Hash(val1)
	val2 := []byte("Second value")
	hash2 := dastree.Hash(val2)

	primary := NewMemoryBackedStorageService(ctx)
	err := primary.Put(ctx, val1, math.MaxUint64)
	Require(t, err)
	fallback := NewMemoryBackedStorageService(ctx)
	err = fallback.Put(ctx, val2, math.MaxUint64)
	Require(t, err)

	fss := NewFallbackStorageService(primary, fallback, fallback, 60*60, true, true)

	res1, err := fss.GetByHash(ctx, hash1)
	Require(t, err)
	if !bytes.Equal(res1, val1) {
		t.Fatal()
	}
	res2, err := fss.GetByHash(ctx, hash2)
	Require(t, err)
	if !bytes.Equal(res2, val2) {
		t.Fatal()
	}

	res2, err = primary.GetByHash(ctx, hash2)
	Require(t, err)
	if !bytes.Equal(res2, val2) {
		t.Fatal()
	}
}

func TestFallbackStorageServiceRecursive(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	val1 := []byte("First value")
	hash1 := dastree.Hash(val1)

	ss := NewMemoryBackedStorageService(ctx)
	fss := NewFallbackStorageService(ss, ss, ss, 60*60, true, true)

	// artificially make fss recursive
	fss.backup = fss

	// try a recursive read of a non-existent item -- should give ErrNotFound
	_, err := fss.GetByHash(ctx, hash1)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
}

'''
'''--- das/ipfs_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// IPFS DAS backend.
// It takes advantage of IPFS' content addressing scheme to be able to directly retrieve
// the batches from IPFS using their root hash from the L1 sequencer inbox contract.

package das

import (
	"bytes"
	"context"
	"errors"
	"io"
	"math/rand"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ipfs/go-cid"
	coreiface "github.com/ipfs/interface-go-ipfs-core"
	"github.com/ipfs/interface-go-ipfs-core/options"
	"github.com/ipfs/interface-go-ipfs-core/path"
	"github.com/multiformats/go-multihash"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/cmd/ipfshelper"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	flag "github.com/spf13/pflag"
)

type IpfsStorageServiceConfig struct {
	Enable      bool          `koanf:"enable"`
	RepoDir     string        `koanf:"repo-dir"`
	ReadTimeout time.Duration `koanf:"read-timeout"`
	Profiles    string        `koanf:"profiles"`
	Peers       []string      `koanf:"peers"`

	// Pinning options
	PinAfterGet   bool    `koanf:"pin-after-get"`
	PinPercentage float64 `koanf:"pin-percentage"`
}

var DefaultIpfsStorageServiceConfig = IpfsStorageServiceConfig{
	Enable:      false,
	RepoDir:     "",
	ReadTimeout: time.Minute,
	Profiles:    "",
	Peers:       []string{},

	PinAfterGet:   true,
	PinPercentage: 100.0,
}

func IpfsStorageServiceConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultIpfsStorageServiceConfig.Enable, "enable storage/retrieval of sequencer batch data from IPFS")
	f.String(prefix+".repo-dir", DefaultIpfsStorageServiceConfig.RepoDir, "directory to use to store the local IPFS repo")
	f.Duration(prefix+".read-timeout", DefaultIpfsStorageServiceConfig.ReadTimeout, "timeout for IPFS reads, since by default it will wait forever. Treat timeout as not found")
	f.String(prefix+".profiles", DefaultIpfsStorageServiceConfig.Profiles, "comma separated list of IPFS profiles to use, see https://docs.ipfs.tech/how-to/default-profile")
	f.StringSlice(prefix+".peers", DefaultIpfsStorageServiceConfig.Peers, "list of IPFS peers to connect to, eg /ip4/1.2.3.4/tcp/12345/p2p/abc...xyz")
	f.Bool(prefix+".pin-after-get", DefaultIpfsStorageServiceConfig.PinAfterGet, "pin sequencer batch data in IPFS")
	f.Float64(prefix+".pin-percentage", DefaultIpfsStorageServiceConfig.PinPercentage, "percent of sequencer batch data to pin, as a floating point number in the range 0.0 to 100.0")
}

type IpfsStorageService struct {
	config     IpfsStorageServiceConfig
	ipfsHelper *ipfshelper.IpfsHelper
	ipfsApi    coreiface.CoreAPI
}

func NewIpfsStorageService(ctx context.Context, config IpfsStorageServiceConfig) (*IpfsStorageService, error) {
	ipfsHelper, err := ipfshelper.CreateIpfsHelper(ctx, config.RepoDir, false, config.Peers, config.Profiles)
	if err != nil {
		return nil, err
	}
	addrs, err := ipfsHelper.GetPeerHostAddresses()
	if err != nil {
		return nil, err
	}
	log.Info("IPFS node started up", "hostAddresses", addrs)

	return &IpfsStorageService{
		config:     config,
		ipfsHelper: ipfsHelper,
		ipfsApi:    ipfsHelper.GetAPI(),
	}, nil
}

func hashToCid(hash common.Hash) (cid.Cid, error) {
	multiEncodedHashBytes, err := multihash.Encode(hash[:], multihash.KECCAK_256)
	if err != nil {
		return cid.Cid{}, err
	}

	_, multiHash, err := multihash.MHFromBytes(multiEncodedHashBytes)
	if err != nil {
		return cid.Cid{}, err
	}

	return cid.NewCidV1(cid.Raw, multiHash), nil
}

// GetByHash retrieves and reconstructs one batch's data, using IPFS to retrieve the preimages
// for each chunk of data and the dastree nodes.
func (s *IpfsStorageService) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	log.Trace("das.IpfsStorageService.GetByHash", "hash", pretty.PrettyHash(hash))

	doPin := false // If true, pin every block related to this batch
	if s.config.PinAfterGet {
		if s.config.PinPercentage == 100.0 {
			doPin = true
		} else if (rand.Float64() * 100.0) <= s.config.PinPercentage {
			doPin = true
		}

	}

	oracle := func(h common.Hash) ([]byte, error) {
		thisCid, err := hashToCid(h)
		if err != nil {
			return nil, err
		}

		ipfsPath := path.IpfsPath(thisCid)
		log.Trace("Retrieving IPFS path", "path", ipfsPath.String())

		parentCtx := ctx
		if doPin {
			// If we want to pin this batch, then detach from the parent context so
			// we are not canceled before s.config.ReadTimeout.
			parentCtx = context.Background()
		}

		timeoutCtx, cancel := context.WithTimeout(parentCtx, s.config.ReadTimeout)
		defer cancel()
		rdr, err := s.ipfsApi.Block().Get(timeoutCtx, ipfsPath)
		if err != nil {
			if timeoutCtx.Err() != nil {
				return nil, ErrNotFound
			}
			return nil, err
		}

		data, err := io.ReadAll(rdr)
		if err != nil {
			return nil, err
		}

		if doPin {
			go func() {
				pinCtx, pinCancel := context.WithTimeout(context.Background(), s.config.ReadTimeout)
				defer pinCancel()
				err := s.ipfsApi.Pin().Add(pinCtx, ipfsPath)
				// Recursive pinning not needed, each dastree preimage fits in a single
				// IPFS block.
				if err != nil {
					// Pinning is best-effort.
					log.Warn("Failed to pin in IPFS", "hash", pretty.PrettyHash(hash), "path", ipfsPath.String())
				} else {
					log.Trace("Pin in IPFS successful", "hash", pretty.PrettyHash(hash), "path", ipfsPath.String())
				}
			}()
		}

		return data, nil
	}

	return dastree.Content(hash, oracle)
}

// Put stores all the preimages required to reconstruct the dastree for single batch,
// ie the hashed data chunks and dastree nodes.
// This takes advantage of IPFS supporting keccak256 on raw data blocks for calculating
// its CIDs, and the fact that the dastree structure uses keccak256 for addressing its
// nodes, to directly store the dastree structure in IPFS.
// IPFS default block size is 256KB and dastree max block size is 64KB so each dastree
// node and data chunk easily fits within an IPFS block.
func (s *IpfsStorageService) Put(ctx context.Context, data []byte, timeout uint64) error {
	logPut("das.IpfsStorageService.Put", data, timeout, s)

	var chunks [][]byte

	record := func(_ common.Hash, value []byte) {
		chunks = append(chunks, value)
	}

	_ = dastree.RecordHash(record, data)

	numChunks := len(chunks)
	resultChan := make(chan error, numChunks)
	for _, chunk := range chunks {
		_chunk := chunk
		go func() {
			blockStat, err := s.ipfsApi.Block().Put(
				ctx,
				bytes.NewReader(_chunk),
				options.Block.CidCodec("raw"), // Store the data in raw form since the hash in the CID must be the hash
				// of the preimage for our lookup scheme to work.
				options.Block.Hash(multihash.KECCAK_256, -1), // Use keccak256 to calculate the hash to put in the block's
				// CID, since it is the same algo used by dastree.
				options.Block.Pin(true)) // Keep the data in the local IPFS repo, don't GC it.
			if err == nil {
				log.Trace("Wrote IPFS path", "path", blockStat.Path().String())
			}
			resultChan <- err
		}()
	}

	successfullyWrittenChunks := 0
	for err := range resultChan {
		if err != nil {
			return err
		}
		successfullyWrittenChunks++
		if successfullyWrittenChunks == numChunks {
			return nil
		}
	}
	panic("unreachable")
}

func (s *IpfsStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.KeepForever, nil
}

func (s *IpfsStorageService) Sync(ctx context.Context) error {
	return nil
}

func (s *IpfsStorageService) Close(ctx context.Context) error {
	return s.ipfsHelper.Close()
}

func (s *IpfsStorageService) String() string {
	return "IpfsStorageService"
}

func (s *IpfsStorageService) HealthCheck(ctx context.Context) error {
	testData := []byte("Test-Data")
	err := s.Put(ctx, testData, 0)
	if err != nil {
		return err
	}
	res, err := s.GetByHash(ctx, dastree.Hash(testData))
	if err != nil {
		return err
	}
	if !bytes.Equal(res, testData) {
		return errors.New("invalid GetByHash result")
	}
	return nil
}

'''
'''--- das/ipfs_storage_service_test.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"math"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/das/dastree"
)

func runAddAndGetTest(t *testing.T, ctx context.Context, svc *IpfsStorageService, size int) {

	data := make([]byte, size)
	_, err := rand.Read(data)
	Require(t, err)

	err = svc.Put(ctx, data, 0)
	Require(t, err)

	hash := dastree.Hash(data).Bytes()
	returnedData, err := svc.GetByHash(ctx, common.BytesToHash(hash))
	Require(t, err)
	if !bytes.Equal(data, returnedData) {
		Fail(t, "Returned data didn't match!")
	}

}

func TestIpfsStorageServiceAddAndGet(t *testing.T) {
	enableLogging()
	ctx := context.Background()
	svc, err := NewIpfsStorageService(ctx,
		IpfsStorageServiceConfig{
			Enable:      true,
			RepoDir:     t.TempDir(),
			ReadTimeout: time.Minute,
			Profiles:    "test",
		})
	defer svc.Close(ctx)
	Require(t, err)

	pow2Size := 1 << 16 // 64kB
	for i := 1; i < 8; i++ {
		runAddAndGetTest(t, ctx, svc, int(math.Pow10(i)))
		runAddAndGetTest(t, ctx, svc, pow2Size)
		runAddAndGetTest(t, ctx, svc, pow2Size-1)
		runAddAndGetTest(t, ctx, svc, pow2Size+1)
		pow2Size = pow2Size << 1
	}
}

'''
'''--- das/iterable_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"strconv"
	"sync"
	"sync/atomic"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/das/dastree"
)

const iteratorStorageKeyPrefix = "iterator_key_prefix_"
const iteratorBegin = "iterator_begin"
const iteratorEnd = "iterator_end"
const expirationTimeKeyPrefix = "expiration_time_key_prefix_"

// IterationCompatibleStorageService is a StorageService which is
// compatible to be used as a backend for IterableStorageService.
type IterationCompatibleStorageService interface {
	putKeyValue(ctx context.Context, key common.Hash, value []byte) error
	StorageService
}

// IterationCompatibleStorageServiceAdaptor is an adaptor used to covert iteration incompatible StorageService
// to IterationCompatibleStorageService (basically adds an empty putKeyValue to the StorageService)
type IterationCompatibleStorageServiceAdaptor struct {
	StorageService
}

func (i *IterationCompatibleStorageServiceAdaptor) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	return nil
}

func ConvertStorageServiceToIterationCompatibleStorageService(storageService StorageService) IterationCompatibleStorageService {
	service, ok := storageService.(IterationCompatibleStorageService)
	if ok {
		return service
	}
	return &IterationCompatibleStorageServiceAdaptor{storageService}
}

// An IterableStorageService is used as a wrapper on top of a storage service,
// to add the capability of iterating over the stored date in a sequential manner.
type IterableStorageService struct {
	// Local copy of iterator end. End can also be accessed by getByHash for iteratorEnd.
	end atomic.Value // atomic access to common.Hash
	IterationCompatibleStorageService

	mutex sync.Mutex
}

func NewIterableStorageService(storageService IterationCompatibleStorageService) *IterableStorageService {
	i := &IterableStorageService{IterationCompatibleStorageService: storageService}
	i.end.Store(common.Hash{})
	return i
}

func (i *IterableStorageService) Put(ctx context.Context, data []byte, expiration uint64) error {
	dataHash := dastree.Hash(data)

	// Do not insert data if data is already present.
	// (This is being done to avoid redundant hash being added to the
	//  linked list ,since it can lead to loops in the linked list.)
	if _, err := i.IterationCompatibleStorageService.GetByHash(ctx, dataHash); err == nil {
		return nil
	}

	if err := i.IterationCompatibleStorageService.Put(ctx, data, expiration); err != nil {
		return err
	}

	if err := i.putKeyValue(ctx, dastree.Hash([]byte(expirationTimeKeyPrefix+EncodeStorageServiceKey(dastree.Hash(data)))), []byte(strconv.FormatUint(expiration, 10))); err != nil {
		return err
	}

	i.mutex.Lock()
	defer i.mutex.Unlock()

	endHash := i.End(ctx)
	if (endHash == common.Hash{}) {
		// First element being inserted in the chain.
		if err := i.putKeyValue(ctx, dastree.Hash([]byte(iteratorBegin)), dataHash.Bytes()); err != nil {
			return err
		}
	} else {
		if err := i.putKeyValue(ctx, dastree.Hash([]byte(iteratorStorageKeyPrefix+EncodeStorageServiceKey(endHash))), dataHash.Bytes()); err != nil {
			return err
		}
	}

	if err := i.putKeyValue(ctx, dastree.Hash([]byte(iteratorEnd)), dataHash.Bytes()); err != nil {
		return err
	}
	i.end.Store(dataHash)

	return nil
}

func (i *IterableStorageService) GetExpirationTime(ctx context.Context, hash common.Hash) (uint64, error) {
	value, err := i.IterationCompatibleStorageService.GetByHash(ctx, dastree.Hash([]byte(expirationTimeKeyPrefix+EncodeStorageServiceKey(hash))))
	if err != nil {
		return 0, err
	}

	expirationTime, err := strconv.ParseUint(string(value), 10, 64)
	if err != nil {
		return 0, err
	}
	return expirationTime, nil
}

func (i *IterableStorageService) DefaultBegin() common.Hash {
	return dastree.Hash([]byte(iteratorBegin))
}

func (i *IterableStorageService) End(ctx context.Context) common.Hash {
	endHash, ok := i.end.Load().(common.Hash)
	if !ok {
		return common.Hash{}
	}
	if (endHash != common.Hash{}) {
		return endHash
	}
	value, err := i.GetByHash(ctx, dastree.Hash([]byte(iteratorEnd)))
	if err != nil {
		return common.Hash{}
	}
	endHash = common.BytesToHash(value)
	i.end.Store(endHash)
	return endHash
}

func (i *IterableStorageService) Next(ctx context.Context, hash common.Hash) common.Hash {
	if hash != i.DefaultBegin() {
		hash = dastree.Hash([]byte(iteratorStorageKeyPrefix + EncodeStorageServiceKey(hash)))
	}
	value, err := i.GetByHash(ctx, hash)
	if err != nil {
		return common.Hash{}
	}
	return common.BytesToHash(value)
}

'''
'''--- das/key_utils.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"encoding/base64"
	"encoding/hex"
	"io"
	"os"

	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/blsSignatures"
)

// Note for Decode functions
// Ethereum's BLS library doesn't like the byte slice containing the BLS keys to be
// any larger than necessary, so we need to create a Decoder to avoid returning any padding.

func DecodeBase64BLSPublicKey(pubKeyEncodedBytes []byte) (*blsSignatures.PublicKey, error) {
	pubKeyDecoder := base64.NewDecoder(base64.StdEncoding, bytes.NewReader(pubKeyEncodedBytes))
	pubKeyBytes, err := io.ReadAll(pubKeyDecoder)
	if err != nil {
		return nil, err
	}
	pubKey, err := blsSignatures.PublicKeyFromBytes(pubKeyBytes, false)
	if err != nil {
		return nil, err
	}
	return &pubKey, nil
}

func DecodeBase64BLSPrivateKey(privKeyEncodedBytes []byte) (blsSignatures.PrivateKey, error) {
	privKeyDecoder := base64.NewDecoder(base64.StdEncoding, bytes.NewReader(privKeyEncodedBytes))
	privKeyBytes, err := io.ReadAll(privKeyDecoder)
	if err != nil {
		return nil, err
	}
	privKey, err := blsSignatures.PrivateKeyFromBytes(privKeyBytes)
	if err != nil {
		return nil, err
	}
	return privKey, nil
}

const DefaultPubKeyFilename = "das_bls.pub"
const DefaultPrivKeyFilename = "das_bls"

func GenerateAndStoreKeys(keyDir string) (*blsSignatures.PublicKey, *blsSignatures.PrivateKey, error) {
	pubKey, privKey, err := blsSignatures.GenerateKeys()
	if err != nil {
		return nil, nil, err
	}
	pubKeyPath := keyDir + "/" + DefaultPubKeyFilename
	pubKeyBytes := blsSignatures.PublicKeyToBytes(pubKey)
	encodedPubKey := make([]byte, base64.StdEncoding.EncodedLen(len(pubKeyBytes)))
	base64.StdEncoding.Encode(encodedPubKey, pubKeyBytes)
	err = os.WriteFile(pubKeyPath, encodedPubKey, 0o600)
	if err != nil {
		return nil, nil, err
	}

	privKeyPath := keyDir + "/" + DefaultPrivKeyFilename
	privKeyBytes := blsSignatures.PrivateKeyToBytes(privKey)
	encodedPrivKey := make([]byte, base64.StdEncoding.EncodedLen(len(privKeyBytes)))
	base64.StdEncoding.Encode(encodedPrivKey, privKeyBytes)
	err = os.WriteFile(privKeyPath, encodedPrivKey, 0o600)
	if err != nil {
		return nil, nil, err
	}
	return &pubKey, &privKey, nil
}

func ReadKeysFromFile(keyDir string) (*blsSignatures.PublicKey, blsSignatures.PrivateKey, error) {
	pubKey, err := ReadPubKeyFromFile(keyDir + "/" + DefaultPubKeyFilename)
	if err != nil {
		return nil, nil, err
	}

	privKey, err := ReadPrivKeyFromFile(keyDir + "/" + DefaultPrivKeyFilename)
	if err != nil {
		return nil, nil, err
	}
	return pubKey, privKey, nil
}

func ReadPubKeyFromFile(pubKeyPath string) (*blsSignatures.PublicKey, error) {
	pubKeyEncodedBytes, err := os.ReadFile(pubKeyPath)
	if err != nil {
		return nil, err
	}
	pubKey, err := DecodeBase64BLSPublicKey(pubKeyEncodedBytes)
	if err != nil {
		return nil, err
	}
	return pubKey, nil
}

func ReadPrivKeyFromFile(privKeyPath string) (blsSignatures.PrivateKey, error) {
	privKeyEncodedBytes, err := os.ReadFile(privKeyPath)
	if err != nil {
		return nil, err
	}
	privKey, err := DecodeBase64BLSPrivateKey(privKeyEncodedBytes)
	if err != nil {
		return nil, err
	}
	return privKey, nil
}

func GenerateAndStoreECDSAKeys(dir string) error {
	privateKey, err := crypto.GenerateKey()
	if err != nil {
		return err
	}

	err = crypto.SaveECDSA(dir+"/ecdsa", privateKey)
	if err != nil {
		return err
	}
	encodedPubKey := hex.EncodeToString(crypto.FromECDSAPub(&privateKey.PublicKey))
	return os.WriteFile(dir+"/ecdsa.pub", []byte(encodedPubKey), 0o600)
}

'''
'''--- das/lifecycle.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/log"
)

type Closer interface {
	Close(ctx context.Context) error
	fmt.Stringer
}

type LifecycleManager struct {
	toClose []Closer
}

func (m *LifecycleManager) Register(c Closer) {
	m.toClose = append(m.toClose, c)
}

func (m *LifecycleManager) StopAndWaitUntil(t time.Duration) {
	if m != nil && m.toClose != nil {
		ctx, cancel := context.WithTimeout(context.Background(), t)
		defer cancel()
		for _, c := range m.toClose {
			err := c.Close(ctx)
			if err != nil {
				log.Warn("Failed to Close DAS component", "err", err)
			}
		}
	}
}

'''
'''--- das/local_file_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/base32"
	"errors"
	"fmt"
	"os"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	flag "github.com/spf13/pflag"
	"golang.org/x/sys/unix"
)

type LocalFileStorageConfig struct {
	Enable                 bool   `koanf:"enable"`
	DataDir                string `koanf:"data-dir"`
	SyncFromStorageService bool   `koanf:"sync-from-storage-service"`
	SyncToStorageService   bool   `koanf:"sync-to-storage-service"`
}

var DefaultLocalFileStorageConfig = LocalFileStorageConfig{
	DataDir: "",
}

func LocalFileStorageConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultLocalFileStorageConfig.Enable, "enable storage/retrieval of sequencer batch data from a directory of files, one per batch")
	f.String(prefix+".data-dir", DefaultLocalFileStorageConfig.DataDir, "local data directory")
	f.Bool(prefix+".sync-from-storage-service", DefaultLocalFileStorageConfig.SyncFromStorageService, "enable local storage to be used as a source for regular sync storage")
	f.Bool(prefix+".sync-to-storage-service", DefaultLocalFileStorageConfig.SyncToStorageService, "enable local storage to be used as a sink for regular sync storage")
}

type LocalFileStorageService struct {
	dataDir string
}

func NewLocalFileStorageService(dataDir string) (StorageService, error) {
	if unix.Access(dataDir, unix.W_OK|unix.R_OK) != nil {
		return nil, fmt.Errorf("couldn't start LocalFileStorageService, directory '%s' must be readable and writeable", dataDir)
	}
	return &LocalFileStorageService{dataDir: dataDir}, nil
}

func (s *LocalFileStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.LocalFileStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", s)
	pathname := s.dataDir + "/" + EncodeStorageServiceKey(key)
	data, err := os.ReadFile(pathname)
	if err != nil {
		// Just for backward compatability.
		pathname = s.dataDir + "/" + base32.StdEncoding.EncodeToString(key.Bytes())
		data, err = os.ReadFile(pathname)
		if err != nil {
			if errors.Is(err, os.ErrNotExist) {
				return nil, ErrNotFound
			}
			return nil, err
		}
		return data, nil
	}
	return data, nil
}

func (s *LocalFileStorageService) Put(ctx context.Context, data []byte, timeout uint64) error {
	logPut("das.LocalFileStorageService.Store", data, timeout, s)
	fileName := EncodeStorageServiceKey(dastree.Hash(data))
	finalPath := s.dataDir + "/" + fileName

	// Use a temp file and rename to achieve atomic writes.
	f, err := os.CreateTemp(s.dataDir, fileName)
	if err != nil {
		return err
	}
	err = f.Chmod(0o600)
	if err != nil {
		return err
	}
	_, err = f.Write(data)
	if err != nil {
		return err
	}
	err = f.Close()
	if err != nil {
		return err
	}

	return os.Rename(f.Name(), finalPath)

}

func (s *LocalFileStorageService) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	fileName := EncodeStorageServiceKey(key)
	finalPath := s.dataDir + "/" + fileName

	// Use a temp file and rename to achieve atomic writes.
	f, err := os.CreateTemp(s.dataDir, fileName)
	if err != nil {
		return err
	}
	err = f.Chmod(0o600)
	if err != nil {
		return err
	}
	_, err = f.Write(value)
	if err != nil {
		return err
	}
	err = f.Close()
	if err != nil {
		return err
	}

	return os.Rename(f.Name(), finalPath)

}

func (s *LocalFileStorageService) Sync(ctx context.Context) error {
	return nil
}

func (s *LocalFileStorageService) Close(ctx context.Context) error {
	return nil
}

func (s *LocalFileStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.KeepForever, nil
}

func (s *LocalFileStorageService) String() string {
	return "LocalFileStorageService(" + s.dataDir + ")"
}

func (s *LocalFileStorageService) HealthCheck(ctx context.Context) error {
	testData := []byte("Test-Data")
	err := s.Put(ctx, testData, uint64(time.Now().Add(time.Minute).Unix()))
	if err != nil {
		return err
	}
	res, err := s.GetByHash(ctx, dastree.Hash(testData))
	if err != nil {
		return err
	}
	if !bytes.Equal(res, testData) {
		return errors.New("invalid GetByHash result")
	}
	return nil
}

'''
'''--- das/memory_backed_storage_service.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"sync"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
)

type MemoryBackedStorageService struct { // intended for testing and debugging
	contents map[[32]byte][]byte
	rwmutex  sync.RWMutex
	closed   bool
}

var ErrClosed = errors.New("cannot access a StorageService that has been Closed")

func NewMemoryBackedStorageService(ctx context.Context) StorageService {
	return &MemoryBackedStorageService{
		contents: make(map[[32]byte][]byte),
	}
}

func (m *MemoryBackedStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.MemoryBackedStorageService.GetByHash", "key", key, "this", m)
	m.rwmutex.RLock()
	defer m.rwmutex.RUnlock()
	if m.closed {
		return nil, ErrClosed
	}
	res, found := m.contents[key]
	if !found {
		return nil, ErrNotFound
	}
	return res, nil
}

func (m *MemoryBackedStorageService) Put(ctx context.Context, data []byte, expirationTime uint64) error {
	logPut("das.MemoryBackedStorageService.Store", data, expirationTime, m)
	m.rwmutex.Lock()
	defer m.rwmutex.Unlock()
	if m.closed {
		return ErrClosed
	}
	m.contents[dastree.Hash(data)] = append([]byte{}, data...)
	return nil
}

func (m *MemoryBackedStorageService) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	m.rwmutex.Lock()
	defer m.rwmutex.Unlock()
	if m.closed {
		return ErrClosed
	}
	m.contents[key] = append([]byte{}, value...)
	return nil
}

func (m *MemoryBackedStorageService) Sync(ctx context.Context) error {
	m.rwmutex.RLock()
	defer m.rwmutex.RUnlock()
	if m.closed {
		return ErrClosed
	}
	return nil
}

func (m *MemoryBackedStorageService) Close(ctx context.Context) error {
	m.rwmutex.Lock()
	defer m.rwmutex.Unlock()
	m.closed = true
	return nil
}

func (m *MemoryBackedStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.KeepForever, nil
}

func (m *MemoryBackedStorageService) String() string {
	return "MemoryBackedStorageService"
}

func (m *MemoryBackedStorageService) HealthCheck(ctx context.Context) error {
	return nil
}

'''
'''--- das/near_aggregator.go ---
package das

import (
	"context"
	"encoding/hex"
	"fmt"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	near "github.com/near/rollup-data-availability/gopkg/da-rpc"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
	flag "github.com/spf13/pflag"
)

// TODO: inject in place of RPC aggregator
type NearService struct {
	near near.Config
	pub  blsSignatures.PublicKey
	priv blsSignatures.PrivateKey
}

func NewNearService(config DataAvailabilityConfig) (*NearService, error) {

	privKey, err := DecodeBase64BLSPrivateKey([]byte(config.Key.PrivKey))
	if err != nil {
		return nil, err
	}

	pubKey, err := blsSignatures.PublicKeyFromPrivateKey(privKey)
	if err != nil {
		return nil, err
	}

	near, err := near.NewConfig(config.NEARAggregator.Account, config.NEARAggregator.Contract, config.NEARAggregator.Key, config.NEARAggregator.Namespace)

	return &NearService{
		near: *near,
		pub:  pubKey,
		priv: privKey,
	}, nil
}

func (s *NearService) String() string {
	return fmt.Sprintf("NearService{}")
}

type NearAggregatorConfig struct {
	Enable    bool
	Account   string
	Contract  string
	Key       string
	Namespace uint32
	StorageConfig NearStorageConfig `koanf:"storage"`
}

func NewNearAggregator(ctx context.Context, config DataAvailabilityConfig, nsvc *NearService) (*Aggregator, error) {
	svc := ServiceDetails{
		service:     (DataAvailabilityServiceWriter)(nsvc),
		pubKey:      nsvc.pub,
		signersMask: 1,
		metricName:  "near",
	}

	services := make([]ServiceDetails, 1)
	services[0] = svc

	return NewAggregator(ctx, config, services)
}

func (s *NearService) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	log.Info("Storing message", "message", message)
	frameRefBytes, err := s.near.ForceSubmit(message)
	if err != nil {
		return nil, err
	}
	frameRef := near.FrameRef{}
	err = frameRef.UnmarshalBinary(frameRefBytes)
	if err != nil {
		return nil, err
	}

	keysetHash, keySet, err := s.KeysetHash()
	if err != nil {
		log.Error("Error getting keyset hash", "err", err, "keyset", keySet)
		return nil, err
	}
	var certificate = arbstate.DataAvailabilityCertificate{
		KeysetHash:  keysetHash,
		DataHash:    [32]byte(frameRef.TxId),
		Timeout:     timeout,
		Sig:         nil,
		SignersMask: 1,
		Version: 255,
	}
	newSig, err := blsSignatures.SignMessage(s.priv, certificate.SerializeSignableFields())
	if err != nil {
		return nil, err
	}
	certificate.Sig = newSig

	return &certificate, nil
}

func (s *NearService) KeysetHash() ([32]byte, []byte, error) {
	svc := ServiceDetails{
		service:     (DataAvailabilityServiceWriter)(s),
		pubKey:      s.pub,
		signersMask: 1,
		metricName:  "near",
	}

	services := make([]ServiceDetails, 1)
	services[0] = svc
	return KeysetHashFromServices(services, 1)
}

func (s *NearService) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	log.Info("Getting message", "hash", hash)
	// Hack to bypass commitment
	bytesPadded := make([]byte, 64)
	copy(bytesPadded[0:32], hash.Bytes())
	bytes, err := s.near.Get(bytesPadded, 0)
	if err != nil {
		return nil, err
	}
	return bytes, nil

}

func (s *NearService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return arbstate.KeepForever, nil
}

var DefaultNearAggregatorConfig = NearAggregatorConfig{
	Enable:    true,
	Account:   "topgunbakugo.testnet",
	Contract:  "nitro.topgunbakugo.testnet",
	Key:       "ed25519:kjdshdfskjdfhsdk",
	Namespace: 1,
	StorageConfig: DefaultNearStorageConfig,
}

func NearAggregatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultNearAggregatorConfig.Enable, "enable retrieval of sequencer batch data from a list of remote REST endpoints; if other DAS storage types are enabled, this mode is used as a fallback")
	f.String(prefix+".account", DefaultNearAggregatorConfig.Account, "Account Id for signing NEAR transactions")
	f.String(prefix+".contract", DefaultNearAggregatorConfig.Contract, "Contract address for submitting NEAR transactions")
	f.String(prefix+".key", DefaultNearAggregatorConfig.Key, "ED25519 Key for signing NEAR transactions, prefixed with 'ed25519:'")
	f.Uint32(prefix+".namespace", DefaultNearAggregatorConfig.Namespace, "Namespace for this rollup")
  NearStorageConfigAddOptions(prefix+".storage", f)	
}

// TODO: add healtchecks
func (s *NearService) HealthCheck(ctx context.Context) error {
	s.near.Get(nil, 0)
	return nil
}

func (s *NearService) Put(ctx context.Context, data []byte, expirationTime uint64) error {
	log.Info("Storing message", "message", data)
	frameRefBytes, err := s.near.ForceSubmit(data)
	if err != nil {
		return err
	}
	log.Info("Frame ref", "frame ref", hex.EncodeToString(frameRefBytes))
	return nil
}

'''
'''--- das/panic_wrapper.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"fmt"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
)

type WriterPanicWrapper struct {
	DataAvailabilityServiceWriter
}

func NewWriterPanicWrapper(dataAvailabilityService DataAvailabilityServiceWriter) DataAvailabilityServiceWriter {
	return &WriterPanicWrapper{
		DataAvailabilityServiceWriter: dataAvailabilityService,
	}
}
func (w *WriterPanicWrapper) String() string {
	return fmt.Sprintf("WriterPanicWrapper{%v}", w.DataAvailabilityServiceWriter)
}

func (w *WriterPanicWrapper) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	cert, err := w.DataAvailabilityServiceWriter.Store(ctx, message, timeout, sig)
	if err != nil {
		panic(fmt.Sprintf("panic wrapper Store: %v", err))
	}
	return cert, nil
}

type ReaderPanicWrapper struct {
	DataAvailabilityServiceReader
}

func NewReaderPanicWrapper(dataAvailabilityService DataAvailabilityServiceReader) DataAvailabilityServiceReader {
	return &ReaderPanicWrapper{
		DataAvailabilityServiceReader: dataAvailabilityService,
	}
}
func (w *ReaderPanicWrapper) String() string {
	return fmt.Sprintf("ReaderPanicWrapper{%v}", w.DataAvailabilityServiceReader)
}

func (w *ReaderPanicWrapper) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	data, err := w.DataAvailabilityServiceReader.GetByHash(ctx, hash)
	if err != nil {
		if errors.Is(err, context.Canceled) {
			log.Error("DAS hash lookup failed from cancelled context")
			return nil, err
		}
		panic(fmt.Sprintf("panic wrapper GetByHash: %v", err))
	}
	return data, nil
}

'''
'''--- das/read_limited.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"

	"github.com/offchainlabs/nitro/arbstate"
)

// These classes are wrappers implementing das.StorageService and das.DataAvailabilityService.
// They are needed to make the DAS factory function uniform for all allowed configurations.
// The wrappers panic if they are used in a situation where writes are needed; panic is used because
// it is a programming error in the code setting up the node or daserver if a non-writeable object
// is used in a writeable context.

func NewReadLimitedStorageService(reader arbstate.DataAvailabilityReader) *readLimitedStorageService {
	return &readLimitedStorageService{reader}
}

type readLimitedStorageService struct {
	arbstate.DataAvailabilityReader
}

func (s *readLimitedStorageService) Put(ctx context.Context, data []byte, expiration uint64) error {
	panic("Logic error: readLimitedStorageService.Put shouldn't be called.")
}

func (s *readLimitedStorageService) Sync(ctx context.Context) error {
	panic("Logic error: readLimitedStorageService.Store shouldn't be called.")
}

func (s *readLimitedStorageService) Close(ctx context.Context) error {
	return nil
}

func (s *readLimitedStorageService) String() string {
	return fmt.Sprintf("readLimitedStorageService(%v)", s.DataAvailabilityReader)

}

type readLimitedDataAvailabilityService struct {
	arbstate.DataAvailabilityReader
}

func NewReadLimitedDataAvailabilityService(da arbstate.DataAvailabilityReader) *readLimitedDataAvailabilityService {
	return &readLimitedDataAvailabilityService{da}
}

func (*readLimitedDataAvailabilityService) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	panic("Logic error: readLimitedDataAvailabilityService.Store shouldn't be called.")
}

func (s *readLimitedDataAvailabilityService) String() string {
	return fmt.Sprintf("ReadLimitedDataAvailabilityService(%v)", s.DataAvailabilityReader)
}

'''
'''--- das/reader_aggregator_strategies.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"errors"
	"math/rand"
	"sort"
	"sync"
	"sync/atomic"

	"github.com/offchainlabs/nitro/arbstate"
)

var ErrNoReadersResponded = errors.New("no DAS readers responded successfully")

type aggregatorStrategy interface {
	newInstance() aggregatorStrategyInstance
	update([]arbstate.DataAvailabilityReader, map[arbstate.DataAvailabilityReader]readerStats)
}

type abstractAggregatorStrategy struct {
	sync.RWMutex
	readers []arbstate.DataAvailabilityReader
	stats   map[arbstate.DataAvailabilityReader]readerStats
}

func (s *abstractAggregatorStrategy) update(readers []arbstate.DataAvailabilityReader, stats map[arbstate.DataAvailabilityReader]readerStats) {
	s.Lock()
	defer s.Unlock()

	s.readers = make([]arbstate.DataAvailabilityReader, len(readers))
	copy(s.readers, readers)

	s.stats = make(map[arbstate.DataAvailabilityReader]readerStats)
	for k, v := range stats {
		s.stats[k] = v
	}
}

// Exponentially growing Explore Exploit Strategy
type simpleExploreExploitStrategy struct {
	iterations        uint32
	exploreIterations uint32
	exploitIterations uint32

	abstractAggregatorStrategy
}

func (s *simpleExploreExploitStrategy) newInstance() aggregatorStrategyInstance {
	iterations := atomic.AddUint32(&s.iterations, 1)

	readerSets := make([][]arbstate.DataAvailabilityReader, 0)
	s.RLock()
	defer s.RUnlock()

	readers := make([]arbstate.DataAvailabilityReader, len(s.readers))
	copy(readers, s.readers)

	if iterations%(s.exploreIterations+s.exploitIterations) < s.exploreIterations {
		// Explore phase
		rand.Shuffle(len(readers), func(i, j int) { readers[i], readers[j] = readers[j], readers[i] })
	} else {
		// Exploit phase
		sort.Slice(readers, func(i, j int) bool {
			a, b := s.stats[readers[i]], s.stats[readers[j]]
			return a.successRatioWeightedMeanLatency() < b.successRatioWeightedMeanLatency()
		})
	}

	for i, maxTake := 0, 1; i < len(readers); maxTake = maxTake * 2 {
		readerSet := make([]arbstate.DataAvailabilityReader, 0, maxTake)
		for taken := 0; taken < maxTake && i < len(readers); i, taken = i+1, taken+1 {
			readerSet = append(readerSet, readers[i])
		}
		readerSets = append(readerSets, readerSet)
	}

	return &basicStrategyInstance{readerSets: readerSets}
}

// Sequential Strategy for Testing
type testingSequentialStrategy struct {
	abstractAggregatorStrategy
}

func (s *testingSequentialStrategy) newInstance() aggregatorStrategyInstance {
	s.RLock()
	defer s.RUnlock()

	si := basicStrategyInstance{}
	for _, reader := range s.readers {
		si.readerSets = append(si.readerSets, []arbstate.DataAvailabilityReader{reader})
	}

	return &si
}

// Instance of a strategy that returns readers in an order according to the strategy
type aggregatorStrategyInstance interface {
	nextReaders() []arbstate.DataAvailabilityReader
}

type basicStrategyInstance struct {
	readerSets [][]arbstate.DataAvailabilityReader
}

func (si *basicStrategyInstance) nextReaders() []arbstate.DataAvailabilityReader {
	if len(si.readerSets) == 0 {
		return nil
	}
	next := si.readerSets[0]
	si.readerSets = si.readerSets[1:]
	return next
}

'''
'''--- das/reader_aggregator_strategies_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"fmt"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbstate"
)

type dummyReader struct {
	int
}

func (*dummyReader) GetByHash(context.Context, common.Hash) ([]byte, error) {
	return nil, errors.New("not implemented")
}

func (*dummyReader) HealthCheck(context.Context) error {
	return errors.New("not implemented")
}

func (*dummyReader) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return -1, errors.New("not implemented")
}

func TestDAS_SimpleExploreExploit(t *testing.T) {
	readers := []arbstate.DataAvailabilityReader{&dummyReader{0}, &dummyReader{1}, &dummyReader{2}, &dummyReader{3}, &dummyReader{4}, &dummyReader{5}}
	stats := make(map[arbstate.DataAvailabilityReader]readerStats)
	stats[readers[0]] = []readerStat{ // weighted avg 10s
		{10 * time.Second, true},
	}
	stats[readers[1]] = []readerStat{ // weighted avg 5s
		{6 * time.Second, true},
		{4 * time.Second, true},
	}
	stats[readers[2]] = []readerStat{ // weighted avg 3 / (1/2) = 6s
		{3 * time.Second, true},
		{3 * time.Second, false},
	}
	stats[readers[3]] = []readerStat{ // weighted avg max int
		{1 * time.Second, false},
		{1 * time.Second, false},
	}
	stats[readers[4]] = []readerStat{ // weighted avg 3 / (1/3) = 9s
		{3 * time.Second, true},
		{3 * time.Second, false},
		{3 * time.Second, false},
	}
	stats[readers[5]] = []readerStat{ // weighted avg 8s
		{8 * time.Second, true},
	}

	expectedOrdering := []arbstate.DataAvailabilityReader{readers[1], readers[2], readers[5], readers[4], readers[0], readers[3]}

	expectedExploreIterations, expectedExploitIterations := uint32(5), uint32(5)
	strategy := simpleExploreExploitStrategy{
		exploreIterations: expectedExploreIterations,
		exploitIterations: expectedExploitIterations,
	}
	strategy.update(readers, stats)

	checkMatch := func(expected, was []arbstate.DataAvailabilityReader, doMatch bool) {
		if len(expected) != len(was) {
			Fail(t, fmt.Sprintf("Incorrect number of nextReaders %d, expected %d", len(was), len(expected)))
		}

		for i := 0; i < len(was) && doMatch; i++ {
			if expected[i].(*dummyReader).int != was[i].(*dummyReader).int {
				Fail(t, fmt.Sprintf("expected %d, was %d", expected[i].(*dummyReader).int, was[i].(*dummyReader).int))
			}
		}
	}

	// In Explore mode we just care about the exponential growth
	for i := uint32(0); i < expectedExploreIterations-1; i++ {
		si := strategy.newInstance()
		checkMatch(expectedOrdering[:1], si.nextReaders(), false)
		checkMatch(expectedOrdering[1:3], si.nextReaders(), false)
		checkMatch(expectedOrdering[3:6], si.nextReaders(), false)
		checkMatch(expectedOrdering[6:], si.nextReaders(), false)
	}

	// In Exploit mode we can check the ordering too.
	for i := uint32(0); i < expectedExploitIterations; i++ {
		si := strategy.newInstance()
		checkMatch(expectedOrdering[:1], si.nextReaders(), true)
		checkMatch(expectedOrdering[1:3], si.nextReaders(), true)
		checkMatch(expectedOrdering[3:6], si.nextReaders(), true)
		checkMatch(expectedOrdering[6:], si.nextReaders(), true)
	}

	// Cycle through explore/exploit one more time
	for i := uint32(0); i < expectedExploreIterations; i++ {
		si := strategy.newInstance()
		checkMatch(expectedOrdering[:1], si.nextReaders(), false)
		checkMatch(expectedOrdering[1:3], si.nextReaders(), false)
		checkMatch(expectedOrdering[3:6], si.nextReaders(), false)
		checkMatch(expectedOrdering[6:], si.nextReaders(), false)
	}

	for i := uint32(0); i < expectedExploitIterations; i++ {
		si := strategy.newInstance()
		checkMatch(expectedOrdering[:1], si.nextReaders(), true)
		checkMatch(expectedOrdering[1:3], si.nextReaders(), true)
		checkMatch(expectedOrdering[3:6], si.nextReaders(), true)
		checkMatch(expectedOrdering[6:], si.nextReaders(), true)
	}

}

'''
'''--- das/redis_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"crypto/hmac"
	"errors"
	"fmt"
	"time"

	"golang.org/x/crypto/sha3"

	"github.com/go-redis/redis/v8"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	"github.com/offchainlabs/nitro/util/redisutil"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
)

type RedisConfig struct {
	Enable                 bool          `koanf:"enable"`
	Url                    string        `koanf:"url"`
	Expiration             time.Duration `koanf:"expiration"`
	KeyConfig              string        `koanf:"key-config"`
	SyncFromStorageService bool          `koanf:"sync-from-storage-service"`
	SyncToStorageService   bool          `koanf:"sync-to-storage-service"`
}

var DefaultRedisConfig = RedisConfig{
	Url:        "",
	Expiration: time.Hour,
	KeyConfig:  "",
}

func RedisConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultRedisConfig.Enable, "enable Redis caching of sequencer batch data")
	f.String(prefix+".url", DefaultRedisConfig.Url, "Redis url")
	f.Duration(prefix+".expiration", DefaultRedisConfig.Expiration, "Redis expiration")
	f.String(prefix+".key-config", DefaultRedisConfig.KeyConfig, "Redis key config")
	f.Bool(prefix+".sync-from-storage-service", DefaultRedisConfig.SyncFromStorageService, "enable Redis to be used as a source for regular sync storage")
	f.Bool(prefix+".sync-to-storage-service", DefaultRedisConfig.SyncToStorageService, "enable Redis to be used as a sink for regular sync storage")
}

type RedisStorageService struct {
	baseStorageService StorageService
	redisConfig        RedisConfig
	signingKey         common.Hash
	client             redis.UniversalClient
}

func NewRedisStorageService(redisConfig RedisConfig, baseStorageService StorageService) (StorageService, error) {
	redisClient, err := redisutil.RedisClientFromURL(redisConfig.Url)
	if err != nil {
		return nil, err
	}
	signingKey := common.HexToHash(redisConfig.KeyConfig)
	if signingKey == (common.Hash{}) {
		return nil, errors.New("signing key file contents are not 32 bytes of hex")
	}
	return &RedisStorageService{
		baseStorageService: baseStorageService,
		redisConfig:        redisConfig,
		signingKey:         signingKey,
		client:             redisClient,
	}, nil
}

func (rs *RedisStorageService) verifyMessageSignature(data []byte) ([]byte, error) {
	if len(data) < 32 {
		return nil, errors.New("data is too short to contain message signature")
	}
	message := data[:len(data)-32]
	haveHmac := common.BytesToHash(data[len(data)-32:])
	mac := hmac.New(sha3.NewLegacyKeccak256, rs.signingKey[:])
	mac.Write(message)
	expectHmac := mac.Sum(nil)
	if !hmac.Equal(haveHmac[:], expectHmac) {
		return nil, errors.New("HMAC signature doesn't match expected value(s)")
	}
	return message, nil
}

func (rs *RedisStorageService) getVerifiedData(ctx context.Context, key common.Hash) ([]byte, error) {
	data, err := rs.client.Get(ctx, string(key.Bytes())).Bytes()
	if err != nil {
		log.Error("das.RedisStorageService.getVerifiedData", "err", err)
		return nil, err
	}
	data, err = rs.verifyMessageSignature(data)
	if err != nil {
		return nil, err
	}
	return data, err
}

func (rs *RedisStorageService) signMessage(message []byte) []byte {
	mac := hmac.New(sha3.NewLegacyKeccak256, rs.signingKey[:])
	mac.Write(message)
	return mac.Sum(message)
}

func (rs *RedisStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.RedisStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", rs)
	ret, err := rs.getVerifiedData(ctx, key)
	if err != nil {
		ret, err = rs.baseStorageService.GetByHash(ctx, key)
		if err != nil {
			return nil, err
		}

		err = rs.client.Set(ctx, string(key.Bytes()), rs.signMessage(ret), rs.redisConfig.Expiration).Err()
		if err != nil {
			return nil, err
		}
		return ret, err
	}

	return ret, err
}

func (rs *RedisStorageService) Put(ctx context.Context, value []byte, timeout uint64) error {
	logPut("das.RedisStorageService.Store", value, timeout, rs)
	err := rs.baseStorageService.Put(ctx, value, timeout)
	if err != nil {
		return err
	}
	err = rs.client.Set(
		ctx, string(dastree.Hash(value).Bytes()), rs.signMessage(value), rs.redisConfig.Expiration,
	).Err()
	if err != nil {
		log.Error("das.RedisStorageService.Store", "err", err)
	}
	return err
}

func (rs *RedisStorageService) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	// Expiration is set to zero here, since we want to keep the index inserted for iterable storage forever.
	err := rs.client.Set(
		ctx, string(key.Bytes()), rs.signMessage(value), 0,
	).Err()
	if err != nil {
		log.Error("das.RedisStorageService.putKeyValue", "err", err)
	}
	return err
}

func (rs *RedisStorageService) Sync(ctx context.Context) error {
	return rs.baseStorageService.Sync(ctx)
}

func (rs *RedisStorageService) Close(ctx context.Context) error {
	err := rs.client.Close()
	if err != nil {
		return err
	}
	return rs.baseStorageService.Close(ctx)
}

func (rs *RedisStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	return rs.baseStorageService.ExpirationPolicy(ctx)
}

func (rs *RedisStorageService) String() string {
	return fmt.Sprintf("RedisStorageService(%+v)", rs.redisConfig)
}

func (rs *RedisStorageService) HealthCheck(ctx context.Context) error {
	err := rs.client.Ping(ctx).Err()
	if err != nil {
		return err
	}
	return rs.baseStorageService.HealthCheck(ctx)
}

'''
'''--- das/redis_storage_service_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"testing"
	"time"

	"github.com/alicebob/miniredis/v2"
	"github.com/offchainlabs/nitro/das/dastree"
)

func TestRedisStorageService(t *testing.T) {
	ctx := context.Background()
	timeout := uint64(time.Now().Add(time.Hour).Unix())
	baseStorageService := NewMemoryBackedStorageService(ctx)
	server, err := miniredis.Run()
	Require(t, err)
	redisService, err := NewRedisStorageService(
		RedisConfig{
			Enable:     true,
			Url:        "redis://" + server.Addr(),
			Expiration: time.Hour,
			KeyConfig:  "b561f5d5d98debc783aa8a1472d67ec3bcd532a1c8d95e5cb23caa70c649f7c9",
		}, baseStorageService)

	Require(t, err)

	val1 := []byte("The first value")
	val1CorrectKey := dastree.Hash(val1)
	val1IncorrectKey := dastree.Hash(append(val1, 0))

	_, err = redisService.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}

	err = redisService.Put(ctx, val1, timeout)
	Require(t, err)

	_, err = redisService.GetByHash(ctx, val1IncorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err := redisService.GetByHash(ctx, val1CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}

	// For Case where the value is present in the base storage but not present in the cache.
	val2 := []byte("The Second value")
	val2CorrectKey := dastree.Hash(val2)
	val2IncorrectKey := dastree.Hash(append(val2, 0))

	err = baseStorageService.Put(ctx, val2, timeout)
	Require(t, err)

	_, err = redisService.GetByHash(ctx, val2IncorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err = redisService.GetByHash(ctx, val2CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val2) {
		t.Fatal(val, val2)
	}

	// For Case where the value is present in the cache storage but not present in the base.
	emptyBaseStorageService := NewMemoryBackedStorageService(ctx)
	redisServiceWithEmptyBaseStorage, err := NewRedisStorageService(
		RedisConfig{
			Enable:     true,
			Url:        "redis://" + server.Addr(),
			Expiration: time.Hour,
			KeyConfig:  "b561f5d5d98debc783aa8a1472d67ec3bcd532a1c8d95e5cb23caa70c649f7c9",
		}, emptyBaseStorageService)
	Require(t, err)
	val, err = redisServiceWithEmptyBaseStorage.GetByHash(ctx, val1CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}

	err = redisService.Close(ctx)
	Require(t, err)
	_, err = redisService.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrClosed) {
		t.Fatal(err)
	}
	// Closes the base storage properly.
	_, err = baseStorageService.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrClosed) {
		t.Fatal(err)
	}
}

'''
'''--- das/redundant_storage_service.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"sync"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/util/pretty"
)

// This is a redundant storage service, which replicates data across a set of StorageServices.
// The implementation assumes that there won't be a large number of replicas.

type RedundantStorageService struct {
	innerServices []StorageService
}

func NewRedundantStorageService(ctx context.Context, services []StorageService) (StorageService, error) {
	innerServices := make([]StorageService, len(services))
	copy(innerServices, services)
	return &RedundantStorageService{innerServices}, nil
}

type readResponse struct {
	data []byte
	err  error
}

func (r *RedundantStorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.RedundantStorageService.GetByHash", "key", pretty.PrettyHash(key), "this", r)
	subCtx, cancel := context.WithCancel(ctx)
	defer cancel()
	var anyError error
	responsesExpected := len(r.innerServices)
	resultChan := make(chan readResponse, responsesExpected)
	for _, serv := range r.innerServices {
		go func(s StorageService) {
			data, err := s.GetByHash(subCtx, key)
			resultChan <- readResponse{data, err}
		}(serv)
	}
	for responsesExpected > 0 {
		select {
		case resp := <-resultChan:
			if resp.err == nil {
				return resp.data, nil
			}
			anyError = resp.err
			responsesExpected--
		case <-ctx.Done():
			return nil, ctx.Err()
		}
	}
	return nil, anyError
}

func (r *RedundantStorageService) Put(ctx context.Context, data []byte, expirationTime uint64) error {
	logPut("das.RedundantStorageService.Store", data, expirationTime, r)
	var wg sync.WaitGroup
	var errorMutex sync.Mutex
	var anyError error
	wg.Add(len(r.innerServices))
	for _, serv := range r.innerServices {
		go func(s StorageService) {
			err := s.Put(ctx, data, expirationTime)
			if err != nil {
				errorMutex.Lock()
				anyError = err
				errorMutex.Unlock()
			}
			wg.Done()
		}(serv)
	}
	wg.Wait()
	return anyError
}

func (r *RedundantStorageService) Sync(ctx context.Context) error {
	var wg sync.WaitGroup
	var errorMutex sync.Mutex
	var anyError error
	wg.Add(len(r.innerServices))
	for _, serv := range r.innerServices {
		go func(s StorageService) {
			err := s.Sync(ctx)
			if err != nil {
				errorMutex.Lock()
				anyError = err
				errorMutex.Unlock()
			}
			wg.Done()
		}(serv)
	}
	wg.Wait()
	return anyError
}

func (r *RedundantStorageService) Close(ctx context.Context) error {
	var wg sync.WaitGroup
	var errorMutex sync.Mutex
	var anyError error
	wg.Add(len(r.innerServices))
	for _, serv := range r.innerServices {
		go func(s StorageService) {
			err := s.Close(ctx)
			if err != nil {
				errorMutex.Lock()
				anyError = err
				errorMutex.Unlock()
			}
			wg.Done()
		}(serv)
	}
	wg.Wait()
	return anyError
}

func (r *RedundantStorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	// If at least one inner service has KeepForever,
	// then whole redundant service can serve after timeout.

	// If no inner service has KeepForever,
	// but at least one inner service has DiscardAfterArchiveTimeout,
	// then whole redundant service can serve till archive timeout.

	// If no inner service has KeepForever, DiscardAfterArchiveTimeout,
	// but at least one inner service has DiscardAfterDataTimeout,
	// then whole redundant service can serve till data timeout.
	var res arbstate.ExpirationPolicy = -1
	for _, serv := range r.innerServices {
		expirationPolicy, err := serv.ExpirationPolicy(ctx)
		if err != nil {
			return -1, err
		}
		switch expirationPolicy {
		case arbstate.KeepForever:
			return arbstate.KeepForever, nil
		case arbstate.DiscardAfterArchiveTimeout:
			res = arbstate.DiscardAfterArchiveTimeout
		case arbstate.DiscardAfterDataTimeout:
			if res != arbstate.DiscardAfterArchiveTimeout {
				res = arbstate.DiscardAfterDataTimeout
			}
		}
	}
	if res == -1 {
		return -1, errors.New("unknown expiration policy")
	}
	return res, nil
}

func (r *RedundantStorageService) String() string {
	str := "RedundantStorageService("
	for _, serv := range r.innerServices {
		str = str + serv.String() + ","
	}
	return str + ")"
}

func (r *RedundantStorageService) HealthCheck(ctx context.Context) error {
	for _, storageService := range r.innerServices {
		err := storageService.HealthCheck(ctx)
		if err != nil {
			return err
		}
	}
	return nil
}

'''
'''--- das/redundant_storage_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/das/dastree"
)

const NumServices = 3

func TestRedundantStorageService(t *testing.T) {
	ctx := context.Background()
	timeout := uint64(time.Now().Add(time.Hour).Unix())
	services := []StorageService{}
	for i := 0; i < NumServices; i++ {
		services = append(services, NewMemoryBackedStorageService(ctx))
	}
	redundantService, err := NewRedundantStorageService(ctx, services)
	Require(t, err)

	val1 := []byte("The first value")
	key1 := dastree.Hash(val1)
	key2 := dastree.Hash(append(val1, 0))

	_, err = redundantService.GetByHash(ctx, key1)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}

	err = redundantService.Put(ctx, val1, timeout)
	Require(t, err)

	_, err = redundantService.GetByHash(ctx, key2)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err := redundantService.GetByHash(ctx, key1)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}

	for _, serv := range services {
		val, err = serv.GetByHash(ctx, key1)
		Require(t, err)
		if !bytes.Equal(val, val1) {
			t.Fatal(val, val1)
		}
	}

	err = redundantService.Close(ctx)
	Require(t, err)

	_, err = redundantService.GetByHash(ctx, key1)
	if !errors.Is(err, ErrClosed) {
		t.Fatal(err)
	}
}

'''
'''--- das/regular_sync_storage_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/das/dastree"
)

func TestRegularSyncStorage(t *testing.T) {
	ctx, cancelFunc := context.WithCancel(context.Background())
	defer cancelFunc()
	syncFromStorageService := []*IterableStorageService{
		NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(NewMemoryBackedStorageService(ctx))),
		NewIterableStorageService(ConvertStorageServiceToIterationCompatibleStorageService(NewMemoryBackedStorageService(ctx))),
	}
	syncToStorageService := []StorageService{
		NewMemoryBackedStorageService(ctx),
		NewMemoryBackedStorageService(ctx),
	}

	regularSyncStorage := NewRegularlySyncStorage(
		syncFromStorageService,
		syncToStorageService, RegularSyncStorageConfig{
			Enable:       true,
			SyncInterval: 100 * time.Millisecond,
		})

	val := [][]byte{
		[]byte("The first value"),
		[]byte("The second value"),
		[]byte("The third value"),
		[]byte("The forth value"),
	}
	valKey := []common.Hash{
		dastree.Hash(val[0]),
		dastree.Hash(val[1]),
		dastree.Hash(val[2]),
		dastree.Hash(val[3]),
	}

	reqCtx := context.Background()
	timeout := uint64(time.Now().Add(time.Hour).Unix())
	for i := 0; i < 2; i++ {
		for j := 0; j < 2; j++ {
			err := syncFromStorageService[i].Put(reqCtx, val[j], timeout)
			Require(t, err)
		}
	}

	regularSyncStorage.Start(ctx)
	time.Sleep(300 * time.Millisecond)

	for i := 0; i < 2; i++ {
		for j := 2; j < 4; j++ {
			err := syncFromStorageService[i].Put(reqCtx, val[j], timeout)
			Require(t, err)
		}
	}

	time.Sleep(300 * time.Millisecond)

	for i := 0; i < 2; i++ {
		for j := 0; j < 4; j++ {
			v, err := syncToStorageService[i].GetByHash(reqCtx, valKey[j])
			Require(t, err)
			if !bytes.Equal(v, val[j]) {
				t.Fatal(v, val[j])
			}
		}
	}
}

'''
'''--- das/regularly_sync_storage.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/util/stopwaiter"

	flag "github.com/spf13/pflag"
)

type RegularSyncStorageConfig struct {
	Enable       bool          `koanf:"enable"`
	SyncInterval time.Duration `koanf:"sync-interval"`
}

var DefaultRegularSyncStorageConfig = RegularSyncStorageConfig{
	Enable:       false,
	SyncInterval: 5 * time.Minute,
}

func RegularSyncStorageConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultRegularSyncStorageConfig.Enable, "enable regular storage syncing")
	f.Duration(prefix+".sync-interval", DefaultRegularSyncStorageConfig.SyncInterval, "interval for running regular storage sync")
}

// A RegularlySyncStorage is used to sync data from syncFromStorageServices to
// all the syncToStorageServices at regular intervals.
// (Only newly added data since the last sync is copied over.)
type RegularlySyncStorage struct {
	stopwaiter.StopWaiter
	syncFromStorageServices                    []*IterableStorageService
	syncToStorageServices                      []StorageService
	lastSyncedHashOfEachSyncFromStorageService map[*IterableStorageService]common.Hash
	syncInterval                               time.Duration
}

func NewRegularlySyncStorage(syncFromStorageServices []*IterableStorageService, syncToStorageServices []StorageService, conf RegularSyncStorageConfig) *RegularlySyncStorage {
	lastSyncedHashOfEachSyncFromStorageService := make(map[*IterableStorageService]common.Hash)
	for _, syncFrom := range syncFromStorageServices {
		lastSyncedHashOfEachSyncFromStorageService[syncFrom] = syncFrom.DefaultBegin()
	}
	return &RegularlySyncStorage{
		syncFromStorageServices:                    syncFromStorageServices,
		syncToStorageServices:                      syncToStorageServices,
		lastSyncedHashOfEachSyncFromStorageService: lastSyncedHashOfEachSyncFromStorageService,
		syncInterval:                               conf.SyncInterval,
	}
}

func (r *RegularlySyncStorage) Start(ctx context.Context) {
	// Start thread for regular sync
	r.StopWaiter.Start(ctx, r)
	r.CallIteratively(r.syncAllStorages)
}

func (r *RegularlySyncStorage) syncAllStorages(ctx context.Context) time.Duration {
	for syncFrom, lastSyncedHash := range r.lastSyncedHashOfEachSyncFromStorageService {
		end := syncFrom.End(ctx)
		if (end == common.Hash{}) {
			continue
		}

		syncHash := lastSyncedHash
		for syncHash != end {
			syncHash = syncFrom.Next(ctx, syncHash)
			data, err := syncFrom.GetByHash(ctx, syncHash)
			if err != nil {
				continue
			}
			expirationTime, err := syncFrom.GetExpirationTime(ctx, syncHash)
			if err != nil {
				continue
			}
			for _, syncTo := range r.syncToStorageServices {
				_, err = syncTo.GetByHash(ctx, syncHash)
				if err == nil {
					continue
				}

				if err = syncTo.Put(ctx, data, expirationTime); err != nil {
					log.Error("Error while running regular storage sync", "err", err)
				}
			}
		}
		r.lastSyncedHashOfEachSyncFromStorageService[syncFrom] = end
	}
	return r.syncInterval
}

'''
'''--- das/rest_server_list.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bufio"
	"context"
	"fmt"
	"net/http"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/log"
)

const initialMaxRecurseDepth uint16 = 8

// RestfulServerURLsFromList reads a list of Restful server URLs from a remote URL.
// The contents at the remote URL are parsed into a series of whitespace-separated words.
// Each word is interpreted as the URL of a Restful server, except that if a word is "LIST"
// (case-insensitive) then the following word is interpreted as the URL of another list,
// which is recursively fetched. The depth of recursion is limited to initialMaxRecurseDepth.
func RestfulServerURLsFromList(ctx context.Context, listUrl string) ([]string, error) {
	client := &http.Client{}
	urls, err := restfulServerURLsFromList(ctx, client, listUrl, initialMaxRecurseDepth, make(map[string]bool))
	if err != nil {
		return nil, err
	}

	// deduplicate the list of URL strings
	seen := make(map[string]bool)
	dedupedUrls := []string{}
	for _, url := range urls {
		if !seen[url] {
			seen[url] = true
			dedupedUrls = append(dedupedUrls, url)
		}
	}

	return dedupedUrls, nil
}

func restfulServerURLsFromList(
	ctx context.Context,
	client *http.Client,
	listUrl string,
	maxRecurseDepth uint16,
	visitedSoFar map[string]bool,
) ([]string, error) {
	if visitedSoFar[listUrl] {
		return []string{}, nil
	}
	visitedSoFar[listUrl] = true
	urls := []string{}
	request, err := http.NewRequestWithContext(ctx, http.MethodGet, listUrl, nil)
	if err != nil {
		return nil, err
	}
	resp, err := client.Do(request)
	if err != nil {
		return nil, err
	}
	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("recieved error response (%d) fetching online-url-list at %s", resp.StatusCode, listUrl)
	}
	scanner := bufio.NewScanner(resp.Body)
	scanner.Split(bufio.ScanWords)
	for scanner.Scan() {
		word := scanner.Text()
		if strings.ToLower(word) == "list" {
			if maxRecurseDepth > 0 && scanner.Scan() {
				word = scanner.Text()
				subUrls, err := restfulServerURLsFromList(ctx, client, word, maxRecurseDepth-1, visitedSoFar)
				if err != nil {
					return nil, err
				}
				urls = append(urls, subUrls...)

			}
		} else {
			urls = append(urls, word)
		}
	}
	return urls, nil
}

const maxListFetchTime = time.Minute

func StartRestfulServerListFetchDaemon(ctx context.Context, listUrl string, updatePeriod time.Duration) <-chan []string {
	updateChan := make(chan []string)
	if listUrl == "" {
		log.Info("Trying to start RestfulServerListFetchDaemon with empty online-url-list, not starting.")
		return updateChan
	}
	if updatePeriod == 0 {
		panic("RestfulServerListFetchDaemon started with zero updatePeriod")
	}

	downloadAndSend := func() error { // download and send once
		subCtx, subCtxCancel := context.WithTimeout(ctx, maxListFetchTime)
		defer subCtxCancel()

		urls, err := RestfulServerURLsFromList(subCtx, listUrl)
		if err != nil {
			return err
		}
		select {
		case updateChan <- urls:
			return nil
		case <-ctx.Done():
			return ctx.Err()
		}
	}

	go func() {
		defer close(updateChan)

		// send the first result immediately
		err := downloadAndSend()
		if err != nil {
			log.Warn("Couldn't download data availability online-url-list, will retry immediately", "err", err)
		}

		// now send periodically
		ticker := time.NewTicker(updatePeriod)
		defer ticker.Stop()
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
				err := downloadAndSend()
				if err != nil {
					log.Warn(fmt.Sprintf("Couldn't download data availability online-url-list, will retry in %s", updatePeriod), "err", err)
				}
			}
		}
	}()

	return updateChan
}

'''
'''--- das/restful_client.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
)

// RestfulDasClient implements DataAvailabilityReader
type RestfulDasClient struct {
	url string
}

func NewRestfulDasClient(protocol string, host string, port int) *RestfulDasClient {
	return &RestfulDasClient{
		url: fmt.Sprintf("%s://%s:%d", protocol, host, port),
	}
}

func NewRestfulDasClientFromURL(url string) (*RestfulDasClient, error) {
	if !(strings.HasPrefix(url, "http://") || strings.HasPrefix(url, "https://")) {
		return nil, fmt.Errorf("protocol prefix 'http://' or 'https://' must be specified for RestfulDasClient; got '%s'", url)

	}
	return &RestfulDasClient{
		url: url,
	}, nil
}

func (c *RestfulDasClient) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	res, err := http.Get(c.url + getByHashRequestPath + EncodeStorageServiceKey(hash))
	if err != nil {
		return nil, err
	}
	if res.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("HTTP error with status %d returned by server: %s", res.StatusCode, http.StatusText(res.StatusCode))
	}

	body, err := io.ReadAll(res.Body)
	if err != nil {
		return nil, err
	}

	var response RestfulDasServerResponse
	err = json.Unmarshal(body, &response)
	if err != nil {
		return nil, err
	}

	decoder := base64.NewDecoder(base64.StdEncoding, bytes.NewReader([]byte(response.Data)))
	decodedBytes, err := io.ReadAll(decoder)
	if err != nil {
		return nil, err
	}
	if !dastree.ValidHash(hash, decodedBytes) {
		return nil, arbstate.ErrHashMismatch
	}

	return decodedBytes, nil
}

func (c *RestfulDasClient) HealthCheck(ctx context.Context) error {
	res, err := http.Get(c.url + healthRequestPath)
	if err != nil {
		return err
	}
	if res.StatusCode != http.StatusOK {
		return fmt.Errorf("HTTP error with status %d returned by server: %s", res.StatusCode, http.StatusText(res.StatusCode))
	}
	return nil
}

func (c *RestfulDasClient) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	res, err := http.Get(c.url + expirationPolicyRequestPath)
	if err != nil {
		return -1, err
	}
	if res.StatusCode != http.StatusOK {
		return -1, err
	}
	body, err := io.ReadAll(res.Body)
	if err != nil {
		return -1, fmt.Errorf("HTTP error with status %d returned by server: %s", res.StatusCode, http.StatusText(res.StatusCode))
	}

	var response RestfulDasServerResponse
	err = json.Unmarshal(body, &response)
	if err != nil {
		return -1, err
	}

	return arbstate.StringToExpirationPolicy(response.ExpirationPolicy)
}

'''
'''--- das/restful_server.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"encoding/base64"
	"encoding/json"
	"errors"
	"fmt"
	"net"
	"net/http"
	"path"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/pretty"
)

var (
	restGetByHashRequestGauge       = metrics.NewRegisteredGauge("arb/das/rest/getbyhash/requests", nil)
	restGetByHashSuccessGauge       = metrics.NewRegisteredGauge("arb/das/rest/getbyhash/success", nil)
	restGetByHashFailureGauge       = metrics.NewRegisteredGauge("arb/das/rest/getbyhash/failure", nil)
	restGetByHashReturnedBytesGauge = metrics.NewRegisteredGauge("arb/das/rest/getbyhash/bytes", nil)
	restGetByHashDurationHistogram  = metrics.NewRegisteredHistogram("arb/das/rest/getbyhash/duration", nil, metrics.NewBoundedHistogramSample())
)

type RestfulDasServer struct {
	server               *http.Server
	daReader             arbstate.DataAvailabilityReader
	daHealthChecker      DataAvailabilityServiceHealthChecker
	httpServerExitedChan chan interface{}
	httpServerError      error
}

func NewRestfulDasServer(address string, port uint64, restServerTimeouts genericconf.HTTPServerTimeoutConfig, daReader arbstate.DataAvailabilityReader, daHealthChecker DataAvailabilityServiceHealthChecker) (*RestfulDasServer, error) {
	listener, err := net.Listen("tcp", fmt.Sprintf("%s:%d", address, port))
	if err != nil {
		return nil, err
	}
	return NewRestfulDasServerOnListener(listener, restServerTimeouts, daReader, daHealthChecker)
}

func NewRestfulDasServerOnListener(listener net.Listener, restServerTimeouts genericconf.HTTPServerTimeoutConfig, daReader arbstate.DataAvailabilityReader, daHealthChecker DataAvailabilityServiceHealthChecker) (*RestfulDasServer, error) {

	ret := &RestfulDasServer{
		daReader:             daReader,
		daHealthChecker:      daHealthChecker,
		httpServerExitedChan: make(chan interface{}),
	}

	ret.server = &http.Server{
		Handler:           ret,
		ReadTimeout:       restServerTimeouts.ReadTimeout,
		ReadHeaderTimeout: restServerTimeouts.ReadHeaderTimeout,
		WriteTimeout:      restServerTimeouts.WriteTimeout,
		IdleTimeout:       restServerTimeouts.IdleTimeout,
	}

	go func() {
		err := ret.server.Serve(listener)
		if err != nil && !errors.Is(err, http.ErrServerClosed) {
			ret.httpServerError = err
		}
		close(ret.httpServerExitedChan)
	}()

	return ret, nil
}

type RestfulDasServerResponse struct {
	Data             string `json:"data,omitempty"`
	ExpirationPolicy string `json:"expirationPolicy,omitempty"`
}

var cacheControlKey = http.CanonicalHeaderKey("cache-control")

const cacheControlValueDefault = "public, max-age=1"                                 // cache for up to 1 second (Used to reduce DOS possibility)
const cacheControlValueForSuccessfulGetByHash = "public, max-age=2419200, immutable" // cache for up to 28 days
const healthRequestPath = "/health"
const expirationPolicyRequestPath = "/expiration-policy/"
const getByHashRequestPath = "/get-by-hash/"

func (rds *RestfulDasServer) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	w.Header()[cacheControlKey] = []string{cacheControlValueDefault}
	requestPath := path.Clean(r.URL.Path)
	log.Debug("Got request", "requestPath", requestPath)
	switch {
	case strings.HasPrefix(requestPath, healthRequestPath):
		rds.HealthHandler(w, r, requestPath)
	case strings.HasPrefix(requestPath, expirationPolicyRequestPath):
		rds.ExpirationPolicyHandler(w, r, requestPath)
	case strings.HasPrefix(requestPath, getByHashRequestPath):
		rds.GetByHashHandler(w, r, requestPath)
	default:
		log.Warn("Unknown requestPath", "requestPath", requestPath)
		w.WriteHeader(http.StatusBadRequest)
		return
	}
}

// HealthHandler implements health requests for remote health-checks
func (rds *RestfulDasServer) HealthHandler(w http.ResponseWriter, r *http.Request, requestPath string) {
	err := rds.daHealthChecker.HealthCheck(r.Context())
	if err != nil {
		log.Warn("Unhealthy service", "path", requestPath, "err", err)
		w.WriteHeader(http.StatusServiceUnavailable)
		return
	}
	w.WriteHeader(http.StatusOK)
}

func (rds *RestfulDasServer) ExpirationPolicyHandler(w http.ResponseWriter, r *http.Request, requestPath string) {
	expirationPolicy, err := rds.daReader.ExpirationPolicy(r.Context())
	if err != nil {
		log.Warn("Error retrieving expiration policy", "path", requestPath, "err", err)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	expirationPolicyString, err := expirationPolicy.String()
	if err != nil {
		log.Warn("Got invalid expiration policy", "path", requestPath, "expirationPolicy", expirationPolicy)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	err = json.NewEncoder(w).Encode(RestfulDasServerResponse{ExpirationPolicy: expirationPolicyString})
	if err != nil {
		log.Warn("Failed encoding and writing response", "path", requestPath, "err", err)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	w.WriteHeader(http.StatusOK)
}

func (rds *RestfulDasServer) GetByHashHandler(w http.ResponseWriter, r *http.Request, requestPath string) {
	log.Debug("Got request", "requestPath", requestPath)
	restGetByHashRequestGauge.Inc(1)
	start := time.Now()
	success := false
	defer func() {
		if success {
			restGetByHashSuccessGauge.Inc(1)
		} else {
			restGetByHashFailureGauge.Inc(1)
		}
		restGetByHashDurationHistogram.Update(time.Since(start).Nanoseconds())
	}()

	hashBytes, err := DecodeStorageServiceKey(strings.TrimPrefix(requestPath, "/get-by-hash/"))
	if err != nil {
		log.Warn("Failed to decode hex-encoded hash", "path", requestPath, "err", err)
		w.WriteHeader(http.StatusBadRequest)
		return
	}
	if len(hashBytes) < 32 {
		log.Warn("Decoded hash was too short", "path", requestPath, "len(hashBytes)", len(hashBytes))
		w.WriteHeader(http.StatusBadRequest)
		return
	}

	responseData, err := rds.daReader.GetByHash(r.Context(), common.BytesToHash(hashBytes[:32]))
	if err != nil {
		log.Warn("Unable to find data", "path", requestPath, "err", err, "remoteAddr", r.RemoteAddr)
		w.WriteHeader(http.StatusNotFound)
		return
	}
	log.Trace("RestfulDasServer.ServeHTTP returning", "message", pretty.FirstFewBytes(responseData), "message length", len(responseData))

	encodedResponseData := make([]byte, base64.StdEncoding.EncodedLen(len(responseData)))
	base64.StdEncoding.Encode(encodedResponseData, responseData)
	var response RestfulDasServerResponse
	response.Data = string(encodedResponseData)
	restGetByHashReturnedBytesGauge.Inc(int64(len(response.Data)))

	err = json.NewEncoder(w).Encode(response)
	if err != nil {
		log.Warn("Failed encoding and writing response", "path", requestPath, "err", err)
		w.WriteHeader(http.StatusInternalServerError)
		return
	}
	w.Header()[cacheControlKey] = []string{cacheControlValueForSuccessfulGetByHash}
	success = true
}

func (rds *RestfulDasServer) GetServerExitedChan() <-chan interface{} { // channel will close when server terminates
	return rds.httpServerExitedChan
}

func (rds *RestfulDasServer) WaitForShutdown() error {
	<-rds.httpServerExitedChan
	return rds.httpServerError
}

func (rds *RestfulDasServer) Shutdown() error {
	err := rds.server.Close()
	if err != nil {
		return err
	}
	<-rds.httpServerExitedChan
	return rds.httpServerError
}

'''
'''--- das/restful_server_list_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"net"
	"net/http"
	"testing"
	"time"
)

func TestRestfulServerList(t *testing.T) {
	initTest(t)

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	urlsIn := []string{"https://supersecret.nowhere.com:9871", "http://www.google.com"}
	listContents := urlsIn[0] + " \t" + urlsIn[1]
	port, server := newListHttpServerForTest(t, &stringHandler{listContents})

	listUrl := fmt.Sprintf("http://localhost:%d", port)
	urls, err := RestfulServerURLsFromList(ctx, listUrl)
	Require(t, err)
	if !stringListIsPermutation(urlsIn, urls) {
		t.Fatal()
	}

	err = server.Shutdown(ctx)
	Require(t, err)
}

func TestRestfulServerListDaemon(t *testing.T) {
	initTest(t)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	urlsIn := []string{"https://supersecret.nowhere.com:9871", "http://www.google.com"}
	listContents := urlsIn[0] + " \t" + urlsIn[1]
	port, server := newListHttpServerForTest(t, &stringHandler{listContents})

	listUrl := fmt.Sprintf("http://localhost:%d", port)

	listChan := StartRestfulServerListFetchDaemon(ctx, listUrl, 200*time.Millisecond)
	for i := 0; i < 4; i++ {
		list := <-listChan
		if !stringListIsPermutation(list, urlsIn) {
			t.Fatal(i)
		}
	}

	err := server.Shutdown(ctx)
	Require(t, err)
}

func TestRestfulServerListDaemonWithErrors(t *testing.T) {
	initTest(t)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	urlsIn := []string{"https://supersecret.nowhere.com:9871", "http://www.google.com"}
	listContents := urlsIn[0] + " \t" + urlsIn[1]
	port, server := newListHttpServerForTest(
		t,
		Handlers(
			&connectionClosingHandler{},
			&connectionClosingHandler{},
			&stringHandler{listContents},
			&erroringHandler{},
			&erroringHandler{},
			&stringHandler{listContents},
			&erroringHandler{},
			&connectionClosingHandler{},
			&stringHandler{listContents},
		),
	)

	listUrl := fmt.Sprintf("http://localhost:%d", port)

	listChan := StartRestfulServerListFetchDaemon(ctx, listUrl, 200*time.Millisecond)
	for i := 0; i < 3; i++ {
		list := <-listChan
		if !stringListIsPermutation(list, urlsIn) {
			t.Fatal(i, "not a match")
		}
	}

	err := server.Shutdown(ctx)
	Require(t, err)
}

func stringListIsPermutation(lis1, lis2 []string) bool {
	if len(lis1) != len(lis2) {
		return false
	}
	lookup := make(map[string]bool)
	for _, s := range lis1 {
		lookup[s] = true
	}
	for _, s := range lis2 {
		if !lookup[s] {
			return false
		}
	}
	return true
}

func newListHttpServerForTest(t *testing.T, handler http.Handler) (int, *http.Server) {
	server := &http.Server{
		Handler:           handler,
		ReadHeaderTimeout: 5 * time.Second,
	}
	listener, err := net.Listen("tcp", "localhost:0")
	Require(t, err)
	go func() {
		_ = server.Serve(listener)
	}()
	tcpAddr, _ := listener.Addr().(*net.TCPAddr)
	return tcpAddr.Port, server
}

type stringHandler struct {
	contents string
}

func (h *stringHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	_, _ = w.Write([]byte(h.contents))
}

type erroringHandler struct {
}

func (h *erroringHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	w.WriteHeader(404)
}

type connectionClosingHandler struct {
}

func (h *connectionClosingHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	panic("close connection")
}

type multiHandler struct {
	current  int
	handlers []http.Handler
}

func Handlers(hs ...http.Handler) *multiHandler {
	return &multiHandler{0, hs}
}

func (h *multiHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	i := h.current % len(h.handlers)
	h.current++
	h.handlers[i].ServeHTTP(w, req)
}

'''
'''--- das/restful_server_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"net"
	"strings"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/das/dastree"
)

const LocalServerAddressForTest = "localhost"

func NewRestfulDasServerOnRandomPort(address string, storageService StorageService) (*RestfulDasServer, int, error) {
	listener, err := net.Listen("tcp", fmt.Sprintf("%s:0", address))
	if err != nil {
		return nil, 0, err
	}
	tcpAddr, ok := listener.Addr().(*net.TCPAddr)
	if !ok {
		return nil, 0, errors.New("attempt to listen on TCP returned non-TCP address")
	}
	rds, err := NewRestfulDasServerOnListener(listener, genericconf.HTTPServerTimeoutConfigDefault, storageService, storageService)
	if err != nil {
		return nil, 0, err
	}
	return rds, tcpAddr.Port, nil
}

func TestRestfulClientServer(t *testing.T) {
	initTest(t)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	storage := NewMemoryBackedStorageService(ctx)
	data := []byte("Testing a restful server now.")
	dataHash := dastree.Hash(data)

	server, port, err := NewRestfulDasServerOnRandomPort(LocalServerAddressForTest, storage)
	Require(t, err)

	err = storage.Put(ctx, data, uint64(time.Now().Add(time.Hour).Unix()))
	Require(t, err)

	time.Sleep(100 * time.Millisecond)

	client := NewRestfulDasClient("http", LocalServerAddressForTest, port)
	returnedData, err := client.GetByHash(ctx, dataHash)
	Require(t, err)
	if !bytes.Equal(data, returnedData) {
		Fail(t, fmt.Sprintf("Returned data '%s' does not match expected '%s'", returnedData, data))
	}

	_, err = client.GetByHash(ctx, dastree.Hash([]byte("absent data")))
	if err == nil || !strings.Contains(err.Error(), "404") {
		Fail(t, "Expected a 404 error")
	}

	err = server.Shutdown()
	Require(t, err)
}

'''
'''--- das/rpc_aggregator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"math/bits"
	"net/url"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/metricsutil"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbutil"
)

type BackendConfig struct {
	URL                 string `json:"url"`
	PubKeyBase64Encoded string `json:"pubkey"`
	SignerMask          uint64 `json:"signermask"`
}

func NewRPCAggregator(ctx context.Context, config DataAvailabilityConfig) (*Aggregator, error) {
	services, err := ParseServices(config.RPCAggregator)
	if err != nil {
		return nil, err
	}
	return NewAggregator(ctx, config, services)
}

func NewRPCAggregatorWithL1Info(config DataAvailabilityConfig, l1client arbutil.L1Interface, seqInboxAddress common.Address) (*Aggregator, error) {
	services, err := ParseServices(config.RPCAggregator)
	if err != nil {
		return nil, err
	}
	return NewAggregatorWithL1Info(config, services, l1client, seqInboxAddress)
}

func NewRPCAggregatorWithSeqInboxCaller(config DataAvailabilityConfig, seqInboxCaller *bridgegen.SequencerInboxCaller) (*Aggregator, error) {
	services, err := ParseServices(config.RPCAggregator)
	if err != nil {
		return nil, err
	}
	return NewAggregatorWithSeqInboxCaller(config, services, seqInboxCaller)
}

func ParseServices(config AggregatorConfig) ([]ServiceDetails, error) {
	var cs []BackendConfig
	err := json.Unmarshal([]byte(config.Backends), &cs)
	if err != nil {
		return nil, err
	}

	var services []ServiceDetails

	for _, b := range cs {
		url, err := url.Parse(b.URL)
		if err != nil {
			return nil, err
		}
		metricName := metricsutil.CanonicalizeMetricName(url.Hostname())

		service, err := NewDASRPCClient(b.URL)
		if err != nil {
			return nil, err
		}

		pubKey, err := DecodeBase64BLSPublicKey([]byte(b.PubKeyBase64Encoded))
		if err != nil {
			return nil, err
		}

		d, err := NewServiceDetails(service, *pubKey, b.SignerMask, metricName)
		if err != nil {
			return nil, err
		}

		services = append(services, *d)
	}

	return services, nil
}

func KeysetHashFromServices(services []ServiceDetails, assumedHonest uint64) ([32]byte, []byte, error) {
	var aggSignersMask uint64
	pubKeys := []blsSignatures.PublicKey{}
	for _, d := range services {
		if bits.OnesCount64(d.signersMask) != 1 {
			return [32]byte{}, nil, fmt.Errorf("tried to configure backend DAS %v with invalid signersMask %X", d.service, d.signersMask)
		}
		aggSignersMask |= d.signersMask
		pubKeys = append(pubKeys, d.pubKey)
	}
	if bits.OnesCount64(aggSignersMask) != len(services) {
		return [32]byte{}, nil, errors.New("at least two signers share a mask")
	}

	keyset := &arbstate.DataAvailabilityKeyset{
		AssumedHonest: uint64(assumedHonest),
		PubKeys:       pubKeys,
	}
	ksBuf := bytes.NewBuffer([]byte{})
	if err := keyset.Serialize(ksBuf); err != nil {
		return [32]byte{}, nil, err
	}
	keysetHash, err := keyset.Hash()
	if err != nil {
		return [32]byte{}, nil, err
	}

	return keysetHash, ksBuf.Bytes(), nil
}

'''
'''--- das/rpc_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"net"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func blsPubToBase64(pubkey *blsSignatures.PublicKey) string {
	pubkeyBytes := blsSignatures.PublicKeyToBytes(*pubkey)
	encodedPubkey := make([]byte, base64.StdEncoding.EncodedLen(len(pubkeyBytes)))
	base64.StdEncoding.Encode(encodedPubkey, pubkeyBytes)
	return string(encodedPubkey)
}

func TestRPC(t *testing.T) {
	ctx := context.Background()
	lis, err := net.Listen("tcp", "localhost:0")
	testhelpers.RequireImpl(t, err)
	keyDir := t.TempDir()
	dataDir := t.TempDir()
	pubkey, _, err := GenerateAndStoreKeys(keyDir)
	testhelpers.RequireImpl(t, err)

	config := DataAvailabilityConfig{
		Enable: true,
		Key: KeyConfig{
			KeyDir: keyDir,
		},
		LocalFileStorage: LocalFileStorageConfig{
			Enable:  true,
			DataDir: dataDir,
		},
		ParentChainNodeURL: "none",
		RequestTimeout:     5 * time.Second,
	}

	var syncFromStorageServices []*IterableStorageService
	var syncToStorageServices []StorageService
	storageService, lifecycleManager, err := CreatePersistentStorageService(ctx, &config, &syncFromStorageServices, &syncToStorageServices)
	testhelpers.RequireImpl(t, err)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	privKey, err := config.Key.BLSPrivKey()
	testhelpers.RequireImpl(t, err)
	localDas, err := NewSignAfterStoreDASWriterWithSeqInboxCaller(privKey, nil, storageService, "")
	testhelpers.RequireImpl(t, err)
	dasServer, err := StartDASRPCServerOnListener(ctx, lis, genericconf.HTTPServerTimeoutConfigDefault, storageService, localDas, storageService)
	defer func() {
		if err := dasServer.Shutdown(ctx); err != nil {
			panic(err)
		}
	}()
	testhelpers.RequireImpl(t, err)
	beConfig := BackendConfig{
		URL:                 "http://" + lis.Addr().String(),
		PubKeyBase64Encoded: blsPubToBase64(pubkey),
		SignerMask:          1,
	}

	backendsJsonByte, err := json.Marshal([]BackendConfig{beConfig})
	testhelpers.RequireImpl(t, err)
	aggConf := DataAvailabilityConfig{
		RPCAggregator: AggregatorConfig{
			AssumedHonest: 1,
			Backends:      string(backendsJsonByte),
		},
		RequestTimeout: 5 * time.Second,
	}
	rpcAgg, err := NewRPCAggregatorWithSeqInboxCaller(aggConf, nil)
	testhelpers.RequireImpl(t, err)

	msg := testhelpers.RandomizeSlice(make([]byte, 100))
	cert, err := rpcAgg.Store(ctx, msg, 0, nil)
	testhelpers.RequireImpl(t, err)

	retrievedMessage, err := storageService.GetByHash(ctx, cert.DataHash)
	testhelpers.RequireImpl(t, err)

	if !bytes.Equal(msg, retrievedMessage) {
		testhelpers.FailImpl(t, "failed to retrieve correct message")
	}

	retrievedMessage, err = storageService.GetByHash(ctx, cert.DataHash)
	testhelpers.RequireImpl(t, err)

	if !bytes.Equal(msg, retrievedMessage) {
		testhelpers.FailImpl(t, "failed to getByHash correct message")
	}
}

'''
'''--- das/s3_storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	awsConfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/feature/s3/manager"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"

	flag "github.com/spf13/pflag"
)

type S3Uploader interface {
	Upload(ctx context.Context, input *s3.PutObjectInput, opts ...func(*manager.Uploader)) (*manager.UploadOutput, error)
}

type S3Downloader interface {
	Download(ctx context.Context, w io.WriterAt, input *s3.GetObjectInput, options ...func(*manager.Downloader)) (n int64, err error)
}

type S3StorageServiceConfig struct {
	Enable                 bool   `koanf:"enable"`
	AccessKey              string `koanf:"access-key"`
	Bucket                 string `koanf:"bucket"`
	ObjectPrefix           string `koanf:"object-prefix"`
	Region                 string `koanf:"region"`
	SecretKey              string `koanf:"secret-key"`
	DiscardAfterTimeout    bool   `koanf:"discard-after-timeout"`
	SyncFromStorageService bool   `koanf:"sync-from-storage-service"`
	SyncToStorageService   bool   `koanf:"sync-to-storage-service"`
}

var DefaultS3StorageServiceConfig = S3StorageServiceConfig{}

func S3ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultS3StorageServiceConfig.Enable, "enable storage/retrieval of sequencer batch data from an AWS S3 bucket")
	f.String(prefix+".access-key", DefaultS3StorageServiceConfig.AccessKey, "S3 access key")
	f.String(prefix+".bucket", DefaultS3StorageServiceConfig.Bucket, "S3 bucket")
	f.String(prefix+".object-prefix", DefaultS3StorageServiceConfig.ObjectPrefix, "prefix to add to S3 objects")
	f.String(prefix+".region", DefaultS3StorageServiceConfig.Region, "S3 region")
	f.String(prefix+".secret-key", DefaultS3StorageServiceConfig.SecretKey, "S3 secret key")
	f.Bool(prefix+".discard-after-timeout", DefaultS3StorageServiceConfig.DiscardAfterTimeout, "discard data after its expiry timeout")
	f.Bool(prefix+".sync-from-storage-service", DefaultRedisConfig.SyncFromStorageService, "enable s3 to be used as a source for regular sync storage")
	f.Bool(prefix+".sync-to-storage-service", DefaultRedisConfig.SyncToStorageService, "enable s3 to be used as a sink for regular sync storage")
}

type S3StorageService struct {
	client              *s3.Client
	bucket              string
	objectPrefix        string
	uploader            S3Uploader
	downloader          S3Downloader
	discardAfterTimeout bool
}

func NewS3StorageService(config S3StorageServiceConfig) (StorageService, error) {
	client, err := buildS3Client(config.AccessKey, config.SecretKey, config.Region)
	if err != nil {
		return nil, err
	}
	return &S3StorageService{
		client:              client,
		bucket:              config.Bucket,
		objectPrefix:        config.ObjectPrefix,
		uploader:            manager.NewUploader(client),
		downloader:          manager.NewDownloader(client),
		discardAfterTimeout: config.DiscardAfterTimeout,
	}, nil
}

func buildS3Client(accessKey, secretKey, region string) (*s3.Client, error) {
	cfg, err := awsConfig.LoadDefaultConfig(context.TODO(), awsConfig.WithRegion(region), func(options *awsConfig.LoadOptions) error {
		// remain backward compatible with accessKey and secretKey credentials provided via cli flags
		if accessKey != "" && secretKey != "" {
			options.Credentials = credentials.NewStaticCredentialsProvider(accessKey, secretKey, "")
		}
		return nil
	})
	if err != nil {
		return nil, err
	}
	return s3.NewFromConfig(cfg), nil
}

func (s3s *S3StorageService) GetByHash(ctx context.Context, key common.Hash) ([]byte, error) {
	log.Trace("das.S3StorageService.GetByHash", "key", pretty.PrettyHash(key), "this", s3s)

	buf := manager.NewWriteAtBuffer([]byte{})
	_, err := s3s.downloader.Download(ctx, buf, &s3.GetObjectInput{
		Bucket: aws.String(s3s.bucket),
		Key:    aws.String(s3s.objectPrefix + EncodeStorageServiceKey(key)),
	})
	return buf.Bytes(), err
}

func (s3s *S3StorageService) Put(ctx context.Context, value []byte, timeout uint64) error {
	logPut("das.S3StorageService.Store", value, timeout, s3s)
	putObjectInput := s3.PutObjectInput{
		Bucket: aws.String(s3s.bucket),
		Key:    aws.String(s3s.objectPrefix + EncodeStorageServiceKey(dastree.Hash(value))),
		Body:   bytes.NewReader(value)}
	if !s3s.discardAfterTimeout {
		expires := time.Unix(int64(timeout), 0)
		putObjectInput.Expires = &expires
	}
	_, err := s3s.uploader.Upload(ctx, &putObjectInput)
	if err != nil {
		log.Error("das.S3StorageService.Store", "err", err)
	}
	return err
}

func (s3s *S3StorageService) putKeyValue(ctx context.Context, key common.Hash, value []byte) error {
	putObjectInput := s3.PutObjectInput{
		Bucket: aws.String(s3s.bucket),
		Key:    aws.String(s3s.objectPrefix + EncodeStorageServiceKey(key)),
		Body:   bytes.NewReader(value)}
	_, err := s3s.uploader.Upload(ctx, &putObjectInput)
	if err != nil {
		log.Error("das.S3StorageService.Store", "err", err)
	}
	return err
}

func (s3s *S3StorageService) Sync(ctx context.Context) error {
	return nil
}

func (s3s *S3StorageService) Close(ctx context.Context) error {
	return nil
}

func (s3s *S3StorageService) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	if s3s.discardAfterTimeout {
		return arbstate.DiscardAfterDataTimeout, nil
	}
	return arbstate.KeepForever, nil
}

func (s3s *S3StorageService) String() string {
	return fmt.Sprintf("S3StorageService(:%s)", s3s.bucket)
}

func (s3s *S3StorageService) HealthCheck(ctx context.Context) error {
	_, err := s3s.client.HeadBucket(ctx, &s3.HeadBucketInput{Bucket: aws.String(s3s.bucket)})
	return err
}

'''
'''--- das/s3_storage_service_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"errors"
	"io"
	"testing"
	"time"

	"github.com/aws/aws-sdk-go-v2/feature/s3/manager"
	"github.com/aws/aws-sdk-go-v2/service/s3"

	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/das/dastree"
)

type mockS3Uploader struct {
	mockStorageService StorageService
}

func (m *mockS3Uploader) Upload(ctx context.Context, input *s3.PutObjectInput, opts ...func(*manager.Uploader)) (*manager.UploadOutput, error) {
	buf := new(bytes.Buffer)
	_, err := buf.ReadFrom(input.Body)
	if err != nil {
		return nil, err
	}

	err = m.mockStorageService.Put(ctx, buf.Bytes(), 0)
	return nil, err
}

type mockS3Downloader struct {
	mockStorageService StorageService
}

func (m *mockS3Downloader) Download(ctx context.Context, w io.WriterAt, input *s3.GetObjectInput, options ...func(*manager.Downloader)) (n int64, err error) {
	key, err := DecodeStorageServiceKey(*input.Key)
	if err != nil {
		return 0, err
	}
	res, err := m.mockStorageService.GetByHash(ctx, key)
	if err != nil {
		return 0, err
	}

	ret, err := w.WriteAt(res, 0)
	if err != nil {
		return 0, err
	}
	return int64(ret), nil
}

func NewTestS3StorageService(ctx context.Context, s3Config genericconf.S3Config) (StorageService, error) {
	mockStorageService := NewMemoryBackedStorageService(ctx)
	return &S3StorageService{
		bucket:     s3Config.Bucket,
		uploader:   &mockS3Uploader{mockStorageService},
		downloader: &mockS3Downloader{mockStorageService}}, nil
}

func TestS3StorageService(t *testing.T) {
	ctx := context.Background()
	timeout := uint64(time.Now().Add(time.Hour).Unix())
	s3Service, err := NewTestS3StorageService(ctx, genericconf.DefaultS3Config)
	Require(t, err)

	val1 := []byte("The first value")
	val1CorrectKey := dastree.Hash(val1)
	val2IncorrectKey := dastree.Hash(append(val1, 0))

	_, err = s3Service.GetByHash(ctx, val1CorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}

	err = s3Service.Put(ctx, val1, timeout)
	Require(t, err)

	_, err = s3Service.GetByHash(ctx, val2IncorrectKey)
	if !errors.Is(err, ErrNotFound) {
		t.Fatal(err)
	}
	val, err := s3Service.GetByHash(ctx, val1CorrectKey)
	Require(t, err)
	if !bytes.Equal(val, val1) {
		t.Fatal(val, val1)
	}
}

'''
'''--- das/sign_after_store_das_writer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"encoding/hex"
	"errors"
	"fmt"
	"os"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/pretty"
)

type KeyConfig struct {
	KeyDir  string `koanf:"key-dir"`
	PrivKey string `koanf:"priv-key"`
}

func (c *KeyConfig) BLSPrivKey() (blsSignatures.PrivateKey, error) {
	var privKeyBytes []byte
	if len(c.PrivKey) != 0 {
		privKeyBytes = []byte(c.PrivKey)
	} else if len(c.KeyDir) != 0 {
		var err error
		privKeyBytes, err = os.ReadFile(c.KeyDir + "/" + DefaultPrivKeyFilename)
		if err != nil {
			if os.IsNotExist(err) {
				return nil, fmt.Errorf("required BLS keypair did not exist at %s", c.KeyDir)
			}
			return nil, err
		}
	} else {
		return nil, errors.New("must specify PrivKey or KeyDir")
	}
	privKey, err := DecodeBase64BLSPrivateKey(privKeyBytes)
	if err != nil {
		return nil, fmt.Errorf("'priv-key' was invalid: %w", err)
	}
	return privKey, nil
}

var DefaultKeyConfig = KeyConfig{}

func KeyConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".key-dir", DefaultKeyConfig.KeyDir, fmt.Sprintf("the directory to read the bls keypair ('%s' and '%s') from; if using any of the DAS storage types exactly one of key-dir or priv-key must be specified", DefaultPubKeyFilename, DefaultPrivKeyFilename))
	f.String(prefix+".priv-key", DefaultKeyConfig.PrivKey, "the base64 BLS private key to use for signing DAS certificates; if using any of the DAS storage types exactly one of key-dir or priv-key must be specified")
}

// SignAfterStoreDASWriter provides DAS signature functionality over a StorageService
// by adapting DataAvailabilityServiceWriter.Store(...) to StorageService.Put(...).
// There are two different signature functionalities it provides:
//
// 1) SignAfterStoreDASWriter.Store(...) assembles the returned hash into a
// DataAvailabilityCertificate and signs it with its BLS private key.
//
// 2) If Sequencer Inbox contract details are provided when a SignAfterStoreDASWriter is
// constructed, calls to Store(...) will try to verify the passed-in data's signature
// is from the batch poster. If the contract details are not provided, then the
// signature is not checked, which is useful for testing.
type SignAfterStoreDASWriter struct {
	privKey        blsSignatures.PrivateKey
	pubKey         *blsSignatures.PublicKey
	keysetHash     [32]byte
	keysetBytes    []byte
	storageService StorageService
	addrVerifier   *contracts.AddressVerifier

	// Extra batch poster verifier, for local installations to have their
	// own way of testing Stores.
	extraBpVerifier func(message []byte, timeout uint64, sig []byte) bool
}

func NewSignAfterStoreDASWriter(ctx context.Context, config DataAvailabilityConfig, storageService StorageService) (*SignAfterStoreDASWriter, error) {
	privKey, err := config.Key.BLSPrivKey()
	if err != nil {
		return nil, err
	}
	if config.ParentChainNodeURL == "none" {
		return NewSignAfterStoreDASWriterWithSeqInboxCaller(privKey, nil, storageService, config.ExtraSignatureCheckingPublicKey)
	}
	l1client, err := GetL1Client(ctx, config.ParentChainConnectionAttempts, config.ParentChainNodeURL)
	if err != nil {
		return nil, err
	}
	seqInboxAddress, err := OptionalAddressFromString(config.SequencerInboxAddress)
	if err != nil {
		return nil, err
	}
	if seqInboxAddress == nil {
		return NewSignAfterStoreDASWriterWithSeqInboxCaller(privKey, nil, storageService, config.ExtraSignatureCheckingPublicKey)
	}

	seqInboxCaller, err := bridgegen.NewSequencerInboxCaller(*seqInboxAddress, l1client)
	if err != nil {
		return nil, err
	}
	return NewSignAfterStoreDASWriterWithSeqInboxCaller(privKey, seqInboxCaller, storageService, config.ExtraSignatureCheckingPublicKey)
}

func NewSignAfterStoreDASWriterWithSeqInboxCaller(
	privKey blsSignatures.PrivateKey,
	seqInboxCaller *bridgegen.SequencerInboxCaller,
	storageService StorageService,
	extraSignatureCheckingPublicKey string,
) (*SignAfterStoreDASWriter, error) {
	publicKey, err := blsSignatures.PublicKeyFromPrivateKey(privKey)
	if err != nil {
		return nil, err
	}

	keyset := &arbstate.DataAvailabilityKeyset{
		AssumedHonest: 1,
		PubKeys:       []blsSignatures.PublicKey{publicKey},
	}
	ksBuf := bytes.NewBuffer([]byte{})
	if err := keyset.Serialize(ksBuf); err != nil {
		return nil, err
	}
	ksHash, err := keyset.Hash()
	if err != nil {
		return nil, err
	}

	var addrVerifier *contracts.AddressVerifier
	if seqInboxCaller != nil {
		addrVerifier = contracts.NewAddressVerifier(seqInboxCaller)
	}

	var extraBpVerifier func(message []byte, timeout uint64, sig []byte) bool
	if extraSignatureCheckingPublicKey != "" {
		var pubkey []byte
		if extraSignatureCheckingPublicKey[:2] == "0x" {
			pubkey, err = hex.DecodeString(extraSignatureCheckingPublicKey[2:])
			if err != nil {
				return nil, err
			}
		} else {
			pubkeyEncoded, err := os.ReadFile(extraSignatureCheckingPublicKey)
			if err != nil {
				return nil, err
			}
			pubkey, err = hex.DecodeString(string(pubkeyEncoded))
			if err != nil {
				return nil, err
			}
		}
		extraBpVerifier = func(message []byte, timeout uint64, sig []byte) bool {
			if len(sig) >= 64 {
				return crypto.VerifySignature(pubkey, dasStoreHash(message, timeout), sig[:64])
			}
			return false
		}
	}

	return &SignAfterStoreDASWriter{
		privKey:         privKey,
		pubKey:          &publicKey,
		keysetHash:      ksHash,
		keysetBytes:     ksBuf.Bytes(),
		storageService:  storageService,
		addrVerifier:    addrVerifier,
		extraBpVerifier: extraBpVerifier,
	}, nil
}

func (d *SignAfterStoreDASWriter) Store(
	ctx context.Context, message []byte, timeout uint64, sig []byte,
) (c *arbstate.DataAvailabilityCertificate, err error) {
	log.Trace("das.SignAfterStoreDASWriter.Store", "message", pretty.FirstFewBytes(message), "timeout", time.Unix(int64(timeout), 0), "sig", pretty.FirstFewBytes(sig), "this", d)
	var verified bool
	if d.extraBpVerifier != nil {
		verified = d.extraBpVerifier(message, timeout, sig)
	}

	if !verified && d.addrVerifier != nil {
		actualSigner, err := DasRecoverSigner(message, timeout, sig)
		if err != nil {
			return nil, err
		}
		isBatchPosterOrSequencer, err := d.addrVerifier.IsBatchPosterOrSequencer(ctx, actualSigner)
		if err != nil {
			return nil, err
		}
		if !isBatchPosterOrSequencer {
			return nil, errors.New("store request not properly signed")
		}
	}

	c = &arbstate.DataAvailabilityCertificate{
		Timeout:     timeout,
		DataHash:    dastree.Hash(message),
		Version:     1,
		SignersMask: 1, // The aggregator will override this if we're part of a committee.
	}

	fields := c.SerializeSignableFields()
	c.Sig, err = blsSignatures.SignMessage(d.privKey, fields)
	if err != nil {
		return nil, err
	}

	err = d.storageService.Put(ctx, message, timeout)
	if err != nil {
		return nil, err
	}
	err = d.storageService.Sync(ctx)
	if err != nil {
		return nil, err
	}

	c.KeysetHash = d.keysetHash

	return c, nil
}

func (d *SignAfterStoreDASWriter) String() string {
	return fmt.Sprintf("SignAfterStoreDASWriter{%v}", hexutil.Encode(blsSignatures.PublicKeyToBytes(*d.pubKey)))
}

'''
'''--- das/simple_das_reader_aggregator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"fmt"
	"math"
	"strings"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

// Most of the time we will use the SimpleDASReaderAggregator only to  aggregate
// RestfulDasClients, so the configuration and factory function are given more
// specific names.
type RestfulClientAggregatorConfig struct {
	Enable                       bool                               `koanf:"enable"`
	Urls                         []string                           `koanf:"urls"`
	OnlineUrlList                string                             `koanf:"online-url-list"`
	OnlineUrlListFetchInterval   time.Duration                      `koanf:"online-url-list-fetch-interval"`
	Strategy                     string                             `koanf:"strategy"`
	StrategyUpdateInterval       time.Duration                      `koanf:"strategy-update-interval"`
	WaitBeforeTryNext            time.Duration                      `koanf:"wait-before-try-next"`
	MaxPerEndpointStats          int                                `koanf:"max-per-endpoint-stats"`
	SimpleExploreExploitStrategy SimpleExploreExploitStrategyConfig `koanf:"simple-explore-exploit-strategy"`
	SyncToStorage                SyncToStorageConfig                `koanf:"sync-to-storage"`
}

var DefaultRestfulClientAggregatorConfig = RestfulClientAggregatorConfig{
	Urls:                         []string{},
	OnlineUrlList:                "",
	OnlineUrlListFetchInterval:   1 * time.Hour,
	Strategy:                     "simple-explore-exploit",
	StrategyUpdateInterval:       10 * time.Second,
	WaitBeforeTryNext:            2 * time.Second,
	MaxPerEndpointStats:          20,
	SimpleExploreExploitStrategy: DefaultSimpleExploreExploitStrategyConfig,
	SyncToStorage:                DefaultSyncToStorageConfig,
}

type SimpleExploreExploitStrategyConfig struct {
	ExploreIterations int `koanf:"explore-iterations"`
	ExploitIterations int `koanf:"exploit-iterations"`
}

var DefaultSimpleExploreExploitStrategyConfig = SimpleExploreExploitStrategyConfig{
	ExploreIterations: 20,
	ExploitIterations: 1000,
}

func RestfulClientAggregatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultRestfulClientAggregatorConfig.Enable, "enable retrieval of sequencer batch data from a list of remote REST endpoints; if other DAS storage types are enabled, this mode is used as a fallback")
	f.StringSlice(prefix+".urls", DefaultRestfulClientAggregatorConfig.Urls, "list of URLs including 'http://' or 'https://' prefixes and port numbers to REST DAS endpoints; additive with the online-url-list option")
	f.String(prefix+".online-url-list", DefaultRestfulClientAggregatorConfig.OnlineUrlList, "a URL to a list of URLs of REST das endpoints that is checked at startup; additive with the url option")
	f.Duration(prefix+".online-url-list-fetch-interval", DefaultRestfulClientAggregatorConfig.OnlineUrlListFetchInterval, "time interval to periodically fetch url list from online-url-list")
	f.String(prefix+".strategy", DefaultRestfulClientAggregatorConfig.Strategy, "strategy to use to determine order and parallelism of calling REST endpoint URLs; valid options are 'simple-explore-exploit'")
	f.Duration(prefix+".strategy-update-interval", DefaultRestfulClientAggregatorConfig.StrategyUpdateInterval, "how frequently to update the strategy with endpoint latency and error rate data")
	f.Duration(prefix+".wait-before-try-next", DefaultRestfulClientAggregatorConfig.WaitBeforeTryNext, "time to wait until trying the next set of REST endpoints while waiting for a response; the next set of REST endpoints is determined by the strategy selected")
	f.Int(prefix+".max-per-endpoint-stats", DefaultRestfulClientAggregatorConfig.MaxPerEndpointStats, "number of stats entries (latency and success rate) to keep for each REST endpoint; controls whether strategy is faster or slower to respond to changing conditions")
	SimpleExploreExploitStrategyConfigAddOptions(prefix+".simple-explore-exploit-strategy", f)
	SyncToStorageConfigAddOptions(prefix+".sync-to-storage", f)
}

func SimpleExploreExploitStrategyConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int(prefix+".explore-iterations", DefaultSimpleExploreExploitStrategyConfig.ExploreIterations, "number of consecutive GetByHash calls to the aggregator where each call will cause it to randomly select from REST endpoints until one returns successfully, before switching to exploit mode")
	f.Int(prefix+".exploit-iterations", DefaultSimpleExploreExploitStrategyConfig.ExploitIterations, "number of consecutive GetByHash calls to the aggregator where each call will cause it to select from REST endpoints in order of best latency and success rate, before switching to explore mode")
}

func NewRestfulClientAggregator(ctx context.Context, config *RestfulClientAggregatorConfig) (*SimpleDASReaderAggregator, error) {
	a := SimpleDASReaderAggregator{
		config: config,
		stats:  make(map[arbstate.DataAvailabilityReader]readerStats),
	}

	combinedUrls := make(map[string]bool)
	for _, url := range config.Urls {
		combinedUrls[url] = true
	}
	if config.OnlineUrlList != DefaultRestfulClientAggregatorConfig.OnlineUrlList {
		onlineUrls, err := RestfulServerURLsFromList(ctx, config.OnlineUrlList)
		if err != nil {
			return nil, err
		}
		for _, url := range onlineUrls {
			combinedUrls[url] = true
		}
	}
	if len(combinedUrls) == 0 {
		return nil, errors.New("no URLs were specified with either of rest-aggregator.urls or rest-aggregator.online-url-list")
	}

	urls := make([]string, 0, len(combinedUrls))
	for url := range combinedUrls {
		urls = append(urls, url)
	}

	log.Info("REST Aggregator URLs", "urls", urls)

	for _, url := range urls {
		reader, err := NewRestfulDasClientFromURL(url)
		if err != nil {
			return nil, err
		}
		a.readers = append(a.readers, reader)
		a.stats[reader] = make([]readerStat, 0, config.MaxPerEndpointStats)
	}
	a.statMessages = make(chan readerStatMessage, len(config.Urls)*2)

	switch strings.ToLower(config.Strategy) {
	case "simple-explore-exploit":
		a.strategy = &simpleExploreExploitStrategy{
			exploreIterations: uint32(config.SimpleExploreExploitStrategy.ExploreIterations),
			exploitIterations: uint32(config.SimpleExploreExploitStrategy.ExploitIterations),
		}
	case "testing-sequential":
		a.strategy = &testingSequentialStrategy{}
	default:
		return nil, fmt.Errorf("unknown RestfulClientAggregator strategy '%s', use --help to see available strategies", config.Strategy)
	}
	a.strategy.update(a.readers, a.stats)
	return &a, nil
}

type readerStats []readerStat

// Return the mean latency, weighted inversely by the ratio of successes : total attempts
func (s *readerStats) successRatioWeightedMeanLatency() time.Duration {
	successes, totalAttempts := 0.0, 0.0
	var totalLatency time.Duration
	for _, stat := range *s {
		if stat.success {
			successes++
			totalLatency += stat.latency
		}
		totalAttempts++
	}
	if successes == 0 {
		return time.Duration(math.MaxInt64)
	}
	avgLatency := float64(totalLatency) / successes
	successRatio := successes / totalAttempts
	return time.Duration(avgLatency / successRatio)
}

type readerStat struct {
	latency time.Duration
	success bool
}

type readerStatMessage struct {
	readerStat
	reader arbstate.DataAvailabilityReader
}

type SimpleDASReaderAggregator struct {
	stopwaiter.StopWaiter

	config *RestfulClientAggregatorConfig

	readersMutex sync.RWMutex
	// readers and stats are only to be updated by the stats goroutine
	readers []arbstate.DataAvailabilityReader
	stats   map[arbstate.DataAvailabilityReader]readerStats

	strategy aggregatorStrategy

	statMessages chan readerStatMessage
}

func (a *SimpleDASReaderAggregator) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	a.readersMutex.RLock()
	defer a.readersMutex.RUnlock()
	log.Trace("das.SimpleDASReaderAggregator.GetByHash", "key", pretty.PrettyHash(hash), "this", a)

	type dataErrorPair struct {
		data []byte
		err  error
	}

	results := make(chan dataErrorPair, len(a.readers))
	subCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	go func() {
		si := a.strategy.newInstance()
		for readers := si.nextReaders(); len(readers) != 0 && subCtx.Err() == nil; readers = si.nextReaders() {
			wg := sync.WaitGroup{}
			waitChan := make(chan interface{})
			for _, reader := range readers {
				wg.Add(1)
				go func(reader arbstate.DataAvailabilityReader) {
					defer wg.Done()
					data, err := a.tryGetByHash(subCtx, hash, reader)
					if err != nil && errors.Is(ctx.Err(), context.Canceled) {
						// Don't record a stats data point when a different
						// client returned faster than this one.
						return
					}
					results <- dataErrorPair{data, err}
				}(reader)
			}
			go func() {
				wg.Wait()
				close(waitChan)
			}()
			select {
			case <-subCtx.Done():
				return
			case <-time.After(a.config.WaitBeforeTryNext):
			case <-waitChan:
				// Yield to give the collector a chance to run in case a request succeeded
				time.Sleep(10 * time.Millisecond)
			}
		}
	}()

	var errorCollection []error
	for i := 0; i < len(a.readers); i++ {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case result := <-results:
			if result.err != nil {
				errorCollection = append(errorCollection, result.err)
			} else {
				return result.data, nil
			}
		}
	}

	return nil, fmt.Errorf("data wasn't able to be retrieved from any DAS Reader: %v", errorCollection)
}

func (a *SimpleDASReaderAggregator) tryGetByHash(
	ctx context.Context, hash common.Hash, reader arbstate.DataAvailabilityReader,
) ([]byte, error) {
	stat := readerStatMessage{reader: reader}
	stat.success = false

	start := time.Now()
	result, err := reader.GetByHash(ctx, hash)
	if err == nil {
		if dastree.ValidHash(hash, result) {
			stat.success = true
		} else {
			err = fmt.Errorf("SimpleDASReaderAggregator got result from reader(%v) not matching hash", reader)
		}
	}
	stat.latency = time.Since(start)

	select {
	case a.statMessages <- stat:
		// Non-blocking write to stat channel
	default:
		log.Warn("SimpleDASReaderAggregator stats processing goroutine is backed up, dropping", "dropped stats", stat)
	}

	return result, err
}

func (a *SimpleDASReaderAggregator) Start(ctx context.Context) {
	a.StopWaiter.Start(ctx, a)
	onlineUrlsChan := StartRestfulServerListFetchDaemon(a.StopWaiter.GetContext(), a.config.OnlineUrlList, a.config.OnlineUrlListFetchInterval)

	updateRestfulDasClients := func(urls []string) {
		a.readersMutex.Lock()
		defer a.readersMutex.Unlock()
		combinedUrls := a.config.Urls
		combinedUrls = append(combinedUrls, urls...)
		combinedReaders := make(map[arbstate.DataAvailabilityReader]bool)
		for _, url := range combinedUrls {
			reader, err := NewRestfulDasClientFromURL(url)
			if err != nil {
				return
			}
			combinedReaders[reader] = true
		}
		a.readers = make([]arbstate.DataAvailabilityReader, 0, len(combinedUrls))
		// Update reader and add newly added stats
		for reader := range combinedReaders {
			a.readers = append(a.readers, reader)
			if _, ok := a.stats[reader]; ok {
				continue
			}
			a.stats[reader] = make([]readerStat, 0, a.config.MaxPerEndpointStats)
		}
		// Delete stats for removed reader
		for reader := range a.stats {
			if combinedReaders[reader] {
				continue
			}
			delete(a.stats, reader)
		}
	}

	a.StopWaiter.LaunchThread(func(innerCtx context.Context) {
		updateStrategyTicker := time.NewTicker(a.config.StrategyUpdateInterval)
		defer updateStrategyTicker.Stop()
		for {
			select {
			case <-innerCtx.Done():
				return
			case stat := <-a.statMessages:
				a.stats[stat.reader] = append(a.stats[stat.reader], stat.readerStat)
				statsLen := len(a.stats[stat.reader])
				if statsLen > a.config.MaxPerEndpointStats {
					a.stats[stat.reader] = a.stats[stat.reader][statsLen-a.config.MaxPerEndpointStats:]
				}
			case <-updateStrategyTicker.C:
				// Strategy update happens in same goroutine as updates to the stats
				// to avoid needing extra synchronization.
				a.strategy.update(a.readers, a.stats)
			case onlineUrls := <-onlineUrlsChan:
				updateRestfulDasClients(onlineUrls)
			}
		}
	})
}

func (a *SimpleDASReaderAggregator) Close(ctx context.Context) error {
	a.StopWaiter.StopOnly()
	waitChan, err := a.StopWaiter.GetWaitChannel()
	if err != nil {
		return err
	}
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-waitChan:
		return nil
	}
}

func (a *SimpleDASReaderAggregator) String() string {
	return fmt.Sprintf("das.SimpleDASReaderAggregator{%v}", a.config.Urls)
}

func (a *SimpleDASReaderAggregator) HealthCheck(ctx context.Context) error {
	return nil
}

func (a *SimpleDASReaderAggregator) ExpirationPolicy(ctx context.Context) (arbstate.ExpirationPolicy, error) {
	a.readersMutex.RLock()
	defer a.readersMutex.RUnlock()
	if len(a.readers) == 0 {
		return -1, errors.New("no DataAvailabilityService present")
	}
	expectedExpirationPolicy, err := a.readers[0].ExpirationPolicy(ctx)
	if err != nil {
		return -1, err
	}
	// Even if a single service is different from the rest,
	// then whole aggregator will be considered for mixed expiration timeout policy.
	for _, serv := range a.readers {
		ep, err := serv.ExpirationPolicy(ctx)
		if err != nil {
			return -1, err
		}
		if ep != expectedExpirationPolicy {
			return arbstate.MixedTimeout, nil
		}
	}
	return expectedExpirationPolicy, nil
}

'''
'''--- das/simple_das_reader_aggregator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"bytes"
	"context"
	"fmt"
	"strconv"
	"strings"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/das/dastree"
)

func TestSimpleDASReaderAggregator(t *testing.T) { //nolint
	initTest(t)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	storage1, storage2, storage3 := NewMemoryBackedStorageService(ctx), NewMemoryBackedStorageService(ctx), NewMemoryBackedStorageService(ctx)

	data1 := []byte("Testing a restful server now.")
	dataHash1 := dastree.Hash(data1)

	server1, port1, err := NewRestfulDasServerOnRandomPort(LocalServerAddressForTest, storage1)
	Require(t, err)
	server2, port2, err := NewRestfulDasServerOnRandomPort(LocalServerAddressForTest, storage2)
	Require(t, err)
	server3, port3, err := NewRestfulDasServerOnRandomPort(LocalServerAddressForTest, storage3)
	Require(t, err)

	err = storage1.Put(ctx, data1, uint64(time.Now().Add(time.Hour).Unix()))
	Require(t, err)
	err = storage2.Put(ctx, data1, uint64(time.Now().Add(time.Hour).Unix()))
	Require(t, err)
	err = storage3.Put(ctx, data1, uint64(time.Now().Add(time.Hour).Unix()))
	Require(t, err)

	time.Sleep(100 * time.Millisecond)

	config := RestfulClientAggregatorConfig{
		Urls:                   []string{"http://localhost:" + strconv.Itoa(port1), "http://localhost:" + strconv.Itoa(port2), "http://localhost:" + strconv.Itoa(port3)},
		Strategy:               "testing-sequential",
		StrategyUpdateInterval: time.Second,
		WaitBeforeTryNext:      500 * time.Millisecond,
		MaxPerEndpointStats:    10,
	}

	agg, err := NewRestfulClientAggregator(ctx, &config)
	Require(t, err)

	returnedData, err := agg.GetByHash(ctx, dataHash1)
	Require(t, err)
	if !bytes.Equal(data1, returnedData) {
		Fail(t, fmt.Sprintf("Returned data '%s' does not match expected '%s'", returnedData, data1))
	}

	_, err = agg.GetByHash(ctx, dastree.Hash([]byte("absent data")))
	if err == nil || !strings.Contains(err.Error(), "404") {
		Fail(t, "Expected a 404 error")
	}

	data2 := []byte("Testing data that is only on the last REST endpoint.")
	dataHash2 := dastree.Hash(data2)

	err = storage3.Put(ctx, data2, uint64(time.Now().Add(time.Hour).Unix()))
	Require(t, err)

	returnedData, err = agg.GetByHash(ctx, dataHash2)
	Require(t, err)
	if !bytes.Equal(data2, returnedData) {
		Fail(t, fmt.Sprintf("Returned data '%s' does not match expected '%s'", returnedData, data2))
	}

	err = server1.Shutdown()
	Require(t, err)
	err = server2.Shutdown()
	Require(t, err)
	err = server3.Shutdown()
	Require(t, err)

}

'''
'''--- das/storage_service.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/offchainlabs/nitro/arbstate"
)

var ErrNotFound = errors.New("not found")

type StorageService interface {
	arbstate.DataAvailabilityReader
	Put(ctx context.Context, data []byte, expirationTime uint64) error
	Sync(ctx context.Context) error
	Closer
	fmt.Stringer
	HealthCheck(ctx context.Context) error
}

func EncodeStorageServiceKey(key common.Hash) string {
	return key.Hex()[2:]
}

func DecodeStorageServiceKey(input string) (common.Hash, error) {
	if !strings.HasPrefix(input, "0x") {
		input = "0x" + input
	}
	key, err := hexutil.Decode(input)
	if err != nil {
		return common.Hash{}, err
	}
	return common.BytesToHash(key), nil
}

'''
'''--- das/store_signing.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"encoding/binary"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/das/dastree"
	"github.com/offchainlabs/nitro/util/pretty"
	"github.com/offchainlabs/nitro/util/signature"
)

var uniquifyingPrefix = []byte("Arbitrum Nitro DAS API Store:")

func applyDasSigner(signer signature.DataSignerFunc, data []byte, timeout uint64) ([]byte, error) {
	return signer(dasStoreHash(data, timeout))
}

func DasRecoverSigner(data []byte, timeout uint64, sig []byte) (common.Address, error) {
	pk, err := crypto.SigToPub(dasStoreHash(data, timeout), sig)
	if err != nil {
		return common.Address{}, err
	}
	return crypto.PubkeyToAddress(*pk), nil
}

func dasStoreHash(data []byte, timeout uint64) []byte {
	var buf8 [8]byte
	binary.BigEndian.PutUint64(buf8[:], timeout)
	return dastree.HashBytes(uniquifyingPrefix, buf8[:], data)
}

type StoreSigningDAS struct {
	DataAvailabilityServiceWriter
	signer signature.DataSignerFunc
	addr   common.Address
}

func NewStoreSigningDAS(inner DataAvailabilityServiceWriter, signer signature.DataSignerFunc) (DataAvailabilityServiceWriter, error) {
	sig, err := applyDasSigner(signer, []byte{}, 0)
	if err != nil {
		return nil, err
	}
	addr, err := DasRecoverSigner([]byte{}, 0, sig)
	if err != nil {
		return nil, err
	}
	return &StoreSigningDAS{inner, signer, addr}, nil
}

func (s *StoreSigningDAS) Store(ctx context.Context, message []byte, timeout uint64, sig []byte) (*arbstate.DataAvailabilityCertificate, error) {
	log.Trace("das.StoreSigningDAS.Store(...)", "message", pretty.FirstFewBytes(message), "timeout", time.Unix(int64(timeout), 0), "sig", pretty.FirstFewBytes(sig), "this", s)
	mySig, err := applyDasSigner(s.signer, message, timeout)
	if err != nil {
		return nil, err
	}
	return s.DataAvailabilityServiceWriter.Store(ctx, message, timeout, mySig)
}

func (s *StoreSigningDAS) String() string {
	return "StoreSigningDAS (" + s.SignerAddress().Hex() + " ," + s.DataAvailabilityServiceWriter.String() + ")"
}

func (s *StoreSigningDAS) SignerAddress() common.Address {
	return s.addr
}

'''
'''--- das/store_signing_test.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/util/signature"
)

func TestStoreSigning(t *testing.T) {
	privateKey, err := crypto.GenerateKey()
	Require(t, err)

	addr := crypto.PubkeyToAddress(privateKey.PublicKey)

	weirdMessage := []byte("The quick brown fox jumped over the lazy dog.")
	timeout := uint64(time.Now().Unix())

	signer := signature.DataSignerFromPrivateKey(privateKey)
	sig, err := applyDasSigner(signer, weirdMessage, timeout)
	Require(t, err)

	recoveredAddr, err := DasRecoverSigner(weirdMessage, timeout, sig)
	Require(t, err)

	if recoveredAddr != addr {
		t.Fatal()
	}
}

'''
'''--- das/syncing_fallback_storage.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math"
	"math/big"
	"os"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

var sequencerInboxABI *abi.ABI
var BatchDeliveredID common.Hash
var addSequencerL2BatchFromOriginCallABI abi.Method
var sequencerBatchDataABI abi.Event

const sequencerBatchDataEvent = "SequencerBatchData"
const sequencerBatchDeliveredEvent = "SequencerBatchDelivered"

// TODO: can we use the generated ABI for BatchDataLocation enum?
type batchDataLocation uint8

const (
	batchDataTxInput batchDataLocation = iota
	batchDataSeparateEvent
)

func init() {
	var err error
	sequencerInboxABI, err = bridgegen.SequencerInboxMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	BatchDeliveredID = sequencerInboxABI.Events[sequencerBatchDeliveredEvent].ID
	sequencerBatchDataABI = sequencerInboxABI.Events[sequencerBatchDataEvent]
	addSequencerL2BatchFromOriginCallABI = sequencerInboxABI.Methods["addSequencerL2BatchFromOrigin"]
}

type SyncToStorageConfig struct {
	CheckAlreadyExists       bool          `koanf:"check-already-exists"`
	Eager                    bool          `koanf:"eager"`
	EagerLowerBoundBlock     uint64        `koanf:"eager-lower-bound-block"`
	RetentionPeriod          time.Duration `koanf:"retention-period"`
	DelayOnError             time.Duration `koanf:"delay-on-error"`
	IgnoreWriteErrors        bool          `koanf:"ignore-write-errors"`
	ParentChainBlocksPerRead uint64        `koanf:"parent-chain-blocks-per-read"`
	StateDir                 string        `koanf:"state-dir"`
}

var DefaultSyncToStorageConfig = SyncToStorageConfig{
	CheckAlreadyExists:       true,
	Eager:                    false,
	EagerLowerBoundBlock:     0,
	RetentionPeriod:          time.Duration(math.MaxInt64),
	DelayOnError:             time.Second,
	IgnoreWriteErrors:        true,
	ParentChainBlocksPerRead: 100,
	StateDir:                 "",
}

func SyncToStorageConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".check-already-exists", DefaultSyncToStorageConfig.CheckAlreadyExists, "check if the data already exists in this DAS's storage. Must be disabled for fast sync with an IPFS backend")
	f.Bool(prefix+".eager", DefaultSyncToStorageConfig.Eager, "eagerly sync batch data to this DAS's storage from the rest endpoints, using L1 as the index of batch data hashes; otherwise only sync lazily")
	f.Uint64(prefix+".eager-lower-bound-block", DefaultSyncToStorageConfig.EagerLowerBoundBlock, "when eagerly syncing, start indexing forward from this L1 block. Only used if there is no sync state")
	f.Uint64(prefix+".parent-chain-blocks-per-read", DefaultSyncToStorageConfig.ParentChainBlocksPerRead, "when eagerly syncing, max l1 blocks to read per poll")
	f.Duration(prefix+".retention-period", DefaultSyncToStorageConfig.RetentionPeriod, "period to retain synced data (defaults to forever)")
	f.Duration(prefix+".delay-on-error", DefaultSyncToStorageConfig.DelayOnError, "time to wait if encountered an error before retrying")
	f.Bool(prefix+".ignore-write-errors", DefaultSyncToStorageConfig.IgnoreWriteErrors, "log only on failures to write when syncing; otherwise treat it as an error")
	f.String(prefix+".state-dir", DefaultSyncToStorageConfig.StateDir, "directory to store the sync state in, ie the block number currently synced up to, so that we don't sync from scratch each time")
}

type l1SyncService struct {
	stopwaiter.StopWaiter

	config     SyncToStorageConfig
	syncTo     StorageService
	dataSource arbstate.DataAvailabilityReader

	l1Reader      *headerreader.HeaderReader
	inboxContract *bridgegen.SequencerInbox
	inboxAddr     common.Address

	catchingUp     bool
	lowBlockNr     uint64
	lastBatchCount *big.Int
	lastBatchAcc   common.Hash
}

const nextBlockNoFilename = "nextBlockNumber"

func readSyncStateOrDefault(syncDir string, dflt uint64) uint64 {
	if syncDir == "" {
		return dflt
	}

	path := syncDir + "/" + nextBlockNoFilename

	f, err := os.Open(path)
	if err != nil {
		log.Info("Couldn't open sync state file, using default sync start block number", "err", err, "path", path, "default", dflt)
		return dflt
	}
	var blockNr uint64
	n, err := fmt.Fscanln(f, &blockNr)
	if err != nil {
		log.Warn("Invalid data in sync state file, using default sync start block number", "err", err, "path", path, "default", dflt)
		return dflt
	}
	if n != 1 {
		log.Warn("Incorrect number of fields in sync state file, using default sync start block number", "n", n, "path", path, "default", dflt)
		return dflt
	}
	return blockNr
}

func writeSyncState(syncDir string, blockNr uint64) error {
	if syncDir == "" {
		return fmt.Errorf("No sync-to-storage.state-dir has been configured")
	}

	path := syncDir + "/" + nextBlockNoFilename

	// Use a temp file and rename to achieve atomic writes.
	f, err := os.CreateTemp(syncDir, nextBlockNoFilename)
	if err != nil {
		return err
	}
	err = f.Chmod(0600)
	if err != nil {
		return err
	}
	_, err = f.WriteString(fmt.Sprintf("%d\n", blockNr))
	if err != nil {
		return err
	}
	err = f.Close()
	if err != nil {
		return err
	}

	return os.Rename(f.Name(), path)
}

func newl1SyncService(config *SyncToStorageConfig, syncTo StorageService, dataSource arbstate.DataAvailabilityReader, l1Reader *headerreader.HeaderReader, inboxAddr common.Address) (*l1SyncService, error) {
	l1Client := l1Reader.Client()
	inboxContract, err := bridgegen.NewSequencerInbox(inboxAddr, l1Client)
	if err != nil {
		return nil, err
	}
	// make sure that as we sync, any Keysets missing from dataSource will fetched from the L1 chain
	dataSource, err = NewChainFetchReader(dataSource, l1Client, inboxAddr)
	if err != nil {
		return nil, err
	}
	return &l1SyncService{
		config:         *config,
		syncTo:         syncTo,
		dataSource:     dataSource,
		l1Reader:       l1Reader,
		inboxContract:  inboxContract,
		inboxAddr:      inboxAddr,
		catchingUp:     true,
		lowBlockNr:     readSyncStateOrDefault(config.StateDir, config.EagerLowerBoundBlock),
		lastBatchCount: big.NewInt(0),
	}, nil
}

func (s *l1SyncService) processBatchDelivered(ctx context.Context, batchDeliveredLog types.Log) error {
	deliveredEvent, err := s.inboxContract.ParseSequencerBatchDelivered(batchDeliveredLog)
	if err != nil {
		return err
	}
	log.Info("BatchDelivered", "log", batchDeliveredLog, "event", deliveredEvent)
	storeUntil := arbmath.SaturatingUAdd(deliveredEvent.TimeBounds.MaxTimestamp, uint64(s.config.RetentionPeriod.Seconds()))
	if storeUntil < uint64(time.Now().Unix()) {
		// old batch - no need to store
		return nil
	}
	data, err := FindDASDataFromLog(ctx, s.inboxContract, deliveredEvent, s.inboxAddr, s.l1Reader.Client(), batchDeliveredLog)
	if err != nil {
		return err
	}
	if data == nil {
		return nil
	}

	header := make([]byte, 40)
	binary.BigEndian.PutUint64(header[:8], deliveredEvent.TimeBounds.MinTimestamp)
	binary.BigEndian.PutUint64(header[8:16], deliveredEvent.TimeBounds.MaxTimestamp)
	binary.BigEndian.PutUint64(header[16:24], deliveredEvent.TimeBounds.MinBlockNumber)
	binary.BigEndian.PutUint64(header[24:32], deliveredEvent.TimeBounds.MaxBlockNumber)
	binary.BigEndian.PutUint64(header[32:40], deliveredEvent.AfterDelayedMessagesRead.Uint64())

	data = append(header, data...)
	preimages := make(map[arbutil.PreimageType]map[common.Hash][]byte)
	if _, err = arbstate.RecoverPayloadFromDasBatch(ctx, deliveredEvent.BatchSequenceNumber.Uint64(), data, s.dataSource, preimages, arbstate.KeysetValidate); err != nil {
		log.Error("recover payload failed", "txhash", batchDeliveredLog.TxHash, "data", data)
		return err
	}
	for _, preimages := range preimages {
		for hash, contents := range preimages {
			var err error
			if s.config.CheckAlreadyExists {
				_, err = s.syncTo.GetByHash(ctx, hash)
			}
			if err == nil || errors.Is(err, ErrNotFound) {
				if err := s.syncTo.Put(ctx, contents, storeUntil); err != nil {
					return err
				}
			} else {
				return err
			}
		}
	}
	seqNumber := deliveredEvent.BatchSequenceNumber
	if seqNumber == nil {
		seqNumber = common.Big0
	}
	updatedBatchCount := new(big.Int).Add(seqNumber, common.Big1)
	if s.lastBatchCount.Cmp(updatedBatchCount) <= 0 {
		s.lastBatchCount.Set(seqNumber)
		s.lastBatchAcc = deliveredEvent.AfterAcc
	}
	return nil
}

func FindDASDataFromLog(
	ctx context.Context,
	inboxContract *bridgegen.SequencerInbox,
	deliveredEvent *bridgegen.SequencerInboxSequencerBatchDelivered,
	inboxAddr common.Address,
	l1Client arbutil.L1Interface,
	batchDeliveredLog types.Log) ([]byte, error) {
	data := []byte{}
	if deliveredEvent.DataLocation == uint8(batchDataSeparateEvent) {
		query := ethereum.FilterQuery{
			BlockHash: &batchDeliveredLog.BlockHash,
			Addresses: []common.Address{inboxAddr},
			Topics:    [][]common.Hash{{sequencerBatchDataABI.ID}, {common.BigToHash(deliveredEvent.BatchSequenceNumber)}},
		}
		logs, err := l1Client.FilterLogs(ctx, query)
		if err != nil {
			return nil, err
		}
		if len(logs) != 1 {
			return nil, fmt.Errorf("found %d data logs for sequence 0x%x (expected 1)", len(logs), deliveredEvent.BatchSequenceNumber)
		}
		dataEvent, err := inboxContract.ParseSequencerBatchData(logs[0])
		if err != nil {
			return nil, err
		}
		data = dataEvent.Data
	} else if deliveredEvent.DataLocation == uint8(batchDataTxInput) {
		txData, err := arbutil.GetLogEmitterTxData(ctx, l1Client, batchDeliveredLog)
		if err != nil {
			return nil, err
		}
		args := make(map[string]interface{})
		err = addSequencerL2BatchFromOriginCallABI.Inputs.UnpackIntoMap(args, txData[4:])
		if err != nil {
			return nil, err
		}
		var ok bool
		data, ok = args["data"].([]byte)
		if !ok {
			return nil, fmt.Errorf("couldn't parse data for sequence 0x%x", deliveredEvent.BatchSequenceNumber)
		}
	}
	if len(data) < 1 {
		// no data - nothing to do
		log.Warn("BatchDelivered - no data found", "data", data)
		return nil, nil
	}
	if !arbstate.IsDASMessageHeaderByte(data[0]) {
		log.Warn("BatchDelivered - data not DAS")
		return nil, nil
	}
	return data, nil
}

func (s *l1SyncService) processBlockRange(ctx context.Context, lowerBound, higherBound uint64) error {
	query := ethereum.FilterQuery{
		FromBlock: new(big.Int).SetUint64(lowerBound),
		ToBlock:   new(big.Int).SetUint64(higherBound),
		Addresses: []common.Address{s.inboxAddr},
		Topics:    [][]common.Hash{{BatchDeliveredID}},
	}
	logs, err := s.l1Reader.Client().FilterLogs(ctx, query)
	if err != nil {
		return err
	}
	for _, deliveredLog := range logs {
		if err := s.processBatchDelivered(ctx, deliveredLog); err != nil {
			return err
		}
	}
	return nil
}

func (s *l1SyncService) readMore(ctx context.Context) error {
	header, err := s.l1Reader.LastHeader(ctx)
	if err != nil {
		return err
	}
	highBlockNr := header.Number.Uint64()
	finalizedHighBlockNr := highBlockNr - 12 // TODO
	callOpts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: header.Number,
	}
	if s.lastBatchCount != nil {
		currentBatchCount, err := s.inboxContract.BatchCount(callOpts)
		if err != nil {
			return err
		}
		if currentBatchCount.Cmp(s.lastBatchCount) == 0 {
			accBytes, err := s.inboxContract.InboxAccs(callOpts, new(big.Int).Sub(currentBatchCount, common.Big1))
			if err != nil {
				return err
			}
			var lastAccHash common.Hash
			copy(lastAccHash[:], accBytes[:])
			if lastAccHash == s.lastBatchAcc {
				// we're up to date
				s.lowBlockNr = finalizedHighBlockNr
				s.catchingUp = false
				return nil
			}
		}
	}
	if highBlockNr > s.lowBlockNr+s.config.ParentChainBlocksPerRead {
		s.catchingUp = true
		highBlockNr = s.lowBlockNr + s.config.ParentChainBlocksPerRead
		if finalizedHighBlockNr > highBlockNr {
			finalizedHighBlockNr = highBlockNr
		}
	} else {
		s.catchingUp = false
	}
	err = s.processBlockRange(ctx, s.lowBlockNr, highBlockNr)
	if err != nil {
		return err
	}
	s.lowBlockNr = finalizedHighBlockNr + 1
	err = writeSyncState(s.config.StateDir, s.lowBlockNr)
	if err != nil {
		log.Warn("sync-to-storage failed to write next block number to sync.", "err", err, "blockNr", s.lowBlockNr)
	}
	return nil
}

func (s *l1SyncService) mainThread(ctx context.Context) {
	headerChan, unsubscribe := s.l1Reader.Subscribe(false)
	defer unsubscribe()
	errCount := 0
	for {
		err := s.readMore(ctx)
		if err != nil {
			if ctx.Err() != nil {
				return
			}
			errCount++
			if errCount > 5 {
				log.Error("error trying to sync from L1", "err", err)
			}
			select {
			case <-ctx.Done():
				return
			case <-time.After(s.config.DelayOnError * time.Duration(errCount)):
			}
			continue
		}
		errCount = 0
		if s.catchingUp {
			// we're behind. Don't wait.
			continue
		}
		select {
		case <-headerChan:
		case <-ctx.Done():
			return
		}
	}
}

func (s *l1SyncService) Start(ctxIn context.Context) {
	s.StopWaiter.Start(ctxIn, s)

	s.LaunchThread(s.mainThread)
}

type SyncingFallbackStorageService struct {
	FallbackStorageService

	syncService *l1SyncService
}

func NewSyncingFallbackStorageService(ctx context.Context,
	primary StorageService,
	backup arbstate.DataAvailabilityReader,
	backupHealthChecker DataAvailabilityServiceHealthChecker,
	l1Reader *headerreader.HeaderReader,
	inboxAddr common.Address,
	syncConf *SyncToStorageConfig) (*SyncingFallbackStorageService, error) {
	syncService, err := newl1SyncService(syncConf, primary, backup, l1Reader, inboxAddr)
	if err != nil {
		return nil, err
	}
	syncService.Start(ctx)
	return &SyncingFallbackStorageService{
		FallbackStorageService{
			primary,
			backup,
			backupHealthChecker,
			uint64(syncConf.RetentionPeriod.Seconds()),
			syncConf.IgnoreWriteErrors,
			true,
			make(map[[32]byte]bool),
			sync.RWMutex{},
		},
		syncService,
	}, nil
}

func (s *SyncingFallbackStorageService) Close(ctx context.Context) error {
	s.syncService.StopOnly()
	s.FallbackStorageService.Close(ctx)
	return nil
}

'''
'''--- das/timeout_wrapper.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"context"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/common"
)

type ReaderTimeoutWrapper struct {
	t time.Duration
	DataAvailabilityServiceReader
}

type TimeoutWrapper struct {
	ReaderTimeoutWrapper
}

func NewReaderTimeoutWrapper(dataAvailabilityServiceReader DataAvailabilityServiceReader, t time.Duration) DataAvailabilityServiceReader {
	return &ReaderTimeoutWrapper{
		t:                             t,
		DataAvailabilityServiceReader: dataAvailabilityServiceReader,
	}
}

func (w *ReaderTimeoutWrapper) GetByHash(ctx context.Context, hash common.Hash) ([]byte, error) {
	deadlineCtx, cancel := context.WithDeadline(ctx, time.Now().Add(w.t))
	// For GetByHash we want fast cancellation of all goroutines started by
	// the aggregator as soon as one returns.
	defer cancel()
	return w.DataAvailabilityServiceReader.GetByHash(deadlineCtx, hash)
}

func (w *ReaderTimeoutWrapper) String() string {
	return fmt.Sprintf("ReaderTimeoutWrapper{%v}", w.DataAvailabilityServiceReader)
}

'''
'''--- das/util.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package das

import (
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/util/pretty"
)

func logPut(store string, data []byte, timeout uint64, reader arbstate.DataAvailabilityReader, more ...interface{}) {
	if len(more) == 0 {
		log.Trace(
			store, "message", pretty.FirstFewBytes(data), "timeout", time.Unix(int64(timeout), 0),
			"this", reader,
		)
	} else {
		log.Trace(
			store, "message", pretty.FirstFewBytes(data), "timeout", time.Unix(int64(timeout), 0),
			"this", reader, more,
		)
	}
}

'''
'''--- deploy/deploy.go ---
package deploy

import (
	"context"
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/solgen/go/ospgen"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/solgen/go/upgrade_executorgen"
	"github.com/offchainlabs/nitro/util/headerreader"
)

func andTxSucceeded(ctx context.Context, l1Reader *headerreader.HeaderReader, tx *types.Transaction, err error) error {
	if err != nil {
		return fmt.Errorf("error submitting tx: %w", err)
	}
	_, err = l1Reader.WaitForTxApproval(ctx, tx)
	if err != nil {
		return fmt.Errorf("error executing tx: %w", err)
	}
	return nil
}

func deployBridgeCreator(ctx context.Context, l1Reader *headerreader.HeaderReader, auth *bind.TransactOpts, maxDataSize *big.Int) (common.Address, error) {
	client := l1Reader.Client()

	/// deploy eth based templates
	bridgeTemplate, tx, _, err := bridgegen.DeployBridge(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("bridge deploy error: %w", err)
	}

	seqInboxTemplate, tx, _, err := bridgegen.DeploySequencerInbox(auth, client, maxDataSize)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("sequencer inbox deploy error: %w", err)
	}

	inboxTemplate, tx, _, err := bridgegen.DeployInbox(auth, client, maxDataSize)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("inbox deploy error: %w", err)
	}

	rollupEventBridgeTemplate, tx, _, err := rollupgen.DeployRollupEventInbox(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("rollup event bridge deploy error: %w", err)
	}

	outboxTemplate, tx, _, err := bridgegen.DeployOutbox(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("outbox deploy error: %w", err)
	}

	ethBasedTemplates := rollupgen.BridgeCreatorBridgeContracts{
		Bridge:           bridgeTemplate,
		SequencerInbox:   seqInboxTemplate,
		Inbox:            inboxTemplate,
		RollupEventInbox: rollupEventBridgeTemplate,
		Outbox:           outboxTemplate,
	}

	/// deploy ERC20 based templates
	erc20BridgeTemplate, tx, _, err := bridgegen.DeployERC20Bridge(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("bridge deploy error: %w", err)
	}

	erc20InboxTemplate, tx, _, err := bridgegen.DeployERC20Inbox(auth, client, maxDataSize)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("inbox deploy error: %w", err)
	}

	erc20RollupEventBridgeTemplate, tx, _, err := rollupgen.DeployERC20RollupEventInbox(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("rollup event bridge deploy error: %w", err)
	}

	erc20OutboxTemplate, tx, _, err := bridgegen.DeployERC20Outbox(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("outbox deploy error: %w", err)
	}

	erc20BasedTemplates := rollupgen.BridgeCreatorBridgeContracts{
		Bridge:           erc20BridgeTemplate,
		SequencerInbox:   seqInboxTemplate,
		Inbox:            erc20InboxTemplate,
		RollupEventInbox: erc20RollupEventBridgeTemplate,
		Outbox:           erc20OutboxTemplate,
	}

	bridgeCreatorAddr, tx, _, err := rollupgen.DeployBridgeCreator(auth, client, ethBasedTemplates, erc20BasedTemplates)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, fmt.Errorf("bridge creator deploy error: %w", err)
	}

	return bridgeCreatorAddr, nil
}

func deployChallengeFactory(ctx context.Context, l1Reader *headerreader.HeaderReader, auth *bind.TransactOpts) (common.Address, common.Address, error) {
	client := l1Reader.Client()
	osp0, tx, _, err := ospgen.DeployOneStepProver0(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("osp0 deploy error: %w", err)
	}

	ospMem, tx, _, err := ospgen.DeployOneStepProverMemory(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("ospMemory deploy error: %w", err)
	}

	ospMath, tx, _, err := ospgen.DeployOneStepProverMath(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("ospMath deploy error: %w", err)
	}

	ospHostIo, tx, _, err := ospgen.DeployOneStepProverHostIo(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("ospHostIo deploy error: %w", err)
	}

	challengeManagerAddr, tx, _, err := challengegen.DeployChallengeManager(auth, client)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("challenge manager deploy error: %w", err)
	}

	ospEntryAddr, tx, _, err := ospgen.DeployOneStepProofEntry(auth, client, osp0, ospMem, ospMath, ospHostIo)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return common.Address{}, common.Address{}, fmt.Errorf("ospEntry deploy error: %w", err)
	}

	return ospEntryAddr, challengeManagerAddr, nil
}

func deployRollupCreator(ctx context.Context, l1Reader *headerreader.HeaderReader, auth *bind.TransactOpts, maxDataSize *big.Int) (*rollupgen.RollupCreator, common.Address, common.Address, common.Address, error) {
	bridgeCreator, err := deployBridgeCreator(ctx, l1Reader, auth, maxDataSize)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("bridge creator deploy error: %w", err)
	}

	ospEntryAddr, challengeManagerAddr, err := deployChallengeFactory(ctx, l1Reader, auth)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, err
	}

	rollupAdminLogic, tx, _, err := rollupgen.DeployRollupAdminLogic(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("rollup admin logic deploy error: %w", err)
	}

	rollupUserLogic, tx, _, err := rollupgen.DeployRollupUserLogic(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("rollup user logic deploy error: %w", err)
	}

	rollupCreatorAddress, tx, rollupCreator, err := rollupgen.DeployRollupCreator(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("rollup creator deploy error: %w", err)
	}

	upgradeExecutor, tx, _, err := upgrade_executorgen.DeployUpgradeExecutor(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("upgrade executor deploy error: %w", err)
	}

	validatorUtils, tx, _, err := rollupgen.DeployValidatorUtils(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("validator utils deploy error: %w", err)
	}

	validatorWalletCreator, tx, _, err := rollupgen.DeployValidatorWalletCreator(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("validator wallet creator deploy error: %w", err)
	}

	l2FactoriesDeployHelper, tx, _, err := rollupgen.DeployDeployHelper(auth, l1Reader.Client())
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("deploy helper creator deploy error: %w", err)
	}

	tx, err = rollupCreator.SetTemplates(
		auth,
		bridgeCreator,
		ospEntryAddr,
		challengeManagerAddr,
		rollupAdminLogic,
		rollupUserLogic,
		upgradeExecutor,
		validatorUtils,
		validatorWalletCreator,
		l2FactoriesDeployHelper,
	)
	err = andTxSucceeded(ctx, l1Reader, tx, err)
	if err != nil {
		return nil, common.Address{}, common.Address{}, common.Address{}, fmt.Errorf("rollup set template error: %w", err)
	}

	return rollupCreator, rollupCreatorAddress, validatorUtils, validatorWalletCreator, nil
}

func DeployOnL1(ctx context.Context, parentChainReader *headerreader.HeaderReader, deployAuth *bind.TransactOpts, batchPoster common.Address, authorizeValidators uint64, config rollupgen.Config, nativeToken common.Address, maxDataSize *big.Int) (*chaininfo.RollupAddresses, error) {
	if config.WasmModuleRoot == (common.Hash{}) {
		return nil, errors.New("no machine specified")
	}

	rollupCreator, _, validatorUtils, validatorWalletCreator, err := deployRollupCreator(ctx, parentChainReader, deployAuth, maxDataSize)
	if err != nil {
		return nil, fmt.Errorf("error deploying rollup creator: %w", err)
	}

	var validatorAddrs []common.Address
	for i := uint64(1); i <= authorizeValidators; i++ {
		validatorAddrs = append(validatorAddrs, crypto.CreateAddress(validatorWalletCreator, i))
	}

	deployParams := rollupgen.RollupCreatorRollupDeploymentParams{
		Config:                    config,
		BatchPoster:               batchPoster,
		Validators:                validatorAddrs,
		MaxDataSize:               maxDataSize,
		NativeToken:               nativeToken,
		DeployFactoriesToL2:       false,
		MaxFeePerGasForRetryables: big.NewInt(0), // needed when utility factories are deployed
	}

	tx, err := rollupCreator.CreateRollup(
		deployAuth,
		deployParams,
	)
	if err != nil {
		return nil, fmt.Errorf("error submitting create rollup tx: %w", err)
	}
	receipt, err := parentChainReader.WaitForTxApproval(ctx, tx)
	if err != nil {
		return nil, fmt.Errorf("error executing create rollup tx: %w", err)
	}
	info, err := rollupCreator.ParseRollupCreated(*receipt.Logs[len(receipt.Logs)-1])
	if err != nil {
		return nil, fmt.Errorf("error parsing rollup created log: %w", err)
	}

	return &chaininfo.RollupAddresses{
		Bridge:                 info.Bridge,
		Inbox:                  info.InboxAddress,
		SequencerInbox:         info.SequencerInbox,
		DeployedAt:             receipt.BlockNumber.Uint64(),
		Rollup:                 info.RollupAddress,
		NativeToken:            nativeToken,
		UpgradeExecutor:        info.UpgradeExecutor,
		ValidatorUtils:         validatorUtils,
		ValidatorWalletCreator: validatorWalletCreator,
	}, nil
}

'''
'''--- docs/notice.md ---
**Note**: Docs now live at https://github.com/OffchainLabs/nitro-docs
'''
'''--- execution/gethexec/api.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethexec

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"math/big"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/util/arbmath"
)

type ArbAPI struct {
	txPublisher TransactionPublisher
}

func NewArbAPI(publisher TransactionPublisher) *ArbAPI {
	return &ArbAPI{publisher}
}

func (a *ArbAPI) CheckPublisherHealth(ctx context.Context) error {
	return a.txPublisher.CheckHealth(ctx)
}

type ArbDebugAPI struct {
	blockchain        *core.BlockChain
	blockRangeBound   uint64
	timeoutQueueBound uint64
}

func NewArbDebugAPI(blockchain *core.BlockChain, blockRangeBound uint64, timeoutQueueBound uint64) *ArbDebugAPI {
	return &ArbDebugAPI{blockchain, blockRangeBound, timeoutQueueBound}
}

type PricingModelHistory struct {
	Start            uint64     `json:"start"`
	End              uint64     `json:"end"`
	Step             uint64     `json:"step"`
	Timestamp        []uint64   `json:"timestamp"`
	BaseFee          []*big.Int `json:"baseFee"`
	GasBacklog       []uint64   `json:"gasBacklog"`
	GasUsed          []uint64   `json:"gasUsed"`
	MinBaseFee       *big.Int   `json:"minBaseFee"`
	SpeedLimit       uint64     `json:"speedLimit"`
	PerBlockGasLimit uint64     `json:"perBlockGasLimit"`
	PricingInertia   uint64     `json:"pricingInertia"`
	BacklogTolerance uint64     `json:"backlogTolerance"`

	L1BaseFeeEstimate      []*big.Int `json:"l1BaseFeeEstimate"`
	L1LastSurplus          []*big.Int `json:"l1LastSurplus"`
	L1FundsDue             []*big.Int `json:"l1FundsDue"`
	L1FundsDueForRewards   []*big.Int `json:"l1FundsDueForRewards"`
	L1UnitsSinceUpdate     []uint64   `json:"l1UnitsSinceUpdate"`
	L1LastUpdateTime       []uint64   `json:"l1LastUpdateTime"`
	L1EquilibrationUnits   *big.Int   `json:"l1EquilibrationUnits"`
	L1PerBatchCost         int64      `json:"l1PerBatchCost"`
	L1AmortizedCostCapBips uint64     `json:"l1AmortizedCostCapBips"`
	L1PricingInertia       uint64     `json:"l1PricingInertia"`
	L1PerUnitReward        uint64     `json:"l1PerUnitReward"`
	L1PayRewardTo          string     `json:"l1PayRewardTo"`
}

func (api *ArbDebugAPI) evenlySpaceBlocks(start, end rpc.BlockNumber) (uint64, uint64, uint64, uint64, error) {
	start, _ = api.blockchain.ClipToPostNitroGenesis(start)
	end, _ = api.blockchain.ClipToPostNitroGenesis(end)

	blocks := end.Int64() - start.Int64() + 1
	bound := int64(api.blockRangeBound)
	step := int64(1)
	if blocks > bound {
		step = int64(float64(blocks)/float64(bound) + 0.5)
		blocks = arbmath.MinInt(bound, blocks/step)
	}
	if blocks <= 0 {
		return 0, 0, 0, 0, fmt.Errorf("invalid block range: %v to %v", start.Int64(), end.Int64())
	}

	first := uint64(end.Int64() - step*(blocks-1)) // minus 1 to include the fact that we start from the last
	return first, uint64(step), uint64(end), uint64(blocks), nil
}

func (api *ArbDebugAPI) PricingModel(ctx context.Context, start, end rpc.BlockNumber) (PricingModelHistory, error) {

	first, step, last, blocks, err := api.evenlySpaceBlocks(start, end)
	if err != nil {
		return PricingModelHistory{}, err
	}

	history := PricingModelHistory{
		Start:                first,
		End:                  last,
		Step:                 step,
		Timestamp:            make([]uint64, blocks),
		BaseFee:              make([]*big.Int, blocks),
		GasBacklog:           make([]uint64, blocks),
		GasUsed:              make([]uint64, blocks),
		L1BaseFeeEstimate:    make([]*big.Int, blocks),
		L1LastSurplus:        make([]*big.Int, blocks),
		L1FundsDue:           make([]*big.Int, blocks),
		L1FundsDueForRewards: make([]*big.Int, blocks),
		L1UnitsSinceUpdate:   make([]uint64, blocks),
		L1LastUpdateTime:     make([]uint64, blocks),
	}

	for i := uint64(0); i < blocks; i++ {
		state, header, err := stateAndHeader(api.blockchain, first+i*step)
		if err != nil {
			return history, err
		}
		l1Pricing := state.L1PricingState()
		l2Pricing := state.L2PricingState()

		history.Timestamp[i] = header.Time
		history.BaseFee[i] = header.BaseFee

		gasBacklog, _ := l2Pricing.GasBacklog()
		l1BaseFeeEstimate, _ := l1Pricing.PricePerUnit()
		l1FundsDue, _ := l1Pricing.BatchPosterTable().TotalFundsDue()
		l1FundsDueForRewards, _ := l1Pricing.FundsDueForRewards()
		l1UnitsSinceUpdate, _ := l1Pricing.UnitsSinceUpdate()
		l1LastUpdateTime, _ := l1Pricing.LastUpdateTime()
		l1LastSurplus, _ := l1Pricing.LastSurplus()

		history.GasBacklog[i] = gasBacklog
		history.GasUsed[i] = header.GasUsed

		history.L1BaseFeeEstimate[i] = l1BaseFeeEstimate
		history.L1FundsDue[i] = l1FundsDue
		history.L1FundsDueForRewards[i] = l1FundsDueForRewards
		history.L1UnitsSinceUpdate[i] = l1UnitsSinceUpdate
		history.L1LastUpdateTime[i] = l1LastUpdateTime
		history.L1LastSurplus[i] = l1LastSurplus

		if i == uint64(blocks)-1 {
			speedLimit, _ := l2Pricing.SpeedLimitPerSecond()
			perBlockGasLimit, _ := l2Pricing.PerBlockGasLimit()
			minBaseFee, _ := l2Pricing.MinBaseFeeWei()
			pricingInertia, _ := l2Pricing.PricingInertia()
			backlogTolerance, _ := l2Pricing.BacklogTolerance()

			l1PricingInertia, _ := l1Pricing.Inertia()
			l1EquilibrationUnits, _ := l1Pricing.EquilibrationUnits()
			l1PerBatchCost, _ := l1Pricing.PerBatchGasCost()
			l1AmortizedCostCapBips, _ := l1Pricing.AmortizedCostCapBips()
			l1PerUnitReward, _ := l1Pricing.PerUnitReward()
			l1PayRewardsTo, err := l1Pricing.PayRewardsTo()

			if err != nil {
				return history, err
			}
			history.MinBaseFee = minBaseFee
			history.SpeedLimit = speedLimit
			history.PerBlockGasLimit = perBlockGasLimit
			history.PricingInertia = pricingInertia
			history.BacklogTolerance = backlogTolerance
			history.L1PricingInertia = l1PricingInertia
			history.L1EquilibrationUnits = l1EquilibrationUnits
			history.L1PerBatchCost = l1PerBatchCost
			history.L1AmortizedCostCapBips = l1AmortizedCostCapBips
			history.L1PerUnitReward = l1PerUnitReward
			history.L1PayRewardTo = l1PayRewardsTo.Hex()
		}
	}
	return history, nil
}

type TimeoutQueueHistory struct {
	Start uint64   `json:"start"`
	End   uint64   `json:"end"`
	Step  uint64   `json:"step"`
	Count []uint64 `json:"count"`
}

func (api *ArbDebugAPI) TimeoutQueueHistory(ctx context.Context, start, end rpc.BlockNumber) (TimeoutQueueHistory, error) {
	first, step, last, blocks, err := api.evenlySpaceBlocks(start, end)
	if err != nil {
		return TimeoutQueueHistory{}, err
	}

	history := TimeoutQueueHistory{
		Start: first,
		End:   last,
		Step:  step,
		Count: make([]uint64, blocks),
	}

	for i := uint64(0); i < blocks; i++ {
		state, _, err := stateAndHeader(api.blockchain, first+i*step)
		if err != nil {
			return history, err
		}
		size, err := state.RetryableState().TimeoutQueue.Size()
		if err != nil {
			return history, err
		}
		history.Count[i] = size
	}
	return history, nil
}

type TimeoutQueue struct {
	BlockNumber uint64        `json:"blockNumber"`
	Tickets     []common.Hash `json:"tickets"`
	Timeouts    []uint64      `json:"timeouts"`
}

func (api *ArbDebugAPI) TimeoutQueue(ctx context.Context, blockNum rpc.BlockNumber) (TimeoutQueue, error) {

	blockNum, _ = api.blockchain.ClipToPostNitroGenesis(blockNum)

	queue := TimeoutQueue{
		BlockNumber: uint64(blockNum),
		Tickets:     []common.Hash{},
		Timeouts:    []uint64{},
	}

	state, _, err := stateAndHeader(api.blockchain, uint64(blockNum))
	if err != nil {
		return queue, err
	}

	closure := func(index uint64, ticket common.Hash) (bool, error) {

		// we don't care if the retryable has expired
		retryable, err := state.RetryableState().OpenRetryable(ticket, 0)
		if err != nil {
			return false, err
		}
		if retryable == nil {
			queue.Tickets = append(queue.Tickets, ticket)
			queue.Timeouts = append(queue.Timeouts, 0)
			return false, nil
		}
		timeout, err := retryable.CalculateTimeout()
		if err != nil {
			return false, err
		}
		windows, err := retryable.TimeoutWindowsLeft()
		if err != nil {
			return false, err
		}
		timeout -= windows * retryables.RetryableLifetimeSeconds

		queue.Tickets = append(queue.Tickets, ticket)
		queue.Timeouts = append(queue.Timeouts, timeout)
		return index == api.timeoutQueueBound, nil
	}

	err = state.RetryableState().TimeoutQueue.ForEach(closure)
	return queue, err
}

func stateAndHeader(blockchain *core.BlockChain, block uint64) (*arbosState.ArbosState, *types.Header, error) {
	header := blockchain.GetHeaderByNumber(block)
	if !blockchain.Config().IsArbitrumNitro(header.Number) {
		return nil, nil, types.ErrUseFallback
	}
	statedb, err := blockchain.StateAt(header.Root)
	if err != nil {
		return nil, nil, err
	}
	state, err := arbosState.OpenSystemArbosState(statedb, nil, true)
	return state, header, err
}

type ArbTraceForwarderAPI struct {
	fallbackClientUrl     string
	fallbackClientTimeout time.Duration

	initialized    atomic.Bool
	mutex          sync.Mutex
	fallbackClient types.FallbackClient
}

func NewArbTraceForwarderAPI(fallbackClientUrl string, fallbackClientTimeout time.Duration) *ArbTraceForwarderAPI {
	return &ArbTraceForwarderAPI{
		fallbackClientUrl:     fallbackClientUrl,
		fallbackClientTimeout: fallbackClientTimeout,
	}
}

func (api *ArbTraceForwarderAPI) getFallbackClient() (types.FallbackClient, error) {
	if api.initialized.Load() {
		return api.fallbackClient, nil
	}
	api.mutex.Lock()
	defer api.mutex.Unlock()
	if api.initialized.Load() {
		return api.fallbackClient, nil
	}
	fallbackClient, err := arbitrum.CreateFallbackClient(api.fallbackClientUrl, api.fallbackClientTimeout)
	if err != nil {
		return nil, err
	}
	api.fallbackClient = fallbackClient
	api.initialized.Store(true)
	return api.fallbackClient, nil
}

func (api *ArbTraceForwarderAPI) forward(ctx context.Context, method string, args ...interface{}) (*json.RawMessage, error) {
	fallbackClient, err := api.getFallbackClient()
	if err != nil {
		return nil, err
	}
	if fallbackClient == nil {
		return nil, errors.New("arbtrace calls forwarding not configured") // TODO(magic)
	}
	var resp *json.RawMessage
	err = fallbackClient.CallContext(ctx, &resp, method, args...)
	if err != nil {
		return nil, err
	}
	return resp, nil
}

func (api *ArbTraceForwarderAPI) Call(ctx context.Context, callArgs json.RawMessage, traceTypes json.RawMessage, blockNum json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_call", callArgs, traceTypes, blockNum)
}

func (api *ArbTraceForwarderAPI) CallMany(ctx context.Context, calls json.RawMessage, blockNum json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_callMany", calls, blockNum)
}

func (api *ArbTraceForwarderAPI) ReplayBlockTransactions(ctx context.Context, blockNum json.RawMessage, traceTypes json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_replayBlockTransactions", blockNum, traceTypes)
}

func (api *ArbTraceForwarderAPI) ReplayTransaction(ctx context.Context, txHash json.RawMessage, traceTypes json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_replayTransaction", txHash, traceTypes)
}

func (api *ArbTraceForwarderAPI) Transaction(ctx context.Context, txHash json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_transaction", txHash)
}

func (api *ArbTraceForwarderAPI) Get(ctx context.Context, txHash json.RawMessage, path json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_get", txHash, path)
}

func (api *ArbTraceForwarderAPI) Block(ctx context.Context, blockNum json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_block", blockNum)
}

func (api *ArbTraceForwarderAPI) Filter(ctx context.Context, filter json.RawMessage) (*json.RawMessage, error) {
	return api.forward(ctx, "arbtrace_filter", filter)
}

'''
'''--- execution/gethexec/arb_interface.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethexec

import (
	"context"

	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
)

type TransactionPublisher interface {
	PublishTransaction(ctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error
	CheckHealth(ctx context.Context) error
	Initialize(context.Context) error
	Start(context.Context) error
	StopAndWait()
	Started() bool
}

type ArbInterface struct {
	exec        *ExecutionEngine
	txPublisher TransactionPublisher
	arbNode     interface{}
}

func NewArbInterface(exec *ExecutionEngine, txPublisher TransactionPublisher) (*ArbInterface, error) {
	return &ArbInterface{
		exec:        exec,
		txPublisher: txPublisher,
	}, nil
}

func (a *ArbInterface) Initialize(arbnode interface{}) {
	a.arbNode = arbnode
}

func (a *ArbInterface) PublishTransaction(ctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	return a.txPublisher.PublishTransaction(ctx, tx, options)
}

func (a *ArbInterface) BlockChain() *core.BlockChain {
	return a.exec.bc
}

func (a *ArbInterface) ArbNode() interface{} {
	return a.arbNode
}

'''
'''--- execution/gethexec/block_recorder.go ---
package gethexec

import (
	"context"
	"fmt"
	"sync"
	"testing"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/validator"
)

// BlockRecorder uses a separate statedatabase from the blockchain.
// It has access to any state in the ethdb (hard-disk) database, and can compute state as needed.
// We keep references for state of:
// Any block that matches PrepareForRecord that was done recently (according to PrepareDelay config)
// Most recent/advanced header we ever computed (lastHdr)
// Hopefully - some recent valid block. For that we always keep one candidate block until it becomes validated.
type BlockRecorder struct {
	recordingDatabase *arbitrum.RecordingDatabase
	execEngine        *ExecutionEngine

	lastHdr     *types.Header
	lastHdrLock sync.Mutex

	validHdrCandidate *types.Header
	validHdr          *types.Header
	validHdrLock      sync.Mutex

	preparedQueue []*types.Header
	preparedLock  sync.Mutex
}

func NewBlockRecorder(config *arbitrum.RecordingDatabaseConfig, execEngine *ExecutionEngine, ethDb ethdb.Database) *BlockRecorder {
	recorder := &BlockRecorder{
		execEngine:        execEngine,
		recordingDatabase: arbitrum.NewRecordingDatabase(config, ethDb, execEngine.bc),
	}
	execEngine.SetRecorder(recorder)
	return recorder
}

func stateLogFunc(targetHeader, header *types.Header, hasState bool) {
	if targetHeader == nil || header == nil {
		return
	}
	gap := targetHeader.Number.Int64() - header.Number.Int64()
	step := int64(500)
	stage := "computing state"
	if !hasState {
		step = 3000
		stage = "looking for full block"
	}
	if (gap >= step) && (gap%step == 0) {
		log.Info("Setting up validation", "stage", stage, "current", header.Number, "target", targetHeader.Number)
	}
}

// If msg is nil, this will record block creation up to the point where message would be accessed (for a "too far" proof)
// If keepreference == true, reference to state of prevHeader is added (no reference added if an error is returned)
func (r *BlockRecorder) RecordBlockCreation(
	ctx context.Context,
	pos arbutil.MessageIndex,
	msg *arbostypes.MessageWithMetadata,
) (*execution.RecordResult, error) {

	blockNum := r.execEngine.MessageIndexToBlockNumber(pos)

	var prevHeader *types.Header
	if pos != 0 {
		prevHeader = r.execEngine.bc.GetHeaderByNumber(uint64(blockNum - 1))
		if prevHeader == nil {
			return nil, fmt.Errorf("pos %d prevHeader not found", pos)
		}
	}

	recordingdb, chaincontext, recordingKV, err := r.recordingDatabase.PrepareRecording(ctx, prevHeader, stateLogFunc)
	if err != nil {
		return nil, err
	}
	defer func() { r.recordingDatabase.Dereference(prevHeader) }()

	chainConfig := r.execEngine.bc.Config()

	// Get the chain ID, both to validate and because the replay binary also gets the chain ID,
	// so we need to populate the recordingdb with preimages for retrieving the chain ID.
	if prevHeader != nil {
		initialArbosState, err := arbosState.OpenSystemArbosState(recordingdb, nil, true)
		if err != nil {
			return nil, fmt.Errorf("error opening initial ArbOS state: %w", err)
		}
		chainId, err := initialArbosState.ChainId()
		if err != nil {
			return nil, fmt.Errorf("error getting chain ID from initial ArbOS state: %w", err)
		}
		if chainId.Cmp(chainConfig.ChainID) != 0 {
			return nil, fmt.Errorf("unexpected chain ID %r in ArbOS state, expected %r", chainId, chainConfig.ChainID)
		}
		genesisNum, err := initialArbosState.GenesisBlockNum()
		if err != nil {
			return nil, fmt.Errorf("error getting genesis block number from initial ArbOS state: %w", err)
		}
		_, err = initialArbosState.ChainConfig()
		if err != nil {
			return nil, fmt.Errorf("error getting chain config from initial ArbOS state: %w", err)
		}
		expectedNum := chainConfig.ArbitrumChainParams.GenesisBlockNum
		if genesisNum != expectedNum {
			return nil, fmt.Errorf("unexpected genesis block number %v in ArbOS state, expected %v", genesisNum, expectedNum)
		}
	}

	var blockHash common.Hash
	var readBatchInfo []validator.BatchInfo
	if msg != nil {
		batchFetcher := func(batchNum uint64) ([]byte, error) {
			data, err := r.execEngine.streamer.FetchBatch(batchNum)
			if err != nil {
				return nil, err
			}
			readBatchInfo = append(readBatchInfo, validator.BatchInfo{
				Number: batchNum,
				Data:   data,
			})
			return data, nil
		}
		// Re-fetch the batch instead of using our cached cost,
		// as the replay binary won't have the cache populated.
		msg.Message.BatchGasCost = nil
		block, _, err := arbos.ProduceBlock(
			msg.Message,
			msg.DelayedMessagesRead,
			prevHeader,
			recordingdb,
			chaincontext,
			chainConfig,
			batchFetcher,
		)
		if err != nil {
			return nil, err
		}
		blockHash = block.Hash()
	}

	preimages, err := r.recordingDatabase.PreimagesFromRecording(chaincontext, recordingKV)
	if err != nil {
		return nil, err
	}

	// check we got the canonical hash
	canonicalHash := r.execEngine.bc.GetCanonicalHash(uint64(blockNum))
	if canonicalHash != blockHash {
		return nil, fmt.Errorf("Blockhash doesn't match when recording got %v canonical %v", blockHash, canonicalHash)
	}

	// these won't usually do much here (they will in preparerecording), but doesn't hurt to check
	r.updateLastHdr(prevHeader)
	r.updateValidCandidateHdr(prevHeader)

	return &execution.RecordResult{
		Pos:       pos,
		BlockHash: blockHash,
		Preimages: preimages,
		BatchInfo: readBatchInfo,
	}, err
}

func (r *BlockRecorder) updateLastHdr(hdr *types.Header) {
	if hdr == nil {
		return
	}
	r.lastHdrLock.Lock()
	defer r.lastHdrLock.Unlock()
	if r.lastHdr != nil {
		if hdr.Number.Cmp(r.lastHdr.Number) <= 0 {
			return
		}
	}
	_, err := r.recordingDatabase.StateFor(hdr)
	if err != nil {
		log.Warn("failed to get state in updateLastHdr", "err", err)
		return
	}
	r.recordingDatabase.Dereference(r.lastHdr)
	r.lastHdr = hdr
}

func (r *BlockRecorder) updateValidCandidateHdr(hdr *types.Header) {
	if hdr == nil {
		return
	}
	r.validHdrLock.Lock()
	defer r.validHdrLock.Unlock()
	// don't need a candidate that's newer than the current one (else it will never become valid)
	if r.validHdrCandidate != nil && r.validHdrCandidate.Number.Cmp(hdr.Number) <= 0 {
		return
	}
	// don't need a candidate that's older than known valid
	if r.validHdr != nil && r.validHdr.Number.Cmp(hdr.Number) >= 0 {
		return
	}
	_, err := r.recordingDatabase.StateFor(hdr)
	if err != nil {
		log.Warn("failed to get state in updateLastHdr", "err", err)
		return
	}
	if r.validHdrCandidate != nil {
		r.recordingDatabase.Dereference(r.validHdrCandidate)
	}
	r.validHdrCandidate = hdr
}

func (r *BlockRecorder) MarkValid(pos arbutil.MessageIndex, resultHash common.Hash) {
	r.validHdrLock.Lock()
	defer r.validHdrLock.Unlock()
	if r.validHdrCandidate == nil {
		return
	}
	validNum := r.execEngine.MessageIndexToBlockNumber(pos)
	if r.validHdrCandidate.Number.Uint64() > validNum {
		return
	}
	// make sure the valid is canonical
	canonicalResultHash := r.execEngine.bc.GetCanonicalHash(uint64(validNum))
	if canonicalResultHash != resultHash {
		log.Warn("markvalid hash not canonical", "pos", pos, "result", resultHash, "canonical", canonicalResultHash)
		return
	}
	// make sure the candidate is still canonical
	canonicalHash := r.execEngine.bc.GetCanonicalHash(r.validHdrCandidate.Number.Uint64())
	candidateHash := r.validHdrCandidate.Hash()
	if canonicalHash != candidateHash {
		log.Error("vlid candidate hash not canonical", "number", r.validHdrCandidate.Number, "candidate", candidateHash, "canonical", canonicalHash)
		r.recordingDatabase.Dereference(r.validHdrCandidate)
		r.validHdrCandidate = nil
		return
	}
	r.recordingDatabase.Dereference(r.validHdr)
	r.validHdr = r.validHdrCandidate
	r.validHdrCandidate = nil
}

// TODO: use config
func (r *BlockRecorder) preparedAddTrim(newRefs []*types.Header, size int) {
	var oldRefs []*types.Header
	r.preparedLock.Lock()
	r.preparedQueue = append(r.preparedQueue, newRefs...)
	if len(r.preparedQueue) > size {
		oldRefs = r.preparedQueue[:len(r.preparedQueue)-size]
		r.preparedQueue = r.preparedQueue[len(r.preparedQueue)-size:]
	}
	r.preparedLock.Unlock()
	for _, ref := range oldRefs {
		r.recordingDatabase.Dereference(ref)
	}
}

func (r *BlockRecorder) preparedTrimBeyond(hdr *types.Header) {
	var oldRefs []*types.Header
	var validRefs []*types.Header
	r.preparedLock.Lock()
	for _, queHdr := range r.preparedQueue {
		if queHdr.Number.Cmp(hdr.Number) > 0 {
			oldRefs = append(oldRefs, queHdr)
		} else {
			validRefs = append(validRefs, queHdr)
		}
	}
	r.preparedQueue = validRefs
	r.preparedLock.Unlock()
	for _, ref := range oldRefs {
		r.recordingDatabase.Dereference(ref)
	}
}

func (r *BlockRecorder) TrimAllPrepared(t *testing.T) {
	r.preparedAddTrim(nil, 0)
}

func (r *BlockRecorder) RecordingDBReferenceCount() int64 {
	return r.recordingDatabase.ReferenceCount()
}

func (r *BlockRecorder) PrepareForRecord(ctx context.Context, start, end arbutil.MessageIndex) error {
	var references []*types.Header
	if end < start {
		return fmt.Errorf("illegal range start %d > end %d", start, end)
	}
	numOfBlocks := uint64(end + 1 - start)
	hdrNum := r.execEngine.MessageIndexToBlockNumber(start)
	if start > 0 {
		hdrNum-- // need to get previous
	} else {
		numOfBlocks-- // genesis block doesn't need preparation, so recording one less block
	}
	lastHdrNum := hdrNum + numOfBlocks
	for hdrNum <= lastHdrNum {
		header := r.execEngine.bc.GetHeaderByNumber(uint64(hdrNum))
		if header == nil {
			log.Warn("prepareblocks asked for non-found block", "hdrNum", hdrNum)
			break
		}
		_, err := r.recordingDatabase.GetOrRecreateState(ctx, header, stateLogFunc)
		if err != nil {
			log.Warn("prepareblocks failed to get state for block", "hdrNum", hdrNum, "err", err)
			break
		}
		references = append(references, header)
		r.updateValidCandidateHdr(header)
		r.updateLastHdr(header)
		hdrNum++
	}
	r.preparedAddTrim(references, 1000)
	return nil
}

func (r *BlockRecorder) ReorgTo(hdr *types.Header) {
	r.validHdrLock.Lock()
	if r.validHdr != nil && r.validHdr.Number.Cmp(hdr.Number) > 0 {
		log.Warn("block recorder: reorging past previously-marked valid block", "reorg target num", hdr.Number, "hash", hdr.Hash(), "reorged past num", r.validHdr.Number, "hash", r.validHdr.Hash())
		r.recordingDatabase.Dereference(r.validHdr)
		r.validHdr = nil
	}
	if r.validHdrCandidate != nil && r.validHdrCandidate.Number.Cmp(hdr.Number) > 0 {
		r.recordingDatabase.Dereference(r.validHdrCandidate)
		r.validHdrCandidate = nil
	}
	r.validHdrLock.Unlock()
	r.lastHdrLock.Lock()
	if r.lastHdr != nil && r.lastHdr.Number.Cmp(hdr.Number) > 0 {
		r.recordingDatabase.Dereference(r.lastHdr)
		r.lastHdr = nil
	}
	r.lastHdrLock.Unlock()
	r.preparedTrimBeyond(hdr)
}

func (r *BlockRecorder) WriteValidStateToDb() error {
	r.validHdrLock.Lock()
	defer r.validHdrLock.Unlock()
	if r.validHdr == nil {
		return nil
	}
	err := r.recordingDatabase.WriteStateToDatabase(r.validHdr)
	r.recordingDatabase.Dereference(r.validHdr)
	return err
}

func (r *BlockRecorder) OrderlyShutdown() {
	err := r.WriteValidStateToDb()
	if err != nil {
		log.Error("failed writing latest valid block state to DB", "err", err)
	}
}

'''
'''--- execution/gethexec/blockchain.go ---
package gethexec

import (
	"errors"
	"fmt"
	"math/big"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/eth/ethconfig"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/gethhook"
	"github.com/offchainlabs/nitro/statetransfer"
)

type CachingConfig struct {
	Archive                            bool          `koanf:"archive"`
	BlockCount                         uint64        `koanf:"block-count"`
	BlockAge                           time.Duration `koanf:"block-age"`
	TrieTimeLimit                      time.Duration `koanf:"trie-time-limit"`
	TrieDirtyCache                     int           `koanf:"trie-dirty-cache"`
	TrieCleanCache                     int           `koanf:"trie-clean-cache"`
	SnapshotCache                      int           `koanf:"snapshot-cache"`
	DatabaseCache                      int           `koanf:"database-cache"`
	SnapshotRestoreGasLimit            uint64        `koanf:"snapshot-restore-gas-limit"`
	MaxNumberOfBlocksToSkipStateSaving uint32        `koanf:"max-number-of-blocks-to-skip-state-saving"`
	MaxAmountOfGasToSkipStateSaving    uint64        `koanf:"max-amount-of-gas-to-skip-state-saving"`
}

func CachingConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".archive", DefaultCachingConfig.Archive, "retain past block state")
	f.Uint64(prefix+".block-count", DefaultCachingConfig.BlockCount, "minimum number of recent blocks to keep in memory")
	f.Duration(prefix+".block-age", DefaultCachingConfig.BlockAge, "minimum age of recent blocks to keep in memory")
	f.Duration(prefix+".trie-time-limit", DefaultCachingConfig.TrieTimeLimit, "maximum block processing time before trie is written to hard-disk")
	f.Int(prefix+".trie-dirty-cache", DefaultCachingConfig.TrieDirtyCache, "amount of memory in megabytes to cache state diffs against disk with (larger cache lowers database growth)")
	f.Int(prefix+".trie-clean-cache", DefaultCachingConfig.TrieCleanCache, "amount of memory in megabytes to cache unchanged state trie nodes with")
	f.Int(prefix+".snapshot-cache", DefaultCachingConfig.SnapshotCache, "amount of memory in megabytes to cache state snapshots with")
	f.Int(prefix+".database-cache", DefaultCachingConfig.DatabaseCache, "amount of memory in megabytes to cache database contents with")
	f.Uint64(prefix+".snapshot-restore-gas-limit", DefaultCachingConfig.SnapshotRestoreGasLimit, "maximum gas rolled back to recover snapshot")
	f.Uint32(prefix+".max-number-of-blocks-to-skip-state-saving", DefaultCachingConfig.MaxNumberOfBlocksToSkipStateSaving, "maximum number of blocks to skip state saving to persistent storage (archive node only) -- warning: this option seems to cause issues")
	f.Uint64(prefix+".max-amount-of-gas-to-skip-state-saving", DefaultCachingConfig.MaxAmountOfGasToSkipStateSaving, "maximum amount of gas in blocks to skip saving state to Persistent storage (archive node only) -- warning: this option seems to cause issues")
}

var DefaultCachingConfig = CachingConfig{
	Archive:                            false,
	BlockCount:                         128,
	BlockAge:                           30 * time.Minute,
	TrieTimeLimit:                      time.Hour,
	TrieDirtyCache:                     1024,
	TrieCleanCache:                     600,
	SnapshotCache:                      400,
	DatabaseCache:                      2048,
	SnapshotRestoreGasLimit:            300_000_000_000,
	MaxNumberOfBlocksToSkipStateSaving: 0,
	MaxAmountOfGasToSkipStateSaving:    0,
}

func DefaultCacheConfigFor(stack *node.Node, cachingConfig *CachingConfig) *core.CacheConfig {
	baseConf := ethconfig.Defaults
	if cachingConfig.Archive {
		baseConf = ethconfig.ArchiveDefaults
	}

	return &core.CacheConfig{
		TrieCleanLimit:                     cachingConfig.TrieCleanCache,
		TrieCleanNoPrefetch:                baseConf.NoPrefetch,
		TrieDirtyLimit:                     cachingConfig.TrieDirtyCache,
		TrieDirtyDisabled:                  cachingConfig.Archive,
		TrieTimeLimit:                      cachingConfig.TrieTimeLimit,
		TriesInMemory:                      cachingConfig.BlockCount,
		TrieRetention:                      cachingConfig.BlockAge,
		SnapshotLimit:                      cachingConfig.SnapshotCache,
		Preimages:                          baseConf.Preimages,
		SnapshotRestoreMaxGas:              cachingConfig.SnapshotRestoreGasLimit,
		MaxNumberOfBlocksToSkipStateSaving: cachingConfig.MaxNumberOfBlocksToSkipStateSaving,
		MaxAmountOfGasToSkipStateSaving:    cachingConfig.MaxAmountOfGasToSkipStateSaving,
	}
}

func WriteOrTestGenblock(chainDb ethdb.Database, initData statetransfer.InitDataReader, chainConfig *params.ChainConfig, initMessage *arbostypes.ParsedInitMessage, accountsPerSync uint) error {
	EmptyHash := common.Hash{}
	prevHash := EmptyHash
	prevDifficulty := big.NewInt(0)
	blockNumber, err := initData.GetNextBlockNumber()
	if err != nil {
		return err
	}
	storedGenHash := rawdb.ReadCanonicalHash(chainDb, blockNumber)
	timestamp := uint64(0)
	if blockNumber > 0 {
		prevHash = rawdb.ReadCanonicalHash(chainDb, blockNumber-1)
		if prevHash == EmptyHash {
			return fmt.Errorf("block number %d not found in database", chainDb)
		}
		prevHeader := rawdb.ReadHeader(chainDb, prevHash, blockNumber-1)
		if prevHeader == nil {
			return fmt.Errorf("block header for block %d not found in database", chainDb)
		}
		timestamp = prevHeader.Time
	}
	stateRoot, err := arbosState.InitializeArbosInDatabase(chainDb, initData, chainConfig, initMessage, timestamp, accountsPerSync)
	if err != nil {
		return err
	}

	genBlock := arbosState.MakeGenesisBlock(prevHash, blockNumber, timestamp, stateRoot, chainConfig)
	blockHash := genBlock.Hash()

	if storedGenHash == EmptyHash {
		// chainDb did not have genesis block. Initialize it.
		core.WriteHeadBlock(chainDb, genBlock, prevDifficulty)
		log.Info("wrote genesis block", "number", blockNumber, "hash", blockHash)
	} else if storedGenHash != blockHash {
		return fmt.Errorf("database contains data inconsistent with initialization: database has genesis hash %v but we built genesis hash %v", storedGenHash, blockHash)
	} else {
		log.Info("recreated existing genesis block", "number", blockNumber, "hash", blockHash)
	}

	return nil
}

func TryReadStoredChainConfig(chainDb ethdb.Database) *params.ChainConfig {
	EmptyHash := common.Hash{}

	block0Hash := rawdb.ReadCanonicalHash(chainDb, 0)
	if block0Hash == EmptyHash {
		return nil
	}
	return rawdb.ReadChainConfig(chainDb, block0Hash)
}

func WriteOrTestChainConfig(chainDb ethdb.Database, config *params.ChainConfig) error {
	EmptyHash := common.Hash{}

	block0Hash := rawdb.ReadCanonicalHash(chainDb, 0)
	if block0Hash == EmptyHash {
		return errors.New("block 0 not found")
	}
	storedConfig := rawdb.ReadChainConfig(chainDb, block0Hash)
	if storedConfig == nil {
		rawdb.WriteChainConfig(chainDb, block0Hash, config)
		return nil
	}
	height := rawdb.ReadHeaderNumber(chainDb, rawdb.ReadHeadHeaderHash(chainDb))
	if height == nil {
		return errors.New("non empty chain config but empty chain")
	}
	err := storedConfig.CheckCompatible(config, *height, 0)
	if err != nil {
		return err
	}
	rawdb.WriteChainConfig(chainDb, block0Hash, config)
	return nil
}

func GetBlockChain(chainDb ethdb.Database, cacheConfig *core.CacheConfig, chainConfig *params.ChainConfig, txLookupLimit uint64) (*core.BlockChain, error) {
	engine := arbos.Engine{
		IsSequencer: true,
	}

	vmConfig := vm.Config{
		EnablePreimageRecording: false,
	}

	return core.NewBlockChain(chainDb, cacheConfig, chainConfig, nil, nil, engine, vmConfig, shouldPreserveFalse, &txLookupLimit)
}

func WriteOrTestBlockChain(chainDb ethdb.Database, cacheConfig *core.CacheConfig, initData statetransfer.InitDataReader, chainConfig *params.ChainConfig, initMessage *arbostypes.ParsedInitMessage, txLookupLimit uint64, accountsPerSync uint) (*core.BlockChain, error) {
	err := WriteOrTestGenblock(chainDb, initData, chainConfig, initMessage, accountsPerSync)
	if err != nil {
		return nil, err
	}
	err = WriteOrTestChainConfig(chainDb, chainConfig)
	if err != nil {
		return nil, err
	}
	return GetBlockChain(chainDb, cacheConfig, chainConfig, txLookupLimit)
}

// Don't preserve reorg'd out blocks
func shouldPreserveFalse(_ *types.Header) bool {
	return false
}

func init() {
	gethhook.RequireHookedGeth()
}

'''
'''--- execution/gethexec/executionengine.go ---
package gethexec

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"sync"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/sharedmetrics"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

type ExecutionEngine struct {
	stopwaiter.StopWaiter

	bc       *core.BlockChain
	streamer execution.TransactionStreamer
	recorder *BlockRecorder

	resequenceChan    chan []*arbostypes.MessageWithMetadata
	createBlocksMutex sync.Mutex

	newBlockNotifier chan struct{}
	latestBlockMutex sync.Mutex
	latestBlock      *types.Block

	nextScheduledVersionCheck time.Time // protected by the createBlocksMutex

	reorgSequencing bool
}

func NewExecutionEngine(bc *core.BlockChain) (*ExecutionEngine, error) {
	return &ExecutionEngine{
		bc:               bc,
		resequenceChan:   make(chan []*arbostypes.MessageWithMetadata),
		newBlockNotifier: make(chan struct{}, 1),
	}, nil
}

func (s *ExecutionEngine) SetRecorder(recorder *BlockRecorder) {
	if s.Started() {
		panic("trying to set recorder after start")
	}
	if s.recorder != nil {
		panic("trying to set recorder policy when already set")
	}
	s.recorder = recorder
}

func (s *ExecutionEngine) EnableReorgSequencing() {
	if s.Started() {
		panic("trying to enable reorg sequencing after start")
	}
	if s.reorgSequencing {
		panic("trying to enable reorg sequencing when already set")
	}
	s.reorgSequencing = true
}

func (s *ExecutionEngine) SetTransactionStreamer(streamer execution.TransactionStreamer) {
	if s.Started() {
		panic("trying to set transaction streamer after start")
	}
	if s.streamer != nil {
		panic("trying to set transaction streamer when already set")
	}
	s.streamer = streamer
}

func (s *ExecutionEngine) Reorg(count arbutil.MessageIndex, newMessages []arbostypes.MessageWithMetadata, oldMessages []*arbostypes.MessageWithMetadata) error {
	if count == 0 {
		return errors.New("cannot reorg out genesis")
	}
	s.createBlocksMutex.Lock()
	resequencing := false
	defer func() {
		// if we are resequencing old messages - don't release the lock
		// lock will be relesed by thread listening to resequenceChan
		if !resequencing {
			s.createBlocksMutex.Unlock()
		}
	}()
	blockNum := s.MessageIndexToBlockNumber(count - 1)
	// We can safely cast blockNum to a uint64 as it comes from MessageCountToBlockNumber
	targetBlock := s.bc.GetBlockByNumber(uint64(blockNum))
	if targetBlock == nil {
		log.Warn("reorg target block not found", "block", blockNum)
		return nil
	}

	err := s.bc.ReorgToOldBlock(targetBlock)
	if err != nil {
		return err
	}
	for i := range newMessages {
		err := s.digestMessageWithBlockMutex(count+arbutil.MessageIndex(i), &newMessages[i])
		if err != nil {
			return err
		}
	}
	if s.recorder != nil {
		s.recorder.ReorgTo(targetBlock.Header())
	}
	if len(oldMessages) > 0 {
		s.resequenceChan <- oldMessages
		resequencing = true
	}
	return nil
}

func (s *ExecutionEngine) getCurrentHeader() (*types.Header, error) {
	currentBlock := s.bc.CurrentBlock()
	if currentBlock == nil {
		return nil, errors.New("failed to get current block")
	}
	return currentBlock, nil
}

func (s *ExecutionEngine) HeadMessageNumber() (arbutil.MessageIndex, error) {
	currentHeader, err := s.getCurrentHeader()
	if err != nil {
		return 0, err
	}
	return s.BlockNumberToMessageIndex(currentHeader.Number.Uint64())
}

func (s *ExecutionEngine) HeadMessageNumberSync(t *testing.T) (arbutil.MessageIndex, error) {
	s.createBlocksMutex.Lock()
	defer s.createBlocksMutex.Unlock()
	return s.HeadMessageNumber()
}

func (s *ExecutionEngine) NextDelayedMessageNumber() (uint64, error) {
	currentHeader, err := s.getCurrentHeader()
	if err != nil {
		return 0, err
	}
	return currentHeader.Nonce.Uint64(), nil
}

func messageFromTxes(header *arbostypes.L1IncomingMessageHeader, txes types.Transactions, txErrors []error) (*arbostypes.L1IncomingMessage, error) {
	var l2Message []byte
	if len(txes) == 1 && txErrors[0] == nil {
		txBytes, err := txes[0].MarshalBinary()
		if err != nil {
			return nil, err
		}
		l2Message = append(l2Message, arbos.L2MessageKind_SignedTx)
		l2Message = append(l2Message, txBytes...)
	} else {
		l2Message = append(l2Message, arbos.L2MessageKind_Batch)
		sizeBuf := make([]byte, 8)
		for i, tx := range txes {
			if txErrors[i] != nil {
				continue
			}
			txBytes, err := tx.MarshalBinary()
			if err != nil {
				return nil, err
			}
			binary.BigEndian.PutUint64(sizeBuf, uint64(len(txBytes)+1))
			l2Message = append(l2Message, sizeBuf...)
			l2Message = append(l2Message, arbos.L2MessageKind_SignedTx)
			l2Message = append(l2Message, txBytes...)
		}
	}
	return &arbostypes.L1IncomingMessage{
		Header: header,
		L2msg:  l2Message,
	}, nil
}

// The caller must hold the createBlocksMutex
func (s *ExecutionEngine) resequenceReorgedMessages(messages []*arbostypes.MessageWithMetadata) {
	if !s.reorgSequencing {
		return
	}

	log.Info("Trying to resequence messages", "number", len(messages))
	lastBlockHeader, err := s.getCurrentHeader()
	if err != nil {
		log.Error("block header not found during resequence", "err", err)
		return
	}

	nextDelayedSeqNum := lastBlockHeader.Nonce.Uint64()

	for _, msg := range messages {
		// Check if the message is non-nil just to be safe
		if msg == nil || msg.Message == nil || msg.Message.Header == nil {
			continue
		}
		header := msg.Message.Header
		if header.RequestId != nil {
			delayedSeqNum := header.RequestId.Big().Uint64()
			if delayedSeqNum != nextDelayedSeqNum {
				log.Info("not resequencing delayed message due to unexpected index", "expected", nextDelayedSeqNum, "found", delayedSeqNum)
				continue
			}
			_, err := s.sequenceDelayedMessageWithBlockMutex(msg.Message, delayedSeqNum)
			if err != nil {
				log.Error("failed to re-sequence old delayed message removed by reorg", "err", err)
			}
			nextDelayedSeqNum += 1
			continue
		}
		if header.Kind != arbostypes.L1MessageType_L2Message || header.Poster != l1pricing.BatchPosterAddress {
			// This shouldn't exist?
			log.Warn("skipping non-standard sequencer message found from reorg", "header", header)
			continue
		}
		// We don't need a batch fetcher as this is an L2 message
		txes, err := arbos.ParseL2Transactions(msg.Message, s.bc.Config().ChainID, nil)
		if err != nil {
			log.Warn("failed to parse sequencer message found from reorg", "err", err)
			continue
		}
		hooks := arbos.NoopSequencingHooks()
		hooks.DiscardInvalidTxsEarly = true
		_, err = s.sequenceTransactionsWithBlockMutex(msg.Message.Header, txes, hooks)
		if err != nil {
			log.Error("failed to re-sequence old user message removed by reorg", "err", err)
			return
		}
	}
}

func (s *ExecutionEngine) sequencerWrapper(sequencerFunc func() (*types.Block, error)) (*types.Block, error) {
	attempts := 0
	for {
		s.createBlocksMutex.Lock()
		block, err := sequencerFunc()
		s.createBlocksMutex.Unlock()
		if !errors.Is(err, execution.ErrSequencerInsertLockTaken) {
			return block, err
		}
		// We got SequencerInsertLockTaken
		// option 1: there was a race, we are no longer main sequencer
		chosenErr := s.streamer.ExpectChosenSequencer()
		if chosenErr != nil {
			return nil, chosenErr
		}
		// option 2: we are in a test without very orderly sequencer coordination
		if !s.bc.Config().ArbitrumChainParams.AllowDebugPrecompiles {
			// option 3: something weird. send warning
			log.Warn("sequence transactions: insert lock takent", "attempts", attempts)
		}
		// options 2/3 fail after too many attempts
		attempts++
		if attempts > 20 {
			return nil, err
		}
		<-time.After(time.Millisecond * 100)
	}
}

func (s *ExecutionEngine) SequenceTransactions(header *arbostypes.L1IncomingMessageHeader, txes types.Transactions, hooks *arbos.SequencingHooks) (*types.Block, error) {
	return s.sequencerWrapper(func() (*types.Block, error) {
		hooks.TxErrors = nil
		return s.sequenceTransactionsWithBlockMutex(header, txes, hooks)
	})
}

func (s *ExecutionEngine) sequenceTransactionsWithBlockMutex(header *arbostypes.L1IncomingMessageHeader, txes types.Transactions, hooks *arbos.SequencingHooks) (*types.Block, error) {
	lastBlockHeader, err := s.getCurrentHeader()
	if err != nil {
		return nil, err
	}

	statedb, err := s.bc.StateAt(lastBlockHeader.Root)
	if err != nil {
		return nil, err
	}

	delayedMessagesRead := lastBlockHeader.Nonce.Uint64()

	startTime := time.Now()
	block, receipts, err := arbos.ProduceBlockAdvanced(
		header,
		txes,
		delayedMessagesRead,
		lastBlockHeader,
		statedb,
		s.bc,
		s.bc.Config(),
		hooks,
	)
	if err != nil {
		return nil, err
	}
	blockCalcTime := time.Since(startTime)
	if len(hooks.TxErrors) != len(txes) {
		return nil, fmt.Errorf("unexpected number of error results: %v vs number of txes %v", len(hooks.TxErrors), len(txes))
	}

	if len(receipts) == 0 {
		return nil, nil
	}

	allTxsErrored := true
	for _, err := range hooks.TxErrors {
		if err == nil {
			allTxsErrored = false
			break
		}
	}
	if allTxsErrored {
		return nil, nil
	}

	msg, err := messageFromTxes(header, txes, hooks.TxErrors)
	if err != nil {
		return nil, err
	}

	msgWithMeta := arbostypes.MessageWithMetadata{
		Message:             msg,
		DelayedMessagesRead: delayedMessagesRead,
	}

	pos, err := s.BlockNumberToMessageIndex(lastBlockHeader.Number.Uint64() + 1)
	if err != nil {
		return nil, err
	}

	err = s.streamer.WriteMessageFromSequencer(pos, msgWithMeta)
	if err != nil {
		return nil, err
	}

	// Only write the block after we've written the messages, so if the node dies in the middle of this,
	// it will naturally recover on startup by regenerating the missing block.
	err = s.appendBlock(block, statedb, receipts, blockCalcTime)
	if err != nil {
		return nil, err
	}

	return block, nil
}

func (s *ExecutionEngine) SequenceDelayedMessage(message *arbostypes.L1IncomingMessage, delayedSeqNum uint64) error {
	_, err := s.sequencerWrapper(func() (*types.Block, error) {
		return s.sequenceDelayedMessageWithBlockMutex(message, delayedSeqNum)
	})
	return err
}

func (s *ExecutionEngine) sequenceDelayedMessageWithBlockMutex(message *arbostypes.L1IncomingMessage, delayedSeqNum uint64) (*types.Block, error) {
	currentHeader, err := s.getCurrentHeader()
	if err != nil {
		return nil, err
	}

	expectedDelayed := currentHeader.Nonce.Uint64()

	lastMsg, err := s.BlockNumberToMessageIndex(currentHeader.Number.Uint64())
	if err != nil {
		return nil, err
	}

	if expectedDelayed != delayedSeqNum {
		return nil, fmt.Errorf("wrong delayed message sequenced got %d expected %d", delayedSeqNum, expectedDelayed)
	}

	messageWithMeta := arbostypes.MessageWithMetadata{
		Message:             message,
		DelayedMessagesRead: delayedSeqNum + 1,
	}

	err = s.streamer.WriteMessageFromSequencer(lastMsg+1, messageWithMeta)
	if err != nil {
		return nil, err
	}

	startTime := time.Now()
	block, statedb, receipts, err := s.createBlockFromNextMessage(&messageWithMeta)
	if err != nil {
		return nil, err
	}

	err = s.appendBlock(block, statedb, receipts, time.Since(startTime))
	if err != nil {
		return nil, err
	}

	log.Info("ExecutionEngine: Added DelayedMessages", "pos", lastMsg+1, "delayed", delayedSeqNum, "block-header", block.Header())

	return block, nil
}

func (s *ExecutionEngine) GetGenesisBlockNumber() uint64 {
	return s.bc.Config().ArbitrumChainParams.GenesisBlockNum
}

func (s *ExecutionEngine) BlockNumberToMessageIndex(blockNum uint64) (arbutil.MessageIndex, error) {
	genesis := s.GetGenesisBlockNumber()
	if blockNum < genesis {
		return 0, fmt.Errorf("blockNum %d < genesis %d", blockNum, genesis)
	}
	return arbutil.MessageIndex(blockNum - genesis), nil
}

func (s *ExecutionEngine) MessageIndexToBlockNumber(messageNum arbutil.MessageIndex) uint64 {
	return uint64(messageNum) + s.GetGenesisBlockNumber()
}

// must hold createBlockMutex
func (s *ExecutionEngine) createBlockFromNextMessage(msg *arbostypes.MessageWithMetadata) (*types.Block, *state.StateDB, types.Receipts, error) {
	currentHeader := s.bc.CurrentBlock()
	if currentHeader == nil {
		return nil, nil, nil, errors.New("failed to get current block header")
	}

	currentBlock := s.bc.GetBlock(currentHeader.Hash(), currentHeader.Number.Uint64())
	if currentBlock == nil {
		return nil, nil, nil, errors.New("can't find block for current header")
	}

	err := s.bc.RecoverState(currentBlock)
	if err != nil {
		return nil, nil, nil, fmt.Errorf("failed to recover block %v state: %w", currentBlock.Number(), err)
	}

	statedb, err := s.bc.StateAt(currentHeader.Root)
	if err != nil {
		return nil, nil, nil, err
	}
	statedb.StartPrefetcher("TransactionStreamer")
	defer statedb.StopPrefetcher()

	block, receipts, err := arbos.ProduceBlock(
		msg.Message,
		msg.DelayedMessagesRead,
		currentHeader,
		statedb,
		s.bc,
		s.bc.Config(),
		s.streamer.FetchBatch,
	)

	return block, statedb, receipts, err
}

// must hold createBlockMutex
func (s *ExecutionEngine) appendBlock(block *types.Block, statedb *state.StateDB, receipts types.Receipts, duration time.Duration) error {
	var logs []*types.Log
	for _, receipt := range receipts {
		logs = append(logs, receipt.Logs...)
	}
	status, err := s.bc.WriteBlockAndSetHeadWithTime(block, receipts, logs, statedb, true, duration)
	if err != nil {
		return err
	}
	if status == core.SideStatTy {
		return errors.New("geth rejected block as non-canonical")
	}
	return nil
}

func (s *ExecutionEngine) resultFromHeader(header *types.Header) (*execution.MessageResult, error) {
	if header == nil {
		return nil, fmt.Errorf("result not found")
	}
	info := types.DeserializeHeaderExtraInformation(header)
	return &execution.MessageResult{
		BlockHash: header.Hash(),
		SendRoot:  info.SendRoot,
	}, nil
}

func (s *ExecutionEngine) ResultAtPos(pos arbutil.MessageIndex) (*execution.MessageResult, error) {
	return s.resultFromHeader(s.bc.GetHeaderByNumber(s.MessageIndexToBlockNumber(pos)))
}

func (s *ExecutionEngine) DigestMessage(num arbutil.MessageIndex, msg *arbostypes.MessageWithMetadata) error {
	if !s.createBlocksMutex.TryLock() {
		return errors.New("createBlock mutex held")
	}
	defer s.createBlocksMutex.Unlock()
	return s.digestMessageWithBlockMutex(num, msg)
}

func (s *ExecutionEngine) digestMessageWithBlockMutex(num arbutil.MessageIndex, msg *arbostypes.MessageWithMetadata) error {
	currentHeader, err := s.getCurrentHeader()
	if err != nil {
		return err
	}
	curMsg, err := s.BlockNumberToMessageIndex(currentHeader.Number.Uint64())
	if err != nil {
		return err
	}
	if curMsg+1 != num {
		return fmt.Errorf("wrong message number in digest got %d expected %d", num, curMsg+1)
	}

	startTime := time.Now()
	block, statedb, receipts, err := s.createBlockFromNextMessage(msg)
	if err != nil {
		return err
	}

	err = s.appendBlock(block, statedb, receipts, time.Since(startTime))
	if err != nil {
		return err
	}

	if time.Now().After(s.nextScheduledVersionCheck) {
		s.nextScheduledVersionCheck = time.Now().Add(time.Minute)
		arbState, err := arbosState.OpenSystemArbosState(statedb, nil, true)
		if err != nil {
			return err
		}
		version, timestampInt, err := arbState.GetScheduledUpgrade()
		if err != nil {
			return err
		}
		var timeUntilUpgrade time.Duration
		var timestamp time.Time
		if timestampInt == 0 {
			// This upgrade will take effect in the next block
			timestamp = time.Now()
		} else {
			// This upgrade is scheduled for the future
			timestamp = time.Unix(int64(timestampInt), 0)
			timeUntilUpgrade = time.Until(timestamp)
		}
		maxSupportedVersion := params.ArbitrumDevTestChainConfig().ArbitrumChainParams.InitialArbOSVersion
		logLevel := log.Warn
		if timeUntilUpgrade < time.Hour*24 {
			logLevel = log.Error
		}
		if version > maxSupportedVersion {
			logLevel(
				"you need to update your node to the latest version before this scheduled ArbOS upgrade",
				"timeUntilUpgrade", timeUntilUpgrade,
				"upgradeScheduledFor", timestamp,
				"maxSupportedArbosVersion", maxSupportedVersion,
				"pendingArbosUpgradeVersion", version,
			)
		}
	}

	sharedmetrics.UpdateSequenceNumberInBlockGauge(num)
	s.latestBlockMutex.Lock()
	s.latestBlock = block
	s.latestBlockMutex.Unlock()
	select {
	case s.newBlockNotifier <- struct{}{}:
	default:
	}
	return nil
}

func (s *ExecutionEngine) Start(ctx_in context.Context) {
	s.StopWaiter.Start(ctx_in, s)
	s.LaunchThread(func(ctx context.Context) {
		for {
			select {
			case <-ctx.Done():
				return
			case resequence := <-s.resequenceChan:
				s.resequenceReorgedMessages(resequence)
				s.createBlocksMutex.Unlock()
			}
		}
	})
	s.LaunchThread(func(ctx context.Context) {
		var lastBlock *types.Block
		for {
			select {
			case <-s.newBlockNotifier:
			case <-ctx.Done():
				return
			}
			s.latestBlockMutex.Lock()
			block := s.latestBlock
			s.latestBlockMutex.Unlock()
			if block != nil && (lastBlock == nil || block.Hash() != lastBlock.Hash()) {
				log.Info(
					"created block",
					"l2Block", block.Number(),
					"l2BlockHash", block.Hash(),
				)
				lastBlock = block
				select {
				case <-time.After(time.Second):
				case <-ctx.Done():
					return
				}
			}
		}
	})
}

'''
'''--- execution/gethexec/forwarder.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethexec

import (
	"context"
	"errors"
	"fmt"
	"net"
	"net/http"
	"sync"
	"sync/atomic"
	"time"

	"github.com/offchainlabs/nitro/util/redisutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rpc"
)

type ForwarderConfig struct {
	ConnectionTimeout     time.Duration `koanf:"connection-timeout"`
	IdleConnectionTimeout time.Duration `koanf:"idle-connection-timeout"`
	MaxIdleConnections    int           `koanf:"max-idle-connections"`
	RedisUrl              string        `koanf:"redis-url"`
	UpdateInterval        time.Duration `koanf:"update-interval"`
	RetryInterval         time.Duration `koanf:"retry-interval"`
}

var DefaultTestForwarderConfig = ForwarderConfig{
	ConnectionTimeout:     2 * time.Second,
	IdleConnectionTimeout: 2 * time.Second,
	MaxIdleConnections:    1,
	RedisUrl:              "",
	UpdateInterval:        time.Millisecond * 10,
	RetryInterval:         time.Millisecond * 3,
}

var DefaultNodeForwarderConfig = ForwarderConfig{
	ConnectionTimeout:     30 * time.Second,
	IdleConnectionTimeout: 15 * time.Second,
	MaxIdleConnections:    1,
	RedisUrl:              "",
	UpdateInterval:        time.Second,
	RetryInterval:         100 * time.Millisecond,
}

var DefaultSequencerForwarderConfig = ForwarderConfig{
	ConnectionTimeout:     30 * time.Second,
	IdleConnectionTimeout: 60 * time.Second,
	MaxIdleConnections:    100,
	RedisUrl:              "",
	UpdateInterval:        time.Second,
	RetryInterval:         100 * time.Millisecond,
}

func AddOptionsForNodeForwarderConfig(prefix string, f *flag.FlagSet) {
	AddOptionsForForwarderConfigImpl(prefix, &DefaultNodeForwarderConfig, f)
}

func AddOptionsForSequencerForwarderConfig(prefix string, f *flag.FlagSet) {
	AddOptionsForForwarderConfigImpl(prefix, &DefaultSequencerForwarderConfig, f)
}

func AddOptionsForForwarderConfigImpl(prefix string, defaultConfig *ForwarderConfig, f *flag.FlagSet) {
	f.Duration(prefix+".connection-timeout", defaultConfig.ConnectionTimeout, "total time to wait before cancelling connection")
	f.Duration(prefix+".idle-connection-timeout", defaultConfig.IdleConnectionTimeout, "time until idle connections are closed")
	f.Int(prefix+".max-idle-connections", defaultConfig.MaxIdleConnections, "maximum number of idle connections to keep open")
	f.String(prefix+".redis-url", defaultConfig.RedisUrl, "the Redis URL to recomend target via")
	f.Duration(prefix+".update-interval", defaultConfig.UpdateInterval, "forwarding target update interval")
	f.Duration(prefix+".retry-interval", defaultConfig.RetryInterval, "minimal time between update retries")
}

type TxForwarder struct {
	enabled   atomic.Bool
	target    string
	timeout   time.Duration
	transport *http.Transport
	rpcClient *rpc.Client
	ethClient *ethclient.Client

	healthMutex   sync.Mutex
	healthErr     error
	healthChecked time.Time
}

func NewForwarder(target string, config *ForwarderConfig) *TxForwarder {
	dialer := net.Dialer{
		Timeout:   5 * time.Second,
		KeepAlive: 2 * time.Second,
	}

	transport := &http.Transport{
		DialContext: func(ctx context.Context, network, addr string) (net.Conn, error) {
			// For tcp connections, prefer IPv4 over IPv6
			if network == "tcp" {
				conn, err := dialer.DialContext(ctx, "tcp4", addr)
				if err == nil {
					return conn, nil
				}
				return dialer.DialContext(ctx, "tcp6", addr)
			}
			return dialer.DialContext(ctx, network, addr)
		},
		MaxIdleConns:          config.MaxIdleConnections,
		MaxIdleConnsPerHost:   config.MaxIdleConnections,
		IdleConnTimeout:       config.IdleConnectionTimeout,
		TLSHandshakeTimeout:   5 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
	}
	return &TxForwarder{
		target:    target,
		timeout:   config.ConnectionTimeout,
		transport: transport,
	}
}

func (f *TxForwarder) ctxWithTimeout(inctx context.Context) (context.Context, context.CancelFunc) {
	if f.timeout == time.Duration(0) {
		return context.WithCancel(inctx)
	}
	return context.WithTimeout(inctx, f.timeout)
}

func (f *TxForwarder) PublishTransaction(inctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	if !f.enabled.Load() {
		return ErrNoSequencer
	}
	ctx, cancelFunc := f.ctxWithTimeout(inctx)
	defer cancelFunc()
	if options == nil {
		return f.ethClient.SendTransaction(ctx, tx)
	}
	return arbitrum.SendConditionalTransactionRPC(ctx, f.rpcClient, tx, options)
}

const cacheUpstreamHealth = 2 * time.Second
const maxHealthTimeout = 10 * time.Second

func (f *TxForwarder) CheckHealth(inctx context.Context) error {
	if !f.enabled.Load() {
		return ErrNoSequencer
	}
	f.healthMutex.Lock()
	defer f.healthMutex.Unlock()
	if time.Since(f.healthChecked) > cacheUpstreamHealth {
		timeout := f.timeout
		if timeout == time.Duration(0) || timeout >= maxHealthTimeout {
			timeout = maxHealthTimeout
		}
		ctx, cancelFunc := context.WithTimeout(context.Background(), timeout)
		defer cancelFunc()
		f.healthErr = f.rpcClient.CallContext(ctx, nil, "arb_checkPublisherHealth")
		f.healthChecked = time.Now()
	}
	return f.healthErr
}

func (f *TxForwarder) Initialize(inctx context.Context) error {
	if f.target == "" {
		f.rpcClient = nil
		f.ethClient = nil
		f.enabled.Store(false)
		return nil
	}
	ctx, cancelFunc := f.ctxWithTimeout(inctx)
	defer cancelFunc()
	rpcClient, err := rpc.DialTransport(ctx, f.target, f.transport)
	if err != nil {
		return err
	}
	f.rpcClient = rpcClient
	f.ethClient = ethclient.NewClient(rpcClient)
	f.enabled.Store(true)
	return nil
}

// Disable is not thread-safe vs. Initialize
func (f *TxForwarder) Disable() {
	f.enabled.Store(false)
}

func (f *TxForwarder) Start(ctx context.Context) error {
	return nil
}

func (f *TxForwarder) StopAndWait() {
	if f.ethClient != nil {
		f.ethClient.Close() // internally closes also the rpc client
	}
}

func (f *TxForwarder) Started() bool {
	return true
}

type TxDropper struct{}

func NewTxDropper() *TxDropper {
	return &TxDropper{}
}

var txDropperErr = errors.New("publishing transactions not supported by this endpoint")

func (f *TxDropper) PublishTransaction(ctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	return txDropperErr
}

func (f *TxDropper) CheckHealth(ctx context.Context) error {
	return txDropperErr
}

func (f *TxDropper) Initialize(ctx context.Context) error { return nil }

func (f *TxDropper) Start(ctx context.Context) error { return nil }

func (f *TxDropper) StopAndWait() {}

func (f *TxDropper) Started() bool {
	return true
}

type RedisTxForwarder struct {
	stopwaiter.StopWaiterSafe

	config         *ForwarderConfig
	fallbackTarget string

	errors           int
	currentTarget    string
	redisCoordinator *redisutil.RedisCoordinator

	mtx       sync.RWMutex
	forwarder *TxForwarder
}

func NewRedisTxForwarder(fallbackTarget string, config *ForwarderConfig) *RedisTxForwarder {
	return &RedisTxForwarder{
		config:         config,
		fallbackTarget: fallbackTarget,
	}
}

func (f *RedisTxForwarder) PublishTransaction(ctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	forwarder := f.getForwarder()
	if forwarder == nil {
		return ErrNoSequencer
	}
	return forwarder.PublishTransaction(ctx, tx, options)
}

func (f *RedisTxForwarder) CheckHealth(ctx context.Context) error {
	forwarder := f.getForwarder()
	if forwarder == nil {
		return ErrNoSequencer
	}
	return forwarder.CheckHealth(ctx)
}

// not thread safe vs update and itself
func (f *RedisTxForwarder) Initialize(ctx context.Context) error {
	var err error
	f.redisCoordinator, err = redisutil.NewRedisCoordinator(f.config.RedisUrl)
	if err != nil {
		return fmt.Errorf("unable to create redis coordinator: %w", err)
	}
	f.update(ctx)
	return nil
}

func (f *RedisTxForwarder) retryAfterError() time.Duration {
	f.errors++
	retryIn := f.config.RetryInterval * time.Duration(f.errors)
	if retryIn > f.config.UpdateInterval {
		retryIn = f.config.UpdateInterval
	}
	return retryIn
}

// returns true when retry interval is saturated and there is a fallback url available
func (f *RedisTxForwarder) shouldFallbackToStatic() bool {
	return f.currentTarget == "" ||
		f.config.RetryInterval*time.Duration(f.errors+1) >= f.config.UpdateInterval && f.fallbackTarget != "" && f.fallbackTarget != f.currentTarget
}

func (f *RedisTxForwarder) noError() time.Duration {
	f.errors = 0
	return f.config.UpdateInterval
}

func (f *RedisTxForwarder) getForwarder() *TxForwarder {
	f.mtx.RLock()
	defer f.mtx.RUnlock()
	return f.forwarder
}

func (f *RedisTxForwarder) setForwarder(forwarder *TxForwarder) {
	f.mtx.Lock()
	defer f.mtx.Unlock()
	if f.forwarder != nil {
		f.forwarder.Disable()
	}
	f.forwarder = forwarder
}

// not thread safe vs initialize and itself
func (f *RedisTxForwarder) update(ctx context.Context) time.Duration {
	nextUpdateIn := f.noError
	var newSequencerUrl string
	var redisErr error
	if f.redisCoordinator != nil {
		newSequencerUrl, redisErr = f.redisCoordinator.CurrentChosenSequencer(ctx)
		if redisErr == nil && newSequencerUrl == "" {
			log.Info("no sequencer is currently chosen, using recommended sequencer instead")
			newSequencerUrl, redisErr = f.redisCoordinator.RecommendSequencerWantingLockout(ctx)
		}
		if redisErr != nil || newSequencerUrl == "" {
			if f.shouldFallbackToStatic() && f.fallbackTarget != "" {
				log.Warn("coordinator failed to find live sequencer, falling back to static url", "err", redisErr, "fallback", f.fallbackTarget)
				newSequencerUrl = f.fallbackTarget
				nextUpdateIn = f.retryAfterError
			} else {
				log.Warn("coordinator failed to find live sequencer", "err", redisErr)
				return f.retryAfterError()
			}
		}
	} else {
		if f.fallbackTarget != "" {
			log.Warn("redis coordinator not initialized, falling back to static url", "fallback", f.fallbackTarget)
			newSequencerUrl = f.fallbackTarget
		} else {
			// TODO panic? - there is no way to recover from this point
			log.Error("redis coordinator not initilized, no fallback available")
			return f.retryAfterError()
		}
	}
	if newSequencerUrl == f.currentTarget {
		return nextUpdateIn()
	}
	var newForwarder *TxForwarder
	for {
		newForwarder = NewForwarder(newSequencerUrl, f.config)
		err := newForwarder.Initialize(ctx)
		if err == nil {
			break
		}
		if f.shouldFallbackToStatic() && newSequencerUrl != f.fallbackTarget {
			log.Error("failed to initialize forward agent, falling back to static url", "err", err, "fallback", f.fallbackTarget)
			newSequencerUrl = f.fallbackTarget
			nextUpdateIn = f.retryAfterError
		} else {
			log.Error("failed to initialize forward agent", "err", err)
			return f.retryAfterError()
		}
	}
	f.currentTarget = newSequencerUrl
	f.setForwarder(newForwarder)
	return nextUpdateIn()
}

func (f *RedisTxForwarder) Start(ctx context.Context) error {
	if err := f.StopWaiterSafe.Start(ctx, f); err != nil {
		return err
	}
	if err := f.CallIterativelySafe(f.update); err != nil {
		return fmt.Errorf("failed to start forwarder update thread: %w", err)
	}
	return nil
}

func (f *RedisTxForwarder) StopAndWait() {
	err := f.StopWaiterSafe.StopAndWait()
	if err != nil {
		log.Error("Failed to stop forwarder", "err", err)
	}
	oldForwarder := f.getForwarder()
	if oldForwarder != nil {
		oldForwarder.StopAndWait()
	}
}

func (f *RedisTxForwarder) Started() bool {
	return f.StopWaiterSafe.Started()
}

'''
'''--- execution/gethexec/node.go ---
package gethexec

import (
	"context"
	"errors"
	"fmt"
	"reflect"
	"sync/atomic"
	"testing"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/eth"
	"github.com/ethereum/go-ethereum/eth/filters"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/headerreader"
	flag "github.com/spf13/pflag"
)

type DangerousConfig struct {
	ReorgToBlock int64 `koanf:"reorg-to-block"`
}

var DefaultDangerousConfig = DangerousConfig{
	ReorgToBlock: -1,
}

func DangerousConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int64(prefix+".reorg-to-block", DefaultDangerousConfig.ReorgToBlock, "DANGEROUS! forces a reorg to an old block height. To be used for testing only. -1 to disable")
}

type Config struct {
	ParentChainReader headerreader.Config              `koanf:"parent-chain-reader" reload:"hot"`
	Sequencer         SequencerConfig                  `koanf:"sequencer" reload:"hot"`
	RecordingDatabase arbitrum.RecordingDatabaseConfig `koanf:"recording-database"`
	TxPreChecker      TxPreCheckerConfig               `koanf:"tx-pre-checker" reload:"hot"`
	Forwarder         ForwarderConfig                  `koanf:"forwarder"`
	ForwardingTarget  string                           `koanf:"forwarding-target"`
	Caching           CachingConfig                    `koanf:"caching"`
	RPC               arbitrum.Config                  `koanf:"rpc"`
	TxLookupLimit     uint64                           `koanf:"tx-lookup-limit"`
	Dangerous         DangerousConfig                  `koanf:"dangerous"`

	forwardingTarget string
}

func (c *Config) Validate() error {
	if err := c.Sequencer.Validate(); err != nil {
		return err
	}
	if !c.Sequencer.Enable && c.ForwardingTarget == "" {
		return errors.New("ForwardingTarget not set and not sequencer (can use \"null\")")
	}
	if c.ForwardingTarget == "null" {
		c.forwardingTarget = ""
	} else {
		c.forwardingTarget = c.ForwardingTarget
	}
	if c.forwardingTarget != "" && c.Sequencer.Enable {
		return errors.New("ForwardingTarget set and sequencer enabled")
	}
	return nil
}

func ConfigAddOptions(prefix string, f *flag.FlagSet) {
	arbitrum.ConfigAddOptions(prefix+".rpc", f)
	SequencerConfigAddOptions(prefix+".sequencer", f)
	headerreader.AddOptions(prefix+".parent-chain-reader", f)
	arbitrum.RecordingDatabaseConfigAddOptions(prefix+".recording-database", f)
	f.String(prefix+".forwarding-target", ConfigDefault.ForwardingTarget, "transaction forwarding target URL, or \"null\" to disable forwarding (iff not sequencer)")
	AddOptionsForNodeForwarderConfig(prefix+".forwarder", f)
	TxPreCheckerConfigAddOptions(prefix+".tx-pre-checker", f)
	CachingConfigAddOptions(prefix+".caching", f)
	f.Uint64(prefix+".tx-lookup-limit", ConfigDefault.TxLookupLimit, "retain the ability to lookup transactions by hash for the past N blocks (0 = all blocks)")
	DangerousConfigAddOptions(prefix+".dangerous", f)
}

var ConfigDefault = Config{
	RPC:               arbitrum.DefaultConfig,
	Sequencer:         DefaultSequencerConfig,
	ParentChainReader: headerreader.DefaultConfig,
	RecordingDatabase: arbitrum.DefaultRecordingDatabaseConfig,
	ForwardingTarget:  "",
	TxPreChecker:      DefaultTxPreCheckerConfig,
	TxLookupLimit:     126_230_400, // 1 year at 4 blocks per second
	Caching:           DefaultCachingConfig,
	Dangerous:         DefaultDangerousConfig,
	Forwarder:         DefaultNodeForwarderConfig,
}

func ConfigDefaultNonSequencerTest() *Config {
	config := ConfigDefault
	config.ParentChainReader = headerreader.TestConfig
	config.Sequencer.Enable = false
	config.Forwarder = DefaultTestForwarderConfig
	config.ForwardingTarget = "null"

	_ = config.Validate()

	return &config
}

func ConfigDefaultTest() *Config {
	config := ConfigDefault
	config.Sequencer = TestSequencerConfig
	config.ForwardingTarget = "null"
	config.ParentChainReader = headerreader.TestConfig

	_ = config.Validate()

	return &config
}

type ConfigFetcher func() *Config

type ExecutionNode struct {
	ChainDB           ethdb.Database
	Backend           *arbitrum.Backend
	FilterSystem      *filters.FilterSystem
	ArbInterface      *ArbInterface
	ExecEngine        *ExecutionEngine
	Recorder          *BlockRecorder
	Sequencer         *Sequencer // either nil or same as TxPublisher
	TxPublisher       TransactionPublisher
	ConfigFetcher     ConfigFetcher
	ParentChainReader *headerreader.HeaderReader
	started           atomic.Bool
}

func CreateExecutionNode(
	ctx context.Context,
	stack *node.Node,
	chainDB ethdb.Database,
	l2BlockChain *core.BlockChain,
	l1client arbutil.L1Interface,
	configFetcher ConfigFetcher,
) (*ExecutionNode, error) {
	config := configFetcher()
	execEngine, err := NewExecutionEngine(l2BlockChain)
	if err != nil {
		return nil, err
	}
	recorder := NewBlockRecorder(&config.RecordingDatabase, execEngine, chainDB)
	var txPublisher TransactionPublisher
	var sequencer *Sequencer

	var parentChainReader *headerreader.HeaderReader
	if l1client != nil && !reflect.ValueOf(l1client).IsNil() {
		arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1client)
		parentChainReader, err = headerreader.New(ctx, l1client, func() *headerreader.Config { return &configFetcher().ParentChainReader }, arbSys)
		if err != nil {
			return nil, err
		}
	}

	if config.Sequencer.Enable {
		seqConfigFetcher := func() *SequencerConfig { return &configFetcher().Sequencer }
		sequencer, err = NewSequencer(execEngine, parentChainReader, seqConfigFetcher)
		if err != nil {
			return nil, err
		}
		txPublisher = sequencer
	} else {
		if config.Forwarder.RedisUrl != "" {
			txPublisher = NewRedisTxForwarder(config.forwardingTarget, &config.Forwarder)
		} else if config.forwardingTarget == "" {
			txPublisher = NewTxDropper()
		} else {
			txPublisher = NewForwarder(config.forwardingTarget, &config.Forwarder)
		}
	}

	txprecheckConfigFetcher := func() *TxPreCheckerConfig { return &configFetcher().TxPreChecker }

	txPublisher = NewTxPreChecker(txPublisher, l2BlockChain, txprecheckConfigFetcher)
	arbInterface, err := NewArbInterface(execEngine, txPublisher)
	if err != nil {
		return nil, err
	}
	filterConfig := filters.Config{
		LogCacheSize: config.RPC.FilterLogCacheSize,
		Timeout:      config.RPC.FilterTimeout,
	}
	backend, filterSystem, err := arbitrum.NewBackend(stack, &config.RPC, chainDB, arbInterface, filterConfig)
	if err != nil {
		return nil, err
	}

	apis := []rpc.API{{
		Namespace: "arb",
		Version:   "1.0",
		Service:   NewArbAPI(txPublisher),
		Public:    false,
	}}
	apis = append(apis, rpc.API{
		Namespace: "arbdebug",
		Version:   "1.0",
		Service: NewArbDebugAPI(
			l2BlockChain,
			config.RPC.ArbDebug.BlockRangeBound,
			config.RPC.ArbDebug.TimeoutQueueBound,
		),
		Public: false,
	})
	apis = append(apis, rpc.API{
		Namespace: "arbtrace",
		Version:   "1.0",
		Service: NewArbTraceForwarderAPI(
			config.RPC.ClassicRedirect,
			config.RPC.ClassicRedirectTimeout,
		),
		Public: false,
	})
	apis = append(apis, rpc.API{
		Namespace: "debug",
		Service:   eth.NewDebugAPI(eth.NewArbEthereum(l2BlockChain, chainDB)),
		Public:    false,
	})

	stack.RegisterAPIs(apis)

	return &ExecutionNode{
		ChainDB:           chainDB,
		Backend:           backend,
		FilterSystem:      filterSystem,
		ArbInterface:      arbInterface,
		ExecEngine:        execEngine,
		Recorder:          recorder,
		Sequencer:         sequencer,
		TxPublisher:       txPublisher,
		ConfigFetcher:     configFetcher,
		ParentChainReader: parentChainReader,
	}, nil

}

func (n *ExecutionNode) Initialize(ctx context.Context, arbnode interface{}, sync arbitrum.SyncProgressBackend) error {
	n.ArbInterface.Initialize(n)
	err := n.Backend.Start()
	if err != nil {
		return fmt.Errorf("error starting geth backend: %w", err)
	}
	err = n.TxPublisher.Initialize(ctx)
	if err != nil {
		return fmt.Errorf("error initializing transaction publisher: %w", err)
	}
	err = n.Backend.APIBackend().SetSyncBackend(sync)
	if err != nil {
		return fmt.Errorf("error setting sync backend: %w", err)
	}
	return nil
}

// not thread safe
func (n *ExecutionNode) Start(ctx context.Context) error {
	if n.started.Swap(true) {
		return errors.New("already started")
	}
	// TODO after separation
	// err := n.Stack.Start()
	// if err != nil {
	// 	return fmt.Errorf("error starting geth stack: %w", err)
	// }
	n.ExecEngine.Start(ctx)
	err := n.TxPublisher.Start(ctx)
	if err != nil {
		return fmt.Errorf("error starting transaction puiblisher: %w", err)
	}
	if n.ParentChainReader != nil {
		n.ParentChainReader.Start(ctx)
	}
	return nil
}

func (n *ExecutionNode) StopAndWait() {
	if !n.started.Load() {
		return
	}
	// TODO after separation
	// n.Stack.StopRPC() // does nothing if not running
	if n.TxPublisher.Started() {
		n.TxPublisher.StopAndWait()
	}
	n.Recorder.OrderlyShutdown()
	if n.ParentChainReader != nil && n.ParentChainReader.Started() {
		n.ParentChainReader.StopAndWait()
	}
	if n.ExecEngine.Started() {
		n.ExecEngine.StopAndWait()
	}
	n.ArbInterface.BlockChain().Stop() // does nothing if not running
	if err := n.Backend.Stop(); err != nil {
		log.Error("backend stop", "err", err)
	}
	// TODO after separation
	// if err := n.Stack.Close(); err != nil {
	// 	log.Error("error on stak close", "err", err)
	// }
}

func (n *ExecutionNode) DigestMessage(num arbutil.MessageIndex, msg *arbostypes.MessageWithMetadata) error {
	return n.ExecEngine.DigestMessage(num, msg)
}
func (n *ExecutionNode) Reorg(count arbutil.MessageIndex, newMessages []arbostypes.MessageWithMetadata, oldMessages []*arbostypes.MessageWithMetadata) error {
	return n.ExecEngine.Reorg(count, newMessages, oldMessages)
}
func (n *ExecutionNode) HeadMessageNumber() (arbutil.MessageIndex, error) {
	return n.ExecEngine.HeadMessageNumber()
}
func (n *ExecutionNode) HeadMessageNumberSync(t *testing.T) (arbutil.MessageIndex, error) {
	return n.ExecEngine.HeadMessageNumberSync(t)
}
func (n *ExecutionNode) NextDelayedMessageNumber() (uint64, error) {
	return n.ExecEngine.NextDelayedMessageNumber()
}
func (n *ExecutionNode) SequenceDelayedMessage(message *arbostypes.L1IncomingMessage, delayedSeqNum uint64) error {
	return n.ExecEngine.SequenceDelayedMessage(message, delayedSeqNum)
}
func (n *ExecutionNode) ResultAtPos(pos arbutil.MessageIndex) (*execution.MessageResult, error) {
	return n.ExecEngine.ResultAtPos(pos)
}

func (n *ExecutionNode) RecordBlockCreation(
	ctx context.Context,
	pos arbutil.MessageIndex,
	msg *arbostypes.MessageWithMetadata,
) (*execution.RecordResult, error) {
	return n.Recorder.RecordBlockCreation(ctx, pos, msg)
}
func (n *ExecutionNode) MarkValid(pos arbutil.MessageIndex, resultHash common.Hash) {
	n.Recorder.MarkValid(pos, resultHash)
}
func (n *ExecutionNode) PrepareForRecord(ctx context.Context, start, end arbutil.MessageIndex) error {
	return n.Recorder.PrepareForRecord(ctx, start, end)
}

func (n *ExecutionNode) Pause() {
	if n.Sequencer != nil {
		n.Sequencer.Pause()
	}
}
func (n *ExecutionNode) Activate() {
	if n.Sequencer != nil {
		n.Sequencer.Activate()
	}
}
func (n *ExecutionNode) ForwardTo(url string) error {
	if n.Sequencer != nil {
		return n.Sequencer.ForwardTo(url)
	} else {
		return errors.New("forwardTo not supported - sequencer not active")
	}
}
func (n *ExecutionNode) SetTransactionStreamer(streamer execution.TransactionStreamer) {
	n.ExecEngine.SetTransactionStreamer(streamer)
}
func (n *ExecutionNode) MessageIndexToBlockNumber(messageNum arbutil.MessageIndex) uint64 {
	return n.ExecEngine.MessageIndexToBlockNumber(messageNum)
}

func (n *ExecutionNode) Maintenance() error {
	return n.ChainDB.Compact(nil, nil)
}

'''
'''--- execution/gethexec/sequencer.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethexec

import (
	"context"
	"errors"
	"fmt"
	"math"
	"math/big"
	"runtime/debug"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/headerreader"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/txpool"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

var (
	sequencerBacklogGauge                   = metrics.NewRegisteredGauge("arb/sequencer/backlog", nil)
	nonceCacheHitCounter                    = metrics.NewRegisteredCounter("arb/sequencer/noncecache/hit", nil)
	nonceCacheMissCounter                   = metrics.NewRegisteredCounter("arb/sequencer/noncecache/miss", nil)
	nonceCacheRejectedCounter               = metrics.NewRegisteredCounter("arb/sequencer/noncecache/rejected", nil)
	nonceCacheClearedCounter                = metrics.NewRegisteredCounter("arb/sequencer/noncecache/cleared", nil)
	nonceFailureCacheSizeGauge              = metrics.NewRegisteredGauge("arb/sequencer/noncefailurecache/size", nil)
	nonceFailureCacheOverflowCounter        = metrics.NewRegisteredGauge("arb/sequencer/noncefailurecache/overflow", nil)
	blockCreationTimer                      = metrics.NewRegisteredTimer("arb/sequencer/block/creation", nil)
	successfulBlocksCounter                 = metrics.NewRegisteredCounter("arb/sequencer/block/successful", nil)
	conditionalTxRejectedBySequencerCounter = metrics.NewRegisteredCounter("arb/sequencer/condtionaltx/rejected", nil)
	conditionalTxAcceptedBySequencerCounter = metrics.NewRegisteredCounter("arb/sequencer/condtionaltx/accepted", nil)
)

type SequencerConfig struct {
	Enable                      bool            `koanf:"enable"`
	MaxBlockSpeed               time.Duration   `koanf:"max-block-speed" reload:"hot"`
	MaxRevertGasReject          uint64          `koanf:"max-revert-gas-reject" reload:"hot"`
	MaxAcceptableTimestampDelta time.Duration   `koanf:"max-acceptable-timestamp-delta" reload:"hot"`
	SenderWhitelist             string          `koanf:"sender-whitelist"`
	Forwarder                   ForwarderConfig `koanf:"forwarder"`
	QueueSize                   int             `koanf:"queue-size"`
	QueueTimeout                time.Duration   `koanf:"queue-timeout" reload:"hot"`
	NonceCacheSize              int             `koanf:"nonce-cache-size" reload:"hot"`
	MaxTxDataSize               int             `koanf:"max-tx-data-size" reload:"hot"`
	NonceFailureCacheSize       int             `koanf:"nonce-failure-cache-size" reload:"hot"`
	NonceFailureCacheExpiry     time.Duration   `koanf:"nonce-failure-cache-expiry" reload:"hot"`
}

func (c *SequencerConfig) Validate() error {
	entries := strings.Split(c.SenderWhitelist, ",")
	for _, address := range entries {
		if len(address) == 0 {
			continue
		}
		if !common.IsHexAddress(address) {
			return fmt.Errorf("sequencer sender whitelist entry \"%v\" is not a valid address", address)
		}
	}
	return nil
}

type SequencerConfigFetcher func() *SequencerConfig

var DefaultSequencerConfig = SequencerConfig{
	Enable:                      false,
	MaxBlockSpeed:               time.Millisecond * 250,
	MaxRevertGasReject:          params.TxGas + 10000,
	MaxAcceptableTimestampDelta: time.Hour,
	Forwarder:                   DefaultSequencerForwarderConfig,
	QueueSize:                   1024,
	QueueTimeout:                time.Second * 12,
	NonceCacheSize:              1024,
	// 95% of the default batch poster limit, leaving 5KB for headers and such
	// This default is overridden for L3 chains in applyChainParameters in cmd/nitro/nitro.go
	MaxTxDataSize:           95000,
	NonceFailureCacheSize:   1024,
	NonceFailureCacheExpiry: time.Second,
}

var TestSequencerConfig = SequencerConfig{
	Enable:                      true,
	MaxBlockSpeed:               time.Millisecond * 10,
	MaxRevertGasReject:          params.TxGas + 10000,
	MaxAcceptableTimestampDelta: time.Hour,
	SenderWhitelist:             "",
	Forwarder:                   DefaultTestForwarderConfig,
	QueueSize:                   128,
	QueueTimeout:                time.Second * 5,
	NonceCacheSize:              4,
	MaxTxDataSize:               95000,
	NonceFailureCacheSize:       1024,
	NonceFailureCacheExpiry:     time.Second,
}

func SequencerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultSequencerConfig.Enable, "act and post to l1 as sequencer")
	f.Duration(prefix+".max-block-speed", DefaultSequencerConfig.MaxBlockSpeed, "minimum delay between blocks (sets a maximum speed of block production)")
	f.Uint64(prefix+".max-revert-gas-reject", DefaultSequencerConfig.MaxRevertGasReject, "maximum gas executed in a revert for the sequencer to reject the transaction instead of posting it (anti-DOS)")
	f.Duration(prefix+".max-acceptable-timestamp-delta", DefaultSequencerConfig.MaxAcceptableTimestampDelta, "maximum acceptable time difference between the local time and the latest L1 block's timestamp")
	f.String(prefix+".sender-whitelist", DefaultSequencerConfig.SenderWhitelist, "comma separated whitelist of authorized senders (if empty, everyone is allowed)")
	AddOptionsForSequencerForwarderConfig(prefix+".forwarder", f)
	f.Int(prefix+".queue-size", DefaultSequencerConfig.QueueSize, "size of the pending tx queue")
	f.Duration(prefix+".queue-timeout", DefaultSequencerConfig.QueueTimeout, "maximum amount of time transaction can wait in queue")
	f.Int(prefix+".nonce-cache-size", DefaultSequencerConfig.NonceCacheSize, "size of the tx sender nonce cache")
	f.Int(prefix+".max-tx-data-size", DefaultSequencerConfig.MaxTxDataSize, "maximum transaction size the sequencer will accept")
	f.Int(prefix+".nonce-failure-cache-size", DefaultSequencerConfig.NonceFailureCacheSize, "number of transactions with too high of a nonce to keep in memory while waiting for their predecessor")
	f.Duration(prefix+".nonce-failure-cache-expiry", DefaultSequencerConfig.NonceFailureCacheExpiry, "maximum amount of time to wait for a predecessor before rejecting a tx with nonce too high")
}

type txQueueItem struct {
	tx              *types.Transaction
	options         *arbitrum_types.ConditionalOptions
	resultChan      chan<- error
	returnedResult  bool
	ctx             context.Context
	firstAppearance time.Time
}

func (i *txQueueItem) returnResult(err error) {
	if i.returnedResult {
		log.Error("attempting to return result to already finished queue item", "err", err)
		return
	}
	i.returnedResult = true
	i.resultChan <- err
	close(i.resultChan)
}

type nonceCache struct {
	cache *containers.LruCache[common.Address, uint64]
	block common.Hash
	dirty *types.Header
}

func newNonceCache(size int) *nonceCache {
	return &nonceCache{
		cache: containers.NewLruCache[common.Address, uint64](size),
		block: common.Hash{},
		dirty: nil,
	}
}

func (c *nonceCache) matches(header *types.Header) bool {
	if c.dirty != nil {
		// Note, even though the of the header changes, c.dirty points to the
		// same header, hence hashes will be the same and this check will pass.
		return headerreader.HeadersEqual(c.dirty, header)
	}
	return c.block == header.ParentHash
}

func (c *nonceCache) Reset(block common.Hash) {
	if c.cache.Len() > 0 {
		nonceCacheClearedCounter.Inc(1)
	}
	c.cache.Clear()
	c.block = block
	c.dirty = nil
}

func (c *nonceCache) BeginNewBlock() {
	if c.dirty != nil {
		c.Reset(common.Hash{})
	}
}

func (c *nonceCache) Get(header *types.Header, statedb *state.StateDB, addr common.Address) uint64 {
	if !c.matches(header) {
		c.Reset(header.ParentHash)
	}
	nonce, ok := c.cache.Get(addr)
	if ok {
		nonceCacheHitCounter.Inc(1)
		return nonce
	}
	nonceCacheMissCounter.Inc(1)
	nonce = statedb.GetNonce(addr)
	c.cache.Add(addr, nonce)
	return nonce
}

func (c *nonceCache) Update(header *types.Header, addr common.Address, nonce uint64) {
	if !c.matches(header) {
		c.Reset(header.ParentHash)
	}
	c.dirty = header
	c.cache.Add(addr, nonce)
}

func (c *nonceCache) Finalize(block *types.Block) {
	// Note: we don't use c.matches here because the header will have changed
	if c.block == block.ParentHash() {
		c.block = block.Hash()
		c.dirty = nil
	} else {
		c.Reset(block.Hash())
	}
}

func (c *nonceCache) Caching() bool {
	return c.cache != nil && c.cache.Size() > 0
}

func (c *nonceCache) Resize(newSize int) {
	c.cache.Resize(newSize)
}

type addressAndNonce struct {
	address common.Address
	nonce   uint64
}

type nonceFailure struct {
	queueItem txQueueItem
	nonceErr  error
	expiry    time.Time
	revived   bool
}

type nonceFailureCache struct {
	*containers.LruCache[addressAndNonce, *nonceFailure]
	getExpiry func() time.Duration
}

func (c nonceFailureCache) Contains(err NonceError) bool {
	key := addressAndNonce{err.sender, err.txNonce}
	return c.LruCache.Contains(key)
}

func (c nonceFailureCache) Add(err NonceError, queueItem txQueueItem) {
	expiry := queueItem.firstAppearance.Add(c.getExpiry())
	if c.Contains(err) || time.Now().After(expiry) {
		queueItem.returnResult(err)
		return
	}
	key := addressAndNonce{err.sender, err.txNonce}
	val := &nonceFailure{
		queueItem: queueItem,
		nonceErr:  err,
		expiry:    expiry,
		revived:   false,
	}
	evicted := c.LruCache.Add(key, val)
	if evicted {
		nonceFailureCacheOverflowCounter.Inc(1)
	}
}

type Sequencer struct {
	stopwaiter.StopWaiter

	execEngine      *ExecutionEngine
	txQueue         chan txQueueItem
	txRetryQueue    containers.Queue[txQueueItem]
	l1Reader        *headerreader.HeaderReader
	config          SequencerConfigFetcher
	senderWhitelist map[common.Address]struct{}
	nonceCache      *nonceCache
	nonceFailures   *nonceFailureCache
	onForwarderSet  chan struct{}

	L1BlockAndTimeMutex sync.Mutex
	l1BlockNumber       uint64
	l1Timestamp         uint64

	// activeMutex manages pauseChan (pauses execution) and forwarder
	// at most one of these is non-nil at any given time
	// both are nil for the active sequencer
	activeMutex sync.Mutex
	pauseChan   chan struct{}
	forwarder   *TxForwarder
}

func NewSequencer(execEngine *ExecutionEngine, l1Reader *headerreader.HeaderReader, configFetcher SequencerConfigFetcher) (*Sequencer, error) {
	config := configFetcher()
	if err := config.Validate(); err != nil {
		return nil, err
	}
	senderWhitelist := make(map[common.Address]struct{})
	entries := strings.Split(config.SenderWhitelist, ",")
	for _, address := range entries {
		if len(address) == 0 {
			continue
		}
		senderWhitelist[common.HexToAddress(address)] = struct{}{}
	}
	s := &Sequencer{
		execEngine:      execEngine,
		txQueue:         make(chan txQueueItem, config.QueueSize),
		l1Reader:        l1Reader,
		config:          configFetcher,
		senderWhitelist: senderWhitelist,
		nonceCache:      newNonceCache(config.NonceCacheSize),
		l1BlockNumber:   0,
		l1Timestamp:     0,
		pauseChan:       nil,
		onForwarderSet:  make(chan struct{}, 1),
	}
	s.nonceFailures = &nonceFailureCache{
		containers.NewLruCacheWithOnEvict(config.NonceCacheSize, s.onNonceFailureEvict),
		func() time.Duration { return configFetcher().NonceFailureCacheExpiry },
	}
	execEngine.EnableReorgSequencing()
	return s, nil
}

func (s *Sequencer) onNonceFailureEvict(_ addressAndNonce, failure *nonceFailure) {
	if failure.revived {
		return
	}
	queueItem := failure.queueItem
	err := queueItem.ctx.Err()
	if err != nil {
		queueItem.returnResult(err)
		return
	}
	_, forwarder := s.GetPauseAndForwarder()
	if forwarder != nil {
		// We might not have gotten the predecessor tx because our forwarder did. Let's try there instead.
		// We run this in a background goroutine because LRU eviction needs to be quick.
		// We use an untracked thread for a few reasons:
		//   - It's guaranteed to run even when stopped (we need to return *some* result).
		//   - It acquires mutexes and this might need to happen a lot.
		//   - We don't need the context because queueItem has its own.
		//   - The RPC handler is on a separate StopWaiter anyways -- we should respect its context.
		s.LaunchUntrackedThread(func() {
			err = forwarder.PublishTransaction(queueItem.ctx, queueItem.tx, queueItem.options)
			queueItem.returnResult(err)
		})
	} else {
		queueItem.returnResult(failure.nonceErr)
	}
}

// ctxWithTimeout is like context.WithTimeout except a timeout of 0 means unlimited instead of instantly expired.
func ctxWithTimeout(ctx context.Context, timeout time.Duration) (context.Context, context.CancelFunc) {
	if timeout == time.Duration(0) {
		return context.WithCancel(ctx)
	}
	return context.WithTimeout(ctx, timeout)
}

func (s *Sequencer) PublishTransaction(parentCtx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	sequencerBacklogGauge.Inc(1)
	defer sequencerBacklogGauge.Dec(1)

	_, forwarder := s.GetPauseAndForwarder()
	if forwarder != nil {
		err := forwarder.PublishTransaction(parentCtx, tx, options)
		if !errors.Is(err, ErrNoSequencer) {
			return err
		}
	}

	if len(s.senderWhitelist) > 0 {
		signer := types.LatestSigner(s.execEngine.bc.Config())
		sender, err := types.Sender(signer, tx)
		if err != nil {
			return err
		}
		_, authorized := s.senderWhitelist[sender]
		if !authorized {
			return errors.New("transaction sender is not on the whitelist")
		}
	}
	if tx.Type() >= types.ArbitrumDepositTxType || tx.Type() == types.BlobTxType {
		// Should be unreachable for Arbitrum types due to UnmarshalBinary not accepting Arbitrum internal txs
		// and we want to disallow BlobTxType since Arbitrum doesn't support EIP-4844 txs yet.
		return types.ErrTxTypeNotSupported
	}

	queueTimeout := s.config().QueueTimeout
	queueCtx, cancelFunc := ctxWithTimeout(parentCtx, queueTimeout)
	defer cancelFunc()

	// Just to be safe, make sure we don't run over twice the queue timeout
	abortCtx, cancel := ctxWithTimeout(parentCtx, queueTimeout*2)
	defer cancel()

	resultChan := make(chan error, 1)
	queueItem := txQueueItem{
		tx,
		options,
		resultChan,
		false,
		queueCtx,
		time.Now(),
	}
	select {
	case s.txQueue <- queueItem:
	case <-queueCtx.Done():
		return queueCtx.Err()
	}

	select {
	case res := <-resultChan:
		return res
	case <-abortCtx.Done():
		// We use abortCtx here and not queueCtx, because the QueueTimeout only applies to the background queue.
		// We want to give the background queue as much time as possible to make a response.
		err := abortCtx.Err()
		if parentCtx.Err() == nil {
			// If we've hit the abort deadline (as opposed to parentCtx being canceled), something went wrong.
			log.Warn("Transaction sequencing hit abort deadline", "err", err, "submittedAt", queueItem.firstAppearance, "queueTimeout", queueTimeout, "txHash", tx.Hash())
		}
		return err
	}
}

func (s *Sequencer) preTxFilter(_ *params.ChainConfig, header *types.Header, statedb *state.StateDB, _ *arbosState.ArbosState, tx *types.Transaction, options *arbitrum_types.ConditionalOptions, sender common.Address, l1Info *arbos.L1Info) error {
	if s.nonceCache.Caching() {
		stateNonce := s.nonceCache.Get(header, statedb, sender)
		err := MakeNonceError(sender, tx.Nonce(), stateNonce)
		if err != nil {
			nonceCacheRejectedCounter.Inc(1)
			return err
		}
	}
	if options != nil {
		err := options.Check(l1Info.L1BlockNumber(), header.Time, statedb)
		if err != nil {
			conditionalTxRejectedBySequencerCounter.Inc(1)
			return err
		}
		conditionalTxAcceptedBySequencerCounter.Inc(1)
	}
	return nil
}

func (s *Sequencer) postTxFilter(header *types.Header, _ *arbosState.ArbosState, tx *types.Transaction, sender common.Address, dataGas uint64, result *core.ExecutionResult) error {
	if result.Err != nil && result.UsedGas > dataGas && result.UsedGas-dataGas <= s.config().MaxRevertGasReject {
		return arbitrum.NewRevertReason(result)
	}
	newNonce := tx.Nonce() + 1
	s.nonceCache.Update(header, sender, newNonce)
	newAddrAndNonce := addressAndNonce{sender, newNonce}
	nonceFailure, haveNonceFailure := s.nonceFailures.Get(newAddrAndNonce)
	if haveNonceFailure {
		nonceFailure.revived = true // prevent the expiry hook from taking effect
		s.nonceFailures.Remove(newAddrAndNonce)
		// Immediately check if the transaction submission has been canceled
		err := nonceFailure.queueItem.ctx.Err()
		if err != nil {
			nonceFailure.queueItem.returnResult(err)
		} else {
			// Add this transaction (whose nonce is now correct) back into the queue
			s.txRetryQueue.Push(nonceFailure.queueItem)
		}
	}
	return nil
}

func (s *Sequencer) CheckHealth(ctx context.Context) error {
	pauseChan, forwarder := s.GetPauseAndForwarder()
	if forwarder != nil {
		return forwarder.CheckHealth(ctx)
	}
	if pauseChan != nil {
		return nil
	}
	return s.execEngine.streamer.ExpectChosenSequencer()
}

func (s *Sequencer) ForwardTarget() string {
	s.activeMutex.Lock()
	defer s.activeMutex.Unlock()
	if s.forwarder == nil {
		return ""
	}
	return s.forwarder.target
}

func (s *Sequencer) ForwardTo(url string) error {
	s.activeMutex.Lock()
	defer s.activeMutex.Unlock()
	if s.forwarder != nil {
		if s.forwarder.target == url {
			log.Warn("attempted to update sequencer forward target with existing target", "url", url)
			return nil
		}
		s.forwarder.Disable()
	}
	s.forwarder = NewForwarder(url, &s.config().Forwarder)
	err := s.forwarder.Initialize(s.GetContext())
	if err != nil {
		log.Error("failed to set forward agent", "err", err)
		s.forwarder = nil
	}
	if s.pauseChan != nil {
		close(s.pauseChan)
		s.pauseChan = nil
	}
	if err == nil {
		// If createBlocks is waiting for a new queue item, notify it that it needs to clear the nonceFailures.
		select {
		case s.onForwarderSet <- struct{}{}:
		default:
		}
	}
	return err
}

func (s *Sequencer) Activate() {
	s.activeMutex.Lock()
	defer s.activeMutex.Unlock()
	if s.forwarder != nil {
		s.forwarder.Disable()
		s.forwarder = nil
	}
	if s.pauseChan != nil {
		close(s.pauseChan)
		s.pauseChan = nil
	}
}

func (s *Sequencer) Pause() {
	s.activeMutex.Lock()
	defer s.activeMutex.Unlock()
	if s.forwarder != nil {
		s.forwarder.Disable()
		s.forwarder = nil
	}
	if s.pauseChan == nil {
		s.pauseChan = make(chan struct{})
	}
}

var ErrNoSequencer = errors.New("sequencer temporarily not available")

func (s *Sequencer) GetPauseAndForwarder() (chan struct{}, *TxForwarder) {
	s.activeMutex.Lock()
	defer s.activeMutex.Unlock()
	return s.pauseChan, s.forwarder
}

// only called from createBlock, may be paused
func (s *Sequencer) handleInactive(ctx context.Context, queueItems []txQueueItem) bool {
	var forwarder *TxForwarder
	for {
		var pause chan struct{}
		pause, forwarder = s.GetPauseAndForwarder()
		if pause == nil {
			if forwarder == nil {
				return false
			}
			// if forwarding: jump to next loop
			break
		}
		// if paused: wait till unpaused
		select {
		case <-ctx.Done():
			return true
		case <-pause:
		}
	}
	publishResults := make(chan *txQueueItem, len(queueItems))
	for _, item := range queueItems {
		item := item
		go func() {
			res := forwarder.PublishTransaction(item.ctx, item.tx, item.options)
			if errors.Is(res, ErrNoSequencer) {
				publishResults <- &item
			} else {
				publishResults <- nil
				item.returnResult(res)
			}
		}()
	}
	for range queueItems {
		remainingItem := <-publishResults
		if remainingItem != nil {
			s.txRetryQueue.Push(*remainingItem)
		}
	}
	// Evict any leftover nonce failures, forwarding them
	s.nonceFailures.Clear()
	return true
}

var sequencerInternalError = errors.New("sequencer internal error")

func (s *Sequencer) makeSequencingHooks() *arbos.SequencingHooks {
	return &arbos.SequencingHooks{
		PreTxFilter:             s.preTxFilter,
		PostTxFilter:            s.postTxFilter,
		DiscardInvalidTxsEarly:  true,
		TxErrors:                []error{},
		ConditionalOptionsForTx: nil,
	}
}

func (s *Sequencer) expireNonceFailures() *time.Timer {
	defer nonceFailureCacheSizeGauge.Update(int64(s.nonceFailures.Len()))
	for {
		_, failure, ok := s.nonceFailures.GetOldest()
		if !ok {
			return nil
		}
		untilExpiry := time.Until(failure.expiry)
		if untilExpiry > 0 {
			return time.NewTimer(untilExpiry)
		}
		s.nonceFailures.RemoveOldest()
	}
}

// There's no guarantee that returned tx nonces will be correct
func (s *Sequencer) precheckNonces(queueItems []txQueueItem) []txQueueItem {
	bc := s.execEngine.bc
	latestHeader := bc.CurrentBlock()
	latestState, err := bc.StateAt(latestHeader.Root)
	if err != nil {
		log.Error("failed to get current state to pre-check nonces", "err", err)
		return queueItems
	}
	nextHeaderNumber := arbmath.BigAdd(latestHeader.Number, common.Big1)
	signer := types.MakeSigner(bc.Config(), nextHeaderNumber, latestHeader.Time)
	outputQueueItems := make([]txQueueItem, 0, len(queueItems))
	var nextQueueItem *txQueueItem
	var queueItemsIdx int
	pendingNonces := make(map[common.Address]uint64)
	for {
		var queueItem txQueueItem
		if nextQueueItem != nil {
			queueItem = *nextQueueItem
			nextQueueItem = nil
		} else if queueItemsIdx < len(queueItems) {
			queueItem = queueItems[queueItemsIdx]
			queueItemsIdx++
		} else {
			break
		}
		tx := queueItem.tx
		sender, err := types.Sender(signer, tx)
		if err != nil {
			queueItem.returnResult(err)
			continue
		}
		stateNonce := s.nonceCache.Get(latestHeader, latestState, sender)
		pendingNonce, pending := pendingNonces[sender]
		if !pending {
			pendingNonce = stateNonce
		}
		txNonce := tx.Nonce()
		if txNonce == pendingNonce {
			pendingNonces[sender] = txNonce + 1
			nextKey := addressAndNonce{sender, txNonce + 1}
			revivingFailure, exists := s.nonceFailures.Get(nextKey)
			if exists {
				// This tx was the predecessor to one that had failed its nonce check
				// Re-enqueue the tx whose nonce should now be correct, unless it expired
				revivingFailure.revived = true
				s.nonceFailures.Remove(nextKey)
				err := revivingFailure.queueItem.ctx.Err()
				if err != nil {
					revivingFailure.queueItem.returnResult(err)
				} else {
					nextQueueItem = &revivingFailure.queueItem
				}
			}
		} else if txNonce < stateNonce || txNonce > pendingNonce {
			// It's impossible for this tx to succeed so far,
			// because its nonce is lower than the state nonce
			// or higher than the highest tx nonce we've seen.
			err := MakeNonceError(sender, txNonce, stateNonce)
			if errors.Is(err, core.ErrNonceTooHigh) {
				var nonceError NonceError
				if !errors.As(err, &nonceError) {
					log.Warn("unreachable nonce error is not nonceError")
					continue
				}
				// Retry this transaction if its predecessor appears
				s.nonceFailures.Add(nonceError, queueItem)
				continue
			} else if err != nil {
				nonceCacheRejectedCounter.Inc(1)
				queueItem.returnResult(err)
				continue
			} else {
				log.Warn("unreachable nonce err == nil condition hit in precheckNonces")
			}
		}
		// If neither if condition was hit, then txNonce >= stateNonce && txNonce < pendingNonce
		// This tx might still go through if previous txs fail.
		// We'll include it in the output queue in case that happens.
		outputQueueItems = append(outputQueueItems, queueItem)
	}
	nonceFailureCacheSizeGauge.Update(int64(s.nonceFailures.Len()))
	return outputQueueItems
}

func (s *Sequencer) createBlock(ctx context.Context) (returnValue bool) {
	var queueItems []txQueueItem
	var totalBatchSize int

	defer func() {
		panicErr := recover()
		if panicErr != nil {
			log.Error("sequencer block creation panicked", "panic", panicErr, "backtrace", string(debug.Stack()))
			// Return an internal error to any queue items we were trying to process
			for _, item := range queueItems {
				if !item.returnedResult {
					item.returnResult(sequencerInternalError)
				}
			}
			// Wait for the MaxBlockSpeed until attempting to create a block again
			returnValue = true
		}
	}()
	defer nonceFailureCacheSizeGauge.Update(int64(s.nonceFailures.Len()))

	config := s.config()

	// Clear out old nonceFailures
	s.nonceFailures.Resize(config.NonceFailureCacheSize)
	nextNonceExpiryTimer := s.expireNonceFailures()
	defer func() {
		// We wrap this in a closure as to not cache the current value of nextNonceExpiryTimer
		if nextNonceExpiryTimer != nil {
			nextNonceExpiryTimer.Stop()
		}
	}()

	for {
		var queueItem txQueueItem
		if s.txRetryQueue.Len() > 0 {
			queueItem = s.txRetryQueue.Pop()
		} else if len(queueItems) == 0 {
			var nextNonceExpiryChan <-chan time.Time
			if nextNonceExpiryTimer != nil {
				nextNonceExpiryChan = nextNonceExpiryTimer.C
			}
			select {
			case queueItem = <-s.txQueue:
			case <-nextNonceExpiryChan:
				// No need to stop the previous timer since it already elapsed
				nextNonceExpiryTimer = s.expireNonceFailures()
				continue
			case <-s.onForwarderSet:
				// Make sure this notification isn't outdated
				_, forwarder := s.GetPauseAndForwarder()
				if forwarder != nil {
					s.nonceFailures.Clear()
				}
				continue
			case <-ctx.Done():
				return false
			}
		} else {
			done := false
			select {
			case queueItem = <-s.txQueue:
			default:
				done = true
			}
			if done {
				break
			}
		}
		err := queueItem.ctx.Err()
		if err != nil {
			queueItem.returnResult(err)
			continue
		}
		txBytes, err := queueItem.tx.MarshalBinary()
		if err != nil {
			queueItem.returnResult(err)
			continue
		}
		if len(txBytes) > config.MaxTxDataSize {
			// This tx is too large
			queueItem.returnResult(txpool.ErrOversizedData)
			continue
		}
		if totalBatchSize+len(txBytes) > config.MaxTxDataSize {
			// This tx would be too large to add to this batch
			s.txRetryQueue.Push(queueItem)
			// End the batch here to put this tx in the next one
			break
		}
		totalBatchSize += len(txBytes)
		queueItems = append(queueItems, queueItem)
	}

	s.nonceCache.Resize(config.NonceCacheSize) // Would probably be better in a config hook but this is basically free
	s.nonceCache.BeginNewBlock()
	queueItems = s.precheckNonces(queueItems)
	txes := make([]*types.Transaction, len(queueItems))
	hooks := s.makeSequencingHooks()
	hooks.ConditionalOptionsForTx = make([]*arbitrum_types.ConditionalOptions, len(queueItems))
	for i, queueItem := range queueItems {
		txes[i] = queueItem.tx
		hooks.ConditionalOptionsForTx[i] = queueItem.options
	}

	if s.handleInactive(ctx, queueItems) {
		return false
	}

	timestamp := time.Now().Unix()
	s.L1BlockAndTimeMutex.Lock()
	l1Block := s.l1BlockNumber
	l1Timestamp := s.l1Timestamp
	s.L1BlockAndTimeMutex.Unlock()

	if s.l1Reader != nil && (l1Block == 0 || math.Abs(float64(l1Timestamp)-float64(timestamp)) > config.MaxAcceptableTimestampDelta.Seconds()) {
		log.Error(
			"cannot sequence: unknown L1 block or L1 timestamp too far from local clock time",
			"l1Block", l1Block,
			"l1Timestamp", time.Unix(int64(l1Timestamp), 0),
			"localTimestamp", time.Unix(int64(timestamp), 0),
		)
		return false
	}

	header := &arbostypes.L1IncomingMessageHeader{
		Kind:        arbostypes.L1MessageType_L2Message,
		Poster:      l1pricing.BatchPosterAddress,
		BlockNumber: l1Block,
		Timestamp:   uint64(timestamp),
		RequestId:   nil,
		L1BaseFee:   nil,
	}

	start := time.Now()
	block, err := s.execEngine.SequenceTransactions(header, txes, hooks)
	elapsed := time.Since(start)
	blockCreationTimer.Update(elapsed)
	if elapsed >= time.Second*5 {
		var blockNum *big.Int
		if block != nil {
			blockNum = block.Number()
		}
		log.Warn("took over 5 seconds to sequence a block", "elapsed", elapsed, "numTxes", len(txes), "success", block != nil, "l2Block", blockNum)
	}
	if err == nil && len(hooks.TxErrors) != len(txes) {
		err = fmt.Errorf("unexpected number of error results: %v vs number of txes %v", len(hooks.TxErrors), len(txes))
	}
	if errors.Is(err, execution.ErrRetrySequencer) {
		log.Warn("error sequencing transactions", "err", err)
		// we changed roles
		// forward if we have where to
		if s.handleInactive(ctx, queueItems) {
			return false
		}
		// try to add back to queue otherwise
		for _, item := range queueItems {
			s.txRetryQueue.Push(item)
		}
		return false
	}
	if err != nil {
		if errors.Is(err, context.Canceled) {
			// thread closed. We'll later try to forward these messages.
			for _, item := range queueItems {
				s.txRetryQueue.Push(item)
			}
			return true // don't return failure to avoid retrying immediately
		}
		log.Error("error sequencing transactions", "err", err)
		for _, queueItem := range queueItems {
			queueItem.returnResult(err)
		}
		return false
	}

	if block != nil {
		successfulBlocksCounter.Inc(1)
		s.nonceCache.Finalize(block)
	}

	madeBlock := false
	for i, err := range hooks.TxErrors {
		if err == nil {
			madeBlock = true
		}
		queueItem := queueItems[i]
		if errors.Is(err, core.ErrGasLimitReached) {
			// There's not enough gas left in the block for this tx.
			if madeBlock {
				// There was already an earlier tx in the block; retry in a fresh block.
				s.txRetryQueue.Push(queueItem)
				continue
			}
		}
		if errors.Is(err, core.ErrIntrinsicGas) {
			// Strip additional information, as it's incorrect due to L1 data gas.
			err = core.ErrIntrinsicGas
		}
		var nonceError NonceError
		if errors.As(err, &nonceError) && nonceError.txNonce > nonceError.stateNonce {
			s.nonceFailures.Add(nonceError, queueItem)
			continue
		}
		queueItem.returnResult(err)
	}
	return madeBlock
}

func (s *Sequencer) updateLatestParentChainBlock(header *types.Header) {
	s.L1BlockAndTimeMutex.Lock()
	defer s.L1BlockAndTimeMutex.Unlock()

	l1BlockNumber := arbutil.ParentHeaderToL1BlockNumber(header)
	if header.Time > s.l1Timestamp || (header.Time == s.l1Timestamp && l1BlockNumber > s.l1BlockNumber) {
		s.l1Timestamp = header.Time
		s.l1BlockNumber = l1BlockNumber
	}
}

func (s *Sequencer) Initialize(ctx context.Context) error {
	if s.l1Reader == nil {
		return nil
	}

	header, err := s.l1Reader.LastHeader(ctx)
	if err != nil {
		return err
	}
	s.updateLatestParentChainBlock(header)
	return nil
}

func (s *Sequencer) Start(ctxIn context.Context) error {
	s.StopWaiter.Start(ctxIn, s)
	if s.l1Reader != nil {
		initialBlockNr := atomic.LoadUint64(&s.l1BlockNumber)
		if initialBlockNr == 0 {
			return errors.New("sequencer not initialized")
		}

		headerChan, cancel := s.l1Reader.Subscribe(false)

		s.LaunchThread(func(ctx context.Context) {
			defer cancel()
			for {
				select {
				case header, ok := <-headerChan:
					if !ok {
						return
					}
					s.updateLatestParentChainBlock(header)
				case <-ctx.Done():
					return
				}
			}
		})

	}

	s.CallIteratively(func(ctx context.Context) time.Duration {
		nextBlock := time.Now().Add(s.config().MaxBlockSpeed)
		madeBlock := s.createBlock(ctx)
		if madeBlock {
			// Note: this may return a negative duration, but timers are fine with that (they treat negative durations as 0).
			return time.Until(nextBlock)
		}
		// If we didn't make a block, try again immediately.
		return 0
	})

	return nil
}

func (s *Sequencer) StopAndWait() {
	s.StopWaiter.StopAndWait()
	if s.txRetryQueue.Len() == 0 && len(s.txQueue) == 0 && s.nonceFailures.Len() == 0 {
		return
	}
	// this usually means that coordinator's safe-shutdown-delay is too low
	log.Warn("Sequencer has queued items while shutting down", "txQueue", len(s.txQueue), "retryQueue", s.txRetryQueue.Len(), "nonceFailures", s.nonceFailures.Len())
	_, forwarder := s.GetPauseAndForwarder()
	if forwarder != nil {
		var wg sync.WaitGroup
	emptyqueues:
		for {
			var item txQueueItem
			source := ""
			if s.txRetryQueue.Len() > 0 {
				item = s.txRetryQueue.Pop()
				source = "retryQueue"
			} else if s.nonceFailures.Len() > 0 {
				_, failure, _ := s.nonceFailures.GetOldest()
				failure.revived = true
				item = failure.queueItem
				source = "nonceFailures"
				s.nonceFailures.RemoveOldest()
			} else {
				select {
				case item = <-s.txQueue:
					source = "txQueue"
				default:
					break emptyqueues
				}
			}
			wg.Add(1)
			go func() {
				defer wg.Done()
				err := forwarder.PublishTransaction(item.ctx, item.tx, item.options)
				if err != nil {
					log.Warn("failed to forward transaction while shutting down", "source", source, "err", err)
				}
			}()
		}
		wg.Wait()
	}
}

'''
'''--- execution/gethexec/tx_pre_checker.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethexec

import (
	"context"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	flag "github.com/spf13/pflag"
)

var (
	conditionalTxRejectedByTxPreCheckerCurrentStateCounter = metrics.NewRegisteredCounter("arb/txprechecker/condtionaltx/currentstate/rejected", nil)
	conditionalTxAcceptedByTxPreCheckerCurrentStateCounter = metrics.NewRegisteredCounter("arb/txprechecker/condtionaltx/currentstate/accepted", nil)
	conditionalTxRejectedByTxPreCheckerOldStateCounter     = metrics.NewRegisteredCounter("arb/txprechecker/condtionaltx/oldstate/rejected", nil)
	conditionalTxAcceptedByTxPreCheckerOldStateCounter     = metrics.NewRegisteredCounter("arb/txprechecker/condtionaltx/oldstate/accepted", nil)
)

const TxPreCheckerStrictnessNone uint = 0
const TxPreCheckerStrictnessAlwaysCompatible uint = 10
const TxPreCheckerStrictnessLikelyCompatible uint = 20
const TxPreCheckerStrictnessFullValidation uint = 30

type TxPreCheckerConfig struct {
	Strictness             uint  `koanf:"strictness" reload:"hot"`
	RequiredStateAge       int64 `koanf:"required-state-age" reload:"hot"`
	RequiredStateMaxBlocks uint  `koanf:"required-state-max-blocks" reload:"hot"`
}

type TxPreCheckerConfigFetcher func() *TxPreCheckerConfig

var DefaultTxPreCheckerConfig = TxPreCheckerConfig{
	Strictness:             TxPreCheckerStrictnessNone,
	RequiredStateAge:       2,
	RequiredStateMaxBlocks: 4,
}

func TxPreCheckerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint(prefix+".strictness", DefaultTxPreCheckerConfig.Strictness, "how strict to be when checking txs before forwarding them. 0 = accept anything, "+
		"10 = should never reject anything that'd succeed, 20 = likely won't reject anything that'd succeed, "+
		"30 = full validation which may reject txs that would succeed")
	f.Int64(prefix+".required-state-age", DefaultTxPreCheckerConfig.RequiredStateAge, "how long ago should the storage conditions from eth_SendRawTransactionConditional be true, 0 = don't check old state")
	f.Uint(prefix+".required-state-max-blocks", DefaultTxPreCheckerConfig.RequiredStateMaxBlocks, "maximum number of blocks to look back while looking for the <required-state-age> seconds old state, 0 = don't limit the search")
}

type TxPreChecker struct {
	TransactionPublisher
	bc     *core.BlockChain
	config TxPreCheckerConfigFetcher
}

func NewTxPreChecker(publisher TransactionPublisher, bc *core.BlockChain, config TxPreCheckerConfigFetcher) *TxPreChecker {
	return &TxPreChecker{
		TransactionPublisher: publisher,
		bc:                   bc,
		config:               config,
	}
}

type NonceError struct {
	sender     common.Address
	txNonce    uint64
	stateNonce uint64
}

func (e NonceError) Error() string {
	if e.txNonce < e.stateNonce {
		return fmt.Sprintf("%v: address %v, tx: %d state: %d", core.ErrNonceTooLow, e.sender, e.txNonce, e.stateNonce)
	}
	if e.txNonce > e.stateNonce {
		return fmt.Sprintf("%v: address %v, tx: %d state: %d", core.ErrNonceTooHigh, e.sender, e.txNonce, e.stateNonce)
	}
	// This should be unreachable
	return fmt.Sprintf("invalid nonce error for address %v nonce %v", e.sender, e.txNonce)
}

func (e NonceError) Unwrap() error {
	if e.txNonce < e.stateNonce {
		return core.ErrNonceTooLow
	}
	if e.txNonce > e.stateNonce {
		return core.ErrNonceTooHigh
	}
	// This should be unreachable
	return nil
}

func MakeNonceError(sender common.Address, txNonce uint64, stateNonce uint64) error {
	if txNonce == stateNonce {
		return nil
	}
	return NonceError{
		sender:     sender,
		txNonce:    txNonce,
		stateNonce: stateNonce,
	}
}

func PreCheckTx(bc *core.BlockChain, chainConfig *params.ChainConfig, header *types.Header, statedb *state.StateDB, arbos *arbosState.ArbosState, tx *types.Transaction, options *arbitrum_types.ConditionalOptions, config *TxPreCheckerConfig) error {
	if config.Strictness < TxPreCheckerStrictnessAlwaysCompatible {
		return nil
	}
	if tx.Gas() < params.TxGas {
		return core.ErrIntrinsicGas
	}
	sender, err := types.Sender(types.MakeSigner(chainConfig, header.Number, header.Time), tx)
	if err != nil {
		return err
	}
	baseFee := header.BaseFee
	if config.Strictness < TxPreCheckerStrictnessLikelyCompatible {
		baseFee, err = arbos.L2PricingState().MinBaseFeeWei()
		if err != nil {
			return err
		}
	}
	if arbmath.BigLessThan(tx.GasFeeCap(), baseFee) {
		return fmt.Errorf("%w: address %v, maxFeePerGas: %s baseFee: %s", core.ErrFeeCapTooLow, sender, tx.GasFeeCap(), header.BaseFee)
	}
	stateNonce := statedb.GetNonce(sender)
	if tx.Nonce() < stateNonce {
		return MakeNonceError(sender, tx.Nonce(), stateNonce)
	}
	extraInfo := types.DeserializeHeaderExtraInformation(header)
	intrinsic, err := core.IntrinsicGas(tx.Data(), tx.AccessList(), tx.To() == nil, chainConfig.IsHomestead(header.Number), chainConfig.IsIstanbul(header.Number), chainConfig.IsShanghai(header.Number, header.Time, extraInfo.ArbOSFormatVersion))
	if err != nil {
		return err
	}
	if tx.Gas() < intrinsic {
		return core.ErrIntrinsicGas
	}
	if config.Strictness < TxPreCheckerStrictnessLikelyCompatible {
		return nil
	}
	if options != nil {
		if err := options.Check(extraInfo.L1BlockNumber, header.Time, statedb); err != nil {
			conditionalTxRejectedByTxPreCheckerCurrentStateCounter.Inc(1)
			return err
		}
		conditionalTxAcceptedByTxPreCheckerCurrentStateCounter.Inc(1)
		if config.RequiredStateAge > 0 {
			now := time.Now().Unix()
			oldHeader := header
			blocksTraversed := uint(0)
			// find a block that's old enough
			for now-int64(oldHeader.Time) < config.RequiredStateAge &&
				(config.RequiredStateMaxBlocks <= 0 || blocksTraversed < config.RequiredStateMaxBlocks) &&
				oldHeader.Number.Uint64() > 0 {
				previousHeader := bc.GetHeader(oldHeader.ParentHash, oldHeader.Number.Uint64()-1)
				if previousHeader == nil {
					break
				}
				oldHeader = previousHeader
				blocksTraversed++
			}
			if !headerreader.HeadersEqual(oldHeader, header) {
				secondOldStatedb, err := bc.StateAt(oldHeader.Root)
				if err != nil {
					return fmt.Errorf("failed to get old state: %w", err)
				}
				oldExtraInfo := types.DeserializeHeaderExtraInformation(oldHeader)
				if err := options.Check(oldExtraInfo.L1BlockNumber, oldHeader.Time, secondOldStatedb); err != nil {
					conditionalTxRejectedByTxPreCheckerOldStateCounter.Inc(1)
					return arbitrum_types.WrapOptionsCheckError(err, "conditions check failed for old state")
				}
			}
			conditionalTxAcceptedByTxPreCheckerOldStateCounter.Inc(1)
		}
	}
	balance := statedb.GetBalance(sender)
	cost := tx.Cost()
	if arbmath.BigLessThan(balance, cost) {
		return fmt.Errorf("%w: address %v have %v want %v", core.ErrInsufficientFunds, sender, balance, cost)
	}
	if config.Strictness >= TxPreCheckerStrictnessFullValidation && tx.Nonce() > stateNonce {
		return MakeNonceError(sender, tx.Nonce(), stateNonce)
	}
	brotliCompressionLevel, err := arbos.BrotliCompressionLevel()
	if err != nil {
		return fmt.Errorf("failed to get brotli compression level: %w", err)
	}
	dataCost, _ := arbos.L1PricingState().GetPosterInfo(tx, l1pricing.BatchPosterAddress, brotliCompressionLevel)
	dataGas := arbmath.BigDiv(dataCost, header.BaseFee)
	if tx.Gas() < intrinsic+dataGas.Uint64() {
		return core.ErrIntrinsicGas
	}
	return nil
}

func (c *TxPreChecker) PublishTransaction(ctx context.Context, tx *types.Transaction, options *arbitrum_types.ConditionalOptions) error {
	block := c.bc.CurrentBlock()
	statedb, err := c.bc.StateAt(block.Root)
	if err != nil {
		return err
	}
	arbos, err := arbosState.OpenSystemArbosState(statedb, nil, true)
	if err != nil {
		return err
	}
	err = PreCheckTx(c.bc, c.bc.Config(), block, statedb, arbos, tx, options, c.config())
	if err != nil {
		return err
	}
	return c.TransactionPublisher.PublishTransaction(ctx, tx, options)
}

'''
'''--- execution/interface.go ---
package execution

import (
	"context"
	"errors"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/validator"
)

type MessageResult struct {
	BlockHash common.Hash
	SendRoot  common.Hash
}

type RecordResult struct {
	Pos       arbutil.MessageIndex
	BlockHash common.Hash
	Preimages map[common.Hash][]byte
	BatchInfo []validator.BatchInfo
}

var ErrRetrySequencer = errors.New("please retry transaction")
var ErrSequencerInsertLockTaken = errors.New("insert lock taken")

// always needed
type ExecutionClient interface {
	DigestMessage(num arbutil.MessageIndex, msg *arbostypes.MessageWithMetadata) error
	Reorg(count arbutil.MessageIndex, newMessages []arbostypes.MessageWithMetadata, oldMessages []*arbostypes.MessageWithMetadata) error
	HeadMessageNumber() (arbutil.MessageIndex, error)
	HeadMessageNumberSync(t *testing.T) (arbutil.MessageIndex, error)
	ResultAtPos(pos arbutil.MessageIndex) (*MessageResult, error)
}

// needed for validators / stakers
type ExecutionRecorder interface {
	RecordBlockCreation(
		ctx context.Context,
		pos arbutil.MessageIndex,
		msg *arbostypes.MessageWithMetadata,
	) (*RecordResult, error)
	MarkValid(pos arbutil.MessageIndex, resultHash common.Hash)
	PrepareForRecord(ctx context.Context, start, end arbutil.MessageIndex) error
}

// needed for sequencer
type ExecutionSequencer interface {
	ExecutionClient
	Pause()
	Activate()
	ForwardTo(url string) error
	SequenceDelayedMessage(message *arbostypes.L1IncomingMessage, delayedSeqNum uint64) error
	NextDelayedMessageNumber() (uint64, error)
	SetTransactionStreamer(streamer TransactionStreamer)
}

type FullExecutionClient interface {
	ExecutionClient
	ExecutionRecorder
	ExecutionSequencer

	Start(ctx context.Context) error
	StopAndWait()

	Maintenance() error

	// TODO: only used to get safe/finalized block numbers
	MessageIndexToBlockNumber(messageNum arbutil.MessageIndex) uint64
}

// not implemented in execution, used as input
type BatchFetcher interface {
	FetchBatch(batchNum uint64) ([]byte, error)
}

type TransactionStreamer interface {
	BatchFetcher
	WriteMessageFromSequencer(pos arbutil.MessageIndex, msgWithMeta arbostypes.MessageWithMetadata) error
	ExpectChosenSequencer() error
}

'''
'''--- gethhook/geth-hook.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethhook

import (
	"errors"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/precompiles"
)

type ArbosPrecompileWrapper struct {
	inner precompiles.ArbosPrecompile
}

func (p ArbosPrecompileWrapper) RequiredGas(input []byte) uint64 {
	panic("Non-advanced precompile method called")
}

func (p ArbosPrecompileWrapper) Run(input []byte) ([]byte, error) {
	panic("Non-advanced precompile method called")
}

func (p ArbosPrecompileWrapper) RunAdvanced(
	input []byte,
	gasSupplied uint64,
	info *vm.AdvancedPrecompileCall,
) (ret []byte, gasLeft uint64, err error) {

	// Precompiles don't actually enter evm execution like normal calls do,
	// so we need to increment the depth here to simulate the callstack change.
	info.Evm.IncrementDepth()
	defer info.Evm.DecrementDepth()

	return p.inner.Call(
		input, info.PrecompileAddress, info.ActingAsAddress,
		info.Caller, info.Value, info.ReadOnly, gasSupplied, info.Evm,
	)
}

func init() {
	core.ReadyEVMForL2 = func(evm *vm.EVM, msg *core.Message) {
		if evm.ChainConfig().IsArbitrum() {
			evm.ProcessingHook = arbos.NewTxProcessor(evm, msg)
		}
	}

	for k, v := range vm.PrecompiledContractsBerlin {
		vm.PrecompiledAddressesArbitrum = append(vm.PrecompiledAddressesArbitrum, k)
		vm.PrecompiledContractsArbitrum[k] = v
	}

	precompileErrors := make(map[[4]byte]abi.Error)
	for addr, precompile := range precompiles.Precompiles() {
		for _, errABI := range precompile.Precompile().GetErrorABIs() {
			var id [4]byte
			copy(id[:], errABI.ID[:4])
			precompileErrors[id] = errABI
		}
		var wrapped vm.AdvancedPrecompile = ArbosPrecompileWrapper{precompile}
		vm.PrecompiledContractsArbitrum[addr] = wrapped
		vm.PrecompiledAddressesArbitrum = append(vm.PrecompiledAddressesArbitrum, addr)
	}

	core.RenderRPCError = func(data []byte) error {
		if len(data) < 4 {
			return nil
		}
		var id [4]byte
		copy(id[:], data[:4])
		errABI, found := precompileErrors[id]
		if !found {
			return nil
		}
		rendered, err := precompiles.RenderSolError(errABI, data)
		if err != nil {
			log.Warn("failed to render rpc error", "err", err)
			return nil
		}
		return errors.New(rendered)
	}
}

// RequireHookedGeth does nothing, but forces an import to let the init function run
func RequireHookedGeth() {}

'''
'''--- gethhook/geth_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package gethhook

import (
	"bytes"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/consensus"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

type TestChainContext struct {
}

func (r *TestChainContext) Engine() consensus.Engine {
	return arbos.Engine{}
}

func (r *TestChainContext) GetHeader(hash common.Hash, num uint64) *types.Header {
	return &types.Header{}
}

var testChainConfig = &params.ChainConfig{
	ChainID:             big.NewInt(0),
	HomesteadBlock:      big.NewInt(0),
	DAOForkBlock:        nil,
	DAOForkSupport:      true,
	EIP150Block:         big.NewInt(0),
	EIP155Block:         big.NewInt(0),
	EIP158Block:         big.NewInt(0),
	ByzantiumBlock:      big.NewInt(0),
	ConstantinopleBlock: big.NewInt(0),
	PetersburgBlock:     big.NewInt(0),
	IstanbulBlock:       big.NewInt(0),
	MuirGlacierBlock:    big.NewInt(0),
	BerlinBlock:         big.NewInt(0),
	LondonBlock:         big.NewInt(0),
	ArbitrumChainParams: params.ArbitrumDevTestParams(),
}

func TestEthDepositMessage(t *testing.T) {

	_, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	addr := common.HexToAddress("0x32abcdeffffff")
	balance := common.BigToHash(big.NewInt(789789897789798))
	balance2 := common.BigToHash(big.NewInt(98))

	if statedb.GetBalance(addr).Sign() != 0 {
		Fail(t)
	}

	firstRequestId := common.BigToHash(big.NewInt(3))
	header := arbostypes.L1IncomingMessageHeader{
		Kind:        arbostypes.L1MessageType_EthDeposit,
		Poster:      addr,
		BlockNumber: 864513,
		Timestamp:   8794561564,
		RequestId:   &firstRequestId,
		L1BaseFee:   big.NewInt(10000000000000),
	}
	msgBuf := bytes.Buffer{}
	if err := util.AddressToWriter(addr, &msgBuf); err != nil {
		t.Error(err)
	}
	if err := util.HashToWriter(balance, &msgBuf); err != nil {
		t.Error(err)
	}
	msg := arbostypes.L1IncomingMessage{
		Header: &header,
		L2msg:  msgBuf.Bytes(),
	}

	serialized, err := msg.Serialize()
	if err != nil {
		t.Error(err)
	}

	secondRequestId := common.BigToHash(big.NewInt(4))
	header.RequestId = &secondRequestId
	header.Poster = util.RemapL1Address(addr)
	msgBuf2 := bytes.Buffer{}
	if err := util.AddressToWriter(addr, &msgBuf2); err != nil {
		t.Error(err)
	}
	if err := util.HashToWriter(balance2, &msgBuf2); err != nil {
		t.Error(err)
	}
	msg2 := arbostypes.L1IncomingMessage{
		Header: &header,
		L2msg:  msgBuf2.Bytes(),
	}
	serialized2, err := msg2.Serialize()
	if err != nil {
		t.Error(err)
	}

	RunMessagesThroughAPI(t, [][]byte{serialized, serialized2}, statedb)

	balanceAfter := statedb.GetBalance(addr)
	if balanceAfter.Cmp(new(big.Int).Add(balance.Big(), balance2.Big())) != 0 {
		Fail(t)
	}
}

func RunMessagesThroughAPI(t *testing.T, msgs [][]byte, statedb *state.StateDB) {
	chainId := big.NewInt(6456554)
	for _, data := range msgs {
		msg, err := arbostypes.ParseIncomingL1Message(bytes.NewReader(data), nil)
		if err != nil {
			t.Error(err)
		}
		txes, err := arbos.ParseL2Transactions(msg, chainId, nil)
		if err != nil {
			t.Error(err)
		}
		chainContext := &TestChainContext{}
		header := &types.Header{
			Number:     big.NewInt(1000),
			Difficulty: big.NewInt(1000),
		}
		gasPool := core.GasPool(100000)
		for _, tx := range txes {
			_, _, err := core.ApplyTransaction(testChainConfig, chainContext, nil, &gasPool, statedb, header, tx, &header.GasUsed, vm.Config{})
			if err != nil {
				Fail(t, err)
			}
		}

		arbos.FinalizeBlock(nil, nil, statedb, testChainConfig)
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- linter/koanf/handlers.go ---
package main

import (
	"fmt"
	"go/ast"
	"go/token"
	"strings"

	"github.com/fatih/structtag"
	"golang.org/x/tools/go/analysis"
)

// handleComposite tracks use of fields in composite literals.
// E.g. `Config{A: 1, B: 2, C: 3}` will increase counters of fields A,B and C.
func handleComposite(pass *analysis.Pass, cl *ast.CompositeLit, cnt map[string]int) {
	id, ok := cl.Type.(*ast.Ident)
	if !ok {
		return
	}
	for _, e := range cl.Elts {
		if kv, ok := e.(*ast.KeyValueExpr); ok {
			if ki, ok := kv.Key.(*ast.Ident); ok {
				fi := pass.TypesInfo.Types[id].Type.String() + "." + ki.Name
				cnt[normalizeID(pass, fi)]++
			}
		}
	}
}

// handleSelector handles selector expression recursively, that is an expression:
// a.B.C.D will update counter for fields: a.B.C.D, a.B.C and a.B.
// It updates counters map in place, increasing corresponding identifiers by
// increaseBy amount.
func handleSelector(pass *analysis.Pass, se *ast.SelectorExpr, increaseBy int, cnt map[string]int) string {
	if e, ok := se.X.(*ast.SelectorExpr); ok {
		// Full field identifier, including package name.
		fi := pass.TypesInfo.Types[e].Type.String() + "." + se.Sel.Name
		cnt[normalizeID(pass, fi)] += increaseBy
		prefix := handleSelector(pass, e, increaseBy, cnt)
		fi = prefix + "." + se.Sel.Name
		cnt[normalizeID(pass, fi)] += increaseBy
		return fi
	}
	// Handle selectors on function calls, e.g. `config().Enabled`.
	if _, ok := se.X.(*ast.CallExpr); ok {
		fi := pass.TypesInfo.Types[se.X].Type.String() + "." + se.Sel.Name
		cnt[normalizeID(pass, fi)] += increaseBy
		return fi
	}
	if ident, ok := se.X.(*ast.Ident); ok {
		if pass.TypesInfo.Types[ident].Type != nil {
			fi := pass.TypesInfo.Types[ident].Type.String() + "." + se.Sel.Name
			cnt[normalizeID(pass, fi)] += increaseBy
			return fi
		}
	}
	return ""
}

// koanfFields returns a map of fields that have koanf tag.
func koanfFields(pass *analysis.Pass) map[string]token.Pos {
	res := make(map[string]token.Pos)
	for _, f := range pass.Files {
		pkgName := f.Name.Name
		ast.Inspect(f, func(node ast.Node) bool {
			if ts, ok := node.(*ast.TypeSpec); ok {
				st, ok := ts.Type.(*ast.StructType)
				if !ok {
					return true
				}
				for _, f := range st.Fields.List {
					if tag := tagFromField(f); tag != "" {
						t := strings.Join([]string{pkgName, ts.Name.Name, f.Names[0].Name}, ".")
						res[t] = f.Pos()
					}
				}
			}
			return true
		})
	}
	return res
}

func containsFlagSet(params []*ast.Field) bool {
	for _, p := range params {
		se, ok := p.Type.(*ast.StarExpr)
		if !ok {
			continue
		}
		sle, ok := se.X.(*ast.SelectorExpr)
		if !ok {
			continue
		}
		if sle.Sel.Name == "FlagSet" {
			return true
		}
	}
	return false
}

// checkFlagDefs checks flag definitions in the function.
// Result contains list of errors where flag name doesn't match field name.
func checkFlagDefs(pass *analysis.Pass, f *ast.FuncDecl, cnt map[string]int) Result {
	// Ignore functions that does not get flagset as parameter.
	if !containsFlagSet(f.Type.Params.List) {
		return Result{}
	}
	var res Result
	for _, s := range f.Body.List {
		es, ok := s.(*ast.ExprStmt)
		if !ok {
			continue
		}
		callE, ok := es.X.(*ast.CallExpr)
		if !ok {
			continue
		}
		if len(callE.Args) != 3 {
			continue
		}
		sl, ok := extractStrLit(callE.Args[0])
		if !ok {
			continue
		}
		s, ok := selectorName(callE.Args[1])
		if !ok {
			continue
		}
		handleSelector(pass, callE.Args[1].(*ast.SelectorExpr), -1, cnt)
		if normSL := normalizeTag(sl); !strings.EqualFold(normSL, s) {
			res.Errors = append(res.Errors, koanfError{
				Pos:     f.Pos(),
				Message: fmt.Sprintf("koanf tag name: %q doesn't match the field: %q", sl, s),
				err:     errIncorrectFlag,
			})
		}

	}
	return res
}

func selectorName(e ast.Expr) (string, bool) {
	n, ok := e.(ast.Node)
	if !ok {
		return "", false
	}
	se, ok := n.(*ast.SelectorExpr)
	if !ok {
		return "", false
	}
	return se.Sel.Name, true
}

// Extracts literal from expression that is either:
// - string literal or
// - sum of variable and string literal.
// E.g.
// strLitFromSum(`"max-size"`) = "max-size"
// - strLitFromSum(`prefix + ".enable"â€œ) = ".enable".
func extractStrLit(e ast.Expr) (string, bool) {
	if s, ok := strLit(e); ok {
		return s, true
	}
	if be, ok := e.(*ast.BinaryExpr); ok {
		if be.Op == token.ADD {
			if s, ok := strLit(be.Y); ok {
				// Drop the prefix dot.
				return s[1:], true
			}
		}
	}
	return "", false
}

func strLit(e ast.Expr) (string, bool) {
	if s, ok := e.(*ast.BasicLit); ok {
		if s.Kind == token.STRING {
			return strings.Trim(s.Value, "\""), true
		}
	}
	return "", false
}

// tagFromField extracts koanf tag from struct field.
func tagFromField(f *ast.Field) string {
	if f.Tag == nil {
		return ""
	}
	tags, err := structtag.Parse(strings.Trim((f.Tag.Value), "`"))
	if err != nil {
		return ""
	}
	tag, err := tags.Get("koanf")
	if err != nil {
		return ""
	}
	return normalizeTag(tag.Name)
}

// checkStruct returns violations where koanf tag name doesn't match field names.
func checkStruct(pass *analysis.Pass, s *ast.StructType) Result {
	var res Result
	for _, f := range s.Fields.List {
		tag := tagFromField(f)
		if tag == "" {
			continue
		}
		fieldName := f.Names[0].Name
		if !strings.EqualFold(tag, fieldName) {
			res.Errors = append(res.Errors, koanfError{
				Pos:     f.Pos(),
				Message: fmt.Sprintf("field name: %q doesn't match tag name: %q\n", fieldName, tag),
				err:     errMismatch,
			})
		}
	}
	return res
}

func normalizeTag(s string) string {
	return strings.ReplaceAll(s, "-", "")
}

func normalizeID(pass *analysis.Pass, id string) string {
	id = strings.TrimPrefix(id, "*")
	return pass.Pkg.Name() + strings.TrimPrefix(id, pass.Pkg.Path())
}

'''
'''--- linter/koanf/koanf.go ---
package main

import (
	"errors"
	"fmt"
	"go/ast"
	"go/token"
	"reflect"

	"golang.org/x/tools/go/analysis"
	"golang.org/x/tools/go/analysis/singlechecker"
)

var (
	errUnused   = errors.New("unused")
	errMismatch = errors.New("mismmatched field name and tag in a struct")
	// e.g. f.Int("max-sz", DefaultBatchPosterConfig.MaxSize, "maximum batch size")
	errIncorrectFlag = errors.New("mismatching flag initialization")
)

func New(conf any) ([]*analysis.Analyzer, error) {
	return []*analysis.Analyzer{Analyzer}, nil
}

var Analyzer = &analysis.Analyzer{
	Name:       "koanfcheck",
	Doc:        "check for koanf misconfigurations",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(false, p) },
	ResultType: reflect.TypeOf(Result{}),
}

var analyzerForTests = &analysis.Analyzer{
	Name:       "testkoanfcheck",
	Doc:        "check for koanf misconfigurations (for tests)",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(true, p) },
	ResultType: reflect.TypeOf(Result{}),
}

// koanfError indicates the position of an error in configuration.
type koanfError struct {
	Pos     token.Pos
	Message string
	err     error
}

// Result is returned from the checkStruct function, and holds all the
// configuration errors.
type Result struct {
	Errors []koanfError
}

func run(dryRun bool, pass *analysis.Pass) (interface{}, error) {
	var (
		ret Result
		cnt = make(map[string]int)
		// koanfFields map contains all the struct koanfFields that have koanf tag.
		// It identifies field as "{pkgName}.{structName}.{field_Name}".
		// e.g. "a.BatchPosterConfig.Enable", "a.BatchPosterConfig.MaxSize"
		koanfFields = koanfFields(pass)
	)
	for _, f := range pass.Files {
		ast.Inspect(f, func(node ast.Node) bool {
			var res Result
			switch v := node.(type) {
			case *ast.StructType:
				res = checkStruct(pass, v)
			case *ast.FuncDecl:
				res = checkFlagDefs(pass, v, cnt)
			case *ast.SelectorExpr:
				handleSelector(pass, v, 1, cnt)
			case *ast.IfStmt:
				if se, ok := v.Cond.(*ast.SelectorExpr); ok {
					handleSelector(pass, se, 1, cnt)
				}
			case *ast.CompositeLit:
				handleComposite(pass, v, cnt)
			default:
			}
			ret.Errors = append(ret.Errors, res.Errors...)
			return true
		})
	}
	for k := range koanfFields {
		if cnt[k] == 0 {
			ret.Errors = append(ret.Errors,
				koanfError{
					Pos:     koanfFields[k],
					Message: fmt.Sprintf("field %v not used", k),
					err:     errUnused,
				})
		}
	}
	for _, err := range ret.Errors {
		if !dryRun {
			pass.Report(analysis.Diagnostic{
				Pos:      err.Pos,
				Message:  err.Message,
				Category: "koanf",
			})
		}
	}
	return ret, nil
}

func main() {
	singlechecker.Main(Analyzer)
}

'''
'''--- linter/koanf/koanf_test.go ---
package main

import (
	"errors"
	"os"
	"path/filepath"
	"testing"

	"github.com/google/go-cmp/cmp"
	"golang.org/x/tools/go/analysis/analysistest"
)

var (
	incorrectFlag = "incorrect_flag"
	mismatch      = "mismatch"
	unused        = "unused"
)

func testData(t *testing.T) string {
	t.Helper()
	wd, err := os.Getwd()
	if err != nil {
		t.Fatalf("Failed to get wd: %s", err)
	}
	return filepath.Join(filepath.Dir(wd), "testdata")
}

// Tests koanf/a package that contains two types of errors where:
// - koanf tag doesn't match field name.
// - flag definition doesn't match field name.
// Errors are marked as comments in the package source file.
func TestMismatch(t *testing.T) {
	testdata := testData(t)
	got := errCounts(analysistest.Run(t, testdata, analyzerForTests, "koanf/a"))
	want := map[string]int{
		incorrectFlag: 2,
		mismatch:      1,
	}
	if diff := cmp.Diff(got, want); diff != "" {
		t.Errorf("analysistest.Run() unexpected diff:\n%s\n", diff)
	}
}

func TestUnused(t *testing.T) {
	testdata := testData(t)
	got := errCounts(analysistest.Run(t, testdata, analyzerForTests, "koanf/b"))
	if diff := cmp.Diff(got, map[string]int{"unused": 2}); diff != "" {
		t.Errorf("analysistest.Run() unexpected diff:\n%s\n", diff)
	}
}

func errCounts(res []*analysistest.Result) map[string]int {
	m := make(map[string]int)
	for _, r := range res {
		if rs, ok := r.Result.(Result); ok {
			for _, e := range rs.Errors {
				var s string
				switch {
				case errors.Is(e.err, errIncorrectFlag):
					s = incorrectFlag
				case errors.Is(e.err, errMismatch):
					s = mismatch
				case errors.Is(e.err, errUnused):
					s = unused
				}
				m[s] = m[s] + 1
			}
		}
	}
	return m
}

'''
'''--- linter/pointercheck/pointer.go ---
package main

import (
	"fmt"
	"go/ast"
	"go/token"
	"go/types"
	"reflect"

	"golang.org/x/tools/go/analysis"
	"golang.org/x/tools/go/analysis/singlechecker"
)

func New(conf any) ([]*analysis.Analyzer, error) {
	return []*analysis.Analyzer{Analyzer}, nil
}

var Analyzer = &analysis.Analyzer{
	Name:       "pointercheck",
	Doc:        "check for pointer comparison",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(false, p) },
	ResultType: reflect.TypeOf(Result{}),
}

var analyzerForTests = &analysis.Analyzer{
	Name:       "testpointercheck",
	Doc:        "check for pointer comparison (for tests)",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(true, p) },
	ResultType: reflect.TypeOf(Result{}),
}

// pointerCmpError indicates the position of pointer comparison.
type pointerCmpError struct {
	Pos     token.Position
	Message string
}

// Result is returned from the checkStruct function, and holds all the
// configuration errors.
type Result struct {
	Errors []pointerCmpError
}

func run(dryRun bool, pass *analysis.Pass) (interface{}, error) {
	var ret Result
	for _, f := range pass.Files {
		ast.Inspect(f, func(node ast.Node) bool {
			var res *Result
			switch e := node.(type) {
			case *ast.BinaryExpr:
				res = checkExpr(pass, e)
			default:
			}
			if res == nil {
				return true
			}
			for _, err := range res.Errors {
				ret.Errors = append(ret.Errors, err)
				if !dryRun {
					pass.Report(analysis.Diagnostic{
						Pos:      pass.Fset.File(f.Pos()).Pos(err.Pos.Offset),
						Message:  err.Message,
						Category: "pointercheck",
					})
				}
			}
			return true
		},
		)
	}
	return ret, nil
}

func checkExpr(pass *analysis.Pass, e *ast.BinaryExpr) *Result {
	if e.Op != token.EQL && e.Op != token.NEQ {
		return nil
	}
	ret := &Result{}
	if ptrIdent(pass, e.X) && ptrIdent(pass, e.Y) {
		ret.Errors = append(ret.Errors, pointerCmpError{
			Pos:     pass.Fset.Position(e.Pos()),
			Message: fmt.Sprintf("comparison of two pointers in expression %v", e),
		})
	}
	return ret
}

func ptrIdent(pass *analysis.Pass, e ast.Expr) bool {
	switch tp := e.(type) {
	case *ast.Ident, *ast.SelectorExpr:
		et := pass.TypesInfo.Types[tp].Type
		_, isPtr := (et).(*types.Pointer)
		return isPtr
	}
	return false
}

func main() {
	singlechecker.Main(Analyzer)
}

'''
'''--- linter/pointercheck/pointer_test.go ---
package main

import (
	"os"
	"path/filepath"
	"testing"

	"golang.org/x/tools/go/analysis/analysistest"
)

func TestAll(t *testing.T) {
	wd, err := os.Getwd()
	if err != nil {
		t.Fatalf("Failed to get wd: %s", err)
	}
	testdata := filepath.Join(filepath.Dir(wd), "testdata")
	res := analysistest.Run(t, testdata, analyzerForTests, "pointercheck")
	if cnt := countErrors(res); cnt != 6 {
		t.Errorf("analysistest.Run() got %v errors, expected 6", cnt)
	}
}

func countErrors(errs []*analysistest.Result) int {
	cnt := 0
	for _, e := range errs {
		if r, ok := e.Result.(Result); ok {
			cnt += len(r.Errors)
		}
	}
	return cnt
}

'''
'''--- linter/structinit/structinit.go ---
package main

import (
	"fmt"
	"go/ast"
	"go/token"
	"reflect"
	"strings"

	"golang.org/x/tools/go/analysis"
	"golang.org/x/tools/go/analysis/singlechecker"
)

// Tip for linter that struct that has this comment should be included in the
// analysis.
// Note: comment should be directly line above the struct definition.
const linterTip = "// lint:require-exhaustive-initialization"

func New(conf any) ([]*analysis.Analyzer, error) {
	return []*analysis.Analyzer{Analyzer}, nil
}

// Analyzer implements struct analyzer for structs that are annotated with
// `linterTip`, it checks that every instantiation initializes all the fields.
var Analyzer = &analysis.Analyzer{
	Name:       "structinit",
	Doc:        "check for struct field initializations",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(false, p) },
	ResultType: reflect.TypeOf(Result{}),
}

var analyzerForTests = &analysis.Analyzer{
	Name:       "teststructinit",
	Doc:        "check for struct field initializations",
	Run:        func(p *analysis.Pass) (interface{}, error) { return run(true, p) },
	ResultType: reflect.TypeOf(Result{}),
}

type structError struct {
	Pos     token.Pos
	Message string
}

type Result struct {
	Errors []structError
}

func run(dryRun bool, pass *analysis.Pass) (interface{}, error) {
	var (
		ret     Result
		structs = markedStructs(pass)
	)
	for _, f := range pass.Files {
		ast.Inspect(f, func(node ast.Node) bool {
			// For every composite literal check that number of elements in
			// the literal match the number of struct fields.
			if cl, ok := node.(*ast.CompositeLit); ok {
				stName := pass.TypesInfo.Types[cl].Type.String()
				if cnt, found := structs[stName]; found && cnt != len(cl.Elts) {
					ret.Errors = append(ret.Errors, structError{
						Pos:     cl.Pos(),
						Message: fmt.Sprintf("struct: %q initialized with: %v of total: %v fields", stName, len(cl.Elts), cnt),
					})

				}

			}
			return true
		})
	}
	for _, err := range ret.Errors {
		if !dryRun {
			pass.Report(analysis.Diagnostic{
				Pos:      err.Pos,
				Message:  err.Message,
				Category: "structinit",
			})
		}
	}
	return ret, nil
}

// markedStructs returns a map of structs that are annotated for linter to check
// that all fields are initialized when the struct is instantiated.
// It maps struct full name (including package path) to number of fields it contains.
func markedStructs(pass *analysis.Pass) map[string]int {
	res := make(map[string]int)
	for _, f := range pass.Files {
		tips := make(map[position]bool)
		ast.Inspect(f, func(node ast.Node) bool {
			switch n := node.(type) {
			case *ast.Comment:
				p := pass.Fset.Position(node.Pos())
				if strings.Contains(n.Text, linterTip) {
					tips[position{p.Filename, p.Line + 1}] = true
				}
			case *ast.TypeSpec:
				if st, ok := n.Type.(*ast.StructType); ok {
					p := pass.Fset.Position(st.Struct)
					if tips[position{p.Filename, p.Line}] {
						fieldsCnt := 0
						for _, field := range st.Fields.List {
							fieldsCnt += len(field.Names)
						}
						res[pass.Pkg.Path()+"."+n.Name.Name] = fieldsCnt
					}
				}
			}
			return true
		})
	}
	return res
}

type position struct {
	fileName string
	line     int
}

func main() {
	singlechecker.Main(Analyzer)
}

'''
'''--- linter/structinit/structinit_test.go ---
package main

import (
	"os"
	"path/filepath"
	"testing"

	"golang.org/x/tools/go/analysis/analysistest"
)

func testData(t *testing.T) string {
	t.Helper()
	wd, err := os.Getwd()
	if err != nil {
		t.Fatalf("Failed to get wd: %s", err)
	}
	return filepath.Join(filepath.Dir(wd), "testdata")
}

func TestLinter(t *testing.T) {
	testdata := testData(t)
	got := errCount(analysistest.Run(t, testdata, analyzerForTests, "structinit/a"))
	if got != 2 {
		t.Errorf("analysistest.Run() got %d errors, expected 2", got)
	}
}

func errCount(res []*analysistest.Result) int {
	cnt := 0
	for _, r := range res {
		if rs, ok := r.Result.(Result); ok {
			cnt += len(rs.Errors)
		}
	}
	return cnt
}

'''
'''--- linter/testdata/src/koanf/a/a.go ---
package a

import (
	"flag"
)

type Config struct {
	L2       int `koanf:"chain"` // Err: mismatch.
	LogLevel int `koanf:"log-level"`
	LogType  int `koanf:"log-type"`
	Metrics  int `koanf:"metrics"`
	PProf    int `koanf:"pprof"`
	Node     int `koanf:"node"`
	Queue    int `koanf:"queue"`
}

// Cover using of all fields in a various way:

// Instantiating a type.
var defaultConfig = Config{
	L2:       1,
	LogLevel: 2,
}

// Instantiating a type an taking reference.
var defaultConfigPtr = &Config{
	LogType: 3,
	Metrics: 4,
}

func init() {
	defaultConfig.PProf = 5
	defaultConfig.Node, _ = 6, 0
	defaultConfigPtr.Queue = 7
}

type BatchPosterConfig struct {
	Enable  bool `koanf:"enable"`
	MaxSize int  `koanf:"max-size" reload:"hot"`
}

var DefaultBatchPosterConfig BatchPosterConfig

func BatchPosterConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enabled", DefaultBatchPosterConfig.Enable, "") // Err: incorrect flag.
	f.Int("max-sz", DefaultBatchPosterConfig.MaxSize, "")          // Err: incorrect flag.
}

func ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultBatchPosterConfig.Enable, "enable posting batches to l1")
	f.Int("max-size", DefaultBatchPosterConfig.MaxSize, "maximum batch size")
}

func init() {
	// Fields must be used outside flag definitions at least once.
	DefaultBatchPosterConfig.Enable = true
	DefaultBatchPosterConfig.MaxSize = 3
}

'''
'''--- linter/testdata/src/koanf/b/b.go ---
package b

import (
	"flag"
	"fmt"
)

type ParCfg struct {
	child      ChildCfg      `koanf:"child"`
	grandChild GrandChildCfg `koanf:grandchild`
}

var defaultCfg = ParCfg{}

type ChildCfg struct {
	A bool `koanf:"A"`
	B bool `koanf:"B"`
	C bool `koanf:"C"`
	D bool `koanf:"D"` // Error: not used outside flag definition.
}

var defaultChildCfg = ChildCfg{}

func childConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".a", defaultChildCfg.A, "")
	f.Bool("b", defaultChildCfg.B, "")
	f.Bool("c", defaultChildCfg.C, "")
	f.Bool("d", defaultChildCfg.D, "")
}

type GrandChildCfg struct {
	A int `koanf:"A"` // Error: unused.
}

func (c *GrandChildCfg) Do() {
}

func configPtr() *ChildCfg {
	return nil
}
func config() ChildCfg {
	return ChildCfg{}
}

func init() {
	fmt.Printf("%v %v", config().A, configPtr().B)
	// This covers usage of both `ParCfg.Child` and `ChildCfg.C`.
	_ = defaultCfg.child.C
	// Covers usage of grandChild.
	defaultCfg.grandChild.Do()

}

'''
'''--- linter/testdata/src/pointercheck/pointercheck.go ---
package pointercheck

import "fmt"

type A struct {
	x, y int
}

// pointerCmp compares pointers, sometimes inside
func pointerCmp() {
	a, b := &A{}, &A{}
	// Simple comparions.
	if a != b {
		fmt.Println("Not Equal")
	}
	if a == b {
		fmt.Println("Equals")
	}
	// Nested binary expressions.
	if (2 > 1) && (a != b) {
		fmt.Println("Still not equal")
	}
	if (174%15 > 3) && (2 > 1 && (1+2 > 2 || a != b)) {
		fmt.Println("Who knows at this point")
	}
	// Nested and inside unary operator.
	if 10 > 5 && !(2 > 1 || a == b) {
		fmt.Println("Not equal")
	}
	c, d := 1, 2
	if &c != &d {
		fmt.Println("Not equal")
	}
}

func legitCmps() {
	a, b := &A{}, &A{}
	if a.x == b.x {
		fmt.Println("Allowed")
	}
}

type cache struct {
	dirty *A
}

// matches does pointer comparison.
func (c *cache) matches(a *A) bool {
	return c.dirty == a
}

'''
'''--- linter/testdata/src/structinit/a/a.go ---
package a

import "fmt"

// lint:require-exhaustive-initialization
type interestingStruct struct {
	x int
	b *boringStruct
}

type boringStruct struct {
	x, y int
}

func init() {
	a := &interestingStruct{ // Error: only single field is initialized.
		x: 1,
	}
	fmt.Println(a)
	b := interestingStruct{ // Error: only single field is initialized.
		b: nil,
	}
	fmt.Println(b)
	c := interestingStruct{ // Not an error, all fields are initialized.
		x: 1,
		b: nil,
	}
	fmt.Println(c)
	d := &boringStruct{ // Not an error since it's not annotated for the linter.
		x: 1,
	}
	fmt.Println(d)
}

'''
'''--- nodeInterface/NodeInterface.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package nodeInterface

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"sort"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/merkletree"
)

// To avoid creating new RPC methods for client-side tooling, nitro Geth's InterceptRPCMessage() hook provides
// an opportunity to swap out the message its handling before deriving a transaction from it.
//
// This mechanism handles messages sent to 0xc8 and uses NodeInterface.sol to determine what to do. No contract
// actually exists at 0xc8, but the abi methods allow the incoming message's calldata to specify the arguments.
type NodeInterface struct {
	Address       addr
	backend       core.NodeInterfaceBackendAPI
	context       context.Context
	header        *types.Header
	sourceMessage *core.Message
	returnMessage struct {
		message *core.Message
		changed *bool
	}
}

var merkleTopic common.Hash
var l2ToL1TxTopic common.Hash
var l2ToL1TransactionTopic common.Hash

var blockInGenesis = errors.New("")
var blockAfterLatestBatch = errors.New("")

func (n NodeInterface) NitroGenesisBlock(c ctx) (huge, error) {
	block := n.backend.ChainConfig().ArbitrumChainParams.GenesisBlockNum
	return arbmath.UintToBig(block), nil
}

func (n NodeInterface) FindBatchContainingBlock(c ctx, evm mech, blockNum uint64) (uint64, error) {
	node, err := arbNodeFromNodeInterfaceBackend(n.backend)
	if err != nil {
		return 0, err
	}
	return findBatchContainingBlock(node, node.TxStreamer.GenesisBlockNumber(), blockNum)
}

func (n NodeInterface) GetL1Confirmations(c ctx, evm mech, blockHash bytes32) (uint64, error) {
	node, err := arbNodeFromNodeInterfaceBackend(n.backend)
	if err != nil {
		return 0, err
	}
	if node.InboxReader == nil {
		return 0, nil
	}
	bc, err := blockchainFromNodeInterfaceBackend(n.backend)
	if err != nil {
		return 0, err
	}
	header := bc.GetHeaderByHash(blockHash)
	if header == nil {
		return 0, errors.New("unknown block hash")
	}
	blockNum := header.Number.Uint64()
	genesis := node.TxStreamer.GenesisBlockNumber()
	batch, err := findBatchContainingBlock(node, genesis, blockNum)
	if err != nil {
		if errors.Is(err, blockInGenesis) {
			batch = 0
		} else if errors.Is(err, blockAfterLatestBatch) {
			return 0, nil
		} else {
			return 0, err
		}
	}
	latestL1Block, latestBatchCount := node.InboxReader.GetLastReadBlockAndBatchCount()
	if latestBatchCount <= batch {
		return 0, nil // batch was reorg'd out?
	}
	meta, err := node.InboxTracker.GetBatchMetadata(batch)
	if err != nil {
		return 0, err
	}
	if latestL1Block < meta.ParentChainBlock || arbutil.BlockNumberToMessageCount(blockNum, genesis) > meta.MessageCount {
		return 0, nil
	}
	canonicalHash := bc.GetCanonicalHash(header.Number.Uint64())
	if canonicalHash != header.Hash() {
		return 0, errors.New("block hash is non-canonical")
	}
	confs := (latestL1Block - meta.ParentChainBlock) + 1 + node.InboxReader.GetDelayBlocks()
	return confs, nil
}

func (n NodeInterface) EstimateRetryableTicket(
	c ctx,
	evm mech,
	sender addr,
	deposit huge,
	to addr,
	l2CallValue huge,
	excessFeeRefundAddress addr,
	callValueRefundAddress addr,
	data []byte,
) error {

	var pRetryTo *addr
	if to != (addr{}) {
		pRetryTo = &to
	}

	l1BaseFee, _ := c.State.L1PricingState().PricePerUnit()
	maxSubmissionFee := retryables.RetryableSubmissionFee(len(data), l1BaseFee)

	submitTx := &types.ArbitrumSubmitRetryableTx{
		ChainId:          nil,
		RequestId:        hash{},
		From:             util.RemapL1Address(sender),
		L1BaseFee:        l1BaseFee,
		DepositValue:     deposit,
		GasFeeCap:        n.sourceMessage.GasPrice,
		Gas:              n.sourceMessage.GasLimit,
		RetryTo:          pRetryTo,
		RetryValue:       l2CallValue,
		Beneficiary:      callValueRefundAddress,
		MaxSubmissionFee: maxSubmissionFee,
		FeeRefundAddr:    excessFeeRefundAddress,
		RetryData:        data,
	}

	// ArbitrumSubmitRetryableTx is unsigned so the following won't panic
	msg, err := core.TransactionToMessage(types.NewTx(submitTx), types.NewArbitrumSigner(nil), nil)
	if err != nil {
		return err
	}

	msg.TxRunMode = core.MessageGasEstimationMode
	*n.returnMessage.message = *msg
	*n.returnMessage.changed = true
	return nil
}

func (n NodeInterface) ConstructOutboxProof(c ctx, evm mech, size, leaf uint64) (bytes32, bytes32, []bytes32, error) {

	hash0 := bytes32{}

	currentBlock := n.backend.CurrentBlock()
	currentBlockInfo := types.DeserializeHeaderExtraInformation(currentBlock)
	if leaf > currentBlockInfo.SendCount {
		return hash0, hash0, nil, errors.New("leaf does not exist")
	}

	balanced := size == arbmath.NextPowerOf2(size)/2
	treeLevels := int(arbmath.Log2ceil(size)) // the # of levels in the tree
	proofLevels := treeLevels - 1             // the # of levels where a hash is needed (all but root)
	walkLevels := treeLevels                  // the # of levels we need to consider when building walks
	if balanced {
		walkLevels -= 1 // skip the root
	}

	// find which nodes we'll want in our proof up to a partial
	start := merkletree.NewLevelAndLeaf(0, leaf)
	query := []merkletree.LevelAndLeaf{start} // the nodes we'll query for
	nodes := []merkletree.LevelAndLeaf{}      // the nodes needed (might not be found from query)
	which := uint64(1)                        // which bit to flip & set
	place := leaf                             // where we are in the tree
	for level := 0; level < walkLevels; level++ {
		sibling := place ^ which
		position := merkletree.NewLevelAndLeaf(uint64(level), sibling)

		if sibling < size {
			// the sibling must not be newer than the root
			query = append(query, position)
		}
		nodes = append(nodes, position)
		place |= which // set the bit so that we approach from the right
		which <<= 1    // advance to the next bit
	}

	// find all the partials
	partials := make(map[merkletree.LevelAndLeaf]hash)
	if !balanced {
		power := uint64(1) << proofLevels
		total := uint64(0)
		for level := proofLevels; level >= 0; level-- {

			if (power & size) > 0 { // the partials map to the binary representation of the size

				total += power    // The leaf for a given partial is the sum of the powers
				leaf := total - 1 // of 2 preceding it. It's 1 less since we count from 0

				partial := merkletree.NewLevelAndLeaf(uint64(level), leaf)

				query = append(query, partial)
				partials[partial] = hash0
			}
			power >>= 1
		}
	}
	sort.Slice(query, func(i, j int) bool {
		return query[i].Leaf < query[j].Leaf
	})

	// collect the logs
	var search func(lo, hi uint64, find []merkletree.LevelAndLeaf)
	var searchLogs []*types.Log
	var searchErr error
	var searchPositions = make(map[hash]struct{})
	for _, item := range query {
		hash := common.BigToHash(item.ToBigInt())
		searchPositions[hash] = struct{}{}
	}
	search = func(lo, hi uint64, find []merkletree.LevelAndLeaf) {

		mid := (lo + hi) / 2

		block, err := n.backend.BlockByNumber(n.context, rpc.BlockNumber(mid))
		if err != nil {
			searchErr = err
			return
		}

		if lo == hi {
			all, err := n.backend.GetLogs(n.context, block.Hash(), block.NumberU64())
			if err != nil {
				searchErr = err
				return
			}
			for _, tx := range all {
				for _, log := range tx {
					if log.Address != types.ArbSysAddress {
						// log not produced by ArbOS
						continue
					}

					// L2ToL1TransactionEventID is deprecated in upgrade 4, but it should to safe to make this code handle
					// both events ignoring the version.
					// TODO: Remove L2ToL1Transaction handling on next chain reset
					if log.Topics[0] != merkleTopic && log.Topics[0] != l2ToL1TxTopic && log.Topics[0] != l2ToL1TransactionTopic {
						// log is unrelated
						continue
					}

					position := log.Topics[3]
					if _, ok := searchPositions[position]; ok {
						// ensure log is one we're looking for
						searchLogs = append(searchLogs, log)
					}
				}
			}
			return
		}

		info := types.DeserializeHeaderExtraInformation(block.Header())

		// Figure out which elements are above and below the midpoint
		//   lower includes leaves older than the midpoint
		//   upper includes leaves at least as new as the midpoint
		//   note: while a binary search is possible here, it doesn't change the complexity
		//
		lower := find
		for len(lower) > 0 && lower[len(lower)-1].Leaf >= info.SendCount {
			lower = lower[:len(lower)-1]
		}
		upper := find[len(lower):]

		if len(lower) > 0 {
			search(lo, mid, lower)
		}
		if len(upper) > 0 {
			search(mid+1, hi, upper)
		}
	}

	search(0, currentBlock.Number.Uint64(), query)

	if searchErr != nil {
		return hash0, hash0, nil, searchErr
	}

	known := make(map[merkletree.LevelAndLeaf]hash) // all values in the tree we know
	partialsByLevel := make(map[uint64]hash)        // maps for each level the partial it may have
	var minPartialPlace *merkletree.LevelAndLeaf    // the lowest-level partial
	var send hash

	for _, log := range searchLogs {

		hash := log.Topics[2]
		position := log.Topics[3]

		level := new(big.Int).SetBytes(position[:8]).Uint64()
		leafAdded := new(big.Int).SetBytes(position[8:]).Uint64()

		if level == 0 && leafAdded == leaf {
			send = hash
		}

		if level == 0 {
			hash = crypto.Keccak256Hash(hash.Bytes())
		}

		place := merkletree.NewLevelAndLeaf(level, leafAdded)
		known[place] = hash

		if zero, ok := partials[place]; ok {
			if zero != hash0 {
				return hash0, hash0, nil, errors.New("internal error constructing proof: duplicate partial")
			}
			partials[place] = hash
			partialsByLevel[level] = hash
			if minPartialPlace == nil || level < minPartialPlace.Level {
				minPartialPlace = &place
			}
		}
	}

	if !balanced {
		// This tree isn't balanced, so we'll need to use the partials to recover the missing info.
		// To do this, we'll walk the boundry of what's known, computing hashes along the way

		step := *minPartialPlace
		step.Leaf += 1 << step.Level // we start on the min partial's zero-hash sibling
		known[step] = hash0

		for step.Level < uint64(treeLevels) {

			curr, ok := known[step]
			if !ok {
				return hash0, hash0, nil, errors.New("internal error constructing proof: bad step in walk")
			}

			left := curr
			right := curr

			if _, ok := partialsByLevel[step.Level]; ok {
				// a partial on the frontier can only appear on the left
				// moving leftward for a level l skips 2^l leaves
				step.Leaf -= 1 << step.Level
				partial, ok := known[step]
				if !ok {
					err := errors.New("internal error constructing proof: incomplete frontier")
					return hash0, hash0, nil, err
				}
				left = partial
			} else {
				// getting to the next partial means covering its mirror subtree, so go right
				// moving rightward for a level l skips 2^l leaves
				step.Leaf += 1 << step.Level
				known[step] = hash0
				right = hash0
			}

			// move to the parent
			step.Level += 1
			step.Leaf |= 1 << (step.Level - 1)
			known[step] = crypto.Keccak256Hash(left.Bytes(), right.Bytes())
		}
	}

	hashes := make([]hash, len(nodes))
	for i, place := range nodes {
		hash, ok := known[place]
		if !ok {
			return hash0, hash0, nil, errors.New("internal error constructing proof: incomplete information")
		}
		hashes[i] = hash
	}

	// recover the root and check correctness
	recovery := crypto.Keccak256Hash(send.Bytes())
	recoveryStep := leaf
	for _, hash := range hashes {
		if recoveryStep&1 == 0 {
			recovery = crypto.Keccak256Hash(recovery.Bytes(), hash.Bytes())
		} else {
			recovery = crypto.Keccak256Hash(hash.Bytes(), recovery.Bytes())
		}
		recoveryStep >>= 1
	}
	root := recovery

	proof := merkletree.MerkleProof{
		RootHash:  root, // now resolved
		LeafHash:  crypto.Keccak256Hash(send.Bytes()),
		LeafIndex: leaf,
		Proof:     hashes,
	}
	if !proof.IsCorrect() {
		return hash0, hash0, nil, errors.New("internal error constructing proof: proof is wrong")
	}

	hashes32 := make([]bytes32, len(hashes))
	for i, hash := range hashes {
		hashes32[i] = hash
	}
	return send, root, hashes32, nil
}

func (n NodeInterface) messageArgs(
	evm mech, value huge, to addr, contractCreation bool, data []byte,
) arbitrum.TransactionArgs {
	msg := n.sourceMessage
	from := msg.From
	gas := msg.GasLimit
	nonce := msg.Nonce
	maxFeePerGas := msg.GasFeeCap
	maxPriorityFeePerGas := msg.GasTipCap
	chainid := evm.ChainConfig().ChainID

	args := arbitrum.TransactionArgs{
		ChainID:              (*hexutil.Big)(chainid),
		From:                 &from,
		Gas:                  (*hexutil.Uint64)(&gas),
		MaxFeePerGas:         (*hexutil.Big)(maxFeePerGas),
		MaxPriorityFeePerGas: (*hexutil.Big)(maxPriorityFeePerGas),
		Value:                (*hexutil.Big)(value),
		Nonce:                (*hexutil.Uint64)(&nonce),
		Data:                 (*hexutil.Bytes)(&data),
	}
	if !contractCreation {
		args.To = &to
	}
	return args
}

func (n NodeInterface) GasEstimateL1Component(
	c ctx, evm mech, value huge, to addr, contractCreation bool, data []byte,
) (uint64, huge, huge, error) {

	// construct a similar message with a random gas limit to avoid underestimating
	args := n.messageArgs(evm, value, to, contractCreation, data)
	randomGas := l1pricing.RandomGas
	args.Gas = (*hexutil.Uint64)(&randomGas)

	// We set the run mode to eth_call mode here because we want an exact estimate, not a padded estimate
	msg, err := args.ToMessage(randomGas, n.header, evm.StateDB.(*state.StateDB), core.MessageEthcallMode)
	if err != nil {
		return 0, nil, nil, err
	}

	pricing := c.State.L1PricingState()
	l1BaseFeeEstimate, err := pricing.PricePerUnit()
	if err != nil {
		return 0, nil, nil, err
	}
	baseFee, err := c.State.L2PricingState().BaseFeeWei()
	if err != nil {
		return 0, nil, nil, err
	}

	// Compute the fee paid for L1 in L2 terms
	//   See in GasChargingHook that this does not induce truncation error
	//
	brotliCompressionLevel, err := c.State.BrotliCompressionLevel()
	if err != nil {
		return 0, nil, nil, fmt.Errorf("failed to get brotli compression level: %w", err)
	}
	feeForL1, _ := pricing.PosterDataCost(msg, l1pricing.BatchPosterAddress, brotliCompressionLevel)
	feeForL1 = arbmath.BigMulByBips(feeForL1, arbos.GasEstimationL1PricePadding)
	gasForL1 := arbmath.BigDiv(feeForL1, baseFee).Uint64()
	return gasForL1, baseFee, l1BaseFeeEstimate, nil
}

func (n NodeInterface) GasEstimateComponents(
	c ctx, evm mech, value huge, to addr, contractCreation bool, data []byte,
) (uint64, uint64, huge, huge, error) {
	if to == types.NodeInterfaceAddress || to == types.NodeInterfaceDebugAddress {
		return 0, 0, nil, nil, errors.New("cannot estimate virtual contract")
	}

	backend, ok := n.backend.(*arbitrum.APIBackend)
	if !ok {
		return 0, 0, nil, nil, errors.New("failed getting API backend")
	}

	context := n.context
	gasCap := backend.RPCGasCap()
	block := rpc.BlockNumberOrHashWithHash(n.header.Hash(), false)
	args := n.messageArgs(evm, value, to, contractCreation, data)

	totalRaw, err := arbitrum.EstimateGas(context, backend, args, block, nil, gasCap)
	if err != nil {
		return 0, 0, nil, nil, err
	}
	total := uint64(totalRaw)

	pricing := c.State.L1PricingState()

	// Setting the gas currently doesn't affect the PosterDataCost,
	// but we do it anyways for accuracy with potential future changes.
	args.Gas = &totalRaw
	msg, err := args.ToMessage(gasCap, n.header, evm.StateDB.(*state.StateDB), core.MessageGasEstimationMode)
	if err != nil {
		return 0, 0, nil, nil, err
	}
	brotliCompressionLevel, err := c.State.BrotliCompressionLevel()
	if err != nil {
		return 0, 0, nil, nil, fmt.Errorf("failed to get brotli compression level: %w", err)
	}
	feeForL1, _ := pricing.PosterDataCost(msg, l1pricing.BatchPosterAddress, brotliCompressionLevel)

	baseFee, err := c.State.L2PricingState().BaseFeeWei()
	if err != nil {
		return 0, 0, nil, nil, err
	}
	l1BaseFeeEstimate, err := pricing.PricePerUnit()
	if err != nil {
		return 0, 0, nil, nil, err
	}

	// Compute the fee paid for L1 in L2 terms
	gasForL1 := arbos.GetPosterGas(c.State, baseFee, core.MessageGasEstimationMode, feeForL1)

	return total, gasForL1, baseFee, l1BaseFeeEstimate, nil
}

func findBatchContainingBlock(node *arbnode.Node, genesis uint64, block uint64) (uint64, error) {
	if block <= genesis {
		return 0, fmt.Errorf("%wblock %v is part of genesis", blockInGenesis, block)
	}
	pos := arbutil.BlockNumberToMessageCount(block, genesis) - 1
	high, err := node.InboxTracker.GetBatchCount()
	if err != nil {
		return 0, err
	}
	high--
	latestCount, err := node.InboxTracker.GetBatchMessageCount(high)
	if err != nil {
		return 0, err
	}
	latestBlock := arbutil.MessageCountToBlockNumber(latestCount, genesis)
	if int64(block) > latestBlock {
		return 0, fmt.Errorf(
			"%wrequested block %v is after latest on-chain block %v published in batch %v",
			blockAfterLatestBatch, block, latestBlock, high,
		)
	}
	return staker.FindBatchContainingMessageIndex(node.InboxTracker, pos, high)
}

func (n NodeInterface) LegacyLookupMessageBatchProof(c ctx, evm mech, batchNum huge, index uint64) (
	proof []bytes32, path huge, l2Sender addr, l1Dest addr, l2Block huge, l1Block huge, timestamp huge, amount huge, calldataForL1 []byte, err error) {

	node, err := arbNodeFromNodeInterfaceBackend(n.backend)
	if err != nil {
		return
	}
	if node.ClassicOutboxRetriever == nil {
		err = errors.New("this node doesnt support classicLookupMessageBatchProof")
		return
	}
	msg, err := node.ClassicOutboxRetriever.GetMsg(batchNum, index)
	if err != nil {
		return
	}
	proof = msg.ProofNodes
	path = msg.PathInt
	data := msg.Data
	if len(data) < 1+6*32 {
		err = errors.New("unexpected L2 to L1 tx result length")
		return
	}
	if data[0] != 0x3 {
		err = errors.New("unexpected type code")
		return
	}
	data = data[1:]
	l2Sender = common.BytesToAddress(data[:32])
	data = data[32:]
	l1Dest = common.BytesToAddress(data[:32])
	data = data[32:]
	l2Block = new(big.Int).SetBytes(data[:32])
	data = data[32:]
	l1Block = new(big.Int).SetBytes(data[:32])
	data = data[32:]
	timestamp = new(big.Int).SetBytes(data[:32])
	data = data[32:]
	amount = new(big.Int).SetBytes(data[:32])
	data = data[32:]
	calldataForL1 = data
	return
}

// L2BlockRangeForL1 fetches the L1 block number of a given l2 block number.
// c ctx and evm mech arguments are not used but supplied to match the precompile function type in NodeInterface contract
func (n NodeInterface) BlockL1Num(c ctx, evm mech, l2BlockNum uint64) (uint64, error) {
	blockHeader, err := n.backend.HeaderByNumber(n.context, rpc.BlockNumber(l2BlockNum))
	if err != nil {
		return 0, err
	}
	if blockHeader == nil {
		return 0, fmt.Errorf("nil header for l2 block: %d", l2BlockNum)
	}
	blockL1Num := types.DeserializeHeaderExtraInformation(blockHeader).L1BlockNumber
	return blockL1Num, nil
}

func (n NodeInterface) matchL2BlockNumWithL1(c ctx, evm mech, l2BlockNum uint64, l1BlockNum uint64) error {
	blockL1Num, err := n.BlockL1Num(c, evm, l2BlockNum)
	if err != nil {
		return fmt.Errorf("failed to get the L1 block number of the L2 block: %v. Error: %w", l2BlockNum, err)
	}
	if blockL1Num != l1BlockNum {
		return fmt.Errorf("no L2 block was found with the given L1 block number. Found L2 block: %v with L1 block number: %v, given L1 block number: %v", l2BlockNum, blockL1Num, l1BlockNum)
	}
	return nil
}

// L2BlockRangeForL1 finds the first and last L2 block numbers that have the given L1 block number
func (n NodeInterface) L2BlockRangeForL1(c ctx, evm mech, l1BlockNum uint64) (uint64, uint64, error) {
	currentBlockNum := n.backend.CurrentBlock().Number.Uint64()
	genesis := n.backend.ChainConfig().ArbitrumChainParams.GenesisBlockNum

	storedMids := map[uint64]uint64{}
	firstL2BlockForL1 := func(target uint64) (uint64, error) {
		low, high := genesis, currentBlockNum
		highBlockL1Num, err := n.BlockL1Num(c, evm, high)
		if err != nil {
			return 0, err
		}
		if highBlockL1Num < target {
			return high + 1, nil
		}
		for low < high {
			mid := arbmath.SaturatingUAdd(low, high) / 2
			if _, ok := storedMids[mid]; !ok {
				midBlockL1Num, err := n.BlockL1Num(c, evm, mid)
				if err != nil {
					return 0, err
				}
				storedMids[mid] = midBlockL1Num
			}
			if storedMids[mid] < target {
				low = mid + 1
			} else {
				high = mid
			}
		}
		return high, nil
	}

	firstBlock, err := firstL2BlockForL1(l1BlockNum)
	if err != nil {
		return 0, 0, fmt.Errorf("failed to get the first L2 block with the L1 block: %v. Error: %w", l1BlockNum, err)
	}
	lastBlock, err := firstL2BlockForL1(l1BlockNum + 1)
	if err != nil {
		return 0, 0, fmt.Errorf("failed to get the last L2 block with the L1 block: %v. Error: %w", l1BlockNum, err)
	}

	if err := n.matchL2BlockNumWithL1(c, evm, firstBlock, l1BlockNum); err != nil {
		return 0, 0, err
	}
	lastBlock -= 1
	if err = n.matchL2BlockNumWithL1(c, evm, lastBlock, l1BlockNum); err != nil {
		return 0, 0, err
	}
	return firstBlock, lastBlock, nil
}

'''
'''--- nodeInterface/NodeInterfaceDebug.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package nodeInterface

import (
	"context"
	"fmt"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
)

type NodeInterfaceDebug struct {
	Address       addr
	backend       core.NodeInterfaceBackendAPI
	context       context.Context
	header        *types.Header
	sourceMessage *core.Message
	returnMessage struct {
		message *core.Message
		changed *bool
	}
}

type RetryableInfo = node_interfacegen.NodeInterfaceDebugRetryableInfo

func (n NodeInterfaceDebug) GetRetryable(c ctx, evm mech, ticket bytes32) (RetryableInfo, error) {
	// we don't care if the retryable has expired
	retryable, err := c.State.RetryableState().OpenRetryable(ticket, 0)
	if err != nil {
		return RetryableInfo{}, err
	}
	if retryable == nil {
		return RetryableInfo{}, fmt.Errorf("no retryable with id %v exists", ticket)
	}

	timeout, _ := retryable.CalculateTimeout()
	from, _ := retryable.From()
	toPointer, _ := retryable.To()
	callvalue, _ := retryable.Callvalue()
	beneficiary, _ := retryable.Beneficiary()
	calldata, _ := retryable.Calldata()
	tries, err := retryable.NumTries()

	to := common.Address{}
	if toPointer != nil {
		to = *toPointer
	}

	return node_interfacegen.NodeInterfaceDebugRetryableInfo{
		Timeout:     timeout,
		From:        from,
		To:          to,
		Value:       callvalue,
		Beneficiary: beneficiary,
		Tries:       tries,
		Data:        calldata,
	}, err
}

'''
'''--- nodeInterface/virtual-contracts.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package nodeInterface

import (
	"context"
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/gethhook"
	"github.com/offchainlabs/nitro/precompiles"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

type addr = common.Address
type mech = *vm.EVM
type huge = *big.Int
type hash = common.Hash
type bytes32 = [32]byte
type ctx = *precompiles.Context

type BackendAPI = core.NodeInterfaceBackendAPI
type ExecutionResult = core.ExecutionResult

func init() {
	gethhook.RequireHookedGeth()

	nodeInterfaceImpl := &NodeInterface{Address: types.NodeInterfaceAddress}
	nodeInterfaceMeta := node_interfacegen.NodeInterfaceMetaData
	_, nodeInterface := precompiles.MakePrecompile(nodeInterfaceMeta, nodeInterfaceImpl)

	nodeInterfaceDebugImpl := &NodeInterfaceDebug{Address: types.NodeInterfaceDebugAddress}
	nodeInterfaceDebugMeta := node_interfacegen.NodeInterfaceDebugMetaData
	_, nodeInterfaceDebug := precompiles.MakePrecompile(nodeInterfaceDebugMeta, nodeInterfaceDebugImpl)

	core.InterceptRPCMessage = func(
		msg *core.Message,
		ctx context.Context,
		statedb *state.StateDB,
		header *types.Header,
		backend core.NodeInterfaceBackendAPI,
		blockCtx *vm.BlockContext,
	) (*core.Message, *ExecutionResult, error) {
		to := msg.To
		arbosVersion := arbosState.ArbOSVersion(statedb) // check ArbOS has been installed
		if to != nil && arbosVersion != 0 {
			var precompile precompiles.ArbosPrecompile
			var swapMessages bool
			returnMessage := &core.Message{}
			var address addr

			switch *to {
			case types.NodeInterfaceAddress:
				address = types.NodeInterfaceAddress
				duplicate := *nodeInterfaceImpl
				duplicate.backend = backend
				duplicate.context = ctx
				duplicate.header = header
				duplicate.sourceMessage = msg
				duplicate.returnMessage.message = returnMessage
				duplicate.returnMessage.changed = &swapMessages
				precompile = nodeInterface.CloneWithImpl(&duplicate)
			case types.NodeInterfaceDebugAddress:
				address = types.NodeInterfaceDebugAddress
				duplicate := *nodeInterfaceDebugImpl
				duplicate.backend = backend
				duplicate.context = ctx
				duplicate.header = header
				duplicate.sourceMessage = msg
				duplicate.returnMessage.message = returnMessage
				duplicate.returnMessage.changed = &swapMessages
				precompile = nodeInterfaceDebug.CloneWithImpl(&duplicate)
			default:
				return msg, nil, nil
			}

			evm, vmError := backend.GetEVM(ctx, msg, statedb, header, &vm.Config{NoBaseFee: true}, blockCtx)
			go func() {
				<-ctx.Done()
				evm.Cancel()
			}()
			core.ReadyEVMForL2(evm, msg)

			output, gasLeft, err := precompile.Call(
				msg.Data, address, address, msg.From, msg.Value, false, msg.GasLimit, evm,
			)
			if err != nil {
				return msg, nil, err
			}
			if swapMessages {
				return returnMessage, nil, nil
			}
			res := &ExecutionResult{
				UsedGas:       msg.GasLimit - gasLeft,
				Err:           nil,
				ReturnData:    output,
				ScheduledTxes: nil,
			}
			return msg, res, vmError()
		}
		return msg, nil, nil
	}

	core.InterceptRPCGasCap = func(gascap *uint64, msg *core.Message, header *types.Header, statedb *state.StateDB) {
		if *gascap == 0 {
			// It's already unlimited
			return
		}
		arbosVersion := arbosState.ArbOSVersion(statedb)
		if arbosVersion == 0 {
			// ArbOS hasn't been installed, so use the vanilla gas cap
			return
		}
		state, err := arbosState.OpenSystemArbosState(statedb, nil, true)
		if err != nil {
			log.Error("failed to open ArbOS state", "err", err)
			return
		}
		if header.BaseFee.Sign() == 0 {
			// if gas is free or there's no reimbursable poster, the user won't pay for L1 data costs
			return
		}

		brotliCompressionLevel, err := state.BrotliCompressionLevel()
		if err != nil {
			log.Error("failed to get brotli compression level", "err", err)
			return
		}
		posterCost, _ := state.L1PricingState().PosterDataCost(msg, l1pricing.BatchPosterAddress, brotliCompressionLevel)
		posterCostInL2Gas := arbos.GetPosterGas(state, header.BaseFee, msg.TxRunMode, posterCost)
		*gascap = arbmath.SaturatingUAdd(*gascap, posterCostInL2Gas)
	}

	core.GetArbOSSpeedLimitPerSecond = func(statedb *state.StateDB) (uint64, error) {
		arbosVersion := arbosState.ArbOSVersion(statedb)
		if arbosVersion == 0 {
			return 0.0, errors.New("ArbOS not installed")
		}
		state, err := arbosState.OpenSystemArbosState(statedb, nil, true)
		if err != nil {
			log.Error("failed to open ArbOS state", "err", err)
			return 0.0, err
		}
		pricing := state.L2PricingState()
		speedLimit, err := pricing.SpeedLimitPerSecond()
		if err != nil {
			log.Error("failed to get the speed limit", "err", err)
			return 0.0, err
		}
		return speedLimit, nil
	}

	arbSys, err := precompilesgen.ArbSysMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	l2ToL1TxTopic = arbSys.Events["L2ToL1Tx"].ID
	l2ToL1TransactionTopic = arbSys.Events["L2ToL1Transaction"].ID
	merkleTopic = arbSys.Events["SendMerkleUpdate"].ID
}

func arbNodeFromNodeInterfaceBackend(backend BackendAPI) (*arbnode.Node, error) {
	apiBackend, ok := backend.(*arbitrum.APIBackend)
	if !ok {
		return nil, errors.New("API backend isn't Arbitrum")
	}
	arbNode, ok := apiBackend.GetArbitrumNode().(*arbnode.Node)
	if !ok {
		return nil, errors.New("failed to get Arbitrum Node from backend")
	}
	return arbNode, nil
}

func blockchainFromNodeInterfaceBackend(backend BackendAPI) (*core.BlockChain, error) {
	apiBackend, ok := backend.(*arbitrum.APIBackend)
	if !ok {
		return nil, errors.New("API backend isn't Arbitrum")
	}
	bc := apiBackend.BlockChain()
	if bc == nil {
		return nil, errors.New("failed to get Blockchain from backend")
	}
	return bc, nil
}

'''
'''--- precompiles/ArbAddressTable.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"
)

// ArbAddressTable precompile provides the ability to create short-hands for commonly used accounts.
type ArbAddressTable struct {
	Address addr // 0x66
}

// AddressExists checks if an address exists in the table
func (con ArbAddressTable) AddressExists(c ctx, evm mech, addr addr) (bool, error) {
	return c.State.AddressTable().AddressExists(addr)
}

// Compress and returns the bytes that represent the address
func (con ArbAddressTable) Compress(c ctx, evm mech, addr addr) ([]uint8, error) {
	return c.State.AddressTable().Compress(addr)
}

// Decompress the compressed bytes at the given offset with those of the corresponding account
func (con ArbAddressTable) Decompress(c ctx, evm mech, buf []uint8, offset huge) (addr, huge, error) {
	if !offset.IsInt64() {
		return addr{}, nil, errors.New("invalid offset in ArbAddressTable.Decompress")
	}
	ioffset := offset.Int64()
	if ioffset > int64(len(buf)) {
		return addr{}, nil, errors.New("invalid offset in ArbAddressTable.Decompress")
	}
	result, nbytes, err := c.State.AddressTable().Decompress(buf[ioffset:])
	return result, big.NewInt(int64(nbytes)), err
}

// Lookup the index of an address in the table
func (con ArbAddressTable) Lookup(c ctx, evm mech, addr addr) (huge, error) {
	result, exists, err := c.State.AddressTable().Lookup(addr)
	if err != nil {
		return nil, err
	}
	if !exists {
		return nil, errors.New("address does not exist in AddressTable")
	}
	return big.NewInt(int64(result)), nil
}

// LookupIndex for  an address in the table by index
func (con ArbAddressTable) LookupIndex(c ctx, evm mech, index huge) (addr, error) {
	if !index.IsUint64() {
		return addr{}, errors.New("invalid index in ArbAddressTable.LookupIndex")
	}
	result, exists, err := c.State.AddressTable().LookupIndex(index.Uint64())
	if err != nil {
		return addr{}, err
	}
	if !exists {
		return addr{}, errors.New("index does not exist in AddressTable")
	}
	return result, nil
}

// Register adds an account to the table, shrinking its compressed representation
func (con ArbAddressTable) Register(c ctx, evm mech, addr addr) (huge, error) {
	slot, err := c.State.AddressTable().Register(addr)
	return big.NewInt(int64(slot)), err
}

// Size gets the number of addresses in the table
func (con ArbAddressTable) Size(c ctx, evm mech) (huge, error) {
	size, err := c.State.AddressTable().Size()
	return big.NewInt(int64(size)), err
}

'''
'''--- precompiles/ArbAddressTable_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"bytes"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestArbAddressTableInit(t *testing.T) {
	evm := newMockEVMForTesting()
	atab := ArbAddressTable{}
	context := testContext(common.Address{}, evm)

	size, err := atab.Size(context, evm)
	Require(t, err)
	if (!size.IsInt64()) || (size.Int64() != 0) {
		t.Fatal()
	}

	_, shouldErr := atab.Lookup(context, evm, common.Address{})
	if shouldErr == nil {
		t.Fatal()
	}

	_, shouldErr = atab.LookupIndex(context, evm, big.NewInt(0))
	if shouldErr == nil {
		t.Fatal()
	}
}

func TestAddressTable1(t *testing.T) {
	evm := newMockEVMForTesting()
	atab := ArbAddressTable{}
	context := testContext(common.Address{}, evm)

	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	// register addr
	slot, err := atab.Register(context, evm, addr)
	Require(t, err)
	if (!slot.IsInt64()) || (slot.Int64() != 0) {
		t.Fatal()
	}

	// verify Size() is 1
	size, err := atab.Size(context, evm)
	Require(t, err)
	if (!size.IsInt64()) || (size.Int64() != 1) {
		t.Fatal()
	}

	// verify Lookup of addr returns 0
	index, err := atab.Lookup(context, evm, addr)
	Require(t, err)
	if (!index.IsInt64()) || (index.Int64() != 0) {
		t.Fatal()
	}

	// verify Lookup of nonexistent address returns error
	_, shouldErr := atab.Lookup(context, evm, common.Address{})
	if shouldErr == nil {
		t.Fatal()
	}

	// verify LookupIndex of 0 returns addr
	addr2, err := atab.LookupIndex(context, evm, big.NewInt(0))
	Require(t, err)
	if addr2 != addr {
		t.Fatal()
	}

	// verify LookupIndex of 1 returns error
	_, shouldErr = atab.LookupIndex(context, evm, big.NewInt(1))
	if shouldErr == nil {
		t.Fatal()
	}
}

func TestAddressTableCompressNotInTable(t *testing.T) {
	evm := newMockEVMForTesting()
	atab := ArbAddressTable{}
	context := testContext(common.Address{}, evm)

	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	// verify that compressing addr produces the 21-byte format
	res, err := atab.Compress(context, evm, addr)
	Require(t, err)
	if len(res) != 21 {
		t.Fatal()
	}
	if !bytes.Equal(addr.Bytes(), res[1:]) {
		t.Fatal()
	}

	// verify that decompressing res consumes 21 bytes and returns the original addr
	dec, nbytes, err := atab.Decompress(context, evm, res, big.NewInt(0))
	Require(t, err)
	if (!nbytes.IsInt64()) || (nbytes.Int64() != 21) {
		t.Fatal()
	}
	if dec != addr {
		t.Fatal()
	}
}

func TestAddressTableCompressInTable(t *testing.T) {
	evm := newMockEVMForTesting()
	atab := ArbAddressTable{}
	context := testContext(common.Address{}, evm)

	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	// Register addr
	if _, err := atab.Register(context, evm, addr); err != nil {
		t.Fatal(err)
	}

	// verify that compressing addr yields the <= 9 byte format
	res, err := atab.Compress(context, evm, addr)
	Require(t, err)
	if len(res) > 9 {
		Fail(t, len(res))
	}

	// add a byte of padding at the beginning and end of res
	res = append([]byte{99}, res...)
	res = append(res, 33)

	// verify that decompressing res consumes all but two bytes of res and produces addr
	dec, nbytes, err := atab.Decompress(context, evm, res, big.NewInt(1))
	Require(t, err)
	if (!nbytes.IsInt64()) || (nbytes.Int64()+2 != int64(len(res))) {
		Fail(t)
	}
	if dec != addr {
		Fail(t)
	}
}

func newMockEVMForTesting() *vm.EVM {
	return newMockEVMForTestingWithVersion(nil)
}

func newMockEVMForTestingWithVersionAndRunMode(version *uint64, runMode core.MessageRunMode) *vm.EVM {
	evm := newMockEVMForTestingWithVersion(version)
	evm.ProcessingHook = arbos.NewTxProcessor(evm, &core.Message{TxRunMode: runMode})
	return evm
}

func newMockEVMForTestingWithVersion(version *uint64) *vm.EVM {
	chainConfig := params.ArbitrumDevTestChainConfig()
	if version != nil {
		chainConfig.ArbitrumChainParams.InitialArbOSVersion = *version
	}
	_, statedb := arbosState.NewArbosMemoryBackedArbOSState()
	context := vm.BlockContext{
		BlockNumber: big.NewInt(0),
		GasLimit:    ^uint64(0),
		Time:        0,
	}
	evm := vm.NewEVM(context, vm.TxContext{}, statedb, chainConfig, vm.Config{})
	evm.ProcessingHook = &arbos.TxProcessor{}
	return evm
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- precompiles/ArbAggregator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"

	"github.com/offchainlabs/nitro/arbos/l1pricing"
)

// ArbAggregator provides aggregators and their users methods for configuring how they participate in L1 aggregation.
// Arbitrum One's default aggregator is the Sequencer, which a user will prefer unless SetPreferredAggregator()
// is invoked to change it.
type ArbAggregator struct {
	Address addr // 0x6d
}

var ErrNotOwner = errors.New("must be called by chain owner")

// GetPreferredAggregator returns the preferred aggregator address.
// Deprecated: Do not use this method.
func (con ArbAggregator) GetPreferredAggregator(c ctx, evm mech, address addr) (prefAgg addr, isDefault bool, err error) {
	return l1pricing.BatchPosterAddress, true, err
}

// GetDefaultAggregator returns the default aggregator address.
// Deprecated: Do not use this method.
func (con ArbAggregator) GetDefaultAggregator(c ctx, evm mech) (addr, error) {
	return l1pricing.BatchPosterAddress, nil
}

// GetBatchPosters gets the addresses of all current batch posters
func (con ArbAggregator) GetBatchPosters(c ctx, evm mech) ([]addr, error) {
	return c.State.L1PricingState().BatchPosterTable().AllPosters(65536)
}

func (con ArbAggregator) AddBatchPoster(c ctx, evm mech, newBatchPoster addr) error {
	isOwner, err := c.State.ChainOwners().IsMember(c.caller)
	if err != nil {
		return err
	}
	if !isOwner {
		return ErrNotOwner
	}
	batchPosterTable := c.State.L1PricingState().BatchPosterTable()
	isBatchPoster, err := batchPosterTable.ContainsPoster(newBatchPoster)
	if err != nil {
		return err
	}
	if !isBatchPoster {
		_, err = batchPosterTable.AddPoster(newBatchPoster, newBatchPoster)
		if err != nil {
			return err
		}
	}
	return nil
}

// GetFeeCollector gets a batch poster's fee collector
func (con ArbAggregator) GetFeeCollector(c ctx, evm mech, batchPoster addr) (addr, error) {
	posterInfo, err := c.State.L1PricingState().BatchPosterTable().OpenPoster(batchPoster, false)
	if err != nil {
		return addr{}, err
	}
	return posterInfo.PayTo()
}

// SetFeeCollector sets a batch poster's fee collector (caller must be the batch poster, its fee collector, or an owner)
func (con ArbAggregator) SetFeeCollector(c ctx, evm mech, batchPoster addr, newFeeCollector addr) error {
	posterInfo, err := c.State.L1PricingState().BatchPosterTable().OpenPoster(batchPoster, false)
	if err != nil {
		return err
	}
	oldFeeCollector, err := posterInfo.PayTo()
	if err != nil {
		return err
	}
	if c.caller != batchPoster && c.caller != oldFeeCollector {
		isOwner, err := c.State.ChainOwners().IsMember(c.caller)
		if err != nil {
			return err
		}
		if !isOwner {
			return errors.New("only a batch poster (or its fee collector / chain owner) may change its fee collector")
		}
	}
	return posterInfo.SetPayTo(newFeeCollector)
}

// GetTxBaseFee gets an aggregator's current fixed fee to submit a tx
func (con ArbAggregator) GetTxBaseFee(c ctx, evm mech, aggregator addr) (huge, error) {
	// This is deprecated and now always returns zero.
	return big.NewInt(0), nil
}

// SetTxBaseFee sets an aggregator's fixed fee (caller must be the aggregator, its fee collector, or an owner)
func (con ArbAggregator) SetTxBaseFee(c ctx, evm mech, aggregator addr, feeInL1Gas huge) error {
	// This is deprecated and is now a no-op.
	return nil
}

'''
'''--- precompiles/ArbAggregator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
)

func TestArbAggregatorBatchPosters(t *testing.T) {
	evm := newMockEVMForTesting()
	context := testContext(common.Address{}, evm)

	addr := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])

	// initially should have one batch poster
	bps, err := ArbAggregator{}.GetBatchPosters(context, evm)
	Require(t, err)
	if len(bps) != 1 {
		Fail(t)
	}

	// add addr as a batch poster
	Require(t, ArbDebug{}.BecomeChainOwner(context, evm))
	Require(t, ArbAggregator{}.AddBatchPoster(context, evm, addr))

	// there should now be two batch posters, and addr should be one of them
	bps, err = ArbAggregator{}.GetBatchPosters(context, evm)
	Require(t, err)
	if len(bps) != 2 {
		Fail(t)
	}
	if bps[0] != addr && bps[1] != addr {
		Fail(t)
	}
}

func TestFeeCollector(t *testing.T) {
	evm := newMockEVMForTesting()
	agg := ArbAggregator{}

	aggAddr := l1pricing.BatchPosterAddress
	collectorAddr := common.BytesToAddress(crypto.Keccak256([]byte{1})[:20])
	impostorAddr := common.BytesToAddress(crypto.Keccak256([]byte{2})[:20])

	aggCtx := testContext(aggAddr, evm)
	callerCtx := testContext(common.Address{}, evm)
	collectorCtx := testContext(collectorAddr, evm)
	imposterCtx := testContext(impostorAddr, evm)

	// initial result should be addr
	coll, err := agg.GetFeeCollector(callerCtx, evm, aggAddr)
	Require(t, err)
	if coll != aggAddr {
		Fail(t)
	}

	// set fee collector to collectorAddr
	Require(t, agg.SetFeeCollector(aggCtx, evm, aggAddr, collectorAddr))

	// fee collector should now be collectorAddr
	coll, err = agg.GetFeeCollector(callerCtx, evm, aggAddr)
	Require(t, err)
	if coll != collectorAddr {
		Fail(t)
	}

	// trying to set someone else's collector is an error
	shouldErr := agg.SetFeeCollector(imposterCtx, evm, aggAddr, impostorAddr)
	if shouldErr == nil {
		Fail(t)
	}

	// but the fee collector can replace itself
	Require(t, agg.SetFeeCollector(collectorCtx, evm, aggAddr, impostorAddr))
}

func TestTxBaseFee(t *testing.T) {
	evm := newMockEVMForTesting()
	agg := ArbAggregator{}

	aggAddr := common.BytesToAddress(crypto.Keccak256([]byte{0})[:20])
	targetFee := big.NewInt(973)

	aggCtx := testContext(aggAddr, evm)
	callerCtx := testContext(common.Address{}, evm)

	// initial result should be zero
	fee, err := agg.GetTxBaseFee(callerCtx, evm, aggAddr)
	Require(t, err)
	if fee.Cmp(big.NewInt(0)) != 0 {
		Fail(t, fee)
	}

	// set base fee to value -- should be ignored
	if err := agg.SetTxBaseFee(aggCtx, evm, aggAddr, targetFee); err != nil {
		Fail(t, err)
	}

	// base fee should still be zero
	fee, err = agg.GetTxBaseFee(callerCtx, evm, aggAddr)
	Require(t, err)
	if fee.Cmp(big.NewInt(0)) != 0 {
		Fail(t, fee)
	}
}

'''
'''--- precompiles/ArbBLS.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

// ArbBLS provides a registry of BLS public keys for accounts.
type ArbBLS struct {
	Address addr
}

'''
'''--- precompiles/ArbDebug.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"

	"github.com/ethereum/go-ethereum/common"
)

// All calls to this precompile are authorized by the DebugPrecompile wrapper,
// which ensures these methods are not accessible in production.
type ArbDebug struct {
	Address      addr                                                     // 0xff
	Basic        func(ctx, mech, bool, bytes32) error                     // index'd: 2nd
	Mixed        func(ctx, mech, bool, bool, bytes32, addr, addr) error   // index'd: 1st 3rd 5th
	Store        func(ctx, mech, bool, addr, huge, bytes32, []byte) error // index'd: 1st 2nd
	BasicGasCost func(bool, bytes32) (uint64, error)
	MixedGasCost func(bool, bool, bytes32, addr, addr) (uint64, error)
	StoreGasCost func(bool, addr, huge, bytes32, []byte) (uint64, error)

	CustomError func(uint64, string, bool) error
	UnusedError func() error
}

func (con ArbDebug) Events(c ctx, evm mech, paid huge, flag bool, value bytes32) (addr, huge, error) {
	// Emits 2 events that cover each case
	//   Basic tests an index'd value & a normal value
	//   Mixed interleaves index'd and normal values that may need to be padded

	err := con.Basic(c, evm, !flag, value)
	if err != nil {
		return addr{}, nil, err
	}

	err = con.Mixed(c, evm, flag, !flag, value, con.Address, c.caller)
	if err != nil {
		return addr{}, nil, err
	}

	return c.caller, paid, nil
}

func (con ArbDebug) EventsView(c ctx, evm mech) error {
	_, _, err := con.Events(c, evm, common.Big0, true, bytes32{})
	return err
}

func (con ArbDebug) CustomRevert(c ctx, number uint64) error {
	return con.CustomError(number, "This spider family wards off bugs: /\\oo/\\ //\\(oo)/\\ /\\oo/\\", true)
}

// Caller becomes a chain owner
func (con ArbDebug) BecomeChainOwner(c ctx, evm mech) error {
	return c.State.ChainOwners().Add(c.caller)
}

func (con ArbDebug) LegacyError(c ctx) error {
	return errors.New("example legacy error")
}

'''
'''--- precompiles/ArbFunctionTable.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"
)

// ArbFunctionTable  precompile provided aggregator's the ability to manage function tables.
// Aggregation works differently in Nitro, so these methods have been stubbed and their effects disabled.
// They are kept for backwards compatibility.
type ArbFunctionTable struct {
	Address addr // 0x68
}

// Upload does nothing
func (con ArbFunctionTable) Upload(c ctx, evm mech, buf []byte) error {
	return nil
}

// Size returns the empty table's size, which is 0
func (con ArbFunctionTable) Size(c ctx, evm mech, addr addr) (huge, error) {
	return big.NewInt(0), nil
}

// Get reverts since the table is empty
func (con ArbFunctionTable) Get(c ctx, evm mech, addr addr, index huge) (huge, bool, huge, error) {
	return nil, false, nil, errors.New("table is empty")
}

'''
'''--- precompiles/ArbGasInfo.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/util/arbmath"
)

// ArbGasInfo provides insight into the cost of using the rollup.
type ArbGasInfo struct {
	Address addr // 0x6c
}

var storageArbGas = big.NewInt(int64(storage.StorageWriteCost))

const AssumedSimpleTxSize = 140

// GetPricesInWeiWithAggregator gets  prices in wei when using the provided aggregator
func (con ArbGasInfo) GetPricesInWeiWithAggregator(
	c ctx,
	evm mech,
	aggregator addr,
) (huge, huge, huge, huge, huge, huge, error) {
	if c.State.ArbOSVersion() < 4 {
		return con._preVersion4_GetPricesInWeiWithAggregator(c, evm, aggregator)
	}

	l1GasPrice, err := c.State.L1PricingState().PricePerUnit()
	if err != nil {
		return nil, nil, nil, nil, nil, nil, err
	}
	l2GasPrice := evm.Context.BaseFee

	// aggregators compress calldata, so we must estimate accordingly
	weiForL1Calldata := arbmath.BigMulByUint(l1GasPrice, params.TxDataNonZeroGasEIP2028)

	// the cost of a simple tx without calldata
	perL2Tx := arbmath.BigMulByUint(weiForL1Calldata, AssumedSimpleTxSize)

	// nitro's compute-centric l2 gas pricing has no special compute component that rises independently
	perArbGasBase, err := c.State.L2PricingState().MinBaseFeeWei()
	if err != nil {
		return nil, nil, nil, nil, nil, nil, err
	}
	if arbmath.BigLessThan(l2GasPrice, perArbGasBase) {
		perArbGasBase = l2GasPrice
	}
	perArbGasCongestion := arbmath.BigSub(l2GasPrice, perArbGasBase)
	perArbGasTotal := l2GasPrice

	weiForL2Storage := arbmath.BigMul(l2GasPrice, storageArbGas)

	return perL2Tx, weiForL1Calldata, weiForL2Storage, perArbGasBase, perArbGasCongestion, perArbGasTotal, nil
}

func (con ArbGasInfo) _preVersion4_GetPricesInWeiWithAggregator(
	c ctx,
	evm mech,
	aggregator addr,
) (huge, huge, huge, huge, huge, huge, error) {
	l1GasPrice, err := c.State.L1PricingState().PricePerUnit()
	if err != nil {
		return nil, nil, nil, nil, nil, nil, err
	}
	l2GasPrice := evm.Context.BaseFee

	// aggregators compress calldata, so we must estimate accordingly
	weiForL1Calldata := arbmath.BigMulByUint(l1GasPrice, params.TxDataNonZeroGasEIP2028)

	// the cost of a simple tx without calldata
	perL2Tx := arbmath.BigMulByUint(weiForL1Calldata, AssumedSimpleTxSize)

	// nitro's compute-centric l2 gas pricing has no special compute component that rises independently
	perArbGasBase := l2GasPrice
	perArbGasCongestion := common.Big0
	perArbGasTotal := l2GasPrice

	weiForL2Storage := arbmath.BigMul(l2GasPrice, storageArbGas)

	return perL2Tx, weiForL1Calldata, weiForL2Storage, perArbGasBase, perArbGasCongestion, perArbGasTotal, nil
}

// GetPricesInWei gets prices in wei when using the caller's preferred aggregator
func (con ArbGasInfo) GetPricesInWei(c ctx, evm mech) (huge, huge, huge, huge, huge, huge, error) {
	return con.GetPricesInWeiWithAggregator(c, evm, addr{})
}

// GetPricesInArbGasWithAggregator gets prices in ArbGas when using the provided aggregator
func (con ArbGasInfo) GetPricesInArbGasWithAggregator(c ctx, evm mech, aggregator addr) (huge, huge, huge, error) {
	if c.State.ArbOSVersion() < 4 {
		return con._preVersion4_GetPricesInArbGasWithAggregator(c, evm, aggregator)
	}
	l1GasPrice, err := c.State.L1PricingState().PricePerUnit()
	if err != nil {
		return nil, nil, nil, err
	}
	l2GasPrice := evm.Context.BaseFee

	// aggregators compress calldata, so we must estimate accordingly
	weiForL1Calldata := arbmath.BigMulByUint(l1GasPrice, params.TxDataNonZeroGasEIP2028)
	weiPerL2Tx := arbmath.BigMulByUint(weiForL1Calldata, AssumedSimpleTxSize)
	gasForL1Calldata := common.Big0
	gasPerL2Tx := common.Big0
	if l2GasPrice.Sign() > 0 {
		gasForL1Calldata = arbmath.BigDiv(weiForL1Calldata, l2GasPrice)
		gasPerL2Tx = arbmath.BigDiv(weiPerL2Tx, l2GasPrice)
	}

	return gasPerL2Tx, gasForL1Calldata, storageArbGas, nil
}

func (con ArbGasInfo) _preVersion4_GetPricesInArbGasWithAggregator(c ctx, evm mech, aggregator addr) (huge, huge, huge, error) {
	l1GasPrice, err := c.State.L1PricingState().PricePerUnit()
	if err != nil {
		return nil, nil, nil, err
	}
	l2GasPrice := evm.Context.BaseFee

	// aggregators compress calldata, so we must estimate accordingly
	weiForL1Calldata := arbmath.BigMulByUint(l1GasPrice, params.TxDataNonZeroGasEIP2028)
	gasForL1Calldata := common.Big0
	if l2GasPrice.Sign() > 0 {
		gasForL1Calldata = arbmath.BigDiv(weiForL1Calldata, l2GasPrice)
	}

	perL2Tx := big.NewInt(AssumedSimpleTxSize)
	return perL2Tx, gasForL1Calldata, storageArbGas, nil
}

// GetPricesInArbGas gets prices in ArbGas when using the caller's preferred aggregator
func (con ArbGasInfo) GetPricesInArbGas(c ctx, evm mech) (huge, huge, huge, error) {
	return con.GetPricesInArbGasWithAggregator(c, evm, addr{})
}

// GetGasAccountingParams gets the rollup's speed limit, pool size, and tx gas limit
func (con ArbGasInfo) GetGasAccountingParams(c ctx, evm mech) (huge, huge, huge, error) {
	l2pricing := c.State.L2PricingState()
	speedLimit, _ := l2pricing.SpeedLimitPerSecond()
	maxTxGasLimit, err := l2pricing.PerBlockGasLimit()
	return arbmath.UintToBig(speedLimit), arbmath.UintToBig(maxTxGasLimit), arbmath.UintToBig(maxTxGasLimit), err
}

// GetMinimumGasPrice gets the minimum gas price needed for a transaction to succeed
func (con ArbGasInfo) GetMinimumGasPrice(c ctx, evm mech) (huge, error) {
	return c.State.L2PricingState().MinBaseFeeWei()
}

// GetL1BaseFeeEstimate gets the current estimate of the L1 basefee
func (con ArbGasInfo) GetL1BaseFeeEstimate(c ctx, evm mech) (huge, error) {
	return c.State.L1PricingState().PricePerUnit()
}

// GetL1BaseFeeEstimateInertia gets how slowly ArbOS updates its estimate of the L1 basefee
func (con ArbGasInfo) GetL1BaseFeeEstimateInertia(c ctx, evm mech) (uint64, error) {
	return c.State.L1PricingState().Inertia()
}

// GetL1RewardRate gets the L1 pricer reward rate
func (con ArbGasInfo) GetL1RewardRate(c ctx, evm mech) (uint64, error) {
	return c.State.L1PricingState().GetRewardsRate()
}

// GetL1RewardRecipient gets the L1 pricer reward recipient
func (con ArbGasInfo) GetL1RewardRecipient(c ctx, evm mech) (common.Address, error) {
	return c.State.L1PricingState().GetRewardsRecepient()
}

// GetL1GasPriceEstimate gets the current estimate of the L1 basefee
func (con ArbGasInfo) GetL1GasPriceEstimate(c ctx, evm mech) (huge, error) {
	return con.GetL1BaseFeeEstimate(c, evm)
}

// GetCurrentTxL1GasFees gets the fee paid to the aggregator for posting this tx
func (con ArbGasInfo) GetCurrentTxL1GasFees(c ctx, evm mech) (huge, error) {
	return c.txProcessor.PosterFee, nil
}

// GetGasBacklog gets the backlogged amount of gas burnt in excess of the speed limit
func (con ArbGasInfo) GetGasBacklog(c ctx, evm mech) (uint64, error) {
	return c.State.L2PricingState().GasBacklog()
}

// GetPricingInertia gets the L2 basefee in response to backlogged gas
func (con ArbGasInfo) GetPricingInertia(c ctx, evm mech) (uint64, error) {
	return c.State.L2PricingState().PricingInertia()
}

// GetGasBacklogTolerance gets the forgivable amount of backlogged gas ArbOS will ignore when raising the basefee
func (con ArbGasInfo) GetGasBacklogTolerance(c ctx, evm mech) (uint64, error) {
	return c.State.L2PricingState().BacklogTolerance()
}

func (con ArbGasInfo) GetL1PricingSurplus(c ctx, evm mech) (*big.Int, error) {
	if c.State.ArbOSVersion() < 10 {
		return con._preversion10_GetL1PricingSurplus(c, evm)
	}
	ps := c.State.L1PricingState()
	fundsDueForRefunds, err := ps.BatchPosterTable().TotalFundsDue()
	if err != nil {
		return nil, err
	}
	fundsDueForRewards, err := ps.FundsDueForRewards()
	if err != nil {
		return nil, err
	}
	haveFunds, err := ps.L1FeesAvailable()
	if err != nil {
		return nil, err
	}
	needFunds := arbmath.BigAdd(fundsDueForRefunds, fundsDueForRewards)
	return arbmath.BigSub(haveFunds, needFunds), nil
}

func (con ArbGasInfo) _preversion10_GetL1PricingSurplus(c ctx, evm mech) (*big.Int, error) {
	ps := c.State.L1PricingState()
	fundsDueForRefunds, err := ps.BatchPosterTable().TotalFundsDue()
	if err != nil {
		return nil, err
	}
	fundsDueForRewards, err := ps.FundsDueForRewards()
	if err != nil {
		return nil, err
	}
	haveFunds := evm.StateDB.GetBalance(l1pricing.L1PricerFundsPoolAddress)
	needFunds := arbmath.BigAdd(fundsDueForRefunds, fundsDueForRewards)
	return arbmath.BigSub(haveFunds, needFunds), nil
}

func (con ArbGasInfo) GetPerBatchGasCharge(c ctx, evm mech) (int64, error) {
	return c.State.L1PricingState().PerBatchGasCost()
}

func (con ArbGasInfo) GetAmortizedCostCapBips(c ctx, evm mech) (uint64, error) {
	return c.State.L1PricingState().AmortizedCostCapBips()
}

func (con ArbGasInfo) GetL1FeesAvailable(c ctx, evm mech) (huge, error) {
	return c.State.L1PricingState().L1FeesAvailable()
}

'''
'''--- precompiles/ArbInfo.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/util/arbmath"
)

// ArbInfo povides the ability to lookup basic info about accounts and contracts.
type ArbInfo struct {
	Address addr // 0x65
}

// GetBalance retrieves an account's balance
func (con ArbInfo) GetBalance(c ctx, evm mech, account addr) (huge, error) {
	if err := c.Burn(params.BalanceGasEIP1884); err != nil {
		return nil, err
	}
	return evm.StateDB.GetBalance(account), nil
}

// GetCode retrieves a contract's deployed code
func (con ArbInfo) GetCode(c ctx, evm mech, account addr) ([]byte, error) {
	if err := c.Burn(params.ColdSloadCostEIP2929); err != nil {
		return nil, err
	}
	code := evm.StateDB.GetCode(account)
	if err := c.Burn(params.CopyGas * arbmath.WordsForBytes(uint64(len(code)))); err != nil {
		return nil, err
	}
	return code, nil
}

'''
'''--- precompiles/ArbOwner.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"math/big"

	"github.com/offchainlabs/nitro/arbos/l1pricing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
)

// ArbOwner precompile provides owners with tools for managing the rollup.
// All calls to this precompile are authorized by the OwnerPrecompile wrapper,
// which ensures only a chain owner can access these methods. For methods that
// are safe for non-owners to call, see ArbOwnerOld
type ArbOwner struct {
	Address          addr // 0x70
	OwnerActs        func(ctx, mech, bytes4, addr, []byte) error
	OwnerActsGasCost func(bytes4, addr, []byte) (uint64, error)
}

var (
	ErrOutOfBounds = errors.New("value out of bounds")
)

// AddChainOwner adds account as a chain owner
func (con ArbOwner) AddChainOwner(c ctx, evm mech, newOwner addr) error {
	return c.State.ChainOwners().Add(newOwner)
}

// RemoveChainOwner removes account from the list of chain owners
func (con ArbOwner) RemoveChainOwner(c ctx, evm mech, addr addr) error {
	member, _ := con.IsChainOwner(c, evm, addr)
	if !member {
		return errors.New("tried to remove non-owner")
	}
	return c.State.ChainOwners().Remove(addr, c.State.ArbOSVersion())
}

// IsChainOwner checks if the account is a chain owner
func (con ArbOwner) IsChainOwner(c ctx, evm mech, addr addr) (bool, error) {
	return c.State.ChainOwners().IsMember(addr)
}

// GetAllChainOwners retrieves the list of chain owners
func (con ArbOwner) GetAllChainOwners(c ctx, evm mech) ([]common.Address, error) {
	return c.State.ChainOwners().AllMembers(65536)
}

// SetL1BaseFeeEstimateInertia sets how slowly ArbOS updates its estimate of the L1 basefee
func (con ArbOwner) SetL1BaseFeeEstimateInertia(c ctx, evm mech, inertia uint64) error {
	return c.State.L1PricingState().SetInertia(inertia)
}

// SetL2BaseFee sets the L2 gas price directly, bypassing the pool calculus
func (con ArbOwner) SetL2BaseFee(c ctx, evm mech, priceInWei huge) error {
	return c.State.L2PricingState().SetBaseFeeWei(priceInWei)
}

// SetMinimumL2BaseFee sets the minimum base fee needed for a transaction to succeed
func (con ArbOwner) SetMinimumL2BaseFee(c ctx, evm mech, priceInWei huge) error {
	return c.State.L2PricingState().SetMinBaseFeeWei(priceInWei)
}

// SetSpeedLimit sets the computational speed limit for the chain
func (con ArbOwner) SetSpeedLimit(c ctx, evm mech, limit uint64) error {
	return c.State.L2PricingState().SetSpeedLimitPerSecond(limit)
}

// SetMaxTxGasLimit sets the maximum size a tx (and block) can be
func (con ArbOwner) SetMaxTxGasLimit(c ctx, evm mech, limit uint64) error {
	return c.State.L2PricingState().SetMaxPerBlockGasLimit(limit)
}

// SetL2GasPricingInertia sets the L2 gas pricing inertia
func (con ArbOwner) SetL2GasPricingInertia(c ctx, evm mech, sec uint64) error {
	return c.State.L2PricingState().SetPricingInertia(sec)
}

// SetL2GasBacklogTolerance sets the L2 gas backlog tolerance
func (con ArbOwner) SetL2GasBacklogTolerance(c ctx, evm mech, sec uint64) error {
	return c.State.L2PricingState().SetBacklogTolerance(sec)
}

// GetNetworkFeeAccount gets the network fee collector
func (con ArbOwner) GetNetworkFeeAccount(c ctx, evm mech) (addr, error) {
	return c.State.NetworkFeeAccount()
}

// GetInfraFeeAccount gets the infrastructure fee collector
func (con ArbOwner) GetInfraFeeAccount(c ctx, evm mech) (addr, error) {
	return c.State.InfraFeeAccount()
}

// SetNetworkFeeAccount sets the network fee collector to the new network fee account
func (con ArbOwner) SetNetworkFeeAccount(c ctx, evm mech, newNetworkFeeAccount addr) error {
	return c.State.SetNetworkFeeAccount(newNetworkFeeAccount)
}

// SetInfraFeeAccount sets the infra fee collector to the new network fee account
func (con ArbOwner) SetInfraFeeAccount(c ctx, evm mech, newNetworkFeeAccount addr) error {
	return c.State.SetInfraFeeAccount(newNetworkFeeAccount)
}

// ScheduleArbOSUpgrade to the requested version at the requested timestamp
func (con ArbOwner) ScheduleArbOSUpgrade(c ctx, evm mech, newVersion uint64, timestamp uint64) error {
	return c.State.ScheduleArbOSUpgrade(newVersion, timestamp)
}

func (con ArbOwner) SetL1PricingEquilibrationUnits(c ctx, evm mech, equilibrationUnits huge) error {
	return c.State.L1PricingState().SetEquilibrationUnits(equilibrationUnits)
}

func (con ArbOwner) SetL1PricingInertia(c ctx, evm mech, inertia uint64) error {
	return c.State.L1PricingState().SetInertia(inertia)
}

func (con ArbOwner) SetL1PricingRewardRecipient(c ctx, evm mech, recipient addr) error {
	return c.State.L1PricingState().SetPayRewardsTo(recipient)
}

func (con ArbOwner) SetL1PricingRewardRate(c ctx, evm mech, weiPerUnit uint64) error {
	return c.State.L1PricingState().SetPerUnitReward(weiPerUnit)
}

func (con ArbOwner) SetL1PricePerUnit(c ctx, evm mech, pricePerUnit *big.Int) error {
	return c.State.L1PricingState().SetPricePerUnit(pricePerUnit)
}

func (con ArbOwner) SetPerBatchGasCharge(c ctx, evm mech, cost int64) error {
	return c.State.L1PricingState().SetPerBatchGasCost(cost)
}

func (con ArbOwner) SetAmortizedCostCapBips(c ctx, evm mech, cap uint64) error {
	return c.State.L1PricingState().SetAmortizedCostCapBips(cap)
}

func (con ArbOwner) SetBrotliCompressionLevel(c ctx, evm mech, level uint64) error {
	return c.State.SetBrotliCompressionLevel(level)
}

func (con ArbOwner) ReleaseL1PricerSurplusFunds(c ctx, evm mech, maxWeiToRelease huge) (huge, error) {
	balance := evm.StateDB.GetBalance(l1pricing.L1PricerFundsPoolAddress)
	l1p := c.State.L1PricingState()
	recognized, err := l1p.L1FeesAvailable()
	if err != nil {
		return nil, err
	}
	weiToTransfer := new(big.Int).Sub(balance, recognized)
	if weiToTransfer.Sign() < 0 {
		return common.Big0, nil
	}
	if weiToTransfer.Cmp(maxWeiToRelease) > 0 {
		weiToTransfer = maxWeiToRelease
	}
	if _, err := l1p.AddToL1FeesAvailable(weiToTransfer); err != nil {
		return nil, err
	}
	return weiToTransfer, nil
}

func (con ArbOwner) SetChainConfig(c ctx, evm mech, serializedChainConfig []byte) error {
	if c == nil {
		return errors.New("nil context")
	}
	if c.txProcessor == nil {
		return errors.New("uninitialized tx processor")
	}
	if c.txProcessor.MsgIsNonMutating() {
		var newConfig params.ChainConfig
		err := json.Unmarshal(serializedChainConfig, &newConfig)
		if err != nil {
			return fmt.Errorf("invalid chain config, can't deserialize: %w", err)
		}
		if newConfig.ChainID == nil {
			return errors.New("invalid chain config, missing chain id")
		}
		chainId, err := c.State.ChainId()
		if err != nil {
			return fmt.Errorf("failed to get chain id from ArbOS state: %w", err)
		}
		if newConfig.ChainID.Cmp(chainId) != 0 {
			return fmt.Errorf("invalid chain config, chain id mismatch, want: %v, have: %v", chainId, newConfig.ChainID)
		}
		oldSerializedConfig, err := c.State.ChainConfig()
		if err != nil {
			return fmt.Errorf("failed to get old chain config from ArbOS state: %w", err)
		}
		if bytes.Equal(oldSerializedConfig, serializedChainConfig) {
			return errors.New("new chain config is the same as old one in ArbOS state")
		}
		if len(oldSerializedConfig) != 0 {
			var oldConfig params.ChainConfig
			err = json.Unmarshal(oldSerializedConfig, &oldConfig)
			if err != nil {
				return fmt.Errorf("failed to deserialize old chain config: %w", err)
			}
			if err := oldConfig.CheckCompatible(&newConfig, evm.Context.BlockNumber.Uint64(), evm.Context.Time); err != nil {
				return fmt.Errorf("invalid chain config, not compatible with previous: %w", err)
			}
		}
		currentConfig := evm.ChainConfig()
		if err := currentConfig.CheckCompatible(&newConfig, evm.Context.BlockNumber.Uint64(), evm.Context.Time); err != nil {
			return fmt.Errorf("invalid chain config, not compatible with EVM's chain config: %w", err)
		}
	}
	return c.State.SetChainConfig(serializedChainConfig)
}

'''
'''--- precompiles/ArbOwnerPublic.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"github.com/ethereum/go-ethereum/common"
)

// ArbOwnerPublic precompile provides non-owners with info about the current chain owners.
// The calls to this precompile do not require the sender be a chain owner.
// For those that are, see ArbOwner
type ArbOwnerPublic struct {
	Address                    addr // 0x6b
	ChainOwnerRectified        func(ctx, mech, addr) error
	ChainOwnerRectifiedGasCost func(addr) (uint64, error)
}

// GetAllChainOwners retrieves the list of chain owners
func (con ArbOwnerPublic) GetAllChainOwners(c ctx, evm mech) ([]common.Address, error) {
	return c.State.ChainOwners().AllMembers(65536)
}

// RectifyChainOwner checks if the account is a chain owner
func (con ArbOwnerPublic) RectifyChainOwner(c ctx, evm mech, addr addr) error {
	err := c.State.ChainOwners().RectifyMapping(addr)
	if err != nil {
		return err
	}
	return con.ChainOwnerRectified(c, evm, addr)
}

// IsChainOwner checks if the user is a chain owner
func (con ArbOwnerPublic) IsChainOwner(c ctx, evm mech, addr addr) (bool, error) {
	return c.State.ChainOwners().IsMember(addr)
}

// GetNetworkFeeAccount gets the network fee collector
func (con ArbOwnerPublic) GetNetworkFeeAccount(c ctx, evm mech) (addr, error) {
	return c.State.NetworkFeeAccount()
}

// GetInfraFeeAccount gets the infrastructure fee collector
func (con ArbOwnerPublic) GetInfraFeeAccount(c ctx, evm mech) (addr, error) {
	if c.State.ArbOSVersion() < 6 {
		return c.State.NetworkFeeAccount()
	}
	return c.State.InfraFeeAccount()
}

// GetBrotliCompressionLevel gets the current brotli compression level used for fast compression
func (con ArbOwnerPublic) GetBrotliCompressionLevel(c ctx, evm mech) (uint64, error) {
	return c.State.BrotliCompressionLevel()
}

'''
'''--- precompiles/ArbOwner_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"bytes"
	"encoding/json"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestArbOwner(t *testing.T) {
	evm := newMockEVMForTesting()
	caller := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])
	tracer := util.NewTracingInfo(evm, testhelpers.RandomAddress(), types.ArbosAddress, util.TracingDuringEVM)
	state, err := arbosState.OpenArbosState(evm.StateDB, burn.NewSystemBurner(tracer, false))
	Require(t, err)
	Require(t, state.ChainOwners().Add(caller))

	addr1 := common.BytesToAddress(crypto.Keccak256([]byte{1})[:20])
	addr2 := common.BytesToAddress(crypto.Keccak256([]byte{2})[:20])
	addr3 := common.BytesToAddress(crypto.Keccak256([]byte{3})[:20])

	prec := &ArbOwner{}
	gasInfo := &ArbGasInfo{}
	callCtx := testContext(caller, evm)

	// the zero address is an owner by default
	Require(t, prec.RemoveChainOwner(callCtx, evm, common.Address{}))

	Require(t, prec.AddChainOwner(callCtx, evm, addr1))
	Require(t, prec.AddChainOwner(callCtx, evm, addr2))
	Require(t, prec.AddChainOwner(callCtx, evm, addr1))

	member, err := prec.IsChainOwner(callCtx, evm, addr1)
	Require(t, err)
	if !member {
		Fail(t)
	}

	member, err = prec.IsChainOwner(callCtx, evm, addr2)
	Require(t, err)
	if !member {
		Fail(t)
	}

	member, err = prec.IsChainOwner(callCtx, evm, addr3)
	Require(t, err)
	if member {
		Fail(t)
	}

	Require(t, prec.RemoveChainOwner(callCtx, evm, addr1))
	member, err = prec.IsChainOwner(callCtx, evm, addr1)
	Require(t, err)
	if member {
		Fail(t)
	}
	member, err = prec.IsChainOwner(callCtx, evm, addr2)
	Require(t, err)
	if !member {
		Fail(t)
	}

	Require(t, prec.AddChainOwner(callCtx, evm, addr1))
	all, err := prec.GetAllChainOwners(callCtx, evm)
	Require(t, err)
	if len(all) != 3 {
		Fail(t)
	}
	if all[0] == all[1] || all[1] == all[2] || all[0] == all[2] {
		Fail(t)
	}
	if all[0] != addr1 && all[1] != addr1 && all[2] != addr1 {
		Fail(t)
	}
	if all[0] != addr2 && all[1] != addr2 && all[2] != addr2 {
		Fail(t)
	}
	if all[0] != caller && all[1] != caller && all[2] != caller {
		Fail(t)
	}

	costCap, err := gasInfo.GetAmortizedCostCapBips(callCtx, evm)
	Require(t, err)
	if costCap != 0 {
		Fail(t, costCap)
	}
	newCostCap := uint64(77734)
	Require(t, prec.SetAmortizedCostCapBips(callCtx, evm, newCostCap))
	costCap, err = gasInfo.GetAmortizedCostCapBips(callCtx, evm)
	Require(t, err)
	if costCap != newCostCap {
		Fail(t)
	}

	avail, err := gasInfo.GetL1FeesAvailable(callCtx, evm)
	Require(t, err)
	if avail.Sign() != 0 {
		Fail(t, avail)
	}
	deposited := big.NewInt(1000000)
	evm.StateDB.AddBalance(l1pricing.L1PricerFundsPoolAddress, deposited)
	avail, err = gasInfo.GetL1FeesAvailable(callCtx, evm)
	Require(t, err)
	if avail.Sign() != 0 {
		Fail(t, avail)
	}
	requested := big.NewInt(200000)
	x, err := prec.ReleaseL1PricerSurplusFunds(callCtx, evm, requested)
	Require(t, err)
	if x.Cmp(requested) != 0 {
		Fail(t, x, requested)
	}
	avail, err = gasInfo.GetL1FeesAvailable(callCtx, evm)
	Require(t, err)
	if avail.Cmp(requested) != 0 {
		Fail(t, avail, requested)
	}
	x, err = prec.ReleaseL1PricerSurplusFunds(callCtx, evm, deposited)
	Require(t, err)
	if x.Cmp(new(big.Int).Sub(deposited, requested)) != 0 {
		Fail(t, x, deposited, requested)
	}
	avail, err = gasInfo.GetL1FeesAvailable(callCtx, evm)
	Require(t, err)
	if avail.Cmp(deposited) != 0 {
		Fail(t, avail, deposited)
	}
	x, err = prec.ReleaseL1PricerSurplusFunds(callCtx, evm, deposited)
	Require(t, err)
	if x.Sign() != 0 {
		Fail(t, x)
	}
	avail, err = gasInfo.GetL1FeesAvailable(callCtx, evm)
	Require(t, err)
	if avail.Cmp(deposited) != 0 {
		Fail(t, avail, deposited)
	}
}

func TestArbOwnerSetChainConfig(t *testing.T) {
	evm := newMockEVMForTestingWithVersionAndRunMode(nil, core.MessageGasEstimationMode)
	caller := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])
	tracer := util.NewTracingInfo(evm, testhelpers.RandomAddress(), types.ArbosAddress, util.TracingDuringEVM)
	state, err := arbosState.OpenArbosState(evm.StateDB, burn.NewSystemBurner(tracer, false))
	Require(t, err)
	Require(t, state.ChainOwners().Add(caller))
	prec := &ArbOwner{}
	callCtx := testContext(caller, evm)

	chainConfig := params.ArbitrumDevTestChainConfig()
	chainConfig.ArbitrumChainParams.AllowDebugPrecompiles = false
	serializedChainConfig, err := json.Marshal(chainConfig)
	Require(t, err)
	err = prec.SetChainConfig(callCtx, evm, serializedChainConfig)
	Require(t, err)
	config, err := state.ChainConfig()
	Require(t, err)
	if !bytes.Equal(config, serializedChainConfig) {
		Fail(t, config, serializedChainConfig)
	}

	chainConfig.ArbitrumChainParams.AllowDebugPrecompiles = true
	serializedChainConfig, err = json.Marshal(chainConfig)
	Require(t, err)
	err = prec.SetChainConfig(callCtx, evm, serializedChainConfig)
	Require(t, err)
	config, err = state.ChainConfig()
	Require(t, err)
	if !bytes.Equal(config, serializedChainConfig) {
		Fail(t, config, serializedChainConfig)
	}
}

func TestArbInfraFeeAccount(t *testing.T) {
	version0 := uint64(0)
	evm := newMockEVMForTestingWithVersion(&version0)
	caller := common.BytesToAddress(crypto.Keccak256([]byte{})[:20])
	newAddr := common.BytesToAddress(crypto.Keccak256([]byte{0})[:20])
	callCtx := testContext(caller, evm)
	prec := &ArbOwner{}
	_, err := prec.GetInfraFeeAccount(callCtx, evm)
	Require(t, err)
	err = prec.SetInfraFeeAccount(callCtx, evm, newAddr) // this should be a no-op (because ArbOS version 0)
	Require(t, err)

	version5 := uint64(5)
	evm = newMockEVMForTestingWithVersion(&version5)
	callCtx = testContext(caller, evm)
	prec = &ArbOwner{}
	precPublic := &ArbOwnerPublic{}
	addr, err := prec.GetInfraFeeAccount(callCtx, evm)
	Require(t, err)
	if addr != (common.Address{}) {
		t.Fatal()
	}
	addr, err = precPublic.GetInfraFeeAccount(callCtx, evm)
	Require(t, err)
	if addr != (common.Address{}) {
		t.Fatal()
	}

	err = prec.SetInfraFeeAccount(callCtx, evm, newAddr)
	Require(t, err)
	addr, err = prec.GetInfraFeeAccount(callCtx, evm)
	Require(t, err)
	if addr != newAddr {
		t.Fatal()
	}
	addr, err = precPublic.GetInfraFeeAccount(callCtx, evm)
	Require(t, err)
	if addr != newAddr {
		t.Fatal()
	}
}

'''
'''--- precompiles/ArbRetryableTx.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"

	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/arbos/storage"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
)

type ArbRetryableTx struct {
	Address                 addr
	TicketCreated           func(ctx, mech, bytes32) error
	LifetimeExtended        func(ctx, mech, bytes32, huge) error
	RedeemScheduled         func(ctx, mech, bytes32, bytes32, uint64, uint64, addr, huge, huge) error
	Canceled                func(ctx, mech, bytes32) error
	TicketCreatedGasCost    func(bytes32) (uint64, error)
	LifetimeExtendedGasCost func(bytes32, huge) (uint64, error)
	RedeemScheduledGasCost  func(bytes32, bytes32, uint64, uint64, addr, huge, huge) (uint64, error)
	CanceledGasCost         func(bytes32) (uint64, error)

	// deprecated event
	Redeemed        func(ctx, mech, bytes32) error
	RedeemedGasCost func(bytes32) (uint64, error)

	NoTicketWithIDError func() error
	NotCallableError    func() error
}

var ErrSelfModifyingRetryable = errors.New("retryable cannot modify itself")

func (con ArbRetryableTx) oldNotFoundError(c ctx) error {
	if c.State.ArbOSVersion() >= 3 {
		return con.NoTicketWithIDError()
	}
	return errors.New("ticketId not found")
}

// Redeem schedules an attempt to redeem the retryable, donating all of the call's gas to the redeem attempt
func (con ArbRetryableTx) Redeem(c ctx, evm mech, ticketId bytes32) (bytes32, error) {
	if c.txProcessor.CurrentRetryable != nil && ticketId == *c.txProcessor.CurrentRetryable {
		return bytes32{}, ErrSelfModifyingRetryable
	}
	retryableState := c.State.RetryableState()
	byteCount, err := retryableState.RetryableSizeBytes(ticketId, evm.Context.Time)
	if err != nil {
		return hash{}, err
	}
	writeBytes := arbmath.WordsForBytes(byteCount)
	if err := c.Burn(params.SloadGas * writeBytes); err != nil {
		return hash{}, err
	}

	retryable, err := retryableState.OpenRetryable(ticketId, evm.Context.Time)
	if err != nil {
		return hash{}, err
	}
	if retryable == nil {
		return hash{}, con.oldNotFoundError(c)
	}
	nextNonce, err := retryable.IncrementNumTries()
	if err != nil {
		return hash{}, err
	}
	nonce := nextNonce - 1

	maxRefund := new(big.Int).Exp(common.Big2, common.Big256, nil)
	maxRefund.Sub(maxRefund, common.Big1)
	retryTxInner, err := retryable.MakeTx(
		evm.ChainConfig().ChainID,
		nonce,
		evm.Context.BaseFee,
		0, // will fill this in below
		ticketId,
		c.caller,
		maxRefund,
		common.Big0,
	)
	if err != nil {
		return hash{}, err
	}

	// figure out how much gas the event issuance will cost, and reduce the donated gas amount in the event
	//     by that much, so that we'll donate the correct amount of gas
	eventCost, err := con.RedeemScheduledGasCost(hash{}, hash{}, 0, 0, addr{}, common.Big0, common.Big0)
	if err != nil {
		return hash{}, err
	}
	// Result is 32 bytes long which is 1 word
	gasCostToReturnResult := params.CopyGas
	gasPoolUpdateCost := storage.StorageReadCost + storage.StorageWriteCost
	futureGasCosts := eventCost + gasCostToReturnResult + gasPoolUpdateCost
	if c.gasLeft < futureGasCosts {
		return hash{}, c.Burn(futureGasCosts) // this will error
	}
	gasToDonate := c.gasLeft - futureGasCosts
	if gasToDonate < params.TxGas {
		return hash{}, errors.New("not enough gas to run redeem attempt")
	}

	// fix up the gas in the retry
	retryTxInner.Gas = gasToDonate

	retryTx := types.NewTx(retryTxInner)
	retryTxHash := retryTx.Hash()

	err = con.RedeemScheduled(c, evm, ticketId, retryTxHash, nonce, gasToDonate, c.caller, maxRefund, common.Big0)
	if err != nil {
		return hash{}, err
	}

	// To prepare for the enqueued retry event, we burn gas here, adding it back to the pool right before retrying.
	// The gas payer for this tx will get a credit for the wei they paid for this gas when retrying.
	// We burn as much gas as we can, leaving only enough to pay for copying out the return data.
	if err := c.Burn(gasToDonate); err != nil {
		return hash{}, err
	}

	// Add the gasToDonate back to the gas pool: the retryable attempt will then consume it.
	// This ensures that the gas pool has enough gas to run the retryable attempt.
	return retryTxHash, c.State.L2PricingState().AddToGasPool(arbmath.SaturatingCast(gasToDonate))
}

// GetLifetime gets the default lifetime period a retryable has at creation
func (con ArbRetryableTx) GetLifetime(c ctx, evm mech) (huge, error) {
	return big.NewInt(retryables.RetryableLifetimeSeconds), nil
}

// GetTimeout gets the timestamp for when ticket will expire
func (con ArbRetryableTx) GetTimeout(c ctx, evm mech, ticketId bytes32) (huge, error) {
	retryableState := c.State.RetryableState()
	retryable, err := retryableState.OpenRetryable(ticketId, evm.Context.Time)
	if err != nil {
		return nil, err
	}
	if retryable == nil {
		return nil, con.NoTicketWithIDError()
	}
	timeout, err := retryable.CalculateTimeout()
	if err != nil {
		return nil, err
	}
	return big.NewInt(int64(timeout)), nil
}

// Keepalive adds one lifetime period to the ticket's expiry
func (con ArbRetryableTx) Keepalive(c ctx, evm mech, ticketId bytes32) (huge, error) {

	// charge for the expiry update
	retryableState := c.State.RetryableState()
	nbytes, err := retryableState.RetryableSizeBytes(ticketId, evm.Context.Time)
	if err != nil {
		return nil, err
	}
	if nbytes == 0 {
		return nil, con.oldNotFoundError(c)
	}
	updateCost := arbmath.WordsForBytes(nbytes) * params.SstoreSetGas / 100
	if err := c.Burn(updateCost); err != nil {
		return big.NewInt(0), err
	}

	currentTime := evm.Context.Time
	window := currentTime + retryables.RetryableLifetimeSeconds
	newTimeout, err := retryableState.Keepalive(ticketId, currentTime, window, retryables.RetryableLifetimeSeconds)
	if err != nil {
		return big.NewInt(0), err
	}

	err = con.LifetimeExtended(c, evm, ticketId, big.NewInt(int64(newTimeout)))
	return big.NewInt(int64(newTimeout)), err
}

// GetBeneficiary gets the beneficiary of the ticket
func (con ArbRetryableTx) GetBeneficiary(c ctx, evm mech, ticketId bytes32) (addr, error) {
	retryableState := c.State.RetryableState()
	retryable, err := retryableState.OpenRetryable(ticketId, evm.Context.Time)
	if err != nil {
		return addr{}, err
	}
	if retryable == nil {
		return addr{}, con.oldNotFoundError(c)
	}
	return retryable.Beneficiary()
}

// Cancel the ticket and refund its callvalue to its beneficiary
func (con ArbRetryableTx) Cancel(c ctx, evm mech, ticketId bytes32) error {
	if c.txProcessor.CurrentRetryable != nil && ticketId == *c.txProcessor.CurrentRetryable {
		return ErrSelfModifyingRetryable
	}
	retryableState := c.State.RetryableState()
	retryable, err := retryableState.OpenRetryable(ticketId, evm.Context.Time)
	if err != nil {
		return err
	}
	if retryable == nil {
		return con.oldNotFoundError(c)
	}
	beneficiary, err := retryable.Beneficiary()
	if err != nil {
		return err
	}
	if c.caller != beneficiary {
		return errors.New("only the beneficiary may cancel a retryable")
	}

	// no refunds are given for deleting retryables because they use rented space
	_, err = retryableState.DeleteRetryable(ticketId, evm, util.TracingDuringEVM)
	if err != nil {
		return err
	}
	return con.Canceled(c, evm, ticketId)
}

func (con ArbRetryableTx) GetCurrentRedeemer(c ctx, evm mech) (common.Address, error) {
	if c.txProcessor.CurrentRefundTo != nil {
		return *c.txProcessor.CurrentRefundTo, nil
	}
	return common.Address{}, nil
}

func (con ArbRetryableTx) SubmitRetryable(
	c ctx, evm mech, requestId bytes32, l1BaseFee, deposit, callvalue, gasFeeCap huge,
	gasLimit uint64, maxSubmissionFee huge,
	feeRefundAddress, beneficiary, retryTo addr,
	retryData []byte,
) error {
	return con.NotCallableError()
}

'''
'''--- precompiles/ArbRetryableTx_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"math/big"
	"testing"

	"github.com/offchainlabs/nitro/arbos/storage"

	"github.com/ethereum/go-ethereum/common"
	templates "github.com/offchainlabs/nitro/solgen/go/precompilesgen"
)

func TestRetryableRedeem(t *testing.T) {
	evm := newMockEVMForTesting()
	precompileCtx := testContext(common.Address{}, evm)

	id := common.BigToHash(big.NewInt(978645611142))
	timeout := evm.Context.Time + 10000000
	from := common.HexToAddress("0x030405")
	to := common.HexToAddress("0x06070809")
	callvalue := big.NewInt(0)
	beneficiary := common.HexToAddress("0x0301040105090206")
	calldata := make([]byte, 42)
	for i := range calldata {
		calldata[i] = byte(i + 3)
	}
	_, err := precompileCtx.State.RetryableState().CreateRetryable(
		id,
		timeout,
		from,
		&to,
		callvalue,
		beneficiary,
		calldata,
	)
	Require(t, err)

	retryABI, err := templates.ArbRetryableTxMetaData.GetAbi()
	Require(t, err)
	redeemCalldata, err := retryABI.Pack("redeem", id)
	Require(t, err)

	retryAddress := common.HexToAddress("6e")
	_, gasLeft, err := Precompiles()[retryAddress].Call(
		redeemCalldata,
		retryAddress,
		retryAddress,
		common.Address{},
		big.NewInt(0),
		false,
		1000000,
		evm,
	)
	Require(t, err)

	if gasLeft != storage.StorageWriteCost-storage.StorageWriteZeroCost {
		// We expect to have some gas left over, because in this test we write a zero, but in other
		//     use cases the precompile would cause a non-zero write. So the precompile allocates enough gas
		//     to handle both cases, and some will be left over in this test's use case.
		Fail(t, "didn't consume all the expected gas")
	}
}

'''
'''--- precompiles/ArbStatistics.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"math/big"
)

// ArbStatistics provides statistics about the rollup right before the Nitro upgrade.
// In Classic, this was how a user would get info such as the total number of accounts,
// but there's now better ways to do that with geth.
type ArbStatistics struct {
	Address addr // 0x6e
}

// GetStats returns the current block number and some statistics about the rollup's pre-Nitro state
func (con ArbStatistics) GetStats(c ctx, evm mech) (huge, huge, huge, huge, huge, huge, error) {
	blockNum := evm.Context.BlockNumber
	classicNumAccounts := big.NewInt(0)  // TODO: hardcode the final value from Arbitrum Classic
	classicStorageSum := big.NewInt(0)   // TODO: hardcode the final value from Arbitrum Classic
	classicGasSum := big.NewInt(0)       // TODO: hardcode the final value from Arbitrum Classic
	classicNumTxes := big.NewInt(0)      // TODO: hardcode the final value from Arbitrum Classic
	classicNumContracts := big.NewInt(0) // TODO: hardcode the final value from Arbitrum Classic
	return blockNum, classicNumAccounts, classicStorageSum, classicGasSum, classicNumTxes, classicNumContracts, nil
}

'''
'''--- precompiles/ArbSys.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/merkletree"
)

// ArbSys provides system-level functionality for interacting with L1 and understanding the call stack.
type ArbSys struct {
	Address                 addr // 0x64
	L2ToL1Tx                func(ctx, mech, addr, addr, huge, huge, huge, huge, huge, huge, []byte) error
	L2ToL1TxGasCost         func(addr, addr, huge, huge, huge, huge, huge, huge, []byte) (uint64, error)
	SendMerkleUpdate        func(ctx, mech, huge, bytes32, huge) error
	SendMerkleUpdateGasCost func(huge, bytes32, huge) (uint64, error)
	InvalidBlockNumberError func(huge, huge) error

	// deprecated event
	L2ToL1Transaction        func(ctx, mech, addr, addr, huge, huge, huge, huge, huge, huge, huge, []byte) error
	L2ToL1TransactionGasCost func(addr, addr, huge, huge, huge, huge, huge, huge, huge, []byte) (uint64, error)
}

// ArbBlockNumber gets the current L2 block number
func (con *ArbSys) ArbBlockNumber(c ctx, evm mech) (huge, error) {
	return evm.Context.BlockNumber, nil
}

// ArbBlockHash gets the L2 block hash, if sufficiently recent
func (con *ArbSys) ArbBlockHash(c ctx, evm mech, arbBlockNumber *big.Int) (bytes32, error) {
	if !arbBlockNumber.IsUint64() {
		if c.State.ArbOSVersion() >= 11 {
			return bytes32{}, con.InvalidBlockNumberError(arbBlockNumber, evm.Context.BlockNumber)
		}
		return bytes32{}, errors.New("invalid block number")
	}
	requestedBlockNum := arbBlockNumber.Uint64()

	currentNumber := evm.Context.BlockNumber.Uint64()
	if requestedBlockNum >= currentNumber || requestedBlockNum+256 < currentNumber {
		if c.State.ArbOSVersion() >= 11 {
			return common.Hash{}, con.InvalidBlockNumberError(arbBlockNumber, evm.Context.BlockNumber)
		}
		return common.Hash{}, errors.New("invalid block number for ArbBlockHAsh")
	}

	return evm.Context.GetHash(requestedBlockNum), nil
}

// ArbChainID gets the rollup's unique chain identifier
func (con *ArbSys) ArbChainID(c ctx, evm mech) (huge, error) {
	return evm.ChainConfig().ChainID, nil
}

// ArbOSVersion gets the current ArbOS version
func (con *ArbSys) ArbOSVersion(c ctx, evm mech) (huge, error) {
	version := new(big.Int).SetUint64(55 + c.State.ArbOSVersion()) // Nitro starts at version 56
	return version, nil
}

// GetStorageGasAvailable returns 0 since Nitro has no concept of storage gas
func (con *ArbSys) GetStorageGasAvailable(c ctx, evm mech) (huge, error) {
	return big.NewInt(0), nil
}

// IsTopLevelCall checks if the call is top-level (deprecated)
func (con *ArbSys) IsTopLevelCall(c ctx, evm mech) (bool, error) {
	return evm.Depth() <= 2, nil
}

// MapL1SenderContractAddressToL2Alias gets the contract's L2 alias
func (con *ArbSys) MapL1SenderContractAddressToL2Alias(c ctx, sender addr, dest addr) (addr, error) {
	return util.RemapL1Address(sender), nil
}

// WasMyCallersAddressAliased checks if the caller's caller was aliased
func (con *ArbSys) WasMyCallersAddressAliased(c ctx, evm mech) (bool, error) {
	topLevel := con.isTopLevel(c, evm)
	if c.State.ArbOSVersion() < 6 {
		topLevel = evm.Depth() == 2
	}
	aliased := topLevel && util.DoesTxTypeAlias(c.txProcessor.TopTxType)
	return aliased, nil
}

// MyCallersAddressWithoutAliasing gets the caller's caller without any potential aliasing
func (con *ArbSys) MyCallersAddressWithoutAliasing(c ctx, evm mech) (addr, error) {

	address := addr{}

	if evm.Depth() > 1 {
		address = c.txProcessor.Callers[evm.Depth()-2]
	}

	aliased, err := con.WasMyCallersAddressAliased(c, evm)
	if aliased {
		address = util.InverseRemapL1Address(address)
	}
	return address, err
}

// SendTxToL1 sends a transaction to L1, adding it to the outbox
func (con *ArbSys) SendTxToL1(c ctx, evm mech, value huge, destination addr, calldataForL1 []byte) (huge, error) {
	l1BlockNum, err := c.txProcessor.L1BlockNumber(vm.BlockContext{})
	if err != nil {
		return nil, err
	}
	bigL1BlockNum := arbmath.UintToBig(l1BlockNum)

	arbosState := c.State
	var t big.Int
	t.SetUint64(evm.Context.Time)
	sendHash, err := arbosState.KeccakHash(
		c.caller.Bytes(),
		destination.Bytes(),
		math.U256Bytes(evm.Context.BlockNumber),
		math.U256Bytes(bigL1BlockNum),
		math.U256Bytes(&t),
		common.BigToHash(value).Bytes(),
		calldataForL1,
	)
	if err != nil {
		return nil, err
	}
	merkleAcc := arbosState.SendMerkleAccumulator()
	merkleUpdateEvents, err := merkleAcc.Append(sendHash)
	if err != nil {
		return nil, err
	}

	size, err := merkleAcc.Size()
	if err != nil {
		return nil, err
	}

	// burn the callvalue, which was previously deposited to this precompile's account
	if err := util.BurnBalance(&con.Address, value, evm, util.TracingDuringEVM, "withdraw"); err != nil {
		return nil, err
	}

	for _, merkleUpdateEvent := range merkleUpdateEvents {
		position := merkletree.LevelAndLeaf{
			Level: merkleUpdateEvent.Level,
			Leaf:  merkleUpdateEvent.NumLeaves,
		}
		err := con.SendMerkleUpdate(
			c,
			evm,
			big.NewInt(0),
			merkleUpdateEvent.Hash,
			position.ToBigInt(),
		)
		if err != nil {
			return nil, err
		}
	}

	leafNum := big.NewInt(int64(size - 1))

	var blockTime big.Int
	blockTime.SetUint64(evm.Context.Time)
	err = con.L2ToL1Tx(
		c,
		evm,
		c.caller,
		destination,
		sendHash.Big(),
		leafNum,
		evm.Context.BlockNumber,
		bigL1BlockNum,
		&blockTime,
		value,
		calldataForL1,
	)

	if c.State.ArbOSVersion() >= 4 {
		return leafNum, nil
	}
	return sendHash.Big(), err
}

// SendMerkleTreeState gets the root, size, and partials of the outbox Merkle tree state (caller must be the 0 address)
func (con ArbSys) SendMerkleTreeState(c ctx, evm mech) (huge, bytes32, []bytes32, error) {
	if c.caller != (addr{}) {
		return nil, bytes32{}, nil, errors.New("method can only be called by address zero")
	}

	// OK to not charge gas, because method is only callable by address zero

	size, rootHash, rawPartials, _ := c.State.SendMerkleAccumulator().StateForExport()
	partials := make([]bytes32, len(rawPartials))
	for i, par := range rawPartials {
		partials[i] = par
	}
	return big.NewInt(int64(size)), rootHash, partials, nil
}

// WithdrawEth send paid eth to the destination on L1
func (con ArbSys) WithdrawEth(c ctx, evm mech, value huge, destination addr) (huge, error) {
	return con.SendTxToL1(c, evm, value, destination, []byte{})
}

func (con ArbSys) isTopLevel(c ctx, evm mech) bool {
	depth := evm.Depth()
	return depth < 2 || evm.Origin == c.txProcessor.Callers[depth-2]
}

'''
'''--- precompiles/ArbosActs.go ---
//
// Copyright 2022, Offchain Labs, Inc. All rights reserved.
//

package precompiles

// ArbosActs precompile represents ArbOS's internal actions as calls it makes to itself
type ArbosActs struct {
	Address addr // 0xa4b05

	CallerNotArbOSError func() error
}

func (con ArbosActs) StartBlock(c ctx, evm mech, l1BaseFee huge, l1BlockNumber, l2BlockNumber, timeLastBlock uint64) error {
	return con.CallerNotArbOSError()
}

func (con ArbosActs) BatchPostingReport(c ctx, evm mech, batchTimestamp huge, batchPosterAddress addr, batchNumber uint64, batchDataGas uint64, l1BaseFeeWei huge) error {
	return con.CallerNotArbOSError()
}

'''
'''--- precompiles/ArbosTest.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
)

// ArbosTest provides a method of burning arbitrary amounts of gas, which exists for historical reasons.
type ArbosTest struct {
	Address addr // 0x69
}

// BurnArbGas unproductively burns the amount of L2 ArbGas
func (con ArbosTest) BurnArbGas(c ctx, gasAmount huge) error {
	if !gasAmount.IsUint64() {
		return errors.New("not a uint64")
	}
	//nolint:errcheck
	c.Burn(gasAmount.Uint64()) // burn the amount, even if it's more than the user has
	return nil
}

'''
'''--- precompiles/context.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/util"
)

type addr = common.Address
type mech = *vm.EVM
type huge = *big.Int
type hash = common.Hash
type bytes4 = [4]byte
type bytes32 = [32]byte
type ctx = *Context

type Context struct {
	caller      addr
	gasSupplied uint64
	gasLeft     uint64
	txProcessor *arbos.TxProcessor
	State       *arbosState.ArbosState
	tracingInfo *util.TracingInfo
	readOnly    bool
}

func (c *Context) Burn(amount uint64) error {
	if c.gasLeft < amount {
		c.gasLeft = 0
		return vm.ErrOutOfGas
	}
	c.gasLeft -= amount
	return nil
}

//nolint:unused
func (c *Context) Burned() uint64 {
	return c.gasSupplied - c.gasLeft
}

func (c *Context) Restrict(err error) {
	log.Crit("A metered burner was used for access-controlled work", "error", err)
}

func (c *Context) HandleError(err error) error {
	return err
}

func (c *Context) ReadOnly() bool {
	return c.readOnly
}

func (c *Context) TracingInfo() *util.TracingInfo {
	return c.tracingInfo
}

func testContext(caller addr, evm mech) *Context {
	tracingInfo := util.NewTracingInfo(evm, common.Address{}, types.ArbosAddress, util.TracingDuringEVM)
	ctx := &Context{
		caller:      caller,
		gasSupplied: ^uint64(0),
		gasLeft:     ^uint64(0),
		tracingInfo: tracingInfo,
		readOnly:    false,
	}
	state, err := arbosState.OpenArbosState(evm.StateDB, burn.NewSystemBurner(tracingInfo, false))
	if err != nil {
		log.Crit("unable to open arbos state", "error", err)
	}
	ctx.State = state
	var ok bool
	ctx.txProcessor, ok = evm.ProcessingHook.(*arbos.TxProcessor)
	if !ok {
		log.Crit("must have tx processor")
	}
	return ctx
}

'''
'''--- precompiles/precompile.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"fmt"
	"math/big"
	"reflect"
	"strconv"
	"strings"
	"unicode"

	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/util"
	templates "github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	glog "github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
)

type ArbosPrecompile interface {
	// Important fields: evm.StateDB and evm.Config.Tracer
	// NOTE: if precompileAddress != actingAsAddress, watch out!
	// This is a delegatecall or callcode, so caller might be wrong.
	// In that case, unless this precompile is pure, it should probably revert.
	Call(
		input []byte,
		precompileAddress common.Address,
		actingAsAddress common.Address,
		caller common.Address,
		value *big.Int,
		readOnly bool,
		gasSupplied uint64,
		evm *vm.EVM,
	) (output []byte, gasLeft uint64, err error)

	Precompile() *Precompile
}

type purity uint8

const (
	pure purity = iota
	view
	write
	payable
)

type Precompile struct {
	methods       map[[4]byte]*PrecompileMethod
	methodsByName map[string]*PrecompileMethod
	events        map[string]PrecompileEvent
	errors        map[string]PrecompileError
	name          string
	implementer   reflect.Value
	address       common.Address
	arbosVersion  uint64
}

type PrecompileMethod struct {
	name         string
	template     abi.Method
	purity       purity
	handler      reflect.Method
	arbosVersion uint64
}

type PrecompileEvent struct {
	name     string
	template abi.Event
}

type PrecompileError struct {
	name     string
	template abi.Error
}

type SolError struct {
	data   []byte
	solErr abi.Error
}

func RenderSolError(solErr abi.Error, data []byte) (string, error) {
	vals, err := solErr.Unpack(data)
	if err != nil {
		return "", err
	}
	valsRange, ok := vals.([]interface{})
	if !ok {
		return "", errors.New("unexpected unpack result")
	}
	strVals := make([]string, 0, len(valsRange))
	for _, val := range valsRange {
		strVals = append(strVals, fmt.Sprintf("%v", val))
	}
	return fmt.Sprintf("error %v(%v)", solErr.Name, strings.Join(strVals, ", ")), nil
}

func (e *SolError) Error() string {
	rendered, err := RenderSolError(e.solErr, e.data)
	if err != nil {
		return "unable to decode execution error"
	}
	return rendered
}

// MakePrecompile makes a precompile for the given hardhat-to-geth bindings, ensuring that the implementer
// supports each method.
func MakePrecompile(metadata *bind.MetaData, implementer interface{}) (addr, *Precompile) {
	source, err := abi.JSON(strings.NewReader(metadata.ABI))
	if err != nil {
		log.Crit("Bad ABI")
	}

	implementerType := reflect.TypeOf(implementer)
	contract := implementerType.Elem().Name()

	_, ok := implementerType.Elem().FieldByName("Address")
	if !ok {
		log.Crit("Implementer for precompile ", contract, " is missing an Address field")
	}

	address, ok := reflect.ValueOf(implementer).Elem().FieldByName("Address").Interface().(addr)
	if !ok {
		log.Crit("Implementer for precompile ", contract, "'s Address field has the wrong type")
	}

	gethAbiFuncTypeEquality := func(actual, geth reflect.Type) bool {
		gethIn := geth.NumIn()
		gethOut := geth.NumOut()
		if actual.NumIn() != gethIn || actual.NumOut() != gethOut {
			return false
		}
		for i := 0; i < gethIn; i++ {
			if !geth.In(i).ConvertibleTo(actual.In(i)) {
				return false
			}
		}
		for i := 0; i < gethOut; i++ {
			if !actual.Out(i).ConvertibleTo(geth.Out(i)) {
				return false
			}
		}
		return true
	}

	methods := make(map[[4]byte]*PrecompileMethod)
	methodsByName := make(map[string]*PrecompileMethod)
	events := make(map[string]PrecompileEvent)
	errors := make(map[string]PrecompileError)

	for _, method := range source.Methods {

		name := method.RawName
		capitalize := string(unicode.ToUpper(rune(name[0])))
		name = capitalize + name[1:]

		if len(method.ID) != 4 {
			log.Crit("Method ID isn't 4 bytes")
		}
		id := *(*[4]byte)(method.ID)

		// check that the implementer has a supporting implementation for this method

		handler, ok := implementerType.MethodByName(name)
		if !ok {
			log.Crit("Precompile " + contract + " must implement " + name)
		}

		var needs = []reflect.Type{
			implementerType,            // the contract itself
			reflect.TypeOf((ctx)(nil)), // this call's context
		}

		var purity purity

		switch method.StateMutability {
		case "pure":
			purity = pure
		case "view":
			needs = append(needs, reflect.TypeOf(&vm.EVM{}))
			purity = view
		case "nonpayable":
			needs = append(needs, reflect.TypeOf(&vm.EVM{}))
			purity = write
		case "payable":
			needs = append(needs, reflect.TypeOf(&vm.EVM{}))
			needs = append(needs, reflect.TypeOf(&big.Int{}))
			purity = payable
		default:
			log.Crit("Unknown state mutability ", method.StateMutability)
		}

		for _, arg := range method.Inputs {
			needs = append(needs, arg.Type.GetType())
		}

		var outputs = []reflect.Type{}
		for _, out := range method.Outputs {
			outputs = append(outputs, out.Type.GetType())
		}
		outputs = append(outputs, reflect.TypeOf((*error)(nil)).Elem())

		expectedHandlerType := reflect.FuncOf(needs, outputs, false)

		if !gethAbiFuncTypeEquality(handler.Type, expectedHandlerType) {
			log.Crit(
				"Precompile "+contract+"'s "+name+"'s implementer has the wrong type\n",
				"\texpected:\t", expectedHandlerType, "\n\tbut have:\t", handler.Type,
			)
		}

		method := PrecompileMethod{
			name,
			method,
			purity,
			handler,
			0,
		}
		methods[id] = &method
		methodsByName[name] = &method
	}

	for i := 0; i < implementerType.NumMethod(); i++ {
		method := implementerType.Method(i)
		name := method.Name
		if method.IsExported() && methodsByName[name] == nil {
			log.Crit(contract + " is missing a solidity interface for " + name)
		}
	}

	// provide the implementer mechanisms to emit logs for the solidity events

	supportedIndices := map[string]struct{}{
		// the solidity value types: https://docs.soliditylang.org/en/v0.8.9/types.html
		"address": {},
		"bool":    {},
	}
	for i := 8; i <= 256; i += 8 {
		supportedIndices["int"+strconv.Itoa(i)] = struct{}{}
		supportedIndices["uint"+strconv.Itoa(i)] = struct{}{}
	}
	for i := 1; i <= 32; i += 1 {
		supportedIndices["bytes"+strconv.Itoa(i)] = struct{}{}
	}

	for _, event := range source.Events {
		name := event.RawName

		var needs = []reflect.Type{
			reflect.TypeOf(&Context{}), // where the emit goes
			reflect.TypeOf(&vm.EVM{}),  // where the emit goes
		}
		for _, arg := range event.Inputs {
			needs = append(needs, arg.Type.GetType())

			if arg.Indexed {
				_, ok := supportedIndices[arg.Type.String()]
				if !ok {
					log.Crit(
						"Please change the solidity for precompile ", contract,
						"'s event ", name, ":\n\tEvent indices of type ",
						arg.Type.String(), " are not supported",
					)
				}
			}
		}

		uint64Type := reflect.TypeOf(uint64(0))
		errorType := reflect.TypeOf((*error)(nil)).Elem()
		expectedFieldType := reflect.FuncOf(needs, []reflect.Type{errorType}, false)
		expectedCostType := reflect.FuncOf(needs[2:], []reflect.Type{uint64Type, errorType}, false)

		context := "Precompile " + contract + "'s implementer"
		missing := context + " is missing a field for "

		field, ok := implementerType.Elem().FieldByName(name)
		if !ok {
			log.Crit(missing, "event ", name, " of type\n\t", expectedFieldType)
		}
		costField, ok := implementerType.Elem().FieldByName(name + "GasCost")
		if !ok {
			log.Crit(missing, "event ", name, "'s GasCost of type\n\t", expectedCostType)
		}
		if !gethAbiFuncTypeEquality(field.Type, expectedFieldType) {
			log.Crit(
				context, "'s field for event ", name, " has the wrong type\n",
				"\texpected:\t", expectedFieldType, "\n\tbut have:\t", field.Type,
			)
		}
		if !gethAbiFuncTypeEquality(costField.Type, expectedCostType) {
			log.Crit(
				context, "'s field for event ", name, "GasCost has the wrong type\n",
				"\texpected:\t", expectedCostType, "\n\tbut have:\t", costField.Type,
			)
		}

		structFields := reflect.ValueOf(implementer).Elem()
		fieldPointer := structFields.FieldByName(name)
		costPointer := structFields.FieldByName(name + "GasCost")

		dataInputs := make(abi.Arguments, 0)
		topicInputs := make(abi.Arguments, 0)

		for _, input := range event.Inputs {
			if input.Indexed {
				topicInputs = append(topicInputs, input)
			} else {
				dataInputs = append(dataInputs, input)
			}
		}

		// we can't capture `event` since the for loop will change its value
		capturedEvent := event
		nilError := reflect.Zero(reflect.TypeOf((*error)(nil)).Elem())

		gascost := func(args []reflect.Value) []reflect.Value {

			cost := params.LogGas
			cost += params.LogTopicGas * uint64(1+len(topicInputs))

			var dataValues []interface{}

			for i := 0; i < len(args); i++ {
				if !capturedEvent.Inputs[i].Indexed {
					dataValues = append(dataValues, args[i].Interface())
				}
			}

			data, err := dataInputs.PackValues(dataValues)
			if err != nil {
				glog.Error(fmt.Sprintf(
					"Could not pack values for event %s's GasCost\nerror %s", name, err,
				))
				return []reflect.Value{reflect.ValueOf(0), reflect.ValueOf(err)}
			}

			// charge for the number of bytes
			cost += params.LogDataGas * uint64(len(data))
			return []reflect.Value{reflect.ValueOf(cost), nilError}
		}

		emit := func(args []reflect.Value) []reflect.Value {

			callerCtx := args[0].Interface().(ctx) //nolint:errcheck
			evm := args[1].Interface().(*vm.EVM)   //nolint:errcheck
			state := evm.StateDB
			args = args[2:]

			version := arbosState.ArbOSVersion(state)
			if callerCtx.readOnly && version >= 11 {
				return []reflect.Value{reflect.ValueOf(vm.ErrWriteProtection)}
			}

			emitCost := gascost(args)
			cost := emitCost[0].Interface().(uint64) //nolint:errcheck
			if !emitCost[1].IsNil() {
				// an error occurred during gascost()
				return []reflect.Value{emitCost[1]}
			}
			if err := callerCtx.Burn(cost); err != nil {
				// the user has run out of gas
				return []reflect.Value{reflect.ValueOf(vm.ErrOutOfGas)}
			}

			// Filter by index'd into data and topics. Indexed values, even if ultimately hashed,
			// aren't supposed to have their contents stored in the general-purpose data portion.
			var dataValues []interface{}
			var topicValues []interface{}

			for i := 0; i < len(args); i++ {
				if capturedEvent.Inputs[i].Indexed {
					topicValues = append(topicValues, args[i].Interface())
				} else {
					dataValues = append(dataValues, args[i].Interface())
				}
			}

			data, err := dataInputs.PackValues(dataValues)
			if err != nil {
				glog.Error(fmt.Sprintf(
					"Couldn't pack values for event %s\nnargs %s\nvalues %s\ntopics %s\nerror %s",
					name, args, dataValues, topicValues, err,
				))
				return []reflect.Value{reflect.ValueOf(err)}
			}

			topics := []common.Hash{capturedEvent.ID}

			for i, input := range topicInputs {
				// Geth provides infrastructure for packing arrays of values,
				// so we create an array with just the value we want to pack.

				packable := []interface{}{topicValues[i]}
				bytes, err := abi.Arguments{input}.PackValues(packable)
				if err != nil {
					glog.Error(fmt.Sprintf(
						"Packing error for event %s\nargs %s\nvalues %s\ntopics %s\nerror %s",
						name, args, dataValues, topicValues, err,
					))
					return []reflect.Value{reflect.ValueOf(err)}
				}

				var topic [32]byte

				if len(bytes) > 32 {
					topic = *(*[32]byte)(crypto.Keccak256(bytes))
				} else {
					offset := 32 - len(bytes)
					copy(topic[offset:], bytes)
				}

				topics = append(topics, topic)
			}

			event := &types.Log{
				Address:     address,
				Topics:      topics,
				Data:        data,
				BlockNumber: evm.Context.BlockNumber.Uint64(),
				// Geth will set all other fields, which include
				//   TxHash, TxIndex, Index, and Removed
			}

			state.AddLog(event)
			return []reflect.Value{nilError}
		}

		fieldPointer.Set(reflect.MakeFunc(field.Type, emit))
		costPointer.Set(reflect.MakeFunc(costField.Type, gascost))

		events[name] = PrecompileEvent{
			name,
			event,
		}
	}

	for _, solErr := range source.Errors {
		name := solErr.Name

		var needs []reflect.Type
		for _, arg := range solErr.Inputs {
			needs = append(needs, arg.Type.GetType())
		}

		errorType := reflect.TypeOf((*error)(nil)).Elem()
		expectedFieldType := reflect.FuncOf(needs, []reflect.Type{errorType}, false)

		context := "Precompile " + contract + "'s implementer"
		missing := context + " is missing a field for "

		field, ok := implementerType.Elem().FieldByName(name + "Error")
		if !ok {
			log.Crit(missing, "custom error ", name, "Error of type\n\t", expectedFieldType)
		}
		if field.Type != expectedFieldType {
			log.Crit(
				context, "'s field for error ", name, "Error has the wrong type\n",
				"\texpected:\t", expectedFieldType, "\n\tbut have:\t", field.Type,
			)
		}

		structFields := reflect.ValueOf(implementer).Elem()
		errorReturnPointer := structFields.FieldByName(name + "Error")

		capturedSolErr := solErr
		errorReturn := func(args []reflect.Value) []reflect.Value {
			var dataValues []interface{}
			for i := 0; i < len(args); i++ {
				dataValues = append(dataValues, args[i].Interface())
			}

			data, err := capturedSolErr.Inputs.PackValues(dataValues)
			if err != nil {
				glog.Error(fmt.Sprintf(
					"Couldn't pack values for error %s\nnargs %s\nvalues %s\nerror %s",
					name, args, dataValues, err,
				))
				return []reflect.Value{reflect.ValueOf(err)}
			}

			customErr := &SolError{data: append(capturedSolErr.ID[:4], data...), solErr: capturedSolErr}

			return []reflect.Value{reflect.ValueOf(customErr)}
		}

		errorReturnPointer.Set(reflect.MakeFunc(field.Type, errorReturn))

		errors[name] = PrecompileError{
			name,
			solErr,
		}
	}

	return address, &Precompile{
		methods,
		methodsByName,
		events,
		errors,
		contract,
		reflect.ValueOf(implementer),
		address,
		0,
	}
}

func Precompiles() map[addr]ArbosPrecompile {

	//nolint:gocritic
	hex := func(s string) addr {
		return common.HexToAddress(s)
	}

	contracts := make(map[addr]ArbosPrecompile)

	insert := func(address addr, impl ArbosPrecompile) *Precompile {
		contracts[address] = impl
		return impl.Precompile()
	}

	insert(MakePrecompile(templates.ArbInfoMetaData, &ArbInfo{Address: hex("65")}))
	insert(MakePrecompile(templates.ArbAddressTableMetaData, &ArbAddressTable{Address: hex("66")}))
	insert(MakePrecompile(templates.ArbBLSMetaData, &ArbBLS{Address: hex("67")}))
	insert(MakePrecompile(templates.ArbFunctionTableMetaData, &ArbFunctionTable{Address: hex("68")}))
	insert(MakePrecompile(templates.ArbosTestMetaData, &ArbosTest{Address: hex("69")}))
	ArbGasInfo := insert(MakePrecompile(templates.ArbGasInfoMetaData, &ArbGasInfo{Address: hex("6c")}))
	ArbGasInfo.methodsByName["GetL1FeesAvailable"].arbosVersion = 10
	ArbGasInfo.methodsByName["GetL1RewardRate"].arbosVersion = 11
	ArbGasInfo.methodsByName["GetL1RewardRecipient"].arbosVersion = 11
	insert(MakePrecompile(templates.ArbAggregatorMetaData, &ArbAggregator{Address: hex("6d")}))
	insert(MakePrecompile(templates.ArbStatisticsMetaData, &ArbStatistics{Address: hex("6f")}))

	eventCtx := func(gasLimit uint64, err error) *Context {
		if err != nil {
			glog.Error("call to event's GasCost field failed", "err", err)
		}
		return &Context{
			gasSupplied: gasLimit,
			gasLeft:     gasLimit,
		}
	}

	ArbOwnerPublic := insert(MakePrecompile(templates.ArbOwnerPublicMetaData, &ArbOwnerPublic{Address: hex("6b")}))
	ArbOwnerPublic.methodsByName["GetInfraFeeAccount"].arbosVersion = 5
	ArbOwnerPublic.methodsByName["RectifyChainOwner"].arbosVersion = 11
	ArbOwnerPublic.methodsByName["GetBrotliCompressionLevel"].arbosVersion = 12

	ArbRetryableImpl := &ArbRetryableTx{Address: types.ArbRetryableTxAddress}
	ArbRetryable := insert(MakePrecompile(templates.ArbRetryableTxMetaData, ArbRetryableImpl))
	arbos.ArbRetryableTxAddress = ArbRetryable.address
	arbos.RedeemScheduledEventID = ArbRetryable.events["RedeemScheduled"].template.ID
	arbos.EmitReedeemScheduledEvent = func(
		evm mech, gas, nonce uint64, ticketId, retryTxHash bytes32,
		donor addr, maxRefund *big.Int, submissionFeeRefund *big.Int,
	) error {
		zero := common.Big0
		context := eventCtx(ArbRetryableImpl.RedeemScheduledGasCost(hash{}, hash{}, 0, 0, addr{}, zero, zero))
		return ArbRetryableImpl.RedeemScheduled(
			context, evm, ticketId, retryTxHash, nonce, gas, donor, maxRefund, submissionFeeRefund,
		)
	}
	arbos.EmitTicketCreatedEvent = func(evm mech, ticketId bytes32) error {
		context := eventCtx(ArbRetryableImpl.TicketCreatedGasCost(hash{}))
		return ArbRetryableImpl.TicketCreated(context, evm, ticketId)
	}

	ArbSys := insert(MakePrecompile(templates.ArbSysMetaData, &ArbSys{Address: types.ArbSysAddress}))
	arbos.ArbSysAddress = ArbSys.address
	arbos.L2ToL1TransactionEventID = ArbSys.events["L2ToL1Transaction"].template.ID
	arbos.L2ToL1TxEventID = ArbSys.events["L2ToL1Tx"].template.ID

	ArbOwnerImpl := &ArbOwner{Address: hex("70")}
	emitOwnerActs := func(evm mech, method bytes4, owner addr, data []byte) error {
		context := eventCtx(ArbOwnerImpl.OwnerActsGasCost(method, owner, data))
		return ArbOwnerImpl.OwnerActs(context, evm, method, owner, data)
	}
	_, ArbOwner := MakePrecompile(templates.ArbOwnerMetaData, ArbOwnerImpl)
	ArbOwner.methodsByName["GetInfraFeeAccount"].arbosVersion = 5
	ArbOwner.methodsByName["SetInfraFeeAccount"].arbosVersion = 5
	ArbOwner.methodsByName["ReleaseL1PricerSurplusFunds"].arbosVersion = 10
	ArbOwner.methodsByName["SetChainConfig"].arbosVersion = 11
	ArbOwner.methodsByName["SetBrotliCompressionLevel"].arbosVersion = 12

	insert(ownerOnly(ArbOwnerImpl.Address, ArbOwner, emitOwnerActs))
	insert(debugOnly(MakePrecompile(templates.ArbDebugMetaData, &ArbDebug{Address: hex("ff")})))

	ArbosActs := insert(MakePrecompile(templates.ArbosActsMetaData, &ArbosActs{Address: types.ArbosAddress}))
	arbos.InternalTxStartBlockMethodID = ArbosActs.GetMethodID("StartBlock")
	arbos.InternalTxBatchPostingReportMethodID = ArbosActs.GetMethodID("BatchPostingReport")

	return contracts
}

func (p *Precompile) CloneWithImpl(impl interface{}) *Precompile {
	clone := *p
	clone.implementer = reflect.ValueOf(impl)
	return &clone
}

func (p *Precompile) GetMethodID(name string) bytes4 {
	method, ok := p.methodsByName[name]
	if !ok {
		panic(fmt.Sprintf("Precompile %v does not have a method with the name %v", p.name, name))
	}
	return *(*bytes4)(method.template.ID)
}

// Call a precompile in typed form, deserializing its inputs and serializing its outputs
func (p *Precompile) Call(
	input []byte,
	precompileAddress common.Address,
	actingAsAddress common.Address,
	caller common.Address,
	value *big.Int,
	readOnly bool,
	gasSupplied uint64,
	evm *vm.EVM,
) (output []byte, gasLeft uint64, err error) {
	arbosVersion := arbosState.ArbOSVersion(evm.StateDB)

	if arbosVersion < p.arbosVersion {
		// the precompile isn't yet active, so treat this call as if it were to a contract that doesn't exist
		return []byte{}, gasSupplied, nil
	}

	if len(input) < 4 {
		// ArbOS precompiles always have canonical method selectors
		return nil, 0, vm.ErrExecutionReverted
	}
	id := *(*[4]byte)(input)
	method, ok := p.methods[id]
	if !ok || arbosVersion < method.arbosVersion {
		// method does not exist or hasn't yet been activated
		return nil, 0, vm.ErrExecutionReverted
	}

	if method.purity >= view && actingAsAddress != precompileAddress {
		// should not access precompile superpowers when not acting as the precompile
		return nil, 0, vm.ErrExecutionReverted
	}

	if method.purity >= write && readOnly {
		// tried to write to global state in read-only mode
		return nil, 0, vm.ErrExecutionReverted
	}

	if method.purity < payable && value.Sign() != 0 {
		// tried to pay something that's non-payable
		return nil, 0, vm.ErrExecutionReverted
	}

	callerCtx := &Context{
		caller:      caller,
		gasSupplied: gasSupplied,
		gasLeft:     gasSupplied,
		readOnly:    method.purity <= view,
		tracingInfo: util.NewTracingInfo(evm, caller, precompileAddress, util.TracingDuringEVM),
	}

	argsCost := params.CopyGas * arbmath.WordsForBytes(uint64(len(input)-4))
	if err := callerCtx.Burn(argsCost); err != nil {
		// user cannot afford the argument data supplied
		return nil, 0, vm.ErrExecutionReverted
	}

	if method.purity != pure {
		// impure methods may need the ArbOS state, so open & update the call context now
		state, err := arbosState.OpenArbosState(evm.StateDB, callerCtx)
		if err != nil {
			return nil, 0, err
		}
		callerCtx.State = state
	}

	switch txProcessor := evm.ProcessingHook.(type) {
	case *arbos.TxProcessor:
		callerCtx.txProcessor = txProcessor
	case *vm.DefaultTxProcessor:
		glog.Error("processing hook not set")
		return nil, 0, vm.ErrExecutionReverted
	default:
		glog.Error("unknown processing hook")
		return nil, 0, vm.ErrExecutionReverted
	}

	reflectArgs := []reflect.Value{
		p.implementer,
		reflect.ValueOf(callerCtx),
	}

	switch method.purity {
	case pure:
	case view:
		reflectArgs = append(reflectArgs, reflect.ValueOf(evm))
	case write:
		reflectArgs = append(reflectArgs, reflect.ValueOf(evm))
	case payable:
		reflectArgs = append(reflectArgs, reflect.ValueOf(evm))
		reflectArgs = append(reflectArgs, reflect.ValueOf(value))
	default:
		log.Crit("Unknown state mutability ", method.purity)
	}

	args, err := method.template.Inputs.Unpack(input[4:])
	if err != nil {
		// calldata does not match the method's signature
		return nil, 0, vm.ErrExecutionReverted
	}
	for _, arg := range args {
		converted := reflect.ValueOf(arg).Convert(method.handler.Type.In(len(reflectArgs)))
		reflectArgs = append(reflectArgs, converted)
	}

	reflectResult := method.handler.Func.Call(reflectArgs)
	resultCount := len(reflectResult) - 1
	if !reflectResult[resultCount].IsNil() {
		// the last arg is always the error status
		errRet, ok := reflectResult[resultCount].Interface().(error)
		if !ok {
			log.Error("final precompile return value must be error")
			return nil, callerCtx.gasLeft, vm.ErrExecutionReverted
		}
		var solErr *SolError
		isSolErr := errors.As(errRet, &solErr)
		if isSolErr {
			resultCost := params.CopyGas * arbmath.WordsForBytes(uint64(len(solErr.data)))
			if err := callerCtx.Burn(resultCost); err != nil {
				// user cannot afford the result data returned
				return nil, 0, vm.ErrExecutionReverted
			}
			return solErr.data, callerCtx.gasLeft, vm.ErrExecutionReverted
		}
		if !errors.Is(errRet, vm.ErrOutOfGas) {
			log.Debug("precompile reverted with non-solidity error", "precompile", precompileAddress, "input", input, "err", errRet)
		}
		// nolint:errorlint
		if arbosVersion >= 11 || errRet == vm.ErrExecutionReverted {
			return nil, callerCtx.gasLeft, vm.ErrExecutionReverted
		}
		// Preserve behavior with old versions which would zero out gas on this type of error
		return nil, 0, errRet
	}
	result := make([]interface{}, resultCount)
	for i := 0; i < resultCount; i++ {
		result[i] = reflectResult[i].Interface()
	}

	encoded, err := method.template.Outputs.PackValues(result)
	if err != nil {
		log.Error("could not encode precompile result", "err", err)
		return nil, callerCtx.gasLeft, vm.ErrExecutionReverted
	}

	resultCost := params.CopyGas * arbmath.WordsForBytes(uint64(len(encoded)))
	if err := callerCtx.Burn(resultCost); err != nil {
		// user cannot afford the result data returned
		return nil, 0, vm.ErrExecutionReverted
	}

	return encoded, callerCtx.gasLeft, nil
}

func (p *Precompile) Precompile() *Precompile {
	return p
}

// Get4ByteMethodSignatures is needed for the fuzzing harness
func (p *Precompile) Get4ByteMethodSignatures() [][4]byte {
	ret := make([][4]byte, 0, len(p.methods))
	for sig := range p.methods {
		ret = append(ret, sig)
	}
	return ret
}

func (p *Precompile) GetErrorABIs() []abi.Error {
	ret := make([]abi.Error, 0, len(p.errors))
	for _, solErr := range p.errors {
		ret = append(ret, solErr.template)
	}
	return ret
}

'''
'''--- precompiles/precompile_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package precompiles

import (
	"bytes"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/core/state"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/storage"
	templates "github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestEvents(t *testing.T) {
	blockNumber := 1024

	evm := newMockEVMForTesting()
	evm.Context.BlockNumber = big.NewInt(int64(blockNumber))

	debugContractAddr := common.HexToAddress("ff")
	contract := Precompiles()[debugContractAddr]

	var method *PrecompileMethod
	for _, available := range contract.Precompile().methods {
		if available.name == "Events" {
			method = available
			break
		}
	}

	zeroHash := crypto.Keccak256([]byte{0x00})
	trueHash := common.Hash{}.Bytes()
	falseHash := common.Hash{}.Bytes()
	trueHash[31] = 0x01

	var data []byte
	payload := [][]byte{
		method.template.ID, // select the `Events` method
		falseHash,          // set the flag to false
		zeroHash,           // set the value to something known
	}
	for _, bytes := range payload {
		data = append(data, bytes...)
	}

	caller := common.HexToAddress("aaaaaaaabbbbbbbbccccccccdddddddd")
	number := big.NewInt(0x9364)

	output, gasLeft, err := contract.Call(
		data,
		debugContractAddr,
		debugContractAddr,
		caller,
		number,
		false,
		^uint64(0),
		evm,
	)
	Require(t, err, "call failed")

	burnedToStorage := storage.StorageReadCost                      // the ArbOS version check costs a read
	burnedToArgs := arbmath.WordsForBytes(32+32) * params.CopyGas   // bool and a bytes32
	burnedToResult := arbmath.WordsForBytes(32+32) * params.CopyGas // addr and a huge
	burnedToEvents := ^uint64(0) - gasLeft - (burnedToStorage + burnedToArgs + burnedToResult)

	if burnedToEvents != 3768 {
		Fail(t, "burned", burnedToEvents, "instead of", 3768, "gas")
	}

	outputAddr := common.BytesToAddress(output[:32])
	outputData := new(big.Int).SetBytes(output[32:])

	if outputAddr != caller {
		Fail(t, "unexpected output address", outputAddr, "instead of", caller)
	}
	if outputData.Cmp(number) != 0 {
		Fail(t, "unexpected output number", outputData, "instead of", number)
	}

	//nolint:errcheck
	logs := evm.StateDB.(*state.StateDB).Logs()
	for _, log := range logs {
		if log.Address != debugContractAddr {
			Fail(t, "address mismatch:", log.Address, "vs", debugContractAddr)
		}
		if log.BlockNumber != uint64(blockNumber) {
			Fail(t, "block number mismatch:", log.BlockNumber, "vs", blockNumber)
		}
		t.Log("topic", len(log.Topics), log.Topics)
		t.Log("data ", len(log.Data), log.Data)
	}

	basicTopics := logs[0].Topics
	mixedTopics := logs[1].Topics

	if !bytes.Equal(basicTopics[1].Bytes(), zeroHash) || !bytes.Equal(mixedTopics[2].Bytes(), zeroHash) {
		Fail(t, "indexing a bytes32 didn't work")
	}
	if !bytes.Equal(mixedTopics[1].Bytes(), falseHash) {
		Fail(t, "indexing a bool didn't work")
	}
	if !bytes.Equal(mixedTopics[3].Bytes(), caller.Hash().Bytes()) {
		Fail(t, "indexing an address didn't work")
	}

	ArbDebugInfo, cerr := templates.NewArbDebug(common.Address{}, nil)
	basic, berr := ArbDebugInfo.ParseBasic(*logs[0])
	mixed, merr := ArbDebugInfo.ParseMixed(*logs[1])
	if cerr != nil || berr != nil || merr != nil {
		Fail(t, "failed to parse event logs", "\nprecompile:", cerr, "\nbasic:", berr, "\nmixed:", merr)
	}

	if basic.Flag != true || !bytes.Equal(basic.Value[:], zeroHash) {
		Fail(t, "event Basic's data isn't correct")
	}
	if mixed.Flag != false || mixed.Not != true || !bytes.Equal(mixed.Value[:], zeroHash) {
		Fail(t, "event Mixed's data isn't correct")
	}
	if mixed.Conn != debugContractAddr || mixed.Caller != caller {
		Fail(t, "event Mixed's data isn't correct")
	}
}

func TestEventCosts(t *testing.T) {
	debugContractAddr := common.HexToAddress("ff")
	contract := Precompiles()[debugContractAddr]

	//nolint:errcheck
	impl := contract.Precompile().implementer.Interface().(*ArbDebug)

	testBytes := [...][]byte{
		nil,
		{0x01},
		{0x02, 0x32, 0x24, 0x48},
		common.Hash{}.Bytes(),
		common.Hash{}.Bytes(),
	}
	testBytes[4] = append(testBytes[4], common.Hash{}.Bytes()...)

	test := func(a bool, b addr, c huge, d hash, e []byte) uint64 {
		cost, err := impl.StoreGasCost(a, b, c, d, e)
		Require(t, err)
		return cost
	}

	tests := [...]uint64{
		test(true, addr{}, big.NewInt(24), common.Hash{}, testBytes[0]),
		test(false, addr{}, big.NewInt(8), common.Hash{}, testBytes[1]),
		test(false, addr{}, big.NewInt(8), common.Hash{}, testBytes[2]),
		test(true, addr{}, big.NewInt(32), common.Hash{}, testBytes[3]),
		test(true, addr{}, big.NewInt(64), common.Hash{}, testBytes[4]),
	}

	expected := [5]uint64{}

	for i, bytes := range testBytes {
		baseCost := params.LogGas + 3*params.LogTopicGas
		addrCost := 32 * params.LogDataGas
		hashCost := 32 * params.LogDataGas

		sizeBytes := 32
		offsetBytes := 32
		storeBytes := sizeBytes + offsetBytes + len(bytes)
		storeBytes = storeBytes + 31 - (storeBytes+31)%32 // round up to a multiple of 32
		storeCost := uint64(storeBytes) * params.LogDataGas

		expected[i] = baseCost + addrCost + hashCost + storeCost
	}

	if tests != expected {
		Fail(t, "Events are mispriced\nexpected:", expected, "\nbut have:", tests)
	}
}

type FatalBurner struct {
	t       *testing.T
	count   uint64
	gasLeft uint64
}

func NewFatalBurner(t *testing.T, limit uint64) FatalBurner {
	return FatalBurner{t, 0, limit}
}

func (burner FatalBurner) Burn(amount uint64) error {
	burner.t.Helper()
	burner.count += 1
	if burner.gasLeft < amount {
		Fail(burner.t, "out of gas after", burner.count, "burns")
	}
	burner.gasLeft -= amount
	return nil
}

'''
'''--- precompiles/wrapper.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package precompiles

import (
	"errors"
	"math/big"

	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/util"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/log"
)

// DebugPrecompile is a precompile wrapper for those not allowed in production
type DebugPrecompile struct {
	precompile ArbosPrecompile
}

// create a debug-only precompile wrapper
func debugOnly(address addr, impl ArbosPrecompile) (addr, ArbosPrecompile) {
	return address, &DebugPrecompile{impl}
}

func (wrapper *DebugPrecompile) Call(
	input []byte,
	precompileAddress common.Address,
	actingAsAddress common.Address,
	caller common.Address,
	value *big.Int,
	readOnly bool,
	gasSupplied uint64,
	evm *vm.EVM,
) ([]byte, uint64, error) {

	debugMode := evm.ChainConfig().DebugMode()

	if debugMode {
		con := wrapper.precompile
		return con.Call(input, precompileAddress, actingAsAddress, caller, value, readOnly, gasSupplied, evm)
	}
	// Take all gas.
	return nil, 0, errors.New("debug precompiles are disabled")
}

func (wrapper *DebugPrecompile) Precompile() *Precompile {
	return wrapper.precompile.Precompile()
}

// OwnerPrecompile is a precompile wrapper for those only chain owners may use
type OwnerPrecompile struct {
	precompile  ArbosPrecompile
	emitSuccess func(mech, bytes4, addr, []byte) error
}

func ownerOnly(address addr, impl ArbosPrecompile, emit func(mech, bytes4, addr, []byte) error) (addr, ArbosPrecompile) {
	return address, &OwnerPrecompile{
		precompile:  impl,
		emitSuccess: emit,
	}
}

func (wrapper *OwnerPrecompile) Call(
	input []byte,
	precompileAddress common.Address,
	actingAsAddress common.Address,
	caller common.Address,
	value *big.Int,
	readOnly bool,
	gasSupplied uint64,
	evm *vm.EVM,
) ([]byte, uint64, error) {
	con := wrapper.precompile

	burner := &Context{
		gasSupplied: gasSupplied,
		gasLeft:     gasSupplied,
		tracingInfo: util.NewTracingInfo(evm, caller, precompileAddress, util.TracingDuringEVM),
	}
	state, err := arbosState.OpenArbosState(evm.StateDB, burner)
	if err != nil {
		return nil, burner.gasLeft, err
	}

	owners := state.ChainOwners()
	isOwner, err := owners.IsMember(caller)
	if err != nil {
		return nil, burner.gasLeft, err
	}

	if !isOwner {
		return nil, burner.gasLeft, errors.New("unauthorized caller to access-controlled method")
	}

	output, _, err := con.Call(input, precompileAddress, actingAsAddress, caller, value, readOnly, gasSupplied, evm)

	if err != nil {
		return output, gasSupplied, err // we don't deduct gas since we don't want to charge the owner
	}

	version := arbosState.ArbOSVersion(evm.StateDB)
	if !readOnly || version < 11 {
		// log that the owner operation succeeded
		if err := wrapper.emitSuccess(evm, *(*[4]byte)(input[:4]), caller, input); err != nil {
			log.Error("failed to emit OwnerActs event", "err", err)
		}
	}

	return output, gasSupplied, err // we don't deduct gas since we don't want to charge the owner
}

func (wrapper *OwnerPrecompile) Precompile() *Precompile {
	con := wrapper.precompile
	return con.Precompile()
}

'''
'''--- relay/relay.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package relay

import (
	"context"
	"errors"
	"net"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/broadcastclient"
	"github.com/offchainlabs/nitro/broadcastclients"
	"github.com/offchainlabs/nitro/broadcaster"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/cmd/util/confighelpers"
	"github.com/offchainlabs/nitro/util/sharedmetrics"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

type Relay struct {
	stopwaiter.StopWaiter
	broadcastClients            *broadcastclients.BroadcastClients
	broadcaster                 *broadcaster.Broadcaster
	confirmedSequenceNumberChan chan arbutil.MessageIndex
	messageChan                 chan broadcaster.BroadcastFeedMessage
}

type MessageQueue struct {
	queue chan broadcaster.BroadcastFeedMessage
}

func (q *MessageQueue) AddBroadcastMessages(feedMessages []*broadcaster.BroadcastFeedMessage) error {
	for _, feedMessage := range feedMessages {
		q.queue <- *feedMessage
	}

	return nil
}

func NewRelay(config *Config, feedErrChan chan error) (*Relay, error) {

	q := MessageQueue{make(chan broadcaster.BroadcastFeedMessage, config.Queue)}

	confirmedSequenceNumberListener := make(chan arbutil.MessageIndex, config.Queue)

	clients, err := broadcastclients.NewBroadcastClients(
		func() *broadcastclient.Config { return &config.Node.Feed.Input },
		config.Chain.ID,
		0,
		&q,
		confirmedSequenceNumberListener,
		feedErrChan,
		nil,
	)
	if err != nil {
		return nil, err
	}
	if clients == nil {
		return nil, errors.New("no feed servers found")
	}

	dataSignerErr := func([]byte) ([]byte, error) {
		return nil, errors.New("relay attempted to sign feed message")
	}
	return &Relay{
		broadcaster:                 broadcaster.NewBroadcaster(func() *wsbroadcastserver.BroadcasterConfig { return &config.Node.Feed.Output }, config.Chain.ID, feedErrChan, dataSignerErr),
		broadcastClients:            clients,
		confirmedSequenceNumberChan: confirmedSequenceNumberListener,
		messageChan:                 q.queue,
	}, nil
}

func (r *Relay) Start(ctx context.Context) error {
	r.StopWaiter.Start(ctx, r)
	err := r.broadcaster.Initialize()
	if err != nil {
		return errors.New("broadcast unable to initialize")
	}
	err = r.broadcaster.Start(ctx)
	if err != nil {
		return errors.New("broadcast unable to start")
	}

	r.broadcastClients.Start(ctx)

	r.LaunchThread(func(ctx context.Context) {
		for {
			select {
			case <-ctx.Done():
				return
			case msg := <-r.messageChan:
				sharedmetrics.UpdateSequenceNumberGauge(msg.SequenceNumber)
				r.broadcaster.BroadcastSingleFeedMessage(&msg)
			case cs := <-r.confirmedSequenceNumberChan:
				r.broadcaster.Confirm(cs)
			}
		}
	})

	return nil
}

func (r *Relay) GetListenerAddr() net.Addr {
	return r.broadcaster.ListenerAddr()
}

func (r *Relay) StopAndWait() {
	r.StopWaiter.StopAndWait()
	r.broadcastClients.StopAndWait()
	r.broadcaster.StopAndWait()
}

type Config struct {
	Conf          genericconf.ConfConfig          `koanf:"conf"`
	Chain         L2Config                        `koanf:"chain"`
	LogLevel      int                             `koanf:"log-level"`
	LogType       string                          `koanf:"log-type"`
	Metrics       bool                            `koanf:"metrics"`
	MetricsServer genericconf.MetricsServerConfig `koanf:"metrics-server"`
	PProf         bool                            `koanf:"pprof"`
	PprofCfg      genericconf.PProf               `koanf:"pprof-cfg"`
	Node          NodeConfig                      `koanf:"node"`
	Queue         int                             `koanf:"queue"`
}

var ConfigDefault = Config{
	Conf:          genericconf.ConfConfigDefault,
	Chain:         L2ConfigDefault,
	LogLevel:      int(log.LvlInfo),
	LogType:       "plaintext",
	Metrics:       false,
	MetricsServer: genericconf.MetricsServerConfigDefault,
	PProf:         false,
	PprofCfg:      genericconf.PProfDefault,
	Node:          NodeConfigDefault,
	Queue:         1024,
}

func ConfigAddOptions(f *flag.FlagSet) {
	genericconf.ConfConfigAddOptions("conf", f)
	L2ConfigAddOptions("chain", f)
	f.Int("log-level", ConfigDefault.LogLevel, "log level")
	f.String("log-type", ConfigDefault.LogType, "log type")
	f.Bool("metrics", ConfigDefault.Metrics, "enable metrics")
	genericconf.MetricsServerAddOptions("metrics-server", f)
	f.Bool("pprof", ConfigDefault.PProf, "enable pprof")
	genericconf.PProfAddOptions("pprof-cfg", f)
	NodeConfigAddOptions("node", f)
	f.Int("queue", ConfigDefault.Queue, "queue for incoming messages from sequencer")
}

type NodeConfig struct {
	Feed broadcastclient.FeedConfig `koanf:"feed"`
}

var NodeConfigDefault = NodeConfig{
	Feed: broadcastclient.FeedConfigDefault,
}

func NodeConfigAddOptions(prefix string, f *flag.FlagSet) {
	broadcastclient.FeedConfigAddOptions(prefix+".feed", f, true, true)
}

type L2Config struct {
	ID uint64 `koanf:"id"`
}

var L2ConfigDefault = L2Config{
	ID: 0,
}

func L2ConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".id", L2ConfigDefault.ID, "L2 chain ID")
}

func ParseRelay(_ context.Context, args []string) (*Config, error) {
	f := flag.NewFlagSet("", flag.ContinueOnError)

	ConfigAddOptions(f)

	k, err := confighelpers.BeginCommonParse(f, args)
	if err != nil {
		return nil, err
	}

	var relayConfig Config
	if err := confighelpers.EndCommonParse(k, &relayConfig); err != nil {
		return nil, err
	}

	if relayConfig.Conf.Dump {
		err = confighelpers.DumpConfig(k, map[string]interface{}{})
		if err != nil {
			return nil, err
		}
	}

	return &relayConfig, nil
}

'''
'''--- rust-toolchain.toml ---
[toolchain]
# This specifies the version of Rust we use to build.
# Individual crates in the workspace may support a lower version, as indicated by `rust-version` field in each crate's `Cargo.toml`.
# The version specified below, should be at least as high as the maximum `rust-version` within the workspace.
channel    = "1.72.0"
components = [ "rustfmt", "rust-src" ]
targets    = [ "wasm32-unknown-unknown" ]

'''
'''--- scripts/build-brotli.sh ---
#!/bin/bash

set -e

mydir=`dirname $0`
cd "$mydir"

BUILD_WASM=false
BUILD_LOCAL=false
BUILD_SOFTFLOAT=false
USE_DOCKER=false
TARGET_DIR=../target/
SOURCE_DIR=../brotli
NITRO_DIR=../

usage(){
    echo "brotli builder for arbitrum"
    echo
    echo "usage: $0 [options]"
    echo
    echo "use one or more of:"
    echo " -w     build wasm (uses emscripten)"
    echo " -l     build local"
    echo " -f     build soft-float"
    echo
    echo "to avoid dependencies you might want:"
    echo " -d     build inside docker container"
    echo
    echo "Other options:"
    echo " -s     source dir default: $SOURCE_DIR"
    echo " -t     target dir default: $TARGET_DIR"
    echo " -n     nitro dir default: $NITRO_DIR"
    echo " -h     help"
    echo
    echo "all relative paths are relative to script location"
}

while getopts "s:t:c:D:wldhf" option; do
    case $option in
        h)
            usage
            exit
            ;;
        w)
            BUILD_WASM=true
            ;;
        l)
            BUILD_LOCAL=true
            ;;
        f)
            BUILD_SOFTFLOAT=true
            ;;
        d)
            USE_DOCKER=true
            ;;
        t)
            TARGET_DIR="$OPTARG"
            ;;
        n)
            NITRO_DIR="$OPTARG"
            ;;
        s)
            SOURCE_DIR="$OPTARG"
            ;;
    esac
done

if ! $BUILD_WASM && ! $BUILD_LOCAL && ! $BUILD_SOFT; then
    usage
    exit
fi

if [ ! -d "$TARGET_DIR" ]; then
    mkdir -p "${TARGET_DIR}lib"
    ln -s "lib" "${TARGET_DIR}lib64" # Fedora build
fi
TARGET_DIR_ABS=`cd -P "$TARGET_DIR"; pwd`

if $USE_DOCKER; then
    if $BUILD_WASM; then
        DOCKER_BUILDKIT=1 docker build --target brotli-wasm-export -o type=local,dest="$TARGET_DIR_ABS" "${NITRO_DIR}"
    fi
    if $BUILD_LOCAL; then
        DOCKER_BUILDKIT=1 docker build --target brotli-library-export -o type=local,dest="$TARGET_DIR_ABS" "${NITRO_DIR}"
    fi
    if $BUILD_SOFTFLOAT; then
        DOCKER_BUILDKIT=1 docker build --target wasm-libs-export -o type=local,dest="$TARGET_DIR_ABS" "${NITRO_DIR}"
    fi
    exit 0
fi

cd "$SOURCE_DIR"
if $BUILD_WASM; then
    mkdir -p buildfiles/build-wasm
    mkdir -p buildfiles/install-wasm
    TEMP_INSTALL_DIR_ABS=`cd -P buildfiles/install-wasm; pwd`
    cd buildfiles/build-wasm
    cmake ../../ -DCMAKE_C_COMPILER=emcc -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_FLAGS=-fPIC -DCMAKE_INSTALL_PREFIX="$TEMP_INSTALL_DIR_ABS" -DCMAKE_AR=`which emar` -DCMAKE_RANLIB=`which touch`
    make -j
    make install
    cp -rv "$TEMP_INSTALL_DIR_ABS/lib" "$TARGET_DIR_ABS/lib-wasm"
    cd ..
fi

if $BUILD_LOCAL; then
    mkdir -p buildfiles/build-local
    cd buildfiles/build-local
    cmake ../../ -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX="$TARGET_DIR_ABS"
    make -j
    make install
    cd ..
fi

'''
'''--- scripts/download-machine.sh ---
#!/usr/bin/env bash
set -e

mkdir "$2"
ln -sfT "$2" latest
cd "$2"
echo "$2" > module-root.txt
url_base="https://github.com/OffchainLabs/nitro/releases/download/$1"
wget "$url_base/machine.wavm.br"

status_code="$(curl -LI "$url_base/replay.wasm" -so /dev/null -w '%{http_code}')"
if [ "$status_code" -ne 404 ]; then
	wget "$url_base/replay.wasm"
fi

'''
'''--- solgen/gen.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"strings"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
)

type HardHatArtifact struct {
	Format       string        `json:"_format"`
	ContractName string        `json:"contractName"`
	SourceName   string        `json:"sourceName"`
	Abi          []interface{} `json:"abi"`
	Bytecode     string        `json:"bytecode"`
}

type moduleInfo struct {
	contractNames []string
	abis          []string
	bytecodes     []string
}

func (m *moduleInfo) addArtifact(artifact HardHatArtifact) {
	abi, err := json.Marshal(artifact.Abi)
	if err != nil {
		log.Fatal(err)
	}
	m.contractNames = append(m.contractNames, artifact.ContractName)
	m.abis = append(m.abis, string(abi))
	m.bytecodes = append(m.bytecodes, artifact.Bytecode)
}

func (m *moduleInfo) exportABIs(dest string) {
	for i, name := range m.contractNames {
		path := filepath.Join(dest, name+".abi")
		abi := m.abis[i] + "\n"

		// #nosec G306
		err := os.WriteFile(path, []byte(abi), 0o644)
		if err != nil {
			log.Fatal(err)
		}
	}
}

func main() {
	_, filename, _, ok := runtime.Caller(0)
	if !ok {
		log.Fatal("bad path")
	}
	root := filepath.Dir(filename)
	parent := filepath.Dir(root)
	filePaths, err := filepath.Glob(filepath.Join(parent, "contracts", "build", "contracts", "src", "*", "*", "*.json"))
	if err != nil {
		log.Fatal(err)
	}

	modules := make(map[string]*moduleInfo)

	for _, path := range filePaths {
		if strings.Contains(path, ".dbg.json") {
			continue
		}

		dir, file := filepath.Split(path)
		dir, _ = filepath.Split(dir[:len(dir)-1])
		_, module := filepath.Split(dir[:len(dir)-1])
		module = strings.ReplaceAll(module, "-", "_")
		module += "gen"

		name := file[:len(file)-5]

		data, err := os.ReadFile(path)
		if err != nil {
			log.Fatal("could not read", path, "for contract", name, err)
		}

		artifact := HardHatArtifact{}
		if err := json.Unmarshal(data, &artifact); err != nil {
			log.Fatal("failed to parse contract", name, err)
		}
		modInfo := modules[module]
		if modInfo == nil {
			modInfo = &moduleInfo{}
			modules[module] = modInfo
		}
		modInfo.addArtifact(artifact)
	}

	// add upgrade executor module which is not compiled locally, but imported from 'nitro-contracts' depedencies
	upgExecutorPath := filepath.Join(parent, "contracts", "node_modules", "@offchainlabs", "upgrade-executor", "build", "contracts", "src", "UpgradeExecutor.sol", "UpgradeExecutor.json")
	_, err = os.Stat(upgExecutorPath)
	if !os.IsNotExist(err) {
		data, err := os.ReadFile(upgExecutorPath)
		if err != nil {
			// log.Fatal(string(output))
			log.Fatal("could not read", upgExecutorPath, "for contract", "UpgradeExecutor", err)
		}
		artifact := HardHatArtifact{}
		if err := json.Unmarshal(data, &artifact); err != nil {
			log.Fatal("failed to parse contract", "UpgradeExecutor", err)
		}
		modInfo := modules["upgrade_executorgen"]
		if modInfo == nil {
			modInfo = &moduleInfo{}
			modules["upgrade_executorgen"] = modInfo
		}
		modInfo.addArtifact(artifact)
	}

	for module, info := range modules {

		code, err := bind.Bind(
			info.contractNames,
			info.abis,
			info.bytecodes,
			nil,
			module,
			bind.LangGo,
			nil,
			nil,
		)
		if err != nil {
			log.Fatal(err)
		}

		folder := filepath.Join(root, "go", module)

		err = os.MkdirAll(folder, 0o755)
		if err != nil {
			log.Fatal(err)
		}

		/*
			#nosec G306
			This file contains no private information so the permissions can be lenient
		*/
		err = os.WriteFile(filepath.Join(folder, module+".go"), []byte(code), 0o644)
		if err != nil {
			log.Fatal(err)
		}
	}

	fmt.Println("successfully generated go abi files")

	blockscout := filepath.Join(parent, "nitro-testnode", "blockscout", "init", "data")
	if _, err := os.Stat(blockscout); err != nil {
		fmt.Println("skipping abi export since blockscout is not present")
	} else {
		modules["precompilesgen"].exportABIs(blockscout)
		modules["node_interfacegen"].exportABIs(blockscout)
		fmt.Println("successfully exported abi files")
	}
}

'''
'''--- staker/assertion.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/validator"
)

func NewAssertionFromSolidity(assertion rollupgen.Assertion) *Assertion {
	return &Assertion{
		BeforeState: validator.NewExecutionStateFromSolidity(assertion.BeforeState),
		AfterState:  validator.NewExecutionStateFromSolidity(assertion.AfterState),
		NumBlocks:   assertion.NumBlocks,
	}
}

func (a *Assertion) AsSolidityStruct() rollupgen.Assertion {
	return rollupgen.Assertion{
		BeforeState: a.BeforeState.AsSolidityStruct(),
		AfterState:  a.AfterState.AsSolidityStruct(),
		NumBlocks:   a.NumBlocks,
	}
}

func HashChallengeState(
	segmentStart uint64,
	segmentLength uint64,
	hashes []common.Hash,
) common.Hash {
	var hashesBytes []byte
	for _, h := range hashes {
		hashesBytes = append(hashesBytes, h[:]...)
	}
	return crypto.Keccak256Hash(
		math.U256Bytes(new(big.Int).SetUint64(segmentStart)),
		math.U256Bytes(new(big.Int).SetUint64(segmentLength)),
		hashesBytes,
	)
}

func (a *Assertion) ExecutionHash() common.Hash {
	return HashChallengeState(
		0,
		a.NumBlocks,
		[]common.Hash{
			a.BeforeState.BlockStateHash(),
			a.AfterState.BlockStateHash(),
		},
	)
}

type Assertion struct {
	BeforeState *validator.ExecutionState
	AfterState  *validator.ExecutionState
	NumBlocks   uint64
}

type NodeInfo struct {
	NodeNum                  uint64
	L1BlockProposed          uint64
	ParentChainBlockProposed uint64
	Assertion                *Assertion
	InboxMaxCount            *big.Int
	AfterInboxBatchAcc       common.Hash
	NodeHash                 common.Hash
	WasmModuleRoot           common.Hash
}

func (n *NodeInfo) AfterState() *validator.ExecutionState {
	return n.Assertion.AfterState
}

func (n *NodeInfo) MachineStatuses() [2]uint8 {
	return [2]uint8{
		uint8(n.Assertion.BeforeState.MachineStatus),
		uint8(n.Assertion.AfterState.MachineStatus),
	}
}

func (n *NodeInfo) GlobalStates() [2]rollupgen.GlobalState {
	return [2]rollupgen.GlobalState{
		rollupgen.GlobalState(n.Assertion.BeforeState.GlobalState.AsSolidityStruct()),
		rollupgen.GlobalState(n.Assertion.AfterState.GlobalState.AsSolidityStruct()),
	}
}

'''
'''--- staker/block_challenge_backend.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/validator"
)

type BlockChallengeBackend struct {
	streamer               TransactionStreamerInterface
	startMsgCount          arbutil.MessageIndex
	startPosition          uint64
	endPosition            uint64
	startGs                validator.GoGlobalState
	endGs                  validator.GoGlobalState
	inboxTracker           InboxTrackerInterface
	tooFarStartsAtPosition uint64
}

// Assert that BlockChallengeBackend implements ChallengeBackend
var _ ChallengeBackend = (*BlockChallengeBackend)(nil)

func NewBlockChallengeBackend(
	initialState *challengegen.ChallengeManagerInitiatedChallenge,
	maxBatchesRead uint64,
	streamer TransactionStreamerInterface,
	inboxTracker InboxTrackerInterface,
) (*BlockChallengeBackend, error) {
	startGs := validator.GoGlobalStateFromSolidity(initialState.StartState)

	var startMsgCount arbutil.MessageIndex
	if startGs.Batch > 0 {
		var err error
		startMsgCount, err = inboxTracker.GetBatchMessageCount(startGs.Batch - 1)
		if err != nil {
			return nil, fmt.Errorf("failed to get challenge start batch metadata: %w", err)
		}
	}
	startMsgCount += arbutil.MessageIndex(startGs.PosInBatch)

	var endMsgCount arbutil.MessageIndex
	if maxBatchesRead > 0 {
		var err error
		endMsgCount, err = inboxTracker.GetBatchMessageCount(maxBatchesRead - 1)
		if err != nil {
			return nil, fmt.Errorf("failed to get challenge end batch metadata: %w", err)
		}
	}

	return &BlockChallengeBackend{
		streamer:               streamer,
		startMsgCount:          startMsgCount,
		startGs:                startGs,
		startPosition:          0,
		endPosition:            math.MaxUint64,
		endGs:                  validator.GoGlobalStateFromSolidity(initialState.EndState),
		inboxTracker:           inboxTracker,
		tooFarStartsAtPosition: uint64(endMsgCount - startMsgCount + 1),
	}, nil
}

func (b *BlockChallengeBackend) findBatchAfterMessageCount(msgCount arbutil.MessageIndex) (uint64, error) {
	if msgCount == 0 {
		return 0, nil
	}
	low := b.startGs.Batch
	high := b.endGs.Batch
	for {
		// Binary search invariants:
		//   - messageCount(high) >= msgCount
		//   - messageCount(low-1) < msgCount
		//   - high >= low
		if high < low {
			return 0, fmt.Errorf("when attempting to find batch for message count %v high %v < low %v", msgCount, high, low)
		}
		mid := (low + high) / 2
		batchMsgCount, err := b.inboxTracker.GetBatchMessageCount(mid)
		if err != nil {
			return 0, fmt.Errorf("failed to get batch metadata while binary searching: %w", err)
		}
		if batchMsgCount < msgCount {
			low = mid + 1
		} else if batchMsgCount == msgCount {
			return mid + 1, nil
		} else if mid == low { // batchMsgCount > msgCount
			return mid, nil
		} else { // batchMsgCount > msgCount
			high = mid
		}
	}
}

func (b *BlockChallengeBackend) FindGlobalStateFromMessageCount(count arbutil.MessageIndex) (validator.GoGlobalState, error) {
	batch, err := b.findBatchAfterMessageCount(count)
	if err != nil {
		return validator.GoGlobalState{}, err
	}
	var prevBatchMsgCount arbutil.MessageIndex
	if batch > 0 {
		prevBatchMsgCount, err = b.inboxTracker.GetBatchMessageCount(batch - 1)
		if err != nil {
			return validator.GoGlobalState{}, err
		}
		if prevBatchMsgCount > count {
			return validator.GoGlobalState{}, errors.New("findBatchFromMessageCount returned bad batch")
		}
	}
	res, err := b.streamer.ResultAtCount(count)
	if err != nil {
		return validator.GoGlobalState{}, err
	}
	return validator.GoGlobalState{
		BlockHash:  res.BlockHash,
		SendRoot:   res.SendRoot,
		Batch:      batch,
		PosInBatch: uint64(count - prevBatchMsgCount),
	}, nil
}

const StatusFinished uint8 = 1
const StatusTooFar uint8 = 3

func (b *BlockChallengeBackend) GetMessageCountAtStep(step uint64) arbutil.MessageIndex {
	return b.startMsgCount + arbutil.MessageIndex(step)
}

func (b *BlockChallengeBackend) GetInfoAtStep(step uint64) (validator.GoGlobalState, uint8, error) {
	msgNum := b.GetMessageCountAtStep(step)
	if step >= b.tooFarStartsAtPosition {
		return validator.GoGlobalState{}, StatusTooFar, nil
	}
	globalState, err := b.FindGlobalStateFromMessageCount(msgNum)
	if err != nil {
		return validator.GoGlobalState{}, 0, err
	}
	return globalState, StatusFinished, nil
}

func (b *BlockChallengeBackend) SetRange(_ context.Context, start uint64, end uint64) error {
	if b.startPosition == start && b.endPosition == end {
		return nil
	}
	newStartGs, _, err := b.GetInfoAtStep(start)
	if err != nil {
		return err
	}
	newEndGs, endStatus, err := b.GetInfoAtStep(end)
	if err != nil {
		return err
	}
	if b.startPosition == start && b.startGs != newStartGs {
		return fmt.Errorf("challenge start position remains at %v but global state changed from %v to %v", start, b.startGs, newStartGs)
	}
	b.startGs = newStartGs
	if endStatus == StatusFinished {
		b.endGs = newEndGs
	}
	return nil
}

func (b *BlockChallengeBackend) GetHashAtStep(_ context.Context, position uint64) (common.Hash, error) {
	gs, status, err := b.GetInfoAtStep(position)
	if err != nil {
		return common.Hash{}, err
	}
	if status == StatusFinished {
		data := []byte("Block state:")
		data = append(data, gs.Hash().Bytes()...)
		return crypto.Keccak256Hash(data), nil
	} else if status == StatusTooFar {
		return crypto.Keccak256Hash([]byte("Block state, too far:")), nil
	} else {
		panic(fmt.Sprintf("Unknown block status: %v", status))
	}
}

func (b *BlockChallengeBackend) IssueExecChallenge(
	core *challengeCore,
	oldState *ChallengeState,
	startSegment int,
	numsteps uint64,
) (*types.Transaction, error) {
	position := oldState.Segments[startSegment].Position
	machineStatuses := [2]uint8{}
	globalStates := [2]validator.GoGlobalState{}
	var err error
	globalStates[0], machineStatuses[0], err = b.GetInfoAtStep(position)
	if err != nil {
		return nil, err
	}
	globalStates[1], machineStatuses[1], err = b.GetInfoAtStep(position + 1)
	if err != nil {
		return nil, err
	}
	globalStateHashes := [2][32]byte{
		globalStates[0].Hash(),
		globalStates[1].Hash(),
	}
	return core.con.ChallengeExecution(
		core.auth,
		core.challengeIndex,
		challengegen.ChallengeLibSegmentSelection{
			OldSegmentsStart:  oldState.Start,
			OldSegmentsLength: new(big.Int).Sub(oldState.End, oldState.Start),
			OldSegments:       oldState.RawSegments,
			ChallengePosition: big.NewInt(int64(startSegment)),
		},
		machineStatuses,
		globalStateHashes,
		big.NewInt(int64(numsteps)),
	)
}

'''
'''--- staker/block_validator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"errors"
	"fmt"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/rpcclient"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
)

var (
	validatorPendingValidationsGauge  = metrics.NewRegisteredGauge("arb/validator/validations/pending", nil)
	validatorValidValidationsCounter  = metrics.NewRegisteredCounter("arb/validator/validations/valid", nil)
	validatorFailedValidationsCounter = metrics.NewRegisteredCounter("arb/validator/validations/failed", nil)
	validatorMsgCountCurrentBatch     = metrics.NewRegisteredGauge("arb/validator/msg_count_current_batch", nil)
	validatorMsgCountValidatedGauge   = metrics.NewRegisteredGauge("arb/validator/msg_count_validated", nil)
)

type BlockValidator struct {
	stopwaiter.StopWaiter
	*StatelessBlockValidator

	reorgMutex sync.RWMutex

	chainCaughtUp bool

	// can only be accessed from creation thread or if holding reorg-write
	nextCreateBatch         []byte
	nextCreateBatchMsgCount arbutil.MessageIndex
	nextCreateBatchReread   bool
	nextCreateStartGS       validator.GoGlobalState
	nextCreatePrevDelayed   uint64

	// can only be accessed from from validation thread or if holding reorg-write
	lastValidGS     validator.GoGlobalState
	valLoopPos      arbutil.MessageIndex
	legacyValidInfo *legacyLastBlockValidatedDbInfo

	// only from logger thread
	lastValidInfoPrinted *GlobalStateValidatedInfo

	// can be read (atomic.Load) by anyone holding reorg-read
	// written (atomic.Set) by appropriate thread or (any way) holding reorg-write
	createdA    uint64
	recordSentA uint64
	validatedA  uint64
	validations containers.SyncMap[arbutil.MessageIndex, *validationStatus]

	config BlockValidatorConfigFetcher

	createNodesChan         chan struct{}
	sendRecordChan          chan struct{}
	progressValidationsChan chan struct{}

	// for testing only
	testingProgressMadeChan chan struct{}

	fatalErr chan<- error
}

type BlockValidatorConfig struct {
	Enable                   bool                          `koanf:"enable"`
	ValidationServer         rpcclient.ClientConfig        `koanf:"validation-server" reload:"hot"`
	ValidationPoll           time.Duration                 `koanf:"validation-poll" reload:"hot"`
	PrerecordedBlocks        uint64                        `koanf:"prerecorded-blocks" reload:"hot"`
	ForwardBlocks            uint64                        `koanf:"forward-blocks" reload:"hot"`
	CurrentModuleRoot        string                        `koanf:"current-module-root"`         // TODO(magic) requires reinitialization on hot reload
	PendingUpgradeModuleRoot string                        `koanf:"pending-upgrade-module-root"` // TODO(magic) requires StatelessBlockValidator recreation on hot reload
	FailureIsFatal           bool                          `koanf:"failure-is-fatal" reload:"hot"`
	Dangerous                BlockValidatorDangerousConfig `koanf:"dangerous"`
}

func (c *BlockValidatorConfig) Validate() error {
	return c.ValidationServer.Validate()
}

type BlockValidatorDangerousConfig struct {
	ResetBlockValidation bool `koanf:"reset-block-validation"`
}

type BlockValidatorConfigFetcher func() *BlockValidatorConfig

func BlockValidatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultBlockValidatorConfig.Enable, "enable block-by-block validation")
	rpcclient.RPCClientAddOptions(prefix+".validation-server", f, &DefaultBlockValidatorConfig.ValidationServer)
	f.Duration(prefix+".validation-poll", DefaultBlockValidatorConfig.ValidationPoll, "poll time to check validations")
	f.Uint64(prefix+".forward-blocks", DefaultBlockValidatorConfig.ForwardBlocks, "prepare entries for up to that many blocks ahead of validation (small footprint)")
	f.Uint64(prefix+".prerecorded-blocks", DefaultBlockValidatorConfig.PrerecordedBlocks, "record that many blocks ahead of validation (larger footprint)")
	f.String(prefix+".current-module-root", DefaultBlockValidatorConfig.CurrentModuleRoot, "current wasm module root ('current' read from chain, 'latest' from machines/latest dir, or provide hash)")
	f.String(prefix+".pending-upgrade-module-root", DefaultBlockValidatorConfig.PendingUpgradeModuleRoot, "pending upgrade wasm module root to additionally validate (hash, 'latest' or empty)")
	f.Bool(prefix+".failure-is-fatal", DefaultBlockValidatorConfig.FailureIsFatal, "failing a validation is treated as a fatal error")
	BlockValidatorDangerousConfigAddOptions(prefix+".dangerous", f)
}

func BlockValidatorDangerousConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".reset-block-validation", DefaultBlockValidatorDangerousConfig.ResetBlockValidation, "resets block-by-block validation, starting again at genesis")
}

var DefaultBlockValidatorConfig = BlockValidatorConfig{
	Enable:                   false,
	ValidationServer:         rpcclient.DefaultClientConfig,
	ValidationPoll:           time.Second,
	ForwardBlocks:            1024,
	PrerecordedBlocks:        128,
	CurrentModuleRoot:        "current",
	PendingUpgradeModuleRoot: "latest",
	FailureIsFatal:           true,
	Dangerous:                DefaultBlockValidatorDangerousConfig,
}

var TestBlockValidatorConfig = BlockValidatorConfig{
	Enable:                   false,
	ValidationServer:         rpcclient.TestClientConfig,
	ValidationPoll:           100 * time.Millisecond,
	ForwardBlocks:            128,
	PrerecordedBlocks:        64,
	CurrentModuleRoot:        "latest",
	PendingUpgradeModuleRoot: "latest",
	FailureIsFatal:           true,
	Dangerous:                DefaultBlockValidatorDangerousConfig,
}

var DefaultBlockValidatorDangerousConfig = BlockValidatorDangerousConfig{
	ResetBlockValidation: false,
}

type valStatusField uint32

const (
	Created valStatusField = iota
	RecordSent
	RecordFailed
	Prepared
	SendingValidation
	ValidationSent
)

type validationStatus struct {
	Status uint32                    // atomic: value is one of validationStatus*
	Cancel func()                    // non-atomic: only read/written to with reorg mutex
	Entry  *validationEntry          // non-atomic: only read if Status >= validationStatusPrepared
	Runs   []validator.ValidationRun // if status >= ValidationSent
}

func (s *validationStatus) getStatus() valStatusField {
	uintStat := atomic.LoadUint32(&s.Status)
	return valStatusField(uintStat)
}

func (s *validationStatus) replaceStatus(old, new valStatusField) bool {
	return atomic.CompareAndSwapUint32(&s.Status, uint32(old), uint32(new))
}

func NewBlockValidator(
	statelessBlockValidator *StatelessBlockValidator,
	inbox InboxTrackerInterface,
	streamer TransactionStreamerInterface,
	config BlockValidatorConfigFetcher,
	fatalErr chan<- error,
) (*BlockValidator, error) {
	ret := &BlockValidator{
		StatelessBlockValidator: statelessBlockValidator,
		createNodesChan:         make(chan struct{}, 1),
		sendRecordChan:          make(chan struct{}, 1),
		progressValidationsChan: make(chan struct{}, 1),
		config:                  config,
		fatalErr:                fatalErr,
	}
	if !config().Dangerous.ResetBlockValidation {
		validated, err := ret.ReadLastValidatedInfo()
		if err != nil {
			return nil, err
		}
		if validated != nil {
			ret.lastValidGS = validated.GlobalState
		} else {
			legacyInfo, err := ret.legacyReadLastValidatedInfo()
			if err != nil {
				return nil, err
			}
			ret.legacyValidInfo = legacyInfo
		}
	}
	// genesis block is impossible to validate unless genesis state is empty
	if ret.lastValidGS.Batch == 0 && ret.legacyValidInfo == nil {
		genesis, err := streamer.ResultAtCount(1)
		if err != nil {
			return nil, err
		}
		ret.lastValidGS = validator.GoGlobalState{
			BlockHash:  genesis.BlockHash,
			SendRoot:   genesis.SendRoot,
			Batch:      1,
			PosInBatch: 0,
		}
	}
	streamer.SetBlockValidator(ret)
	inbox.SetBlockValidator(ret)
	return ret, nil
}

func atomicStorePos(addr *uint64, val arbutil.MessageIndex) {
	atomic.StoreUint64(addr, uint64(val))
}

func atomicLoadPos(addr *uint64) arbutil.MessageIndex {
	return arbutil.MessageIndex(atomic.LoadUint64(addr))
}

func (v *BlockValidator) created() arbutil.MessageIndex {
	return atomicLoadPos(&v.createdA)
}

func (v *BlockValidator) recordSent() arbutil.MessageIndex {
	return atomicLoadPos(&v.recordSentA)
}

func (v *BlockValidator) validated() arbutil.MessageIndex {
	return atomicLoadPos(&v.validatedA)
}

func (v *BlockValidator) Validated(t *testing.T) arbutil.MessageIndex {
	return v.validated()
}

func (v *BlockValidator) possiblyFatal(err error) {
	if v.Stopped() {
		return
	}
	if err == nil {
		return
	}
	log.Error("Error during validation", "err", err)
	if v.config().FailureIsFatal {
		select {
		case v.fatalErr <- err:
		default:
		}
	}
}

func nonBlockingTrigger(channel chan struct{}) {
	select {
	case channel <- struct{}{}:
	default:
	}
}

// called from NewBlockValidator, doesn't need to catch locks
func ReadLastValidatedInfo(db ethdb.Database) (*GlobalStateValidatedInfo, error) {
	exists, err := db.Has(lastGlobalStateValidatedInfoKey)
	if err != nil {
		return nil, err
	}
	var validated GlobalStateValidatedInfo
	if !exists {
		return nil, nil
	}
	gsBytes, err := db.Get(lastGlobalStateValidatedInfoKey)
	if err != nil {
		return nil, err
	}
	err = rlp.DecodeBytes(gsBytes, &validated)
	if err != nil {
		return nil, err
	}
	return &validated, nil
}

func (v *BlockValidator) ReadLastValidatedInfo() (*GlobalStateValidatedInfo, error) {
	return ReadLastValidatedInfo(v.db)
}

func (v *BlockValidator) legacyReadLastValidatedInfo() (*legacyLastBlockValidatedDbInfo, error) {
	exists, err := v.db.Has(legacyLastBlockValidatedInfoKey)
	if err != nil {
		return nil, err
	}
	var validated legacyLastBlockValidatedDbInfo
	if !exists {
		return nil, nil
	}
	gsBytes, err := v.db.Get(legacyLastBlockValidatedInfoKey)
	if err != nil {
		return nil, err
	}
	err = rlp.DecodeBytes(gsBytes, &validated)
	if err != nil {
		return nil, err
	}
	return &validated, nil
}

var ErrGlobalStateNotInChain = errors.New("globalstate not in chain")

// false if chain not caught up to globalstate
// error is ErrGlobalStateNotInChain if globalstate not in chain (and chain caught up)
func GlobalStateToMsgCount(tracker InboxTrackerInterface, streamer TransactionStreamerInterface, gs validator.GoGlobalState) (bool, arbutil.MessageIndex, error) {
	batchCount, err := tracker.GetBatchCount()
	if err != nil {
		return false, 0, err
	}
	requiredBatchCount := gs.Batch + 1
	if gs.PosInBatch == 0 {
		requiredBatchCount -= 1
	}
	if batchCount < requiredBatchCount {
		return false, 0, nil
	}
	var prevBatchMsgCount arbutil.MessageIndex
	if gs.Batch > 0 {
		prevBatchMsgCount, err = tracker.GetBatchMessageCount(gs.Batch - 1)
		if err != nil {
			return false, 0, err
		}
	}
	count := prevBatchMsgCount
	if gs.PosInBatch > 0 {
		curBatchMsgCount, err := tracker.GetBatchMessageCount(gs.Batch)
		if err != nil {
			return false, 0, fmt.Errorf("%w: getBatchMsgCount %d batchCount %d", err, gs.Batch, batchCount)
		}
		count += arbutil.MessageIndex(gs.PosInBatch)
		if curBatchMsgCount < count {
			return false, 0, fmt.Errorf("%w: batch %d posInBatch %d, maxPosInBatch %d", ErrGlobalStateNotInChain, gs.Batch, gs.PosInBatch, curBatchMsgCount-prevBatchMsgCount)
		}
	}
	processed, err := streamer.GetProcessedMessageCount()
	if err != nil {
		return false, 0, err
	}
	if processed < count {
		return false, 0, nil
	}
	res, err := streamer.ResultAtCount(count)
	if err != nil {
		return false, 0, err
	}
	if res.BlockHash != gs.BlockHash || res.SendRoot != gs.SendRoot {
		return false, 0, fmt.Errorf("%w: count %d hash %v expected %v, sendroot %v expected %v", ErrGlobalStateNotInChain, count, gs.BlockHash, res.BlockHash, gs.SendRoot, res.SendRoot)
	}
	return true, count, nil
}

func (v *BlockValidator) sendRecord(s *validationStatus) error {
	if !v.Started() {
		return nil
	}
	if !s.replaceStatus(Created, RecordSent) {
		return fmt.Errorf("failed status check for send record. Status: %v", s.getStatus())
	}
	v.LaunchThread(func(ctx context.Context) {
		err := v.ValidationEntryRecord(ctx, s.Entry)
		if ctx.Err() != nil {
			return
		}
		if err != nil {
			s.replaceStatus(RecordSent, RecordFailed) // after that - could be removed from validations map
			log.Error("Error while recording", "err", err, "status", s.getStatus())
			return
		}
		if !s.replaceStatus(RecordSent, Prepared) {
			log.Error("Fault trying to update validation with recording", "entry", s.Entry, "status", s.getStatus())
			return
		}
		nonBlockingTrigger(v.progressValidationsChan)
	})
	return nil
}

//nolint:gosec
func (v *BlockValidator) writeToFile(validationEntry *validationEntry, moduleRoot common.Hash) error {
	input, err := validationEntry.ToInput()
	if err != nil {
		return err
	}
	_, err = v.execSpawner.WriteToFile(input, validationEntry.End, moduleRoot).Await(v.GetContext())
	return err
}

func (v *BlockValidator) SetCurrentWasmModuleRoot(hash common.Hash) error {
	v.moduleMutex.Lock()
	defer v.moduleMutex.Unlock()

	if (hash == common.Hash{}) {
		return errors.New("trying to set zero as wasmModuleRoot")
	}
	if hash == v.currentWasmModuleRoot {
		return nil
	}
	if (v.currentWasmModuleRoot == common.Hash{}) {
		v.currentWasmModuleRoot = hash
		return nil
	}
	if v.pendingWasmModuleRoot == hash {
		log.Info("Block validator: detected progressing to pending machine", "hash", hash)
		v.currentWasmModuleRoot = hash
		return nil
	}
	if v.config().CurrentModuleRoot != "current" {
		return nil
	}
	return fmt.Errorf(
		"unexpected wasmModuleRoot! cannot validate! found %v , current %v, pending %v",
		hash, v.currentWasmModuleRoot, v.pendingWasmModuleRoot,
	)
}

func (v *BlockValidator) readBatch(ctx context.Context, batchNum uint64) (bool, []byte, arbutil.MessageIndex, error) {
	batchCount, err := v.inboxTracker.GetBatchCount()
	if err != nil {
		return false, nil, 0, err
	}
	if batchCount <= batchNum {
		return false, nil, 0, nil
	}
	batchMsgCount, err := v.inboxTracker.GetBatchMessageCount(batchNum)
	if err != nil {
		return false, nil, 0, err
	}
	batch, err := v.inboxReader.GetSequencerMessageBytes(ctx, batchNum)
	if err != nil {
		return false, nil, 0, err
	}
	return true, batch, batchMsgCount, nil
}

func (v *BlockValidator) createNextValidationEntry(ctx context.Context) (bool, error) {
	v.reorgMutex.RLock()
	defer v.reorgMutex.RUnlock()
	pos := v.created()
	if pos > v.validated()+arbutil.MessageIndex(v.config().ForwardBlocks) {
		log.Trace("create validation entry: nothing to do", "pos", pos, "validated", v.validated())
		return false, nil
	}
	streamerMsgCount, err := v.streamer.GetProcessedMessageCount()
	if err != nil {
		return false, err
	}
	if pos >= streamerMsgCount {
		log.Trace("create validation entry: nothing to do", "pos", pos, "streamerMsgCount", streamerMsgCount)
		return false, nil
	}
	msg, err := v.streamer.GetMessage(pos)
	if err != nil {
		return false, err
	}
	endRes, err := v.streamer.ResultAtCount(pos + 1)
	if err != nil {
		return false, err
	}
	if v.nextCreateStartGS.PosInBatch == 0 || v.nextCreateBatchReread {
		// new batch
		found, batch, count, err := v.readBatch(ctx, v.nextCreateStartGS.Batch)
		if !found {
			return false, err
		}
		v.nextCreateBatch = batch
		v.nextCreateBatchMsgCount = count
		validatorMsgCountCurrentBatch.Update(int64(count))
		v.nextCreateBatchReread = false
	}
	endGS := validator.GoGlobalState{
		BlockHash: endRes.BlockHash,
		SendRoot:  endRes.SendRoot,
	}
	if pos+1 < v.nextCreateBatchMsgCount {
		endGS.Batch = v.nextCreateStartGS.Batch
		endGS.PosInBatch = v.nextCreateStartGS.PosInBatch + 1
	} else if pos+1 == v.nextCreateBatchMsgCount {
		endGS.Batch = v.nextCreateStartGS.Batch + 1
		endGS.PosInBatch = 0
	} else {
		return false, fmt.Errorf("illegal batch msg count %d pos %d batch %d", v.nextCreateBatchMsgCount, pos, endGS.Batch)
	}
	entry, err := newValidationEntry(pos, v.nextCreateStartGS, endGS, msg, v.nextCreateBatch, v.nextCreatePrevDelayed)
	if err != nil {
		return false, err
	}
	status := &validationStatus{
		Status: uint32(Created),
		Entry:  entry,
	}
	v.validations.Store(pos, status)
	v.nextCreateStartGS = endGS
	v.nextCreatePrevDelayed = msg.DelayedMessagesRead
	atomicStorePos(&v.createdA, pos+1)
	log.Trace("create validation entry: created", "pos", pos)
	return true, nil
}

func (v *BlockValidator) iterativeValidationEntryCreator(ctx context.Context, ignored struct{}) time.Duration {
	moreWork, err := v.createNextValidationEntry(ctx)
	if err != nil {
		processed, processedErr := v.streamer.GetProcessedMessageCount()
		log.Error("error trying to create validation node", "err", err, "created", v.created()+1, "processed", processed, "processedErr", processedErr)
	}
	if moreWork {
		return 0
	}
	return v.config().ValidationPoll
}

func (v *BlockValidator) sendNextRecordRequests(ctx context.Context) (bool, error) {
	v.reorgMutex.RLock()
	pos := v.recordSent()
	created := v.created()
	validated := v.validated()
	v.reorgMutex.RUnlock()

	recordUntil := validated + arbutil.MessageIndex(v.config().PrerecordedBlocks) - 1
	if recordUntil > created-1 {
		recordUntil = created - 1
	}
	if recordUntil < pos {
		return false, nil
	}
	log.Trace("preparing to record", "pos", pos, "until", recordUntil)
	// prepare could take a long time so we do it without a lock
	err := v.recorder.PrepareForRecord(ctx, pos, recordUntil)
	if err != nil {
		return false, err
	}

	v.reorgMutex.RLock()
	defer v.reorgMutex.RUnlock()
	createdNew := v.created()
	recordSentNew := v.recordSent()
	if createdNew < created || recordSentNew < pos {
		// there was a relevant reorg - quit and restart
		return true, nil
	}
	for pos <= recordUntil {
		validationStatus, found := v.validations.Load(pos)
		if !found {
			return false, fmt.Errorf("not found entry for pos %d", pos)
		}
		currentStatus := validationStatus.getStatus()
		if currentStatus != Created {
			return false, fmt.Errorf("bad status trying to send recordings for pos %d status: %v", pos, currentStatus)
		}
		err := v.sendRecord(validationStatus)
		if err != nil {
			return false, err
		}
		pos += 1
		atomicStorePos(&v.recordSentA, pos)
		log.Trace("next record request: sent", "pos", pos)
	}

	return true, nil
}

func (v *BlockValidator) iterativeValidationEntryRecorder(ctx context.Context, ignored struct{}) time.Duration {
	moreWork, err := v.sendNextRecordRequests(ctx)
	if err != nil {
		log.Error("error trying to record for validation node", "err", err)
	}
	if moreWork {
		return 0
	}
	return v.config().ValidationPoll
}

func (v *BlockValidator) iterativeValidationPrint(ctx context.Context) time.Duration {
	validated, err := v.ReadLastValidatedInfo()
	if err != nil {
		log.Error("cannot read last validated data from database", "err", err)
		return time.Second * 30
	}
	if validated == nil {
		return time.Second
	}
	if v.lastValidInfoPrinted != nil {
		if v.lastValidInfoPrinted.GlobalState.BlockHash == validated.GlobalState.BlockHash {
			return time.Second
		}
	}
	var batchMsgs arbutil.MessageIndex
	var printedCount int64
	if validated.GlobalState.Batch > 0 {
		batchMsgs, err = v.inboxTracker.GetBatchMessageCount(validated.GlobalState.Batch - 1)
	}
	if err != nil {
		printedCount = -1
	} else {
		printedCount = int64(batchMsgs) + int64(validated.GlobalState.PosInBatch)
	}
	log.Info("validated execution", "messageCount", printedCount, "globalstate", validated.GlobalState, "WasmRoots", validated.WasmRoots)
	v.lastValidInfoPrinted = validated
	return time.Second
}

// return val:
// *MessageIndex - pointer to bad entry if there is one (requires reorg)
func (v *BlockValidator) advanceValidations(ctx context.Context) (*arbutil.MessageIndex, error) {
	v.reorgMutex.RLock()
	defer v.reorgMutex.RUnlock()

	wasmRoots := v.GetModuleRootsToValidate()
	room := 100 // even if there is more room then that it's fine
	for _, spawner := range v.validationSpawners {
		here := spawner.Room() / len(wasmRoots)
		if here <= 0 {
			room = 0
		}
		if here < room {
			room = here
		}
	}
	pos := v.validated() - 1 // to reverse the first +1 in the loop
validationsLoop:
	for {
		if ctx.Err() != nil {
			return nil, ctx.Err()
		}
		v.valLoopPos = pos + 1
		v.reorgMutex.RUnlock()
		v.reorgMutex.RLock()
		pos = v.valLoopPos
		if pos >= v.recordSent() {
			log.Trace("advanceValidations: nothing to validate", "pos", pos)
			return nil, nil
		}
		validationStatus, found := v.validations.Load(pos)
		if !found {
			return nil, fmt.Errorf("not found entry for pos %d", pos)
		}
		currentStatus := validationStatus.getStatus()
		if currentStatus == RecordFailed {
			// retry
			log.Warn("Recording for validation failed, retrying..", "pos", pos)
			return &pos, nil
		}
		if currentStatus == ValidationSent && pos == v.validated() {
			if validationStatus.Entry.Start != v.lastValidGS {
				log.Warn("Validation entry has wrong start state", "pos", pos, "start", validationStatus.Entry.Start, "expected", v.lastValidGS)
				validationStatus.Cancel()
				return &pos, nil
			}
			var wasmRoots []common.Hash
			for i, run := range validationStatus.Runs {
				if !run.Ready() {
					log.Trace("advanceValidations: validation not ready", "pos", pos, "run", i)
					continue validationsLoop
				}
				wasmRoots = append(wasmRoots, run.WasmModuleRoot())
				runEnd, err := run.Current()
				if err == nil && runEnd != validationStatus.Entry.End {
					err = fmt.Errorf("validation failed: expected %v got %v", validationStatus.Entry.End, runEnd)
					writeErr := v.writeToFile(validationStatus.Entry, run.WasmModuleRoot())
					if writeErr != nil {
						log.Warn("failed to write debug results file", "err", writeErr)
					}
				}
				if err != nil {
					validatorFailedValidationsCounter.Inc(1)
					v.possiblyFatal(err)
					return &pos, nil // if not fatal - retry
				}
				validatorValidValidationsCounter.Inc(1)
			}
			err := v.writeLastValidated(validationStatus.Entry.End, wasmRoots)
			if err != nil {
				log.Error("failed writing new validated to database", "pos", pos, "err", err)
			}
			go v.recorder.MarkValid(pos, v.lastValidGS.BlockHash)
			atomicStorePos(&v.validatedA, pos+1)
			v.validations.Delete(pos)
			nonBlockingTrigger(v.createNodesChan)
			nonBlockingTrigger(v.sendRecordChan)
			validatorMsgCountValidatedGauge.Update(int64(pos + 1))
			if v.testingProgressMadeChan != nil {
				nonBlockingTrigger(v.testingProgressMadeChan)
			}
			log.Trace("result validated", "count", v.validated(), "blockHash", v.lastValidGS.BlockHash)
			continue
		}
		if room == 0 {
			log.Trace("advanceValidations: no more room", "pos", pos)
			return nil, nil
		}
		if currentStatus == Prepared {
			input, err := validationStatus.Entry.ToInput()
			if err != nil && ctx.Err() == nil {
				v.possiblyFatal(fmt.Errorf("%w: error preparing validation", err))
				continue
			}
			replaced := validationStatus.replaceStatus(Prepared, SendingValidation)
			if !replaced {
				v.possiblyFatal(errors.New("failed to set SendingValidation status"))
			}
			validatorPendingValidationsGauge.Inc(1)
			defer validatorPendingValidationsGauge.Dec(1)
			var runs []validator.ValidationRun
			for _, moduleRoot := range wasmRoots {
				for i, spawner := range v.validationSpawners {
					run := spawner.Launch(input, moduleRoot)
					log.Trace("advanceValidations: launched", "pos", validationStatus.Entry.Pos, "moduleRoot", moduleRoot, "spawner", i)
					runs = append(runs, run)
				}
			}
			validationCtx, cancel := context.WithCancel(ctx)
			validationStatus.Runs = runs
			validationStatus.Cancel = cancel
			v.LaunchUntrackedThread(func() {
				defer cancel()
				replaced = validationStatus.replaceStatus(SendingValidation, ValidationSent)
				if !replaced {
					v.possiblyFatal(errors.New("failed to set status to ValidationSent"))
				}

				// validationStatus might be removed from under us
				// trigger validation progress when done
				for _, run := range runs {
					_, err := run.Await(validationCtx)
					if err != nil {
						return
					}
				}
				nonBlockingTrigger(v.progressValidationsChan)
			})
			room--
		}
	}
}

func (v *BlockValidator) iterativeValidationProgress(ctx context.Context, ignored struct{}) time.Duration {
	reorg, err := v.advanceValidations(ctx)
	if err != nil {
		log.Error("error trying to record for validation node", "err", err)
	} else if reorg != nil {
		err := v.Reorg(ctx, *reorg)
		if err != nil {
			log.Error("error trying to reorg validation", "pos", *reorg-1, "err", err)
			v.possiblyFatal(err)
		}
	}
	return v.config().ValidationPoll
}

var ErrValidationCanceled = errors.New("validation of block cancelled")

func (v *BlockValidator) writeLastValidated(gs validator.GoGlobalState, wasmRoots []common.Hash) error {
	v.lastValidGS = gs
	info := GlobalStateValidatedInfo{
		GlobalState: gs,
		WasmRoots:   wasmRoots,
	}
	encoded, err := rlp.EncodeToBytes(info)
	if err != nil {
		return err
	}
	err = v.db.Put(lastGlobalStateValidatedInfoKey, encoded)
	if err != nil {
		return err
	}
	return nil
}

func (v *BlockValidator) validGSIsNew(globalState validator.GoGlobalState) bool {
	if v.legacyValidInfo != nil {
		if v.legacyValidInfo.AfterPosition.BatchNumber > globalState.Batch {
			return false
		}
		if v.legacyValidInfo.AfterPosition.BatchNumber == globalState.Batch && v.legacyValidInfo.AfterPosition.PosInBatch >= globalState.PosInBatch {
			return false
		}
		return true
	}
	if v.lastValidGS.Batch > globalState.Batch {
		return false
	}
	if v.lastValidGS.Batch == globalState.Batch && v.lastValidGS.PosInBatch >= globalState.PosInBatch {
		return false
	}
	return true
}

// this accepts globalstate even if not caught up
func (v *BlockValidator) InitAssumeValid(globalState validator.GoGlobalState) error {
	if v.Started() {
		return fmt.Errorf("cannot handle InitAssumeValid while running")
	}

	// don't do anything if we already validated past that
	if !v.validGSIsNew(globalState) {
		return nil
	}

	v.legacyValidInfo = nil

	err := v.writeLastValidated(globalState, nil)
	if err != nil {
		log.Error("failed writing new validated to database", "pos", v.lastValidGS, "err", err)
	}

	return nil
}

func (v *BlockValidator) UpdateLatestStaked(count arbutil.MessageIndex, globalState validator.GoGlobalState) {

	if count <= v.validated() {
		return
	}

	v.reorgMutex.Lock()
	defer v.reorgMutex.Unlock()

	if count <= v.validated() {
		return
	}

	if !v.chainCaughtUp {
		if !v.validGSIsNew(globalState) {
			return
		}
		v.legacyValidInfo = nil
		err := v.writeLastValidated(globalState, nil)
		if err != nil {
			log.Error("error writing last validated", "err", err)
		}
		return
	}

	countUint64 := uint64(count)
	msg, err := v.streamer.GetMessage(count - 1)
	if err != nil {
		log.Error("getMessage error", "err", err, "count", count)
		return
	}
	// delete no-longer relevant entries
	for iPos := v.validated(); iPos < count && iPos < v.created(); iPos++ {
		status, found := v.validations.Load(iPos)
		if found && status != nil && status.Cancel != nil {
			status.Cancel()
		}
		v.validations.Delete(iPos)
	}
	if v.created() < count {
		v.nextCreateStartGS = globalState
		v.nextCreatePrevDelayed = msg.DelayedMessagesRead
		v.nextCreateBatchReread = true
		v.createdA = countUint64
	}
	// under the reorg mutex we don't need atomic access
	if v.recordSentA < countUint64 {
		v.recordSentA = countUint64
	}
	v.validatedA = countUint64
	v.valLoopPos = count
	validatorMsgCountValidatedGauge.Update(int64(countUint64))
	err = v.writeLastValidated(globalState, nil) // we don't know which wasm roots were validated
	if err != nil {
		log.Error("failed writing valid state after reorg", "err", err)
	}
	nonBlockingTrigger(v.createNodesChan)
}

// Because batches and blocks are handled at separate layers in the node,
// and because block generation from messages is asynchronous,
// this call is different than Reorg, which is currently called later.
func (v *BlockValidator) ReorgToBatchCount(count uint64) {
	v.reorgMutex.Lock()
	defer v.reorgMutex.Unlock()
	if v.nextCreateStartGS.Batch >= count {
		v.nextCreateBatchReread = true
	}
}

func (v *BlockValidator) Reorg(ctx context.Context, count arbutil.MessageIndex) error {
	v.reorgMutex.Lock()
	defer v.reorgMutex.Unlock()
	if count <= 1 {
		return errors.New("cannot reorg out genesis")
	}
	if !v.chainCaughtUp {
		return nil
	}
	if v.created() < count {
		return nil
	}
	_, endPosition, err := v.GlobalStatePositionsAtCount(count)
	if err != nil {
		v.possiblyFatal(err)
		return err
	}
	res, err := v.streamer.ResultAtCount(count)
	if err != nil {
		v.possiblyFatal(err)
		return err
	}
	msg, err := v.streamer.GetMessage(count - 1)
	if err != nil {
		v.possiblyFatal(err)
		return err
	}
	for iPos := count; iPos < v.created(); iPos++ {
		status, found := v.validations.Load(iPos)
		if found && status != nil && status.Cancel != nil {
			status.Cancel()
		}
		v.validations.Delete(iPos)
	}
	v.nextCreateStartGS = buildGlobalState(*res, endPosition)
	v.nextCreatePrevDelayed = msg.DelayedMessagesRead
	v.nextCreateBatchReread = true
	countUint64 := uint64(count)
	v.createdA = countUint64
	// under the reorg mutex we don't need atomic access
	if v.recordSentA > countUint64 {
		v.recordSentA = countUint64
	}
	if v.validatedA > countUint64 {
		v.validatedA = countUint64
		validatorMsgCountValidatedGauge.Update(int64(countUint64))
		err := v.writeLastValidated(v.nextCreateStartGS, nil) // we don't know which wasm roots were validated
		if err != nil {
			log.Error("failed writing valid state after reorg", "err", err)
		}
	}
	nonBlockingTrigger(v.createNodesChan)
	return nil
}

// Initialize must be called after SetCurrentWasmModuleRoot sets the current one
func (v *BlockValidator) Initialize(ctx context.Context) error {
	config := v.config()
	currentModuleRoot := config.CurrentModuleRoot
	switch currentModuleRoot {
	case "latest":
		latest, err := v.execSpawner.LatestWasmModuleRoot().Await(ctx)
		if err != nil {
			return err
		}
		v.currentWasmModuleRoot = latest
	case "current":
		if (v.currentWasmModuleRoot == common.Hash{}) {
			return errors.New("wasmModuleRoot set to 'current' - but info not set from chain")
		}
	default:
		v.currentWasmModuleRoot = common.HexToHash(currentModuleRoot)
		if (v.currentWasmModuleRoot == common.Hash{}) {
			return errors.New("current-module-root config value illegal")
		}
	}
	log.Info("BlockValidator initialized", "current", v.currentWasmModuleRoot, "pending", v.pendingWasmModuleRoot)
	return nil
}

func (v *BlockValidator) checkLegacyValid() error {
	v.reorgMutex.Lock()
	defer v.reorgMutex.Unlock()
	if v.legacyValidInfo == nil {
		return nil
	}
	batchCount, err := v.inboxTracker.GetBatchCount()
	if err != nil {
		return err
	}
	requiredBatchCount := v.legacyValidInfo.AfterPosition.BatchNumber + 1
	if v.legacyValidInfo.AfterPosition.PosInBatch == 0 {
		requiredBatchCount -= 1
	}
	if batchCount < requiredBatchCount {
		log.Warn("legacy valid batch ahead of db", "current", batchCount, "required", requiredBatchCount)
		return nil
	}
	var msgCount arbutil.MessageIndex
	if v.legacyValidInfo.AfterPosition.BatchNumber > 0 {
		msgCount, err = v.inboxTracker.GetBatchMessageCount(v.legacyValidInfo.AfterPosition.BatchNumber - 1)
		if err != nil {
			return err
		}
	}
	msgCount += arbutil.MessageIndex(v.legacyValidInfo.AfterPosition.PosInBatch)
	processedCount, err := v.streamer.GetProcessedMessageCount()
	if err != nil {
		return err
	}
	if processedCount < msgCount {
		log.Warn("legacy valid message count ahead of db", "current", processedCount, "required", msgCount)
		return nil
	}
	result, err := v.streamer.ResultAtCount(msgCount)
	if err != nil {
		return err
	}
	if result.BlockHash != v.legacyValidInfo.BlockHash {
		log.Error("legacy validated blockHash does not fit chain", "info.BlockHash", v.legacyValidInfo.BlockHash, "chain", result.BlockHash, "count", msgCount)
		return fmt.Errorf("legacy validated blockHash does not fit chain")
	}
	validGS := validator.GoGlobalState{
		BlockHash:  result.BlockHash,
		SendRoot:   result.SendRoot,
		Batch:      v.legacyValidInfo.AfterPosition.BatchNumber,
		PosInBatch: v.legacyValidInfo.AfterPosition.PosInBatch,
	}
	err = v.writeLastValidated(validGS, nil)
	if err == nil {
		err = v.db.Delete(legacyLastBlockValidatedInfoKey)
		if err != nil {
			err = fmt.Errorf("deleting legacy: %w", err)
		}
	}
	if err != nil {
		log.Error("failed writing initial lastValid on upgrade from legacy", "new-info", v.lastValidGS, "err", err)
	} else {
		log.Info("updated last-valid from legacy", "lastValid", v.lastValidGS)
	}
	v.legacyValidInfo = nil
	return nil
}

// checks that the chain caught up to lastValidGS, used in startup
func (v *BlockValidator) checkValidatedGSCaughtUp() (bool, error) {
	v.reorgMutex.Lock()
	defer v.reorgMutex.Unlock()
	if v.chainCaughtUp {
		return true, nil
	}
	if v.legacyValidInfo != nil {
		return false, nil
	}
	if v.lastValidGS.Batch == 0 {
		return false, errors.New("lastValid not initialized. cannot validate genesis")
	}
	caughtUp, count, err := GlobalStateToMsgCount(v.inboxTracker, v.streamer, v.lastValidGS)
	if err != nil {
		return false, err
	}
	if !caughtUp {
		batchCount, err := v.inboxTracker.GetBatchCount()
		if err != nil {
			log.Error("failed reading batch count", "err", err)
			batchCount = 0
		}
		batchMsgCount, err := v.inboxTracker.GetBatchMessageCount(batchCount - 1)
		if err != nil {
			log.Error("failed reading batchMsgCount", "err", err)
			batchMsgCount = 0
		}
		processedMsgCount, err := v.streamer.GetProcessedMessageCount()
		if err != nil {
			log.Error("failed reading processedMsgCount", "err", err)
			processedMsgCount = 0
		}
		log.Info("validator catching up to last valid", "lastValid.Batch", v.lastValidGS.Batch, "lastValid.PosInBatch", v.lastValidGS.PosInBatch, "batchCount", batchCount, "batchMsgCount", batchMsgCount, "processedMsgCount", processedMsgCount)
		return false, nil
	}
	msg, err := v.streamer.GetMessage(count - 1)
	if err != nil {
		return false, err
	}
	v.nextCreateBatchReread = true
	v.nextCreateStartGS = v.lastValidGS
	v.nextCreatePrevDelayed = msg.DelayedMessagesRead
	atomicStorePos(&v.createdA, count)
	atomicStorePos(&v.recordSentA, count)
	atomicStorePos(&v.validatedA, count)
	validatorMsgCountValidatedGauge.Update(int64(count))
	v.chainCaughtUp = true
	return true, nil
}

func (v *BlockValidator) LaunchWorkthreadsWhenCaughtUp(ctx context.Context) {
	for {
		err := v.checkLegacyValid()
		if err != nil {
			log.Error("validator got error updating legacy validated info. Consider restarting with dangerous.reset-block-validation", "err", err)
		}
		caughtUp, err := v.checkValidatedGSCaughtUp()
		if err != nil {
			log.Error("validator got error waiting for chain to catch up. Consider restarting with dangerous.reset-block-validation", "err", err)
		}
		if caughtUp {
			break
		}
		select {
		case <-ctx.Done():
			return
		case <-time.After(v.config().ValidationPoll):
		}
	}
	err := stopwaiter.CallIterativelyWith[struct{}](&v.StopWaiterSafe, v.iterativeValidationEntryCreator, v.createNodesChan)
	if err != nil {
		v.possiblyFatal(err)
	}
	err = stopwaiter.CallIterativelyWith[struct{}](&v.StopWaiterSafe, v.iterativeValidationEntryRecorder, v.sendRecordChan)
	if err != nil {
		v.possiblyFatal(err)
	}
	err = stopwaiter.CallIterativelyWith[struct{}](&v.StopWaiterSafe, v.iterativeValidationProgress, v.progressValidationsChan)
	if err != nil {
		v.possiblyFatal(err)
	}
}

func (v *BlockValidator) Start(ctxIn context.Context) error {
	v.StopWaiter.Start(ctxIn, v)
	v.LaunchThread(v.LaunchWorkthreadsWhenCaughtUp)
	v.CallIteratively(v.iterativeValidationPrint)
	return nil
}

func (v *BlockValidator) StopAndWait() {
	v.StopWaiter.StopAndWait()
}

// WaitForPos can only be used from One thread
func (v *BlockValidator) WaitForPos(t *testing.T, ctx context.Context, pos arbutil.MessageIndex, timeout time.Duration) bool {
	triggerchan := make(chan struct{})
	v.testingProgressMadeChan = triggerchan
	timer := time.NewTimer(timeout)
	defer timer.Stop()
	lastLoop := false
	for {
		if v.validated() > pos {
			return true
		}
		if lastLoop {
			return false
		}
		select {
		case <-timer.C:
			lastLoop = true
		case <-triggerchan:
		case <-ctx.Done():
			lastLoop = true
		}
	}
}

'''
'''--- staker/block_validator_schema.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator"
)

type legacyLastBlockValidatedDbInfo struct {
	BlockNumber   uint64
	BlockHash     common.Hash
	AfterPosition GlobalStatePosition
}

type GlobalStateValidatedInfo struct {
	GlobalState validator.GoGlobalState
	WasmRoots   []common.Hash
}

var (
	lastGlobalStateValidatedInfoKey = []byte("_lastGlobalStateValidatedInfo") // contains a rlp encoded lastBlockValidatedDbInfo
	legacyLastBlockValidatedInfoKey = []byte("_lastBlockValidatedInfo")       // LEGACY - contains a rlp encoded lastBlockValidatedDbInfo
)

'''
'''--- staker/challenge_manager.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/abi/bind/backends"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/validator"
)

const maxBisectionDegree uint64 = 40

const challengeModeExecution = 2

var initiatedChallengeID common.Hash
var challengeBisectedID common.Hash
var executionChallengeBegunID common.Hash

func init() {
	parsedChallengeManagerABI, err := challengegen.ChallengeManagerMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	initiatedChallengeID = parsedChallengeManagerABI.Events["InitiatedChallenge"].ID
	challengeBisectedID = parsedChallengeManagerABI.Events["Bisected"].ID
	executionChallengeBegunID = parsedChallengeManagerABI.Events["ExecutionChallengeBegun"].ID
}

type ChallengeBackend interface {
	SetRange(ctx context.Context, start uint64, end uint64) error
	GetHashAtStep(ctx context.Context, position uint64) (common.Hash, error)
}

// Assert that ExecutionChallengeBackend implements ChallengeBackend
var _ ChallengeBackend = (*ExecutionChallengeBackend)(nil)

type challengeCore struct {
	con                  *challengegen.ChallengeManager
	challengeManagerAddr common.Address
	challengeIndex       uint64
	client               bind.ContractBackend
	auth                 *bind.TransactOpts
	actingAs             common.Address
	startL1Block         *big.Int
	confirmationBlocks   int64
}

type ChallengeManager struct {
	// fields used in both block and execution challenge
	*challengeCore

	// fields below are used while working on block challenge
	blockChallengeBackend *BlockChallengeBackend

	// fields below are only used to create execution challenge from block challenge
	validator      *StatelessBlockValidator
	maxBatchesRead uint64
	wasmModuleRoot common.Hash

	// these fields are empty until working on execution challenge
	initialMachineMessageCount arbutil.MessageIndex
	executionChallengeBackend  *ExecutionChallengeBackend
	machineFinalStepCount      uint64
}

// NewChallengeManager constructs a new challenge manager.
// Note: latestMachineLoader may be nil if the block validator is disabled
func NewChallengeManager(
	ctx context.Context,
	l1client bind.ContractBackend,
	auth *bind.TransactOpts,
	fromAddr common.Address,
	challengeManagerAddr common.Address,
	challengeIndex uint64,
	val *StatelessBlockValidator,
	startL1Block uint64,
	confirmationBlocks int64,
) (*ChallengeManager, error) {
	con, err := challengegen.NewChallengeManager(challengeManagerAddr, l1client)
	if err != nil {
		return nil, fmt.Errorf("error creating bindgen ChallengeManager: %w", err)
	}

	logs, err := l1client.FilterLogs(ctx, ethereum.FilterQuery{
		FromBlock: new(big.Int).SetUint64(startL1Block),
		Addresses: []common.Address{challengeManagerAddr},
		Topics:    [][]common.Hash{{initiatedChallengeID}, {uint64ToIndex(challengeIndex)}},
	})
	if err != nil {
		return nil, fmt.Errorf("error searching logs for InitiatedChallenge event from block %v: %w", startL1Block, err)
	}
	if len(logs) == 0 {
		return nil, fmt.Errorf("didn't find InitiatedChallenge event for challenge %v starting at block %v", challengeIndex, startL1Block)
	}
	if len(logs) > 1 {
		log.Warn("found multiple InitiatedChallenge logs", "challenge", challengeIndex, "count", len(logs), "fromBlock", startL1Block)
	}
	// Multiple logs are in theory fine, as they should all reveal the same preimage.
	// We'll use the most recent log to be safe.
	evmLog := logs[len(logs)-1]
	parsedLog, err := con.ParseInitiatedChallenge(evmLog)
	if err != nil {
		return nil, fmt.Errorf("error parsing InitiatedChallenge event for challenge %v: %w", challengeIndex, err)
	}

	callOpts := &bind.CallOpts{Context: ctx}
	challengeInfo, err := con.Challenges(callOpts, new(big.Int).SetUint64(challengeIndex))
	if err != nil {
		return nil, fmt.Errorf("error getting challenge %v info: %w", challengeIndex, err)
	}

	backend, err := NewBlockChallengeBackend(
		parsedLog,
		challengeInfo.MaxInboxMessages,
		val.streamer,
		val.inboxTracker,
	)
	if err != nil {
		return nil, fmt.Errorf("error creating block challenge backend for challenge %v: %w", challengeIndex, err)
	}
	return &ChallengeManager{
		challengeCore: &challengeCore{
			con:                  con,
			challengeManagerAddr: challengeManagerAddr,
			challengeIndex:       challengeIndex,
			client:               l1client,
			auth:                 auth,
			actingAs:             fromAddr,
			startL1Block:         new(big.Int).SetUint64(startL1Block),
			confirmationBlocks:   confirmationBlocks,
		},
		blockChallengeBackend: backend,
		validator:             val,
		wasmModuleRoot:        challengeInfo.WasmModuleRoot,
		maxBatchesRead:        challengeInfo.MaxInboxMessages,
	}, nil
}

// NewExecutionChallengeManager is for testing only - skips block challenges
func NewExecutionChallengeManager(
	l1client bind.ContractBackend,
	auth *bind.TransactOpts,
	challengeManagerAddr common.Address,
	challengeIndex uint64,
	exec validator.ExecutionRun,
	startL1Block uint64,
	confirmationBlocks int64,
) (*ChallengeManager, error) {
	con, err := challengegen.NewChallengeManager(challengeManagerAddr, l1client)
	if err != nil {
		return nil, err
	}
	backend, err := NewExecutionChallengeBackend(exec)
	if err != nil {
		return nil, err
	}
	return &ChallengeManager{
		challengeCore: &challengeCore{
			con:                  con,
			challengeManagerAddr: challengeManagerAddr,
			challengeIndex:       challengeIndex,
			client:               l1client,
			auth:                 auth,
			actingAs:             auth.From,
			startL1Block:         new(big.Int).SetUint64(startL1Block),
			confirmationBlocks:   confirmationBlocks,
		},
		executionChallengeBackend: backend,
	}, nil
}

type ChallengeSegment struct {
	Hash     common.Hash
	Position uint64
}

type ChallengeState struct {
	Start       *big.Int
	End         *big.Int
	Segments    []ChallengeSegment
	RawSegments [][32]byte
}

// Returns nil if client is a SimulatedBackend
func (m *ChallengeManager) latestConfirmedBlock(ctx context.Context) (*big.Int, error) {
	_, isSimulated := m.client.(*backends.SimulatedBackend)
	if isSimulated {
		return nil, nil
	}
	latestBlock, err := m.client.HeaderByNumber(ctx, nil)
	if err != nil {
		return nil, fmt.Errorf("error getting latest header from client: %w", err)
	}
	if latestBlock.Difficulty.Sign() == 0 {
		latestConfirmed, err := m.client.HeaderByNumber(ctx, big.NewInt(int64(rpc.FinalizedBlockNumber)))
		if err != nil {
			return nil, fmt.Errorf("error getting finalized block from client: %w", err)
		}
		return latestConfirmed.Number, nil
	}
	block := new(big.Int).Sub(latestBlock.Number, big.NewInt(m.confirmationBlocks))
	if block.Sign() < 0 {
		block.SetInt64(0)
	}
	return block, nil
}

func (m *ChallengeManager) ChallengeIndex() uint64 {
	return m.challengeIndex
}

func uint64ToIndex(val uint64) common.Hash {
	var challengeIndex common.Hash
	binary.BigEndian.PutUint64(challengeIndex[(32-8):], val)
	return challengeIndex
}

// Given the challenge's state hash, resolve the full challenge state via the Bisected event.
func (m *ChallengeManager) resolveStateHash(ctx context.Context, stateHash common.Hash) (ChallengeState, error) {
	logs, err := m.client.FilterLogs(ctx, ethereum.FilterQuery{
		FromBlock: m.startL1Block,
		Addresses: []common.Address{m.challengeManagerAddr},
		Topics:    [][]common.Hash{{challengeBisectedID}, {uint64ToIndex(m.challengeIndex)}, {stateHash}},
	})
	if err != nil {
		return ChallengeState{}, fmt.Errorf("error searching logs for Bisected event from block %v: %w", m.startL1Block, err)
	}
	if len(logs) == 0 {
		return ChallengeState{}, fmt.Errorf("didn't find Bisected event for challenge %v state hash %v starting at block %v", m.challengeIndex, stateHash, m.startL1Block)
	}
	if len(logs) > 1 {
		log.Warn("found multiple Bisected logs", "challenge", m.challengeIndex, "count", len(logs), "fromBlock", m.startL1Block)
	}
	// Multiple logs are in theory fine, as they should all reveal the same preimage.
	// We'll use the most recent log to be safe.
	evmLog := logs[len(logs)-1]
	parsedLog, err := m.con.ParseBisected(evmLog)
	if err != nil {
		return ChallengeState{}, fmt.Errorf("error parsing Bisected event log for challenge %v state hash %v: %w", m.challengeIndex, stateHash, err)
	}
	state := ChallengeState{
		Start:       parsedLog.ChallengedSegmentStart,
		End:         new(big.Int).Add(parsedLog.ChallengedSegmentStart, parsedLog.ChallengedSegmentLength),
		Segments:    make([]ChallengeSegment, len(parsedLog.ChainHashes)),
		RawSegments: parsedLog.ChainHashes,
	}
	degree := len(parsedLog.ChainHashes) - 1
	currentPosition := new(big.Int).Set(parsedLog.ChallengedSegmentStart)
	normalSegmentLength := new(big.Int).Div(parsedLog.ChallengedSegmentLength, big.NewInt(int64(degree)))
	for i, h := range parsedLog.ChainHashes {
		hash := common.Hash(h)
		if i == len(parsedLog.ChainHashes)-1 {
			if currentPosition.Cmp(state.End) > 0 {
				return ChallengeState{}, errors.New("computed last segment position past end")
			}
			currentPosition.Set(state.End)
		}
		if !currentPosition.IsUint64() {
			return ChallengeState{}, errors.New("challenge segment position doesn't fit in a uint64")
		}
		state.Segments[i] = ChallengeSegment{
			Hash:     hash,
			Position: currentPosition.Uint64(),
		}
		currentPosition.Add(currentPosition, normalSegmentLength)
	}
	return state, nil
}

func (m *ChallengeManager) bisect(ctx context.Context, backend ChallengeBackend, oldState *ChallengeState, startSegment int) (*types.Transaction, error) {
	startSegmentPosition := oldState.Segments[startSegment].Position
	endSegmentPosition := oldState.Segments[startSegment+1].Position
	newChallengeLength := endSegmentPosition - startSegmentPosition
	err := backend.SetRange(ctx, startSegmentPosition, endSegmentPosition)
	if err != nil {
		return nil, fmt.Errorf("error setting challenge %v range of %v to %v on backend: %w", m.challengeIndex, startSegmentPosition, endSegmentPosition, err)
	}
	bisectionDegree := maxBisectionDegree
	if newChallengeLength < bisectionDegree {
		bisectionDegree = newChallengeLength
	}
	newSegments := make([][32]byte, int(bisectionDegree+1))
	position := startSegmentPosition
	normalSegmentLength := newChallengeLength / bisectionDegree
	for i := range newSegments {
		if i == len(newSegments)-1 {
			if position > endSegmentPosition {
				return nil, errors.New("computed last segment position past end when bisecting")
			}
			position = endSegmentPosition
		}
		newSegments[i], err = backend.GetHashAtStep(ctx, position)
		if err != nil {
			return nil, fmt.Errorf("error getting challenge %v hash at step %v: %w", m.challengeIndex, position, err)
		}
		position += normalSegmentLength
	}
	return m.con.BisectExecution(
		m.auth,
		m.challengeIndex,
		challengegen.ChallengeLibSegmentSelection{
			OldSegmentsStart:  oldState.Start,
			OldSegmentsLength: new(big.Int).Sub(oldState.End, oldState.Start),
			OldSegments:       oldState.RawSegments,
			ChallengePosition: big.NewInt(int64(startSegment)),
		},
		newSegments,
	)
}

func (m *ChallengeManager) IsMyTurn(ctx context.Context) (bool, error) {
	callOpts := &bind.CallOpts{Context: ctx}
	responder, err := m.con.CurrentResponder(callOpts, m.challengeIndex)
	if err != nil {
		return false, fmt.Errorf("error getting current responder of challenge %v: %w", m.challengeIndex, err)
	}
	if responder != m.actingAs {
		return false, nil
	}
	// Perform future checks against the latest confirmed block
	callOpts.BlockNumber, err = m.latestConfirmedBlock(ctx)
	if err != nil {
		return false, err
	}
	responder, err = m.con.CurrentResponder(callOpts, m.challengeIndex)
	if err != nil {
		return false, fmt.Errorf("error getting confirmed current responder of challenge %v: %w", m.challengeIndex, err)
	}
	if responder != m.actingAs {
		return false, nil
	}
	return true, nil
}

func (m *ChallengeManager) GetChallengeState(ctx context.Context) (*ChallengeState, error) {
	callOpts := &bind.CallOpts{Context: ctx}
	var err error
	callOpts.BlockNumber, err = m.latestConfirmedBlock(ctx)
	if err != nil {
		return nil, err
	}
	challengeState, err := m.con.ChallengeInfo(callOpts, m.challengeIndex)
	if err != nil {
		return nil, fmt.Errorf("error getting challenge %v info: %w", m.challengeIndex, err)
	}
	if challengeState.ChallengeStateHash == (common.Hash{}) {
		return nil, errors.New("lost challenge (state hash 0)")
	}
	state, err := m.resolveStateHash(ctx, challengeState.ChallengeStateHash)
	if err != nil {
		return nil, fmt.Errorf("error resolving challenge %v state hash %v: %w", m.challengeIndex, challengeState.ChallengeStateHash, err)
	}
	return &state, nil
}

func (m *ChallengeManager) ScanChallengeState(ctx context.Context, backend ChallengeBackend, state *ChallengeState) (int, error) {
	for i, segment := range state.Segments {
		ourHash, err := backend.GetHashAtStep(ctx, segment.Position)
		if err != nil {
			return 0, fmt.Errorf("error getting hash from challenge %v backend at step %v: %w", m.challengeIndex, segment.Position, err)
		}
		log.Debug("checking challenge segment", "challenge", m.challengeIndex, "position", segment.Position, "ourHash", ourHash, "segmentHash", segment.Hash)
		if segment.Hash != ourHash {
			if i == 0 {
				return 0, fmt.Errorf(
					"first segment of challenge %v doesn't match: at step count %v challenge has %v but resolved %v",
					m.challengeIndex, segment.Position, segment.Hash, ourHash,
				)
			}
			return i - 1, nil
		}
	}
	return 0, fmt.Errorf("agreed with entire challenge %v (start step count %v and end step count %v)", m.challengeIndex, state.Start.String(), state.End.String())
}

// Checks if an execution challenge exists on-chain.
// If it exists on-chain but we don't have a backend for it, it creates the execution challenge backend.
// If we have a backend for it but it doesn't exist on-chain, it removes the execution challenge backend.
func (m *ChallengeManager) LoadExecChallengeIfExists(ctx context.Context) error {
	latestConfirmedBlock, err := m.latestConfirmedBlock(ctx)
	if err != nil {
		return err
	}
	callOpts := &bind.CallOpts{
		Context:     ctx,
		BlockNumber: latestConfirmedBlock,
	}
	challengeState, err := m.con.ChallengeInfo(callOpts, m.challengeIndex)
	if err != nil {
		return fmt.Errorf("error getting challenge %v info: %w", m.challengeIndex, err)
	}
	if challengeState.Mode != challengeModeExecution {
		m.executionChallengeBackend = nil
		return nil
	}
	if m.executionChallengeBackend != nil {
		return nil
	}
	logs, err := m.client.FilterLogs(ctx, ethereum.FilterQuery{
		FromBlock: m.startL1Block,
		Addresses: []common.Address{m.challengeManagerAddr},
		Topics:    [][]common.Hash{{executionChallengeBegunID}, {uint64ToIndex(m.challengeIndex)}},
	})
	if err != nil {
		return fmt.Errorf("error searching challenge %v logs for ExecutionChallengeBegun event from block %v: %w", m.challengeIndex, m.startL1Block, err)
	}
	if len(logs) == 0 {
		return fmt.Errorf("didn't find ExecutionChallengeBegun event for challenge %v starting at block %v", m.challengeIndex, m.startL1Block)
	}
	if len(logs) > 1 {
		return errors.New("expected only one ExecutionChallengeBegun event")
	}
	ev, err := m.con.ParseExecutionChallengeBegun(logs[0])
	if err != nil {
		return fmt.Errorf("error parsing ExecutionChallengeBegun event of challenge %v: %w", m.challengeIndex, err)
	}
	if !ev.BlockSteps.IsUint64() {
		return fmt.Errorf("ExecutionChallengeBegun event has non-uint64 blockSteps of %v", ev.BlockSteps)
	}
	return m.createExecutionBackend(ctx, ev.BlockSteps.Uint64())
}

func (m *ChallengeManager) IssueOneStepProof(
	ctx context.Context,
	oldState *ChallengeState,
	startSegment int,
) (*types.Transaction, error) {
	position := oldState.Segments[startSegment].Position
	proof, err := m.executionChallengeBackend.GetProofAt(ctx, position)
	if err != nil {
		return nil, fmt.Errorf("error getting OSP from challenge %v backend at step %v: %w", m.challengeIndex, position, err)
	}
	return m.challengeCore.con.OneStepProveExecution(
		m.challengeCore.auth,
		m.challengeCore.challengeIndex,
		challengegen.ChallengeLibSegmentSelection{
			OldSegmentsStart:  oldState.Start,
			OldSegmentsLength: new(big.Int).Sub(oldState.End, oldState.Start),
			OldSegments:       oldState.RawSegments,
			ChallengePosition: big.NewInt(int64(startSegment)),
		},
		proof,
	)
}

func (m *ChallengeManager) createExecutionBackend(ctx context.Context, step uint64) error {
	initialCount := m.blockChallengeBackend.GetMessageCountAtStep(step)
	if m.initialMachineMessageCount == initialCount && m.executionChallengeBackend != nil {
		return nil
	}
	m.executionChallengeBackend = nil
	entry, err := m.validator.CreateReadyValidationEntry(ctx, initialCount)
	if err != nil {
		return fmt.Errorf("error creating validation entry for challenge %v msg %v for execution challenge: %w", m.challengeIndex, initialCount, err)
	}
	input, err := entry.ToInput()
	if err != nil {
		return fmt.Errorf("error getting validation entry input of challenge %v msg %v: %w", m.challengeIndex, initialCount, err)
	}
	var prunedBatches []validator.BatchInfo
	for _, batch := range input.BatchInfo {
		if batch.Number < m.maxBatchesRead {
			prunedBatches = append(prunedBatches, batch)
		}
	}
	input.BatchInfo = prunedBatches
	execRun, err := m.validator.execSpawner.CreateExecutionRun(m.wasmModuleRoot, input).Await(ctx)
	if err != nil {
		return fmt.Errorf("error creating execution backend for msg %v: %w", initialCount, err)
	}
	backend, err := NewExecutionChallengeBackend(execRun)
	if err != nil {
		return err
	}
	expectedState, expectedStatus, err := m.blockChallengeBackend.GetInfoAtStep(step + 1)
	if err != nil {
		return fmt.Errorf("error getting info from block challenge backend: %w", err)
	}
	machineStepCount, computedState, computedStatus, err := backend.GetFinalState(ctx)
	if err != nil {
		return fmt.Errorf("error getting execution challenge final state: %w", err)
	}
	if expectedStatus != computedStatus {
		return fmt.Errorf("after msg %v expected status %v but got %v", initialCount, expectedStatus, computedStatus)
	}
	if computedStatus == StatusFinished {
		if computedState != expectedState {
			return fmt.Errorf("after msg %v expected global state %v but got %v", initialCount, expectedState, computedState)
		}
	}
	m.executionChallengeBackend = backend
	m.machineFinalStepCount = machineStepCount
	m.initialMachineMessageCount = initialCount
	return nil
}

func (m *ChallengeManager) Act(ctx context.Context) (*types.Transaction, error) {
	err := m.LoadExecChallengeIfExists(ctx)
	if err != nil {
		return nil, fmt.Errorf("error loading execution challenge: %w", err)
	}
	myTurn, err := m.IsMyTurn(ctx)
	if err != nil {
		return nil, fmt.Errorf("error checking if it's our turn: %w", err)
	}
	if !myTurn {
		return nil, nil
	}
	state, err := m.GetChallengeState(ctx)
	if err != nil {
		return nil, fmt.Errorf("error getting challenge state: %w", err)
	}

	var backend ChallengeBackend
	if m.executionChallengeBackend != nil {
		backend = m.executionChallengeBackend
	} else {
		backend = m.blockChallengeBackend
	}

	err = backend.SetRange(ctx, state.Start.Uint64(), state.End.Uint64())
	if err != nil {
		return nil, fmt.Errorf("error setting challenge range on backend: %w", err)
	}

	nextMovePos, err := m.ScanChallengeState(ctx, backend, state)
	if err != nil {
		return nil, fmt.Errorf("error scanning challenge state: %w", err)
	}
	startPosition := state.Segments[nextMovePos].Position
	endPosition := state.Segments[nextMovePos+1].Position
	if startPosition+1 != endPosition {
		log.Info("bisecting execution", "challenge", m.challengeIndex, "startPosition", startPosition, "endPosition", endPosition)
		return m.bisect(ctx, backend, state, nextMovePos)
	}
	if m.executionChallengeBackend != nil {
		log.Info("sending onestepproof", "challenge", m.challengeIndex, "startPosition", startPosition, "endPosition", endPosition)
		return m.IssueOneStepProof(
			ctx,
			state,
			nextMovePos,
		)
	}
	err = m.createExecutionBackend(ctx, uint64(nextMovePos))
	if err != nil {
		return nil, fmt.Errorf("error creating execution backend: %w", err)
	}
	machineStepCount := m.machineFinalStepCount
	log.Info("issuing one step proof", "challenge", m.challengeIndex, "machineStepCount", machineStepCount, "initialCount", m.initialMachineMessageCount)
	return m.blockChallengeBackend.IssueExecChallenge(
		m.challengeCore,
		state,
		nextMovePos,
		machineStepCount,
	)
}

'''
'''--- staker/challenge_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"math/big"
	"os"
	"path"
	"runtime"
	"strings"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/abi/bind/backends"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/ospgen"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_arb"
)

func DeployOneStepProofEntry(t *testing.T, auth *bind.TransactOpts, client bind.ContractBackend) common.Address {
	osp0, _, _, err := ospgen.DeployOneStepProver0(auth, client)
	Require(t, err)

	ospMem, _, _, err := ospgen.DeployOneStepProverMemory(auth, client)
	Require(t, err)

	ospMath, _, _, err := ospgen.DeployOneStepProverMath(auth, client)
	Require(t, err)

	ospHostIo, _, _, err := ospgen.DeployOneStepProverHostIo(auth, client)
	Require(t, err)

	ospEntry, _, _, err := ospgen.DeployOneStepProofEntry(auth, client, osp0, ospMem, ospMath, ospHostIo)
	Require(t, err)
	return ospEntry
}

func CreateChallenge(
	t *testing.T,
	ctx context.Context,
	auth *bind.TransactOpts,
	client bind.ContractBackend,
	ospEntry common.Address,
	inputMachine server_arb.MachineInterface,
	maxInboxMessage uint64,
	asserter common.Address,
	challenger common.Address,
) (*mocksgen.MockResultReceiver, common.Address) {
	resultReceiverAddr, _, resultReceiver, err := mocksgen.DeployMockResultReceiver(auth, client, common.Address{})
	Require(t, err)

	machine := inputMachine.CloneMachineInterface()
	startMachineHash := machine.Hash()

	Require(t, machine.Step(ctx, ^uint64(0)))

	endMachineHash := machine.Hash()
	endMachineSteps := machine.GetStepCount()

	var startHashBytes [32]byte
	var endHashBytes [32]byte
	copy(startHashBytes[:], startMachineHash[:])
	copy(endHashBytes[:], endMachineHash[:])
	challenge, _, _, err := mocksgen.DeploySingleExecutionChallenge(
		auth,
		client,
		ospEntry,
		resultReceiverAddr,
		maxInboxMessage,
		[2][32]byte{startHashBytes, endHashBytes},
		big.NewInt(int64(endMachineSteps)),
		asserter,
		challenger,
		big.NewInt(100),
		big.NewInt(100),
	)
	Require(t, err)

	return resultReceiver, challenge
}

func createTransactOpts(t *testing.T) *bind.TransactOpts {
	key, err := crypto.GenerateKey()
	Require(t, err)

	opts, err := bind.NewKeyedTransactorWithChainID(key, big.NewInt(1337))
	Require(t, err)
	return opts
}

func createGenesisAlloc(accts ...*bind.TransactOpts) core.GenesisAlloc {
	alloc := make(core.GenesisAlloc)
	amount := big.NewInt(10)
	amount.Exp(amount, big.NewInt(20), nil)
	for _, opts := range accts {
		alloc[opts.From] = core.GenesisAccount{
			Balance: new(big.Int).Set(amount),
		}
	}
	return alloc
}

func runChallengeTest(
	t *testing.T,
	baseMachine *server_arb.ArbitratorMachine,
	incorrectMachine server_arb.MachineInterface,
	asserterIsCorrect bool,
	testTimeout bool,
	maxInboxMessage uint64,
) {
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlDebug)
	log.Root().SetHandler(glogger)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	deployer := createTransactOpts(t)
	asserter := createTransactOpts(t)
	challenger := createTransactOpts(t)
	alloc := createGenesisAlloc(deployer, asserter, challenger)
	backend := backends.NewSimulatedBackend(alloc, 1_000_000_000)
	backend.Commit()

	ospEntry := DeployOneStepProofEntry(t, deployer, backend)
	backend.Commit()

	var asserterMachine, challengerMachine server_arb.MachineInterface
	var expectedWinner common.Address
	if asserterIsCorrect {
		expectedWinner = asserter.From
		asserterMachine = baseMachine.Clone()
		challengerMachine = incorrectMachine
	} else {
		expectedWinner = challenger.From
		asserterMachine = incorrectMachine
		challengerMachine = baseMachine.Clone()
	}

	resultReceiver, challengeManager := CreateChallenge(
		t,
		ctx,
		deployer,
		backend,
		ospEntry,
		asserterMachine,
		maxInboxMessage,
		asserter.From,
		challenger.From,
	)

	backend.Commit()

	asserterRun, err := server_arb.NewExecutionRun(ctx,
		func(context.Context) (server_arb.MachineInterface, error) { return asserterMachine, nil },
		&server_arb.DefaultMachineCacheConfig)
	Require(t, err)

	asserterManager, err := NewExecutionChallengeManager(
		backend,
		asserter,
		challengeManager,
		1,
		asserterRun,
		0,
		12,
	)
	Require(t, err)

	challengerRun, err := server_arb.NewExecutionRun(ctx,
		func(context.Context) (server_arb.MachineInterface, error) { return challengerMachine, nil },
		&server_arb.DefaultMachineCacheConfig)
	Require(t, err)
	challengerManager, err := NewExecutionChallengeManager(
		backend,
		challenger,
		challengeManager,
		1,
		challengerRun,
		0,
		12,
	)
	Require(t, err)

	for i := 0; i < 100; i++ {
		if testTimeout {
			err = backend.AdjustTime(time.Second * 40)
		}
		Require(t, err)
		backend.Commit()

		var currentCorrect bool
		if i%2 == 0 {
			_, err = challengerManager.Act(ctx)
			currentCorrect = !asserterIsCorrect
		} else {
			_, err = asserterManager.Act(ctx)
			currentCorrect = asserterIsCorrect
		}
		if err != nil {
			if testTimeout && strings.Contains(err.Error(), "CHAL_DEADLINE") {
				t.Log("challenge completed in timeout")
				return
			}
			if !currentCorrect &&
				(strings.Contains(err.Error(), "lost challenge") || strings.Contains(err.Error(), "SAME_OSP_END")) {
				if testTimeout {
					t.Fatal("expected challenge to end in timeout")
				}
				t.Log("challenge completed! asserter hit expected error:", err)
				return
			}
			t.Fatal(err)
		}

		backend.Commit()

		winner, err := resultReceiver.Winner(&bind.CallOpts{})
		Require(t, err)

		if winner == (common.Address{}) {
			continue
		}
		if winner != expectedWinner {
			t.Fatal("wrong party won challenge")
		}
	}

	t.Fatal("challenge timed out without winner")
}

func createBaseMachine(t *testing.T, wasmname string, wasmModules []string) *server_arb.ArbitratorMachine {
	_, filename, _, _ := runtime.Caller(0)
	wasmDir := path.Join(path.Dir(filename), "../arbitrator/prover/test-cases/")

	wasmPath := path.Join(wasmDir, wasmname)

	var modulePaths []string
	for _, moduleName := range wasmModules {
		modulePaths = append(modulePaths, path.Join(wasmDir, moduleName))
	}

	machine, err := server_arb.LoadSimpleMachine(wasmPath, modulePaths)
	Require(t, err)

	return machine
}

func TestChallengeToOSP(t *testing.T) {
	machine := createBaseMachine(t, "global-state.wasm", []string{"global-state-wrapper.wasm"})
	IncorrectMachine := server_arb.NewIncorrectMachine(machine, 200)
	runChallengeTest(t, machine, IncorrectMachine, false, false, 0)
}

func TestChallengeToFailedOSP(t *testing.T) {
	machine := createBaseMachine(t, "global-state.wasm", []string{"global-state-wrapper.wasm"})
	IncorrectMachine := server_arb.NewIncorrectMachine(machine, 200)
	runChallengeTest(t, machine, IncorrectMachine, true, false, 0)
}

func TestChallengeToErroredOSP(t *testing.T) {
	machine := createBaseMachine(t, "const.wasm", nil)
	IncorrectMachine := server_arb.NewIncorrectMachine(machine, 10)
	runChallengeTest(t, machine, IncorrectMachine, false, false, 0)
}

func TestChallengeToFailedErroredOSP(t *testing.T) {
	machine := createBaseMachine(t, "const.wasm", nil)
	IncorrectMachine := server_arb.NewIncorrectMachine(machine, 10)
	runChallengeTest(t, machine, IncorrectMachine, true, false, 0)
}

func TestChallengeToTimeout(t *testing.T) {
	machine := createBaseMachine(t, "global-state.wasm", []string{"global-state-wrapper.wasm"})
	IncorrectMachine := server_arb.NewIncorrectMachine(machine, 200)
	runChallengeTest(t, machine, IncorrectMachine, false, true, 0)
}

func TestChallengeToTooFar(t *testing.T) {
	machine := createBaseMachine(t, "read-inboxmsg-10.wasm", []string{"global-state-wrapper.wasm"})
	Require(t, machine.SetGlobalState(validator.GoGlobalState{PosInBatch: 10}))
	incorrectMachine := machine.Clone()
	Require(t, incorrectMachine.AddSequencerInboxMessage(10, []byte{0, 1, 2, 3}))
	runChallengeTest(t, machine, incorrectMachine, false, false, 9)
}

func TestChallengeToFailedTooFar(t *testing.T) {
	machine := createBaseMachine(t, "read-inboxmsg-10.wasm", []string{"global-state-wrapper.wasm"})
	Require(t, machine.SetGlobalState(validator.GoGlobalState{PosInBatch: 10}))
	incorrectMachine := machine.Clone()
	Require(t, machine.AddSequencerInboxMessage(10, []byte{0, 1, 2, 3}))
	runChallengeTest(t, machine, incorrectMachine, true, false, 11)
}

'''
'''--- staker/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- staker/execution_challenge_bakend.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator"
)

type ExecutionChallengeBackend struct {
	exec validator.ExecutionRun
}

// NewExecutionChallengeBackend creates a backend with the given arguments.
// Note: machineCache may be nil, but if present, it must not have a restricted range.
func NewExecutionChallengeBackend(executionRun validator.ExecutionRun) (*ExecutionChallengeBackend, error) {
	return &ExecutionChallengeBackend{
		exec: executionRun,
	}, nil
}

func (b *ExecutionChallengeBackend) SetRange(ctx context.Context, start uint64, end uint64) error {
	_, err := b.exec.PrepareRange(start, end).Await(ctx)
	return err
}

func (b *ExecutionChallengeBackend) GetHashAtStep(ctx context.Context, position uint64) (common.Hash, error) {
	step := b.exec.GetStepAt(position)
	result, err := step.Await(ctx)
	if err != nil {
		return common.Hash{}, err
	}
	return result.Hash, nil
}

func (b *ExecutionChallengeBackend) GetProofAt(
	ctx context.Context,
	position uint64,
) ([]byte, error) {
	return b.exec.GetProofAt(position).Await(ctx)
}

func (b *ExecutionChallengeBackend) GetFinalState(ctx context.Context) (uint64, validator.GoGlobalState, uint8, error) {
	step := b.exec.GetLastStep()
	res, err := step.Await(ctx)
	if err != nil {
		return 0, validator.GoGlobalState{}, 0, err
	}
	return res.Position, res.GlobalState, uint8(res.Status), nil
}

'''
'''--- staker/execution_reverted_test.go ---
package staker

import (
	"io"
	"testing"
)

func TestExecutionRevertedRegexp(t *testing.T) {
	executionRevertedErrors := []string{
		// go-ethereum and most other execution clients return "execution reverted"
		"execution reverted",
		// execution clients may decode the EVM revert data as a string and include it in the error
		"execution reverted: FOO",
		// besu returns "Execution reverted"
		"Execution reverted",
	}
	for _, errString := range executionRevertedErrors {
		if !executionRevertedRegexp.MatchString(errString) {
			t.Fatalf("execution reverted regexp didn't match %q", errString)
		}
	}
	// This regexp should not match random IO errors
	if executionRevertedRegexp.MatchString(io.ErrUnexpectedEOF.Error()) {
		t.Fatal("execution reverted regexp matched unexpected EOF")
	}
}

'''
'''--- staker/l1_validator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"time"

	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/staker/txbuilder"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/validator"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
)

type ConfirmType uint8

const (
	CONFIRM_TYPE_NONE ConfirmType = iota
	CONFIRM_TYPE_VALID
	CONFIRM_TYPE_INVALID
)

type ConflictType uint8

const (
	CONFLICT_TYPE_NONE ConflictType = iota
	CONFLICT_TYPE_FOUND
	CONFLICT_TYPE_INDETERMINATE
	CONFLICT_TYPE_INCOMPLETE
)

type L1Validator struct {
	rollup         *RollupWatcher
	rollupAddress  common.Address
	validatorUtils *rollupgen.ValidatorUtils
	client         arbutil.L1Interface
	builder        *txbuilder.Builder
	wallet         ValidatorWalletInterface
	callOpts       bind.CallOpts

	das                arbstate.DataAvailabilityReader
	inboxTracker       InboxTrackerInterface
	txStreamer         TransactionStreamerInterface
	blockValidator     *BlockValidator
	lastWasmModuleRoot common.Hash
}

func NewL1Validator(
	client arbutil.L1Interface,
	wallet ValidatorWalletInterface,
	validatorUtilsAddress common.Address,
	callOpts bind.CallOpts,
	das arbstate.DataAvailabilityReader,
	inboxTracker InboxTrackerInterface,
	txStreamer TransactionStreamerInterface,
	blockValidator *BlockValidator,
) (*L1Validator, error) {
	builder, err := txbuilder.NewBuilder(wallet)
	if err != nil {
		return nil, err
	}
	rollup, err := NewRollupWatcher(wallet.RollupAddress(), builder, callOpts)
	if err != nil {
		return nil, err
	}
	validatorUtils, err := rollupgen.NewValidatorUtils(
		validatorUtilsAddress,
		client,
	)
	if err != nil {
		return nil, err
	}
	return &L1Validator{
		rollup:         rollup,
		rollupAddress:  wallet.RollupAddress(),
		validatorUtils: validatorUtils,
		client:         client,
		builder:        builder,
		wallet:         wallet,
		callOpts:       callOpts,
		das:            das,
		inboxTracker:   inboxTracker,
		txStreamer:     txStreamer,
		blockValidator: blockValidator,
	}, nil
}

func (v *L1Validator) getCallOpts(ctx context.Context) *bind.CallOpts {
	opts := v.callOpts
	opts.Context = ctx
	return &opts
}

func (v *L1Validator) Initialize(ctx context.Context) error {
	err := v.rollup.Initialize(ctx)
	if err != nil {
		return err
	}
	return v.updateBlockValidatorModuleRoot(ctx)
}

func (v *L1Validator) updateBlockValidatorModuleRoot(ctx context.Context) error {
	if v.blockValidator == nil {
		return nil
	}
	moduleRoot, err := v.rollup.WasmModuleRoot(v.getCallOpts(ctx))
	if err != nil {
		return err
	}
	if moduleRoot != v.lastWasmModuleRoot {
		err := v.blockValidator.SetCurrentWasmModuleRoot(moduleRoot)
		if err != nil {
			return err
		}
		v.lastWasmModuleRoot = moduleRoot
	} else if (moduleRoot == common.Hash{}) {
		return errors.New("wasmModuleRoot in rollup is zero")
	}
	return nil
}

func (v *L1Validator) resolveTimedOutChallenges(ctx context.Context) (*types.Transaction, error) {
	challengesToEliminate, _, err := v.validatorUtils.TimedOutChallenges(v.getCallOpts(ctx), v.rollupAddress, 0, 10)
	if err != nil {
		return nil, err
	}
	if len(challengesToEliminate) == 0 {
		return nil, nil
	}
	log.Info("timing out challenges", "count", len(challengesToEliminate))
	return v.wallet.TimeoutChallenges(ctx, challengesToEliminate)
}

func (v *L1Validator) resolveNextNode(ctx context.Context, info *StakerInfo, latestConfirmedNode *uint64) (bool, error) {
	callOpts := v.getCallOpts(ctx)
	confirmType, err := v.validatorUtils.CheckDecidableNextNode(callOpts, v.rollupAddress)
	if err != nil {
		return false, err
	}
	unresolvedNodeIndex, err := v.rollup.FirstUnresolvedNode(callOpts)
	if err != nil {
		return false, err
	}
	switch ConfirmType(confirmType) {
	case CONFIRM_TYPE_INVALID:
		addr := v.wallet.Address()
		if info == nil || addr == nil || info.LatestStakedNode <= unresolvedNodeIndex {
			// We aren't an example of someone staked on a competitor
			return false, nil
		}
		log.Warn("rejecting node", "node", unresolvedNodeIndex)
		auth, err := v.builder.Auth(ctx)
		if err != nil {
			return false, err
		}
		_, err = v.rollup.RejectNextNode(auth, *addr)
		return true, err
	case CONFIRM_TYPE_VALID:
		nodeInfo, err := v.rollup.LookupNode(ctx, unresolvedNodeIndex)
		if err != nil {
			return false, err
		}
		afterGs := nodeInfo.AfterState().GlobalState
		log.Info("confirming node", "node", unresolvedNodeIndex)
		auth, err := v.builder.Auth(ctx)
		if err != nil {
			return false, err
		}
		_, err = v.rollup.ConfirmNextNode(auth, afterGs.BlockHash, afterGs.SendRoot)
		if err != nil {
			return false, err
		}
		*latestConfirmedNode = unresolvedNodeIndex
		return true, nil
	default:
		return false, nil
	}
}

func (v *L1Validator) isRequiredStakeElevated(ctx context.Context) (bool, error) {
	callOpts := v.getCallOpts(ctx)
	requiredStake, err := v.rollup.CurrentRequiredStake(callOpts)
	if err != nil {
		return false, err
	}
	baseStake, err := v.rollup.BaseStake(callOpts)
	if err != nil {
		return false, err
	}
	return requiredStake.Cmp(baseStake) > 0, nil
}

type createNodeAction struct {
	assertion         *Assertion
	prevInboxMaxCount *big.Int
	hash              common.Hash
}

type existingNodeAction struct {
	number uint64
	hash   [32]byte
}

type nodeAction interface{}

type OurStakerInfo struct {
	LatestStakedNode     uint64
	LatestStakedNodeHash common.Hash
	CanProgress          bool
	StakeExists          bool
	*StakerInfo
}

func (v *L1Validator) generateNodeAction(
	ctx context.Context,
	stakerInfo *OurStakerInfo,
	strategy StakerStrategy,
	stakerConfig *L1ValidatorConfig,
) (nodeAction, bool, error) {
	startState, prevInboxMaxCount, startStateProposedL1, startStateProposedParentChain, err := lookupNodeStartState(
		ctx, v.rollup, stakerInfo.LatestStakedNode, stakerInfo.LatestStakedNodeHash,
	)
	if err != nil {
		return nil, false, fmt.Errorf(
			"error looking up node %v (hash %v) start state: %w",
			stakerInfo.LatestStakedNode, stakerInfo.LatestStakedNodeHash, err,
		)
	}

	startStateProposedHeader, err := v.client.HeaderByNumber(ctx, arbmath.UintToBig(startStateProposedParentChain))
	if err != nil {
		return nil, false, fmt.Errorf(
			"error looking up L1 header of block %v of node start state: %w",
			startStateProposedParentChain, err,
		)
	}
	startStateProposedTime := time.Unix(int64(startStateProposedHeader.Time), 0)

	v.txStreamer.PauseReorgs()
	defer v.txStreamer.ResumeReorgs()

	localBatchCount, err := v.inboxTracker.GetBatchCount()
	if err != nil {
		return nil, false, fmt.Errorf("error getting batch count from inbox tracker: %w", err)
	}
	if localBatchCount < startState.RequiredBatches() || localBatchCount == 0 {
		log.Info(
			"catching up to chain batches", "localBatches", localBatchCount,
			"target", startState.RequiredBatches(),
		)
		return nil, false, nil
	}

	caughtUp, startCount, err := GlobalStateToMsgCount(v.inboxTracker, v.txStreamer, startState.GlobalState)
	if err != nil {
		return nil, false, fmt.Errorf("start state not in chain: %w", err)
	}
	if !caughtUp {
		target := GlobalStatePosition{
			BatchNumber: startState.GlobalState.Batch,
			PosInBatch:  startState.GlobalState.PosInBatch,
		}
		var current GlobalStatePosition
		head, err := v.txStreamer.GetProcessedMessageCount()
		if err != nil {
			_, current, err = v.blockValidator.GlobalStatePositionsAtCount(head)
		}
		if err != nil {
			log.Info("catching up to chain messages", "target", target)
		} else {
			log.Info("catching up to chain blocks", "target", target, "current", current)
		}
		return nil, false, nil
	}

	var validatedCount arbutil.MessageIndex
	var validatedGlobalState validator.GoGlobalState
	if v.blockValidator != nil {
		valInfo, err := v.blockValidator.ReadLastValidatedInfo()
		if err != nil || valInfo == nil {
			return nil, false, err
		}
		validatedGlobalState = valInfo.GlobalState
		caughtUp, validatedCount, err = GlobalStateToMsgCount(
			v.inboxTracker, v.txStreamer, valInfo.GlobalState,
		)
		if err != nil {
			return nil, false, fmt.Errorf("%w: not found validated block in blockchain", err)
		}
		if !caughtUp {
			log.Info("catching up to last validated block", "target", valInfo.GlobalState)
			return nil, false, nil
		}
		if err := v.updateBlockValidatorModuleRoot(ctx); err != nil {
			return nil, false, fmt.Errorf("error updating block validator module root: %w", err)
		}
		wasmRootValid := false
		for _, root := range valInfo.WasmRoots {
			if v.lastWasmModuleRoot == root {
				wasmRootValid = true
				break
			}
		}
		if !wasmRootValid {
			if !stakerConfig.Dangerous.IgnoreRollupWasmModuleRoot {
				return nil, false, fmt.Errorf(
					"wasmroot doesn't match rollup : %v, valid: %v",
					v.lastWasmModuleRoot, valInfo.WasmRoots,
				)
			}
			log.Warn("wasmroot doesn't match rollup", "rollup", v.lastWasmModuleRoot, "blockValidator", valInfo.WasmRoots)
		}
	} else {
		validatedCount, err = v.txStreamer.GetProcessedMessageCount()
		if err != nil || validatedCount == 0 {
			return nil, false, err
		}
		var batchNum uint64
		messageCount, err := v.inboxTracker.GetBatchMessageCount(localBatchCount - 1)
		if err != nil {
			return nil, false, fmt.Errorf("error getting latest batch %v message count: %w", localBatchCount-1, err)
		}
		if validatedCount >= messageCount {
			batchNum = localBatchCount - 1
			validatedCount = messageCount
		} else {
			batchNum, err = FindBatchContainingMessageIndex(v.inboxTracker, validatedCount-1, localBatchCount)
			if err != nil {
				return nil, false, err
			}
		}
		execResult, err := v.txStreamer.ResultAtCount(validatedCount)
		if err != nil {
			return nil, false, err
		}
		_, gsPos, err := GlobalStatePositionsAtCount(v.inboxTracker, validatedCount, batchNum)
		if err != nil {
			return nil, false, fmt.Errorf("%w: failed calculating GSposition for count %d", err, validatedCount)
		}
		validatedGlobalState = buildGlobalState(*execResult, gsPos)
	}

	currentL1BlockNum, err := v.client.BlockNumber(ctx)
	if err != nil {
		return nil, false, fmt.Errorf("error getting latest L1 block number: %w", err)
	}

	l1BlockNumber, err := arbutil.CorrespondingL1BlockNumber(ctx, v.client, currentL1BlockNum)
	if err != nil {
		return nil, false, err
	}

	minAssertionPeriod, err := v.rollup.MinimumAssertionPeriod(v.getCallOpts(ctx))
	if err != nil {
		return nil, false, fmt.Errorf("error getting rollup minimum assertion period: %w", err)
	}

	timeSinceProposed := big.NewInt(int64(l1BlockNumber) - int64(startStateProposedL1))
	if timeSinceProposed.Cmp(minAssertionPeriod) < 0 {
		// Too soon to assert
		return nil, false, nil
	}

	successorNodes, err := v.rollup.LookupNodeChildren(ctx, stakerInfo.LatestStakedNode, stakerInfo.LatestStakedNodeHash)
	if err != nil {
		return nil, false, fmt.Errorf("error looking up node %v (hash %v) children: %w", stakerInfo.LatestStakedNode, stakerInfo.LatestStakedNodeHash, err)
	}

	var correctNode nodeAction
	wrongNodesExist := false
	if len(successorNodes) > 0 {
		log.Info("examining existing potential successors", "count", len(successorNodes))
	}
	for _, nd := range successorNodes {
		if correctNode != nil && wrongNodesExist {
			// We've found everything we could hope to find
			break
		}
		if correctNode != nil {
			log.Error("found younger sibling to correct assertion (implicitly invalid)", "node", nd.NodeNum)
			wrongNodesExist = true
			continue
		}
		afterGS := nd.AfterState().GlobalState
		requiredBatch := afterGS.Batch
		if afterGS.PosInBatch == 0 && afterGS.Batch > 0 {
			requiredBatch -= 1
		}
		if localBatchCount <= requiredBatch {
			log.Info("staker: waiting for node to catch up to assertion batch", "current", localBatchCount, "target", requiredBatch-1)
			return nil, false, nil
		}
		nodeBatchMsgCount, err := v.inboxTracker.GetBatchMessageCount(requiredBatch)
		if err != nil {
			return nil, false, err
		}
		if validatedCount < nodeBatchMsgCount {
			log.Info("staker: waiting for validator to catch up to assertion batch messages", "current", validatedCount, "target", nodeBatchMsgCount)
			return nil, false, nil
		}
		if nd.Assertion.AfterState.MachineStatus != validator.MachineStatusFinished {
			wrongNodesExist = true
			log.Error("Found incorrect assertion: Machine status not finished", "node", nd.NodeNum, "machineStatus", nd.Assertion.AfterState.MachineStatus)
			continue
		}
		caughtUp, nodeMsgCount, err := GlobalStateToMsgCount(v.inboxTracker, v.txStreamer, afterGS)
		if errors.Is(err, ErrGlobalStateNotInChain) {
			wrongNodesExist = true
			log.Error("Found incorrect assertion", "node", nd.NodeNum, "afterGS", afterGS, "err", err)
			continue
		}
		if err != nil {
			return nil, false, fmt.Errorf("error getting message number from global state: %w", err)
		}
		if !caughtUp {
			return nil, false, fmt.Errorf("unexpected no-caught-up parsing assertion. Current: %d target: %v", validatedCount, afterGS)
		}
		log.Info(
			"found correct assertion",
			"node", nd.NodeNum,
			"count", nodeMsgCount,
			"blockHash", afterGS.BlockHash,
		)
		correctNode = existingNodeAction{
			number: nd.NodeNum,
			hash:   nd.NodeHash,
		}
	}

	if correctNode != nil || strategy == WatchtowerStrategy {
		return correctNode, wrongNodesExist, nil
	}

	makeAssertionInterval := stakerConfig.MakeAssertionInterval
	if wrongNodesExist || (strategy >= MakeNodesStrategy && time.Since(startStateProposedTime) >= makeAssertionInterval) {
		// There's no correct node; create one.
		var lastNodeHashIfExists *common.Hash
		if len(successorNodes) > 0 {
			lastNodeHashIfExists = &successorNodes[len(successorNodes)-1].NodeHash
		}
		action, err := v.createNewNodeAction(ctx, stakerInfo, prevInboxMaxCount, startCount, startState, validatedCount, validatedGlobalState, lastNodeHashIfExists)
		if err != nil {
			return nil, wrongNodesExist, fmt.Errorf("error generating create new node action (from pos %d to %d): %w", startCount, validatedCount, err)
		}
		return action, wrongNodesExist, nil
	}

	return nil, wrongNodesExist, nil
}

func (v *L1Validator) createNewNodeAction(
	ctx context.Context,
	stakerInfo *OurStakerInfo,
	prevInboxMaxCount *big.Int,
	startCount arbutil.MessageIndex,
	startState *validator.ExecutionState,
	validatedCount arbutil.MessageIndex,
	validatedGS validator.GoGlobalState,
	lastNodeHashIfExists *common.Hash,
) (nodeAction, error) {
	if !prevInboxMaxCount.IsUint64() {
		return nil, fmt.Errorf("inbox max count %v isn't a uint64", prevInboxMaxCount)
	}
	if validatedCount <= startCount {
		// we haven't validated any new blocks
		return nil, nil
	}
	if validatedGS.Batch < prevInboxMaxCount.Uint64() {
		// didn't validate enough batches
		log.Info("staker: not enough batches validated to create new assertion", "validated.Batch", validatedGS.Batch, "posInBatch", validatedGS.PosInBatch, "required batch", prevInboxMaxCount)
		return nil, nil
	}
	batchValidated := validatedGS.Batch
	if validatedGS.PosInBatch == 0 {
		batchValidated--
	}
	validatedBatchAcc, err := v.inboxTracker.GetBatchAcc(batchValidated)
	if err != nil {
		return nil, fmt.Errorf("error getting batch %v accumulator: %w", batchValidated, err)
	}

	hasSiblingByte := [1]byte{0}
	prevNum := stakerInfo.LatestStakedNode
	lastHash := stakerInfo.LatestStakedNodeHash
	if lastNodeHashIfExists != nil {
		lastHash = *lastNodeHashIfExists
		hasSiblingByte[0] = 1
	}
	assertionNumBlocks := uint64(validatedCount - startCount)
	assertion := &Assertion{
		BeforeState: startState,
		AfterState: &validator.ExecutionState{
			GlobalState:   validatedGS,
			MachineStatus: validator.MachineStatusFinished,
		},
		NumBlocks: assertionNumBlocks,
	}

	wasmModuleRoot := v.lastWasmModuleRoot
	if v.blockValidator == nil {
		wasmModuleRoot, err = v.rollup.WasmModuleRoot(v.getCallOpts(ctx))
		if err != nil {
			return nil, fmt.Errorf("error rollup wasm module root: %w", err)
		}
	}

	executionHash := assertion.ExecutionHash()
	newNodeHash := crypto.Keccak256Hash(hasSiblingByte[:], lastHash[:], executionHash[:], validatedBatchAcc[:], wasmModuleRoot[:])

	action := createNodeAction{
		assertion:         assertion,
		hash:              newNodeHash,
		prevInboxMaxCount: prevInboxMaxCount,
	}
	log.Info("creating node", "hash", newNodeHash, "lastNode", prevNum, "parentNode", stakerInfo.LatestStakedNode)
	return action, nil
}

// Returns (execution state, inbox max count, L1 block proposed, parent chain block proposed, error)
func lookupNodeStartState(ctx context.Context, rollup *RollupWatcher, nodeNum uint64, nodeHash common.Hash) (*validator.ExecutionState, *big.Int, uint64, uint64, error) {
	if nodeNum == 0 {
		creationEvent, err := rollup.LookupCreation(ctx)
		if err != nil {
			return nil, nil, 0, 0, fmt.Errorf("error looking up rollup creation event: %w", err)
		}
		l1BlockNumber, err := arbutil.CorrespondingL1BlockNumber(ctx, rollup.client, creationEvent.Raw.BlockNumber)
		if err != nil {
			return nil, nil, 0, 0, err
		}
		return &validator.ExecutionState{
			GlobalState:   validator.GoGlobalState{},
			MachineStatus: validator.MachineStatusFinished,
		}, big.NewInt(1), l1BlockNumber, creationEvent.Raw.BlockNumber, nil
	}
	node, err := rollup.LookupNode(ctx, nodeNum)
	if err != nil {
		return nil, nil, 0, 0, err
	}
	if node.NodeHash != nodeHash {
		return nil, nil, 0, 0, errors.New("looked up starting node but found wrong hash")
	}
	return node.AfterState(), node.InboxMaxCount, node.L1BlockProposed, node.ParentChainBlockProposed, nil
}

'''
'''--- staker/rollup_watcher.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"math/big"
	"regexp"
	"sync/atomic"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
)

var rollupInitializedID common.Hash
var nodeCreatedID common.Hash
var challengeCreatedID common.Hash

func init() {
	parsedRollup, err := rollupgen.RollupUserLogicMetaData.GetAbi()
	if err != nil {
		panic(err)
	}
	rollupInitializedID = parsedRollup.Events["RollupInitialized"].ID
	nodeCreatedID = parsedRollup.Events["NodeCreated"].ID
	challengeCreatedID = parsedRollup.Events["RollupChallengeStarted"].ID
}

type StakerInfo struct {
	Index            uint64
	LatestStakedNode uint64
	AmountStaked     *big.Int
	CurrentChallenge *uint64
}

type RollupWatcher struct {
	*rollupgen.RollupUserLogic
	address             common.Address
	fromBlock           *big.Int
	client              arbutil.L1Interface
	baseCallOpts        bind.CallOpts
	unSupportedL3Method atomic.Bool
}

func NewRollupWatcher(address common.Address, client arbutil.L1Interface, callOpts bind.CallOpts) (*RollupWatcher, error) {
	con, err := rollupgen.NewRollupUserLogic(address, client)
	if err != nil {
		return nil, err
	}

	return &RollupWatcher{
		address:         address,
		client:          client,
		baseCallOpts:    callOpts,
		RollupUserLogic: con,
	}, nil
}

func (r *RollupWatcher) getCallOpts(ctx context.Context) *bind.CallOpts {
	opts := r.baseCallOpts
	opts.Context = ctx
	return &opts
}

// A regexp matching "execution reverted" errors returned from the parent chain RPC.
var executionRevertedRegexp = regexp.MustCompile("(?i)execution reverted")

func (r *RollupWatcher) getNodeCreationBlock(ctx context.Context, nodeNum uint64) (*big.Int, error) {
	callOpts := r.getCallOpts(ctx)
	if !r.unSupportedL3Method.Load() {
		createdAtBlock, err := r.GetNodeCreationBlockForLogLookup(callOpts, nodeNum)
		if err == nil {
			return createdAtBlock, nil
		}
		log.Trace("failed to call getNodeCreationBlockForLogLookup, falling back on node CreatedAtBlock field", "err", err)
		if executionRevertedRegexp.MatchString(err.Error()) {
			r.unSupportedL3Method.Store(true)
		} else {
			return nil, err
		}
	}
	node, err := r.GetNode(callOpts, nodeNum)
	if err != nil {
		return nil, err
	}
	createdAtBlock := new(big.Int).SetUint64(node.CreatedAtBlock)
	return createdAtBlock, nil
}

func (r *RollupWatcher) Initialize(ctx context.Context) error {
	var err error
	r.fromBlock, err = r.getNodeCreationBlock(ctx, 0)
	return err
}

func (r *RollupWatcher) LookupCreation(ctx context.Context) (*rollupgen.RollupUserLogicRollupInitialized, error) {
	var query = ethereum.FilterQuery{
		FromBlock: r.fromBlock,
		ToBlock:   r.fromBlock,
		Addresses: []common.Address{r.address},
		Topics:    [][]common.Hash{{rollupInitializedID}},
	}
	logs, err := r.client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	if len(logs) == 0 {
		return nil, errors.New("rollup not created")
	}
	if len(logs) > 1 {
		return nil, errors.New("rollup created multiple times")
	}
	ev, err := r.ParseRollupInitialized(logs[0])
	return ev, err
}

func (r *RollupWatcher) LookupNode(ctx context.Context, number uint64) (*NodeInfo, error) {
	createdAtBlock, err := r.getNodeCreationBlock(ctx, number)
	if err != nil {
		return nil, err
	}
	var numberAsHash common.Hash
	binary.BigEndian.PutUint64(numberAsHash[(32-8):], number)
	var query = ethereum.FilterQuery{
		FromBlock: createdAtBlock,
		ToBlock:   createdAtBlock,
		Addresses: []common.Address{r.address},
		Topics:    [][]common.Hash{{nodeCreatedID}, {numberAsHash}},
	}
	logs, err := r.client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	if len(logs) == 0 {
		return nil, fmt.Errorf("couldn't find requested node %v", number)
	}
	if len(logs) > 1 {
		return nil, fmt.Errorf("found multiple instances of requested node %v", number)
	}
	ethLog := logs[0]
	parsedLog, err := r.ParseNodeCreated(ethLog)
	if err != nil {
		return nil, err
	}
	l1BlockProposed, err := arbutil.CorrespondingL1BlockNumber(ctx, r.client, ethLog.BlockNumber)
	if err != nil {
		return nil, err
	}
	return &NodeInfo{
		NodeNum:                  parsedLog.NodeNum,
		L1BlockProposed:          l1BlockProposed,
		ParentChainBlockProposed: ethLog.BlockNumber,
		Assertion:                NewAssertionFromSolidity(parsedLog.Assertion),
		InboxMaxCount:            parsedLog.InboxMaxCount,
		AfterInboxBatchAcc:       parsedLog.AfterInboxBatchAcc,
		NodeHash:                 parsedLog.NodeHash,
		WasmModuleRoot:           parsedLog.WasmModuleRoot,
	}, nil
}

func (r *RollupWatcher) LookupNodeChildren(ctx context.Context, nodeNum uint64, nodeHash common.Hash) ([]*NodeInfo, error) {
	node, err := r.RollupUserLogic.GetNode(r.getCallOpts(ctx), nodeNum)
	if err != nil {
		return nil, err
	}
	if node.LatestChildNumber == 0 {
		return nil, nil
	}
	if node.NodeHash != nodeHash {
		return nil, fmt.Errorf("got unexpected node hash %v looking for node number %v with expected hash %v (reorg?)", node.NodeHash, nodeNum, nodeHash)
	}
	latestChild, err := r.RollupUserLogic.GetNode(r.getCallOpts(ctx), node.LatestChildNumber)
	if err != nil {
		return nil, err
	}
	var query = ethereum.FilterQuery{
		FromBlock: new(big.Int).SetUint64(node.CreatedAtBlock),
		ToBlock:   new(big.Int).SetUint64(latestChild.CreatedAtBlock),
		Addresses: []common.Address{r.address},
		Topics:    [][]common.Hash{{nodeCreatedID}, nil, {nodeHash}},
	}
	logs, err := r.client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	infos := make([]*NodeInfo, 0, len(logs))
	lastHash := nodeHash
	for i, ethLog := range logs {
		parsedLog, err := r.ParseNodeCreated(ethLog)
		if err != nil {
			return nil, err
		}
		lastHashIsSibling := [1]byte{0}
		if i > 0 {
			lastHashIsSibling[0] = 1
		}
		lastHash = crypto.Keccak256Hash(lastHashIsSibling[:], lastHash[:], parsedLog.ExecutionHash[:], parsedLog.AfterInboxBatchAcc[:], parsedLog.WasmModuleRoot[:])
		l1BlockProposed, err := arbutil.CorrespondingL1BlockNumber(ctx, r.client, ethLog.BlockNumber)
		if err != nil {
			return nil, err
		}
		infos = append(infos, &NodeInfo{
			NodeNum:                  parsedLog.NodeNum,
			L1BlockProposed:          l1BlockProposed,
			ParentChainBlockProposed: ethLog.BlockNumber,
			Assertion:                NewAssertionFromSolidity(parsedLog.Assertion),
			InboxMaxCount:            parsedLog.InboxMaxCount,
			AfterInboxBatchAcc:       parsedLog.AfterInboxBatchAcc,
			NodeHash:                 lastHash,
			WasmModuleRoot:           parsedLog.WasmModuleRoot,
		})
	}
	return infos, nil
}

func (r *RollupWatcher) LatestConfirmedCreationBlock(ctx context.Context) (uint64, error) {
	latestConfirmed, err := r.LatestConfirmed(r.getCallOpts(ctx))
	if err != nil {
		return 0, err
	}
	creation, err := r.getNodeCreationBlock(ctx, latestConfirmed)
	if err != nil {
		return 0, err
	}
	if !creation.IsUint64() {
		return 0, fmt.Errorf("node %v creation block %v is not a uint64", latestConfirmed, creation)
	}
	return creation.Uint64(), nil
}

func (r *RollupWatcher) LookupChallengedNode(ctx context.Context, address common.Address) (uint64, error) {
	// TODO: This function is currently unused

	// Assuming this function is only used to find information about an active challenge, it
	// must be a challenge over an unconfirmed node and thus must have been created after the
	// latest confirmed node was created
	latestConfirmedCreated, err := r.LatestConfirmedCreationBlock(ctx)
	if err != nil {
		return 0, err
	}

	addressQuery := common.Hash{}
	copy(addressQuery[12:], address.Bytes())

	query := ethereum.FilterQuery{
		FromBlock: new(big.Int).SetUint64(latestConfirmedCreated),
		ToBlock:   nil,
		Addresses: []common.Address{r.address},
		Topics:    [][]common.Hash{{challengeCreatedID}, {addressQuery}},
	}
	logs, err := r.client.FilterLogs(ctx, query)
	if err != nil {
		return 0, err
	}

	if len(logs) == 0 {
		return 0, errors.New("no matching challenge")
	}

	if len(logs) > 1 {
		return 0, errors.New("too many matching challenges")
	}

	challenge, err := r.ParseRollupChallengeStarted(logs[0])
	if err != nil {
		return 0, err
	}

	return challenge.ChallengedNode, nil
}

func (r *RollupWatcher) StakerInfo(ctx context.Context, staker common.Address) (*StakerInfo, error) {
	info, err := r.StakerMap(r.getCallOpts(ctx), staker)
	if err != nil {
		return nil, err
	}
	if !info.IsStaked {
		return nil, nil
	}
	stakerInfo := &StakerInfo{
		Index:            info.Index,
		LatestStakedNode: info.LatestStakedNode,
		AmountStaked:     info.AmountStaked,
	}
	if info.CurrentChallenge != 0 {
		chal := info.CurrentChallenge
		stakerInfo.CurrentChallenge = &chal
	}
	return stakerInfo, nil
}

'''
'''--- staker/staker.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"runtime/debug"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/rpc"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbnode/redislock"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/staker/txbuilder"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
)

var (
	stakerBalanceGauge              = metrics.NewRegisteredGaugeFloat64("arb/staker/balance", nil)
	stakerAmountStakedGauge         = metrics.NewRegisteredGauge("arb/staker/amount_staked", nil)
	stakerLatestStakedNodeGauge     = metrics.NewRegisteredGauge("arb/staker/staked_node", nil)
	stakerLatestConfirmedNodeGauge  = metrics.NewRegisteredGauge("arb/staker/confirmed_node", nil)
	stakerLastSuccessfulActionGauge = metrics.NewRegisteredGauge("arb/staker/action/last_success", nil)
	stakerActionSuccessCounter      = metrics.NewRegisteredCounter("arb/staker/action/success", nil)
	stakerActionFailureCounter      = metrics.NewRegisteredCounter("arb/staker/action/failure", nil)
	validatorGasRefunderBalance     = metrics.NewRegisteredGaugeFloat64("arb/validator/gasrefunder/balanceether", nil)
)

type StakerStrategy uint8

const (
	// Watchtower: don't do anything on L1, but log if there's a bad assertion
	WatchtowerStrategy StakerStrategy = iota
	// Defensive: stake if there's a bad assertion
	DefensiveStrategy
	// Stake latest: stay staked on the latest node, challenging bad assertions
	StakeLatestStrategy
	// Resolve nodes: stay staked on the latest node and resolve any unconfirmed nodes, challenging bad assertions
	ResolveNodesStrategy
	// Make nodes: continually create new nodes, challenging bad assertions
	MakeNodesStrategy
)

type L1PostingStrategy struct {
	HighGasThreshold   float64 `koanf:"high-gas-threshold"`
	HighGasDelayBlocks int64   `koanf:"high-gas-delay-blocks"`
}

var DefaultL1PostingStrategy = L1PostingStrategy{
	HighGasThreshold:   0,
	HighGasDelayBlocks: 0,
}

func L1PostingStrategyAddOptions(prefix string, f *flag.FlagSet) {
	f.Float64(prefix+".high-gas-threshold", DefaultL1PostingStrategy.HighGasThreshold, "high gas threshold")
	f.Int64(prefix+".high-gas-delay-blocks", DefaultL1PostingStrategy.HighGasDelayBlocks, "high gas delay blocks")
}

type L1ValidatorConfig struct {
	Enable                    bool                        `koanf:"enable"`
	Strategy                  string                      `koanf:"strategy"`
	StakerInterval            time.Duration               `koanf:"staker-interval"`
	MakeAssertionInterval     time.Duration               `koanf:"make-assertion-interval"`
	PostingStrategy           L1PostingStrategy           `koanf:"posting-strategy"`
	DisableChallenge          bool                        `koanf:"disable-challenge"`
	ConfirmationBlocks        int64                       `koanf:"confirmation-blocks"`
	UseSmartContractWallet    bool                        `koanf:"use-smart-contract-wallet"`
	OnlyCreateWalletContract  bool                        `koanf:"only-create-wallet-contract"`
	StartValidationFromStaked bool                        `koanf:"start-validation-from-staked"`
	ContractWalletAddress     string                      `koanf:"contract-wallet-address"`
	GasRefunderAddress        string                      `koanf:"gas-refunder-address"`
	DataPoster                dataposter.DataPosterConfig `koanf:"data-poster" reload:"hot"`
	RedisUrl                  string                      `koanf:"redis-url"`
	RedisLock                 redislock.SimpleCfg         `koanf:"redis-lock" reload:"hot"`
	ExtraGas                  uint64                      `koanf:"extra-gas" reload:"hot"`
	Dangerous                 DangerousConfig             `koanf:"dangerous"`
	ParentChainWallet         genericconf.WalletConfig    `koanf:"parent-chain-wallet"`

	strategy    StakerStrategy
	gasRefunder common.Address
}

func (c *L1ValidatorConfig) ParseStrategy() (StakerStrategy, error) {
	switch strings.ToLower(c.Strategy) {
	case "watchtower":
		return WatchtowerStrategy, nil
	case "defensive":
		return DefensiveStrategy, nil
	case "stakelatest":
		return StakeLatestStrategy, nil
	case "resolvenodes":
		return ResolveNodesStrategy, nil
	case "makenodes":
		return MakeNodesStrategy, nil
	default:
		return WatchtowerStrategy, fmt.Errorf("unknown staker strategy \"%v\"", c.Strategy)
	}
}

func (c *L1ValidatorConfig) ValidatorRequired() bool {
	if !c.Enable {
		return false
	}
	if c.Dangerous.WithoutBlockValidator {
		return false
	}
	if c.strategy == WatchtowerStrategy {
		return false
	}
	return true
}

func (c *L1ValidatorConfig) Validate() error {
	strategy, err := c.ParseStrategy()
	if err != nil {
		return err
	}
	c.strategy = strategy
	if len(c.GasRefunderAddress) > 0 && !common.IsHexAddress(c.GasRefunderAddress) {
		return errors.New("invalid validator gas refunder address")
	}
	c.gasRefunder = common.HexToAddress(c.GasRefunderAddress)
	return nil
}

var DefaultL1ValidatorConfig = L1ValidatorConfig{
	Enable:                    true,
	Strategy:                  "Watchtower",
	StakerInterval:            time.Minute,
	MakeAssertionInterval:     time.Hour,
	PostingStrategy:           L1PostingStrategy{},
	DisableChallenge:          false,
	ConfirmationBlocks:        12,
	UseSmartContractWallet:    false,
	OnlyCreateWalletContract:  false,
	StartValidationFromStaked: true,
	ContractWalletAddress:     "",
	GasRefunderAddress:        "",
	DataPoster:                dataposter.DefaultDataPosterConfigForValidator,
	RedisUrl:                  "",
	RedisLock:                 redislock.DefaultCfg,
	ExtraGas:                  50000,
	Dangerous:                 DefaultDangerousConfig,
	ParentChainWallet:         DefaultValidatorL1WalletConfig,
}

var TestL1ValidatorConfig = L1ValidatorConfig{
	Enable:                    true,
	Strategy:                  "Watchtower",
	StakerInterval:            time.Millisecond * 10,
	MakeAssertionInterval:     -time.Hour * 1000,
	PostingStrategy:           L1PostingStrategy{},
	DisableChallenge:          false,
	ConfirmationBlocks:        0,
	UseSmartContractWallet:    false,
	OnlyCreateWalletContract:  false,
	StartValidationFromStaked: true,
	ContractWalletAddress:     "",
	GasRefunderAddress:        "",
	DataPoster:                dataposter.TestDataPosterConfigForValidator,
	RedisUrl:                  "",
	RedisLock:                 redislock.DefaultCfg,
	ExtraGas:                  50000,
	Dangerous:                 DefaultDangerousConfig,
	ParentChainWallet:         DefaultValidatorL1WalletConfig,
}

var DefaultValidatorL1WalletConfig = genericconf.WalletConfig{
	Pathname:      "validator-wallet",
	Password:      genericconf.WalletConfigDefault.Password,
	PrivateKey:    genericconf.WalletConfigDefault.PrivateKey,
	Account:       genericconf.WalletConfigDefault.Account,
	OnlyCreateKey: genericconf.WalletConfigDefault.OnlyCreateKey,
}

func L1ValidatorConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultL1ValidatorConfig.Enable, "enable validator")
	f.String(prefix+".strategy", DefaultL1ValidatorConfig.Strategy, "L1 validator strategy, either watchtower, defensive, stakeLatest, or makeNodes")
	f.Duration(prefix+".staker-interval", DefaultL1ValidatorConfig.StakerInterval, "how often the L1 validator should check the status of the L1 rollup and maybe take action with its stake")
	f.Duration(prefix+".make-assertion-interval", DefaultL1ValidatorConfig.MakeAssertionInterval, "if configured with the makeNodes strategy, how often to create new assertions (bypassed in case of a dispute)")
	L1PostingStrategyAddOptions(prefix+".posting-strategy", f)
	f.Bool(prefix+".disable-challenge", DefaultL1ValidatorConfig.DisableChallenge, "disable validator challenge")
	f.Int64(prefix+".confirmation-blocks", DefaultL1ValidatorConfig.ConfirmationBlocks, "confirmation blocks")
	f.Bool(prefix+".use-smart-contract-wallet", DefaultL1ValidatorConfig.UseSmartContractWallet, "use a smart contract wallet instead of an EOA address")
	f.Bool(prefix+".only-create-wallet-contract", DefaultL1ValidatorConfig.OnlyCreateWalletContract, "only create smart wallet contract and exit")
	f.Bool(prefix+".start-validation-from-staked", DefaultL1ValidatorConfig.StartValidationFromStaked, "assume staked nodes are valid")
	f.String(prefix+".contract-wallet-address", DefaultL1ValidatorConfig.ContractWalletAddress, "validator smart contract wallet public address")
	f.String(prefix+".gas-refunder-address", DefaultL1ValidatorConfig.GasRefunderAddress, "The gas refunder contract address (optional)")
	f.String(prefix+".redis-url", DefaultL1ValidatorConfig.RedisUrl, "redis url for L1 validator")
	f.Uint64(prefix+".extra-gas", DefaultL1ValidatorConfig.ExtraGas, "use this much more gas than estimation says is necessary to post transactions")
	dataposter.DataPosterConfigAddOptions(prefix+".data-poster", f, dataposter.DefaultDataPosterConfigForValidator)
	redislock.AddConfigOptions(prefix+".redis-lock", f)
	DangerousConfigAddOptions(prefix+".dangerous", f)
	genericconf.WalletConfigAddOptions(prefix+".parent-chain-wallet", f, DefaultL1ValidatorConfig.ParentChainWallet.Pathname)
}

type DangerousConfig struct {
	IgnoreRollupWasmModuleRoot bool `koanf:"ignore-rollup-wasm-module-root"`
	WithoutBlockValidator      bool `koanf:"without-block-validator"`
}

var DefaultDangerousConfig = DangerousConfig{
	IgnoreRollupWasmModuleRoot: false,
	WithoutBlockValidator:      false,
}

func DangerousConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".ignore-rollup-wasm-module-root", DefaultL1ValidatorConfig.Dangerous.IgnoreRollupWasmModuleRoot, "DANGEROUS! make assertions even when the wasm module root is wrong")
	f.Bool(prefix+".without-block-validator", DefaultL1ValidatorConfig.Dangerous.WithoutBlockValidator, "DANGEROUS! allows running an L1 validator without a block validator")
}

type nodeAndHash struct {
	id   uint64
	hash common.Hash
}

type LatestStakedNotifier interface {
	UpdateLatestStaked(count arbutil.MessageIndex, globalState validator.GoGlobalState)
}

type LatestConfirmedNotifier interface {
	UpdateLatestConfirmed(count arbutil.MessageIndex, globalState validator.GoGlobalState)
}

type Staker struct {
	*L1Validator
	stopwaiter.StopWaiter
	l1Reader                *headerreader.HeaderReader
	stakedNotifiers         []LatestStakedNotifier
	confirmedNotifiers      []LatestConfirmedNotifier
	activeChallenge         *ChallengeManager
	baseCallOpts            bind.CallOpts
	config                  L1ValidatorConfig
	highGasBlocksBuffer     *big.Int
	lastActCalledBlock      *big.Int
	inactiveLastCheckedNode *nodeAndHash
	bringActiveUntilNode    uint64
	inboxReader             InboxReaderInterface
	statelessBlockValidator *StatelessBlockValidator
	fatalErr                chan<- error
}

type ValidatorWalletInterface interface {
	Initialize(context.Context) error
	// Address must be able to be called concurrently with other functions
	Address() *common.Address
	// Address must be able to be called concurrently with other functions
	AddressOrZero() common.Address
	TxSenderAddress() *common.Address
	RollupAddress() common.Address
	ChallengeManagerAddress() common.Address
	L1Client() arbutil.L1Interface
	TestTransactions(context.Context, []*types.Transaction) error
	ExecuteTransactions(context.Context, *txbuilder.Builder, common.Address) (*types.Transaction, error)
	TimeoutChallenges(context.Context, []uint64) (*types.Transaction, error)
	CanBatchTxs() bool
	AuthIfEoa() *bind.TransactOpts
	Start(context.Context)
	StopAndWait()
	// May be nil
	DataPoster() *dataposter.DataPoster
}

func NewStaker(
	l1Reader *headerreader.HeaderReader,
	wallet ValidatorWalletInterface,
	callOpts bind.CallOpts,
	config L1ValidatorConfig,
	blockValidator *BlockValidator,
	statelessBlockValidator *StatelessBlockValidator,
	stakedNotifiers []LatestStakedNotifier,
	confirmedNotifiers []LatestConfirmedNotifier,
	validatorUtilsAddress common.Address,
	fatalErr chan<- error,
) (*Staker, error) {

	if err := config.Validate(); err != nil {
		return nil, err
	}
	client := l1Reader.Client()
	val, err := NewL1Validator(client, wallet, validatorUtilsAddress, callOpts,
		statelessBlockValidator.daService, statelessBlockValidator.inboxTracker, statelessBlockValidator.streamer, blockValidator)
	if err != nil {
		return nil, err
	}
	stakerLastSuccessfulActionGauge.Update(time.Now().Unix())
	if config.StartValidationFromStaked && blockValidator != nil {
		stakedNotifiers = append(stakedNotifiers, blockValidator)
	}
	return &Staker{
		L1Validator:             val,
		l1Reader:                l1Reader,
		stakedNotifiers:         stakedNotifiers,
		confirmedNotifiers:      confirmedNotifiers,
		baseCallOpts:            callOpts,
		config:                  config,
		highGasBlocksBuffer:     big.NewInt(config.PostingStrategy.HighGasDelayBlocks),
		lastActCalledBlock:      nil,
		inboxReader:             statelessBlockValidator.inboxReader,
		statelessBlockValidator: statelessBlockValidator,
		fatalErr:                fatalErr,
	}, nil
}

func (s *Staker) Initialize(ctx context.Context) error {
	err := s.L1Validator.Initialize(ctx)
	if err != nil {
		return err
	}
	walletAddressOrZero := s.wallet.AddressOrZero()
	if walletAddressOrZero != (common.Address{}) {
		s.updateStakerBalanceMetric(ctx)
	}
	if s.blockValidator != nil && s.config.StartValidationFromStaked {
		latestStaked, _, err := s.validatorUtils.LatestStaked(&s.baseCallOpts, s.rollupAddress, walletAddressOrZero)
		if err != nil {
			return err
		}
		stakerLatestStakedNodeGauge.Update(int64(latestStaked))
		if latestStaked == 0 {
			return nil
		}

		stakedInfo, err := s.rollup.LookupNode(ctx, latestStaked)
		if err != nil {
			return err
		}

		return s.blockValidator.InitAssumeValid(stakedInfo.AfterState().GlobalState)
	}
	return nil
}

func (s *Staker) getLatestStakedState(ctx context.Context, staker common.Address) (uint64, arbutil.MessageIndex, *validator.GoGlobalState, error) {
	callOpts := s.getCallOpts(ctx)
	if s.l1Reader.UseFinalityData() {
		callOpts.BlockNumber = big.NewInt(int64(rpc.FinalizedBlockNumber))
	}
	latestStaked, _, err := s.validatorUtils.LatestStaked(s.getCallOpts(ctx), s.rollupAddress, staker)
	if err != nil {
		return 0, 0, nil, fmt.Errorf("couldn't get LatestStaked(%v): %w", staker, err)
	}
	if latestStaked == 0 {
		return latestStaked, 0, nil, nil
	}

	stakedInfo, err := s.rollup.LookupNode(ctx, latestStaked)
	if err != nil {
		return 0, 0, nil, fmt.Errorf("couldn't look up latest assertion of %v (%v): %w", staker, latestStaked, err)
	}

	globalState := stakedInfo.AfterState().GlobalState
	caughtUp, count, err := GlobalStateToMsgCount(s.inboxTracker, s.txStreamer, globalState)
	if err != nil {
		if errors.Is(err, ErrGlobalStateNotInChain) && s.fatalErr != nil {
			fatal := fmt.Errorf("latest assertion of %v (%v) not in chain: %w", staker, latestStaked, err)
			s.fatalErr <- fatal
		}
		return 0, 0, nil, fmt.Errorf("latest assertion of %v (%v): %w", staker, latestStaked, err)
	}

	if !caughtUp {
		log.Info("latest assertion not yet in our node", "staker", staker, "assertion", latestStaked, "state", globalState)
		return latestStaked, 0, nil, nil
	}

	processedCount, err := s.txStreamer.GetProcessedMessageCount()
	if err != nil {
		return 0, 0, nil, err
	}

	if processedCount < count {
		log.Info("execution catching up to rollup", "staker", staker, "rollupCount", count, "processedCount", processedCount)
		return latestStaked, 0, nil, nil
	}

	return latestStaked, count, &globalState, nil
}

func (s *Staker) StopAndWait() {
	s.StopWaiter.StopAndWait()
	if s.Strategy() != WatchtowerStrategy {
		s.wallet.StopAndWait()
	}
}

func (s *Staker) Start(ctxIn context.Context) {
	if s.Strategy() != WatchtowerStrategy {
		s.wallet.Start(ctxIn)
	}
	s.StopWaiter.Start(ctxIn, s)
	backoff := time.Second
	s.CallIteratively(func(ctx context.Context) (returningWait time.Duration) {
		defer func() {
			panicErr := recover()
			if panicErr != nil {
				log.Error("staker Act call panicked", "panic", panicErr, "backtrace", string(debug.Stack()))
				s.builder.ClearTransactions()
				returningWait = time.Minute
			}
		}()
		var err error
		if common.HexToAddress(s.config.GasRefunderAddress) != (common.Address{}) {
			gasRefunderBalance, err := s.client.BalanceAt(ctx, common.HexToAddress(s.config.GasRefunderAddress), nil)
			if err != nil {
				log.Warn("error fetching validator gas refunder balance", "err", err)
			} else {
				validatorGasRefunderBalance.Update(arbmath.BalancePerEther(gasRefunderBalance))
			}
		}
		err = s.updateBlockValidatorModuleRoot(ctx)
		if err != nil {
			log.Warn("error updating latest wasm module root", "err", err)
		}
		arbTx, err := s.Act(ctx)
		if err == nil && arbTx != nil {
			_, err = s.l1Reader.WaitForTxApproval(ctx, arbTx)
			if err == nil {
				log.Info("successfully executed staker transaction", "hash", arbTx.Hash())
			} else {
				err = fmt.Errorf("error waiting for tx receipt: %w", err)
			}
		}
		if err == nil {
			backoff = time.Second
			stakerLastSuccessfulActionGauge.Update(time.Now().Unix())
			stakerActionSuccessCounter.Inc(1)
			if arbTx != nil && !s.wallet.CanBatchTxs() {
				// Try to create another tx
				return 0
			}
			return s.config.StakerInterval
		}
		stakerActionFailureCounter.Inc(1)
		backoff *= 2
		if backoff > time.Minute {
			backoff = time.Minute
			log.Error("error acting as staker", "err", err)
		} else {
			log.Warn("error acting as staker", "err", err)
		}
		return backoff
	})
	s.CallIteratively(func(ctx context.Context) time.Duration {
		wallet := s.wallet.AddressOrZero()
		staked, stakedMsgCount, stakedGlobalState, err := s.getLatestStakedState(ctx, wallet)
		if err != nil && ctx.Err() == nil {
			log.Error("staker: error checking latest staked", "err", err)
		}
		stakerLatestStakedNodeGauge.Update(int64(staked))
		if stakedGlobalState != nil {
			for _, notifier := range s.stakedNotifiers {
				notifier.UpdateLatestStaked(stakedMsgCount, *stakedGlobalState)
			}
		}
		confirmed := staked
		confirmedMsgCount := stakedMsgCount
		confirmedGlobalState := stakedGlobalState
		if wallet != (common.Address{}) {
			confirmed, confirmedMsgCount, confirmedGlobalState, err = s.getLatestStakedState(ctx, common.Address{})
			if err != nil && ctx.Err() == nil {
				log.Error("staker: error checking latest confirmed", "err", err)
			}
		}
		stakerLatestConfirmedNodeGauge.Update(int64(confirmed))
		if confirmedGlobalState != nil {
			for _, notifier := range s.confirmedNotifiers {
				notifier.UpdateLatestConfirmed(confirmedMsgCount, *confirmedGlobalState)
			}
		}
		return s.config.StakerInterval
	})
}

func (s *Staker) IsWhitelisted(ctx context.Context) (bool, error) {
	callOpts := s.getCallOpts(ctx)
	whitelistDisabled, err := s.rollup.ValidatorWhitelistDisabled(callOpts)
	if err != nil {
		return false, err
	}
	if whitelistDisabled {
		return true, nil
	}
	addr := s.wallet.Address()
	if addr != nil {
		return s.rollup.IsValidator(callOpts, *addr)
	}
	return false, nil
}

func (s *Staker) shouldAct(ctx context.Context) bool {
	var gasPriceHigh = false
	var gasPriceFloat float64
	gasPrice, err := s.client.SuggestGasPrice(ctx)
	if err != nil {
		log.Warn("error getting gas price", "err", err)
	} else {
		gasPriceFloat = float64(gasPrice.Int64()) / 1e9
		if gasPriceFloat >= s.config.PostingStrategy.HighGasThreshold {
			gasPriceHigh = true
		}
	}
	latestBlockInfo, err := s.client.HeaderByNumber(ctx, nil)
	if err != nil {
		log.Warn("error getting latest block", "err", err)
		return true
	}
	latestBlockNum := latestBlockInfo.Number
	if s.lastActCalledBlock == nil {
		s.lastActCalledBlock = latestBlockNum
	}
	blocksSinceActCalled := new(big.Int).Sub(latestBlockNum, s.lastActCalledBlock)
	s.lastActCalledBlock = latestBlockNum
	if gasPriceHigh {
		// We're eating into the high gas buffer to delay our tx
		s.highGasBlocksBuffer.Sub(s.highGasBlocksBuffer, blocksSinceActCalled)
	} else {
		// We'll try to make a tx if necessary, so we can add to the buffer for future high gas
		s.highGasBlocksBuffer.Add(s.highGasBlocksBuffer, blocksSinceActCalled)
	}
	// Clamp `s.highGasBlocksBuffer` to between 0 and HighGasDelayBlocks
	if s.highGasBlocksBuffer.Sign() < 0 {
		s.highGasBlocksBuffer.SetInt64(0)
	} else if s.highGasBlocksBuffer.Cmp(big.NewInt(s.config.PostingStrategy.HighGasDelayBlocks)) > 0 {
		s.highGasBlocksBuffer.SetInt64(s.config.PostingStrategy.HighGasDelayBlocks)
	}
	if gasPriceHigh && s.highGasBlocksBuffer.Sign() > 0 {
		log.Warn(
			"not acting yet as gas price is high",
			"gasPrice", gasPriceFloat,
			"highGasPriceConfig", s.config.PostingStrategy.HighGasThreshold,
			"highGasBuffer", s.highGasBlocksBuffer,
		)
		return false
	}
	return true
}

func (s *Staker) confirmDataPosterIsReady(ctx context.Context) error {
	dp := s.wallet.DataPoster()
	if dp == nil {
		return nil
	}
	dataPosterNonce, _, err := dp.GetNextNonceAndMeta(ctx)
	if err != nil {
		return err
	}
	latestNonce, err := s.l1Reader.Client().NonceAt(ctx, dp.Sender(), nil)
	if err != nil {
		return err
	}
	if dataPosterNonce > latestNonce {
		return fmt.Errorf("data poster nonce %v is ahead of on-chain nonce %v -- probably waiting for a pending transaction to be included in a block", dataPosterNonce, latestNonce)
	}
	if dataPosterNonce < latestNonce {
		return fmt.Errorf("data poster nonce %v is behind on-chain nonce %v -- is something else making transactions on this address?", dataPosterNonce, latestNonce)
	}
	return nil
}

func (s *Staker) Act(ctx context.Context) (*types.Transaction, error) {
	if s.config.strategy != WatchtowerStrategy {
		err := s.confirmDataPosterIsReady(ctx)
		if err != nil {
			return nil, err
		}
		whitelisted, err := s.IsWhitelisted(ctx)
		if err != nil {
			return nil, fmt.Errorf("error checking if whitelisted: %w", err)
		}
		if !whitelisted {
			log.Warn("validator address isn't whitelisted", "address", s.wallet.Address(), "txSender", s.wallet.TxSenderAddress())
		}
	}
	if !s.shouldAct(ctx) {
		// The fact that we're delaying acting is already logged in `shouldAct`
		return nil, nil
	}
	callOpts := s.getCallOpts(ctx)
	s.builder.ClearTransactions()
	var rawInfo *StakerInfo
	walletAddressOrZero := s.wallet.AddressOrZero()
	if walletAddressOrZero != (common.Address{}) {
		var err error
		rawInfo, err = s.rollup.StakerInfo(ctx, walletAddressOrZero)
		if err != nil {
			return nil, fmt.Errorf("error getting own staker (%v) info: %w", walletAddressOrZero, err)
		}
		if rawInfo != nil {
			stakerAmountStakedGauge.Update(rawInfo.AmountStaked.Int64())
		} else {
			stakerAmountStakedGauge.Update(0)
		}
		s.updateStakerBalanceMetric(ctx)
	}
	// If the wallet address is zero, or the wallet address isn't staked,
	// this will return the latest node and its hash (atomically).
	latestStakedNodeNum, latestStakedNodeInfo, err := s.validatorUtils.LatestStaked(
		callOpts, s.rollupAddress, walletAddressOrZero,
	)
	if err != nil {
		return nil, fmt.Errorf("error getting latest staked node of own wallet %v: %w", walletAddressOrZero, err)
	}
	stakerLatestStakedNodeGauge.Update(int64(latestStakedNodeNum))
	if rawInfo != nil {
		rawInfo.LatestStakedNode = latestStakedNodeNum
	}
	info := OurStakerInfo{
		CanProgress:          true,
		LatestStakedNode:     latestStakedNodeNum,
		LatestStakedNodeHash: latestStakedNodeInfo.NodeHash,
		StakerInfo:           rawInfo,
		StakeExists:          rawInfo != nil,
	}

	effectiveStrategy := s.config.strategy
	nodesLinear, err := s.validatorUtils.AreUnresolvedNodesLinear(callOpts, s.rollupAddress)
	if err != nil {
		return nil, fmt.Errorf("error checking for rollup assertion fork: %w", err)
	}
	if !nodesLinear {
		log.Warn("rollup assertion fork detected")
		if effectiveStrategy == DefensiveStrategy {
			effectiveStrategy = StakeLatestStrategy
		}
		s.inactiveLastCheckedNode = nil
	}
	if s.bringActiveUntilNode != 0 {
		if info.LatestStakedNode < s.bringActiveUntilNode {
			if effectiveStrategy == DefensiveStrategy {
				effectiveStrategy = StakeLatestStrategy
			}
		} else {
			log.Info("defensive validator staked past incorrect node; waiting here")
			s.bringActiveUntilNode = 0
		}
		s.inactiveLastCheckedNode = nil
	}
	if effectiveStrategy <= DefensiveStrategy && s.inactiveLastCheckedNode != nil {
		info.LatestStakedNode = s.inactiveLastCheckedNode.id
		info.LatestStakedNodeHash = s.inactiveLastCheckedNode.hash
	}

	latestConfirmedNode, err := s.rollup.LatestConfirmed(callOpts)
	if err != nil {
		return nil, fmt.Errorf("error getting latest confirmed node: %w", err)
	}

	requiredStakeElevated, err := s.isRequiredStakeElevated(ctx)
	if err != nil {
		return nil, fmt.Errorf("error checking if required stake is elevated: %w", err)
	}
	// Resolve nodes if either we're on the make nodes strategy,
	// or we're on the stake latest strategy but don't have a stake
	// (attempt to reduce the current required stake).
	shouldResolveNodes := effectiveStrategy >= ResolveNodesStrategy ||
		(effectiveStrategy >= StakeLatestStrategy && rawInfo == nil && requiredStakeElevated)
	resolvingNode := false
	if shouldResolveNodes {
		arbTx, err := s.resolveTimedOutChallenges(ctx)
		if err != nil {
			return nil, fmt.Errorf("error resolving timed out challenges: %w", err)
		}
		if arbTx != nil {
			return arbTx, nil
		}
		resolvingNode, err = s.resolveNextNode(ctx, rawInfo, &latestConfirmedNode)
		if err != nil {
			return nil, fmt.Errorf("error resolving node %v: %w", latestConfirmedNode+1, err)
		}
		if resolvingNode && rawInfo == nil && latestConfirmedNode > info.LatestStakedNode {
			// If we hit this condition, we've resolved what was previously the latest confirmed node,
			// and we don't have a stake yet. That means we were planning to enter the rollup on
			// the latest confirmed node, which has now changed. We fix this by updating our staker info
			// to indicate that we're now entering the rollup on the newly confirmed node.
			nodeInfo, err := s.rollup.GetNode(callOpts, latestConfirmedNode)
			if err != nil {
				return nil, fmt.Errorf("error getting latest confirmed node %v info: %w", latestConfirmedNode, err)
			}
			info.LatestStakedNode = latestConfirmedNode
			info.LatestStakedNodeHash = nodeInfo.NodeHash
		}
	}

	canActFurther := func() bool {
		return s.wallet.CanBatchTxs() || s.builder.BuildingTransactionCount() == 0
	}

	// If we have an old stake, remove it
	if rawInfo != nil && rawInfo.LatestStakedNode <= latestConfirmedNode && canActFurther() {
		stakeIsTooOutdated := rawInfo.LatestStakedNode < latestConfirmedNode
		// We're not trying to stake anyways
		stakeIsUnwanted := effectiveStrategy < StakeLatestStrategy
		if stakeIsTooOutdated || stakeIsUnwanted {
			// Note: we must have an address if rawInfo != nil
			auth, err := s.builder.Auth(ctx)
			if err != nil {
				return nil, err
			}
			_, err = s.rollup.ReturnOldDeposit(auth, walletAddressOrZero)
			if err != nil {
				return nil, fmt.Errorf("error returning old deposit (from our staker %v): %w", walletAddressOrZero, err)
			}
			auth, err = s.builder.Auth(ctx)
			if err != nil {
				return nil, err
			}
			_, err = s.rollup.WithdrawStakerFunds(auth)
			if err != nil {
				return nil, fmt.Errorf("error withdrawing staker funds from our staker %v: %w", walletAddressOrZero, err)
			}
			log.Info("removing old stake and withdrawing funds")
			return s.wallet.ExecuteTransactions(ctx, s.builder, s.config.gasRefunder)
		}
	}

	if walletAddressOrZero != (common.Address{}) && canActFurther() {
		withdrawable, err := s.rollup.WithdrawableFunds(callOpts, walletAddressOrZero)
		if err != nil {
			return nil, fmt.Errorf("error checking withdrawable funds of our staker %v: %w", walletAddressOrZero, err)
		}
		if withdrawable.Sign() > 0 {
			auth, err := s.builder.Auth(ctx)
			if err != nil {
				return nil, err
			}
			_, err = s.rollup.WithdrawStakerFunds(auth)
			if err != nil {
				return nil, fmt.Errorf("error withdrawing our staker %v funds: %w", walletAddressOrZero, err)
			}
		}
	}

	if rawInfo != nil && canActFurther() {
		if err = s.handleConflict(ctx, rawInfo); err != nil {
			return nil, fmt.Errorf("error handling conflict: %w", err)
		}
	}

	// Don't attempt to create a new stake if we're resolving a node and the stake is elevated,
	// as that might affect the current required stake.
	if (rawInfo != nil || !resolvingNode || !requiredStakeElevated) && canActFurther() {
		// Advance stake up to 20 times in one transaction
		for i := 0; info.CanProgress && i < 20; i++ {
			if err := s.advanceStake(ctx, &info, effectiveStrategy); err != nil {
				return nil, fmt.Errorf("error advancing stake from node %v (hash %v): %w", info.LatestStakedNode, info.LatestStakedNodeHash, err)
			}
			if !s.wallet.CanBatchTxs() && effectiveStrategy >= StakeLatestStrategy {
				info.CanProgress = false
			}
		}
	}

	if rawInfo != nil && s.builder.BuildingTransactionCount() == 0 && canActFurther() {
		if err := s.createConflict(ctx, rawInfo); err != nil {
			return nil, fmt.Errorf("error creating conflict: %w", err)
		}
	}

	if s.builder.BuildingTransactionCount() == 0 {
		return nil, nil
	}

	if info.StakerInfo == nil && info.StakeExists {
		log.Info("staking to execute transactions")
	}
	return s.wallet.ExecuteTransactions(ctx, s.builder, s.config.gasRefunder)
}

func (s *Staker) handleConflict(ctx context.Context, info *StakerInfo) error {
	if info.CurrentChallenge == nil {
		s.activeChallenge = nil
		return nil
	}

	if s.activeChallenge == nil || s.activeChallenge.ChallengeIndex() != *info.CurrentChallenge {
		log.Error("entered challenge", "challenge", *info.CurrentChallenge)

		latestConfirmedCreated, err := s.rollup.LatestConfirmedCreationBlock(ctx)
		if err != nil {
			return fmt.Errorf("error getting latest confirmed creation block: %w", err)
		}

		newChallengeManager, err := NewChallengeManager(
			ctx,
			s.builder,
			s.builder.BuilderAuth(),
			*s.builder.WalletAddress(),
			s.wallet.ChallengeManagerAddress(),
			*info.CurrentChallenge,
			s.statelessBlockValidator,
			latestConfirmedCreated,
			s.config.ConfirmationBlocks,
		)
		if err != nil {
			return fmt.Errorf("error creating challenge manager: %w", err)
		}

		s.activeChallenge = newChallengeManager
	}

	_, err := s.activeChallenge.Act(ctx)
	return err
}

func (s *Staker) advanceStake(ctx context.Context, info *OurStakerInfo, effectiveStrategy StakerStrategy) error {
	active := effectiveStrategy >= StakeLatestStrategy
	action, wrongNodesExist, err := s.generateNodeAction(ctx, info, effectiveStrategy, &s.config)
	if err != nil {
		return fmt.Errorf("error generating node action: %w", err)
	}
	if wrongNodesExist && effectiveStrategy == WatchtowerStrategy {
		log.Error("found incorrect assertion in watchtower mode")
	}
	if action == nil {
		info.CanProgress = false
		return nil
	}

	switch action := action.(type) {
	case createNodeAction:
		if wrongNodesExist && s.config.DisableChallenge {
			log.Error("refusing to challenge assertion as config disables challenges")
			info.CanProgress = false
			return nil
		}
		if !active {
			if wrongNodesExist && effectiveStrategy >= DefensiveStrategy {
				log.Error("bringing defensive validator online because of incorrect assertion")
				s.bringActiveUntilNode = info.LatestStakedNode + 1
			}
			info.CanProgress = false
			return nil
		}

		// Details are already logged with more details in generateNodeAction
		info.CanProgress = false
		info.LatestStakedNode = 0
		info.LatestStakedNodeHash = action.hash

		// We'll return early if we already have a stake
		if info.StakeExists {
			auth, err := s.builder.Auth(ctx)
			if err != nil {
				return err
			}
			_, err = s.rollup.StakeOnNewNode(auth, action.assertion.AsSolidityStruct(), action.hash, action.prevInboxMaxCount)
			if err != nil {
				return fmt.Errorf("error staking on new node: %w", err)
			}
			return nil
		}

		// If we have no stake yet, we'll put one down
		stakeAmount, err := s.rollup.CurrentRequiredStake(s.getCallOpts(ctx))
		if err != nil {
			return fmt.Errorf("error getting current required stake: %w", err)
		}
		auth, err := s.builder.AuthWithAmount(ctx, stakeAmount)
		if err != nil {
			return err
		}
		_, err = s.rollup.NewStakeOnNewNode(
			auth,
			action.assertion.AsSolidityStruct(),
			action.hash,
			action.prevInboxMaxCount,
		)
		if err != nil {
			return fmt.Errorf("error placing new stake on new node: %w", err)
		}
		info.StakeExists = true
		return nil
	case existingNodeAction:
		info.LatestStakedNode = action.number
		info.LatestStakedNodeHash = action.hash
		if !active {
			if wrongNodesExist && effectiveStrategy >= DefensiveStrategy {
				log.Error("bringing defensive validator online because of incorrect assertion")
				s.bringActiveUntilNode = action.number
				info.CanProgress = false
			} else {
				s.inactiveLastCheckedNode = &nodeAndHash{
					id:   action.number,
					hash: action.hash,
				}
			}
			return nil
		}
		log.Info("staking on existing node", "node", action.number)
		// We'll return early if we already havea stake
		if info.StakeExists {
			auth, err := s.builder.Auth(ctx)
			if err != nil {
				return err
			}
			_, err = s.rollup.StakeOnExistingNode(auth, action.number, action.hash)
			if err != nil {
				return fmt.Errorf("error staking on existing node: %w", err)
			}
			return nil
		}

		// If we have no stake yet, we'll put one down
		stakeAmount, err := s.rollup.CurrentRequiredStake(s.getCallOpts(ctx))
		if err != nil {
			return fmt.Errorf("error getting current required stake: %w", err)
		}
		auth, err := s.builder.AuthWithAmount(ctx, stakeAmount)
		if err != nil {
			return err
		}
		_, err = s.rollup.NewStakeOnExistingNode(
			auth,
			action.number,
			action.hash,
		)
		if err != nil {
			return fmt.Errorf("error placing new stake on existing node: %w", err)
		}
		info.StakeExists = true
		return nil
	default:
		panic("invalid action type")
	}
}

func (s *Staker) createConflict(ctx context.Context, info *StakerInfo) error {
	if info.CurrentChallenge != nil {
		return nil
	}

	callOpts := s.getCallOpts(ctx)
	stakers, moreStakers, err := s.validatorUtils.GetStakers(callOpts, s.rollupAddress, 0, 1024)
	if err != nil {
		return fmt.Errorf("error getting stakers list: %w", err)
	}
	for moreStakers {
		var newStakers []common.Address
		newStakers, moreStakers, err = s.validatorUtils.GetStakers(callOpts, s.rollupAddress, uint64(len(stakers)), 1024)
		if err != nil {
			return fmt.Errorf("error getting more stakers: %w", err)
		}
		stakers = append(stakers, newStakers...)
	}
	latestNode, err := s.rollup.LatestConfirmed(callOpts)
	if err != nil {
		return err
	}
	// Safe to dereference as createConflict is only called when we have a wallet address
	walletAddr := *s.wallet.Address()
	for _, staker := range stakers {
		stakerInfo, err := s.rollup.StakerInfo(ctx, staker)
		if err != nil {
			return fmt.Errorf("error getting staker %v info: %w", staker, err)
		}
		if stakerInfo == nil {
			return fmt.Errorf("staker %v (returned from ValidatorUtils's GetStakers function) not found in rollup", staker)
		}
		if stakerInfo.CurrentChallenge != nil {
			continue
		}
		conflictInfo, err := s.validatorUtils.FindStakerConflict(callOpts, s.rollupAddress, walletAddr, staker, big.NewInt(1024))
		if err != nil {
			return fmt.Errorf("error finding conflict with staker %v: %w", staker, err)
		}
		if ConflictType(conflictInfo.Ty) != CONFLICT_TYPE_FOUND {
			continue
		}
		staker1 := walletAddr
		staker2 := staker
		if conflictInfo.Node2 < conflictInfo.Node1 {
			staker1, staker2 = staker2, staker1
			conflictInfo.Node1, conflictInfo.Node2 = conflictInfo.Node2, conflictInfo.Node1
		}
		if conflictInfo.Node1 <= latestNode {
			// Immaterial as this is past the confirmation point; this must be a zombie
			continue
		}

		node1Info, err := s.rollup.LookupNode(ctx, conflictInfo.Node1)
		if err != nil {
			return fmt.Errorf("error looking up node %v: %w", conflictInfo.Node1, err)
		}
		node2Info, err := s.rollup.LookupNode(ctx, conflictInfo.Node2)
		if err != nil {
			return fmt.Errorf("error looking up node %v: %w", conflictInfo.Node2, err)
		}
		log.Warn("creating challenge", "node1", conflictInfo.Node1, "node2", conflictInfo.Node2, "otherStaker", staker)
		auth, err := s.builder.Auth(ctx)
		if err != nil {
			return err
		}
		_, err = s.rollup.CreateChallenge(
			auth,
			[2]common.Address{staker1, staker2},
			[2]uint64{conflictInfo.Node1, conflictInfo.Node2},
			node1Info.MachineStatuses(),
			node1Info.GlobalStates(),
			node1Info.Assertion.NumBlocks,
			node2Info.Assertion.ExecutionHash(),
			[2]*big.Int{new(big.Int).SetUint64(node1Info.L1BlockProposed), new(big.Int).SetUint64(node2Info.L1BlockProposed)},
			[2][32]byte{node1Info.WasmModuleRoot, node2Info.WasmModuleRoot},
		)
		if err != nil {
			return fmt.Errorf("error creating challenge: %w", err)
		}
	}
	// No conflicts exist
	return nil
}

func (s *Staker) Strategy() StakerStrategy {
	return s.config.strategy
}

func (s *Staker) Rollup() *RollupWatcher {
	return s.rollup
}

func (s *Staker) updateStakerBalanceMetric(ctx context.Context) {
	txSenderAddress := s.wallet.TxSenderAddress()
	if txSenderAddress == nil {
		stakerBalanceGauge.Update(0)
		return
	}
	balance, err := s.client.BalanceAt(ctx, *txSenderAddress, nil)
	if err != nil {
		log.Error("error getting staker balance", "txSenderAddress", *txSenderAddress, "err", err)
		return
	}
	stakerBalanceGauge.Update(arbmath.BalancePerEther(balance))
}

'''
'''--- staker/stateless_block_validator.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package staker

import (
	"context"
	"errors"
	"fmt"
	"regexp"
	"sync"
	"testing"

	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/util/rpcclient"
	"github.com/offchainlabs/nitro/validator/server_api"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/validator"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbstate"
)

type StatelessBlockValidator struct {
	config *BlockValidatorConfig

	execSpawner        validator.ExecutionSpawner
	validationSpawners []validator.ValidationSpawner

	recorder execution.ExecutionRecorder

	inboxReader  InboxReaderInterface
	inboxTracker InboxTrackerInterface
	streamer     TransactionStreamerInterface
	db           ethdb.Database
	daService    arbstate.DataAvailabilityReader

	moduleMutex           sync.Mutex
	currentWasmModuleRoot common.Hash
	pendingWasmModuleRoot common.Hash
}

type BlockValidatorRegistrer interface {
	SetBlockValidator(*BlockValidator)
}

type InboxTrackerInterface interface {
	BlockValidatorRegistrer
	GetDelayedMessageBytes(uint64) ([]byte, error)
	GetBatchMessageCount(seqNum uint64) (arbutil.MessageIndex, error)
	GetBatchAcc(seqNum uint64) (common.Hash, error)
	GetBatchCount() (uint64, error)
}

type TransactionStreamerInterface interface {
	BlockValidatorRegistrer
	GetProcessedMessageCount() (arbutil.MessageIndex, error)
	GetMessage(seqNum arbutil.MessageIndex) (*arbostypes.MessageWithMetadata, error)
	ResultAtCount(count arbutil.MessageIndex) (*execution.MessageResult, error)
	PauseReorgs()
	ResumeReorgs()
}

type InboxReaderInterface interface {
	GetSequencerMessageBytes(ctx context.Context, seqNum uint64) ([]byte, error)
}

type GlobalStatePosition struct {
	BatchNumber uint64
	PosInBatch  uint64
}

// return the globalState position before and after processing message at the specified count
// batch-number must be provided by caller
func GlobalStatePositionsAtCount(
	tracker InboxTrackerInterface,
	count arbutil.MessageIndex,
	batch uint64,
) (GlobalStatePosition, GlobalStatePosition, error) {
	msgCountInBatch, err := tracker.GetBatchMessageCount(batch)
	if err != nil {
		return GlobalStatePosition{}, GlobalStatePosition{}, err
	}
	var firstInBatch arbutil.MessageIndex
	if batch > 0 {
		firstInBatch, err = tracker.GetBatchMessageCount(batch - 1)
		if err != nil {
			return GlobalStatePosition{}, GlobalStatePosition{}, err
		}
	}
	if msgCountInBatch < count {
		return GlobalStatePosition{}, GlobalStatePosition{}, fmt.Errorf("batch %d has msgCount %d, failed getting for %d", batch, msgCountInBatch-1, count)
	}
	if firstInBatch >= count {
		return GlobalStatePosition{}, GlobalStatePosition{}, fmt.Errorf("batch %d starts from %d, failed getting for %d", batch, firstInBatch, count)
	}
	posInBatch := uint64(count - firstInBatch - 1)
	startPos := GlobalStatePosition{batch, posInBatch}
	if msgCountInBatch == count {
		return startPos, GlobalStatePosition{batch + 1, 0}, nil
	}
	return startPos, GlobalStatePosition{batch, posInBatch + 1}, nil
}

func FindBatchContainingMessageIndex(
	tracker InboxTrackerInterface, pos arbutil.MessageIndex, high uint64,
) (uint64, error) {
	var low uint64
	// Iteration preconditions:
	// - high >= low
	// - msgCount(low - 1) <= pos implies low <= target
	// - msgCount(high) > pos implies high >= target
	// Therefore, if low == high, then low == high == target
	for high > low {
		// Due to integer rounding, mid >= low && mid < high
		mid := (low + high) / 2
		count, err := tracker.GetBatchMessageCount(mid)
		if err != nil {
			return 0, err
		}
		if count < pos {
			// Must narrow as mid >= low, therefore mid + 1 > low, therefore newLow > oldLow
			// Keeps low precondition as msgCount(mid) < pos
			low = mid + 1
		} else if count == pos {
			return mid + 1, nil
		} else if count == pos+1 || mid == low { // implied: count > pos
			return mid, nil
		} else { // implied: count > pos + 1
			// Must narrow as mid < high, therefore newHigh < lowHigh
			// Keeps high precondition as msgCount(mid) > pos
			high = mid
		}
	}
	return low, nil
}

type ValidationEntryStage uint32

const (
	Empty ValidationEntryStage = iota
	ReadyForRecord
	Ready
)

type validationEntry struct {
	Stage ValidationEntryStage
	// Valid since ReadyforRecord:
	Pos           arbutil.MessageIndex
	Start         validator.GoGlobalState
	End           validator.GoGlobalState
	HasDelayedMsg bool
	DelayedMsgNr  uint64
	// valid when created, removed after recording
	msg *arbostypes.MessageWithMetadata
	// Has batch when created - others could be added on record
	BatchInfo []validator.BatchInfo
	// Valid since Ready
	Preimages  map[arbutil.PreimageType]map[common.Hash][]byte
	DelayedMsg []byte
}

func (e *validationEntry) ToInput() (*validator.ValidationInput, error) {
	if e.Stage != Ready {
		return nil, errors.New("cannot create input from non-ready entry")
	}
	return &validator.ValidationInput{
		Id:            uint64(e.Pos),
		HasDelayedMsg: e.HasDelayedMsg,
		DelayedMsgNr:  e.DelayedMsgNr,
		Preimages:     e.Preimages,
		BatchInfo:     e.BatchInfo,
		DelayedMsg:    e.DelayedMsg,
		StartState:    e.Start,
	}, nil
}

func newValidationEntry(
	pos arbutil.MessageIndex,
	start validator.GoGlobalState,
	end validator.GoGlobalState,
	msg *arbostypes.MessageWithMetadata,
	batch []byte,
	prevDelayed uint64,
) (*validationEntry, error) {
	batchInfo := validator.BatchInfo{
		Number: start.Batch,
		Data:   batch,
	}
	hasDelayed := false
	var delayedNum uint64
	if msg.DelayedMessagesRead == prevDelayed+1 {
		hasDelayed = true
		delayedNum = prevDelayed
	} else if msg.DelayedMessagesRead != prevDelayed {
		return nil, fmt.Errorf("illegal validation entry delayedMessage %d, previous %d", msg.DelayedMessagesRead, prevDelayed)
	}
	return &validationEntry{
		Stage:         ReadyForRecord,
		Pos:           pos,
		Start:         start,
		End:           end,
		HasDelayedMsg: hasDelayed,
		DelayedMsgNr:  delayedNum,
		msg:           msg,
		BatchInfo:     []validator.BatchInfo{batchInfo},
	}, nil
}

func NewStatelessBlockValidator(
	inboxReader InboxReaderInterface,
	inbox InboxTrackerInterface,
	streamer TransactionStreamerInterface,
	recorder execution.ExecutionRecorder,
	arbdb ethdb.Database,
	das arbstate.DataAvailabilityReader,
	config func() *BlockValidatorConfig,
	stack *node.Node,
) (*StatelessBlockValidator, error) {
	valConfFetcher := func() *rpcclient.ClientConfig { return &config().ValidationServer }
	valClient := server_api.NewValidationClient(valConfFetcher, stack)
	execClient := server_api.NewExecutionClient(valConfFetcher, stack)
	validator := &StatelessBlockValidator{
		config:             config(),
		execSpawner:        execClient,
		recorder:           recorder,
		validationSpawners: []validator.ValidationSpawner{valClient},
		inboxReader:        inboxReader,
		inboxTracker:       inbox,
		streamer:           streamer,
		db:                 arbdb,
		daService:          das,
	}
	return validator, nil
}

func (v *StatelessBlockValidator) GetModuleRootsToValidate() []common.Hash {
	v.moduleMutex.Lock()
	defer v.moduleMutex.Unlock()

	validatingModuleRoots := []common.Hash{v.currentWasmModuleRoot}
	if (v.currentWasmModuleRoot != v.pendingWasmModuleRoot && v.pendingWasmModuleRoot != common.Hash{}) {
		validatingModuleRoots = append(validatingModuleRoots, v.pendingWasmModuleRoot)
	}
	return validatingModuleRoots
}

func (v *StatelessBlockValidator) ValidationEntryRecord(ctx context.Context, e *validationEntry) error {
	if e.Stage != ReadyForRecord {
		return fmt.Errorf("validation entry should be ReadyForRecord, is: %v", e.Stage)
	}
	e.Preimages = make(map[arbutil.PreimageType]map[common.Hash][]byte)
	if e.Pos != 0 {
		recording, err := v.recorder.RecordBlockCreation(ctx, e.Pos, e.msg)
		if err != nil {
			return err
		}
		if recording.BlockHash != e.End.BlockHash {
			return fmt.Errorf("recording failed: pos %d, hash expected %v, got %v", e.Pos, e.End.BlockHash, recording.BlockHash)
		}
		e.BatchInfo = append(e.BatchInfo, recording.BatchInfo...)

		if recording.Preimages != nil {
			e.Preimages[arbutil.Keccak256PreimageType] = recording.Preimages
		}
	}
	if e.HasDelayedMsg {
		delayedMsg, err := v.inboxTracker.GetDelayedMessageBytes(e.DelayedMsgNr)
		if err != nil {
			log.Error(
				"error while trying to read delayed msg for proving",
				"err", err, "seq", e.DelayedMsgNr, "pos", e.Pos,
			)
			return fmt.Errorf("error while trying to read delayed msg for proving: %w", err)
		}
		e.DelayedMsg = delayedMsg
	}
	for _, batch := range e.BatchInfo {
		if len(batch.Data) <= 40 {
			continue
		}
		if !arbstate.IsDASMessageHeaderByte(batch.Data[40]) {
			continue
		}
		if v.daService == nil {
			log.Warn("No DAS configured, but sequencer message found with DAS header")
		} else {
			_, err := arbstate.RecoverPayloadFromDasBatch(
				ctx, batch.Number, batch.Data, v.daService, e.Preimages, arbstate.KeysetValidate,
			)
			if err != nil {
				return err
			}
		}
	}

	e.msg = nil // no longer needed
	e.Stage = Ready
	return nil
}

func buildGlobalState(res execution.MessageResult, pos GlobalStatePosition) validator.GoGlobalState {
	return validator.GoGlobalState{
		BlockHash:  res.BlockHash,
		SendRoot:   res.SendRoot,
		Batch:      pos.BatchNumber,
		PosInBatch: pos.PosInBatch,
	}
}

// return the globalState position before and after processing message at the specified count
func (v *StatelessBlockValidator) GlobalStatePositionsAtCount(count arbutil.MessageIndex) (GlobalStatePosition, GlobalStatePosition, error) {
	if count == 0 {
		return GlobalStatePosition{}, GlobalStatePosition{}, errors.New("no initial state for count==0")
	}
	if count == 1 {
		return GlobalStatePosition{}, GlobalStatePosition{1, 0}, nil
	}
	batchCount, err := v.inboxTracker.GetBatchCount()
	if err != nil {
		return GlobalStatePosition{}, GlobalStatePosition{}, err
	}
	batch, err := FindBatchContainingMessageIndex(v.inboxTracker, count-1, batchCount)
	if err != nil {
		return GlobalStatePosition{}, GlobalStatePosition{}, err
	}
	return GlobalStatePositionsAtCount(v.inboxTracker, count, batch)
}

func (v *StatelessBlockValidator) CreateReadyValidationEntry(ctx context.Context, pos arbutil.MessageIndex) (*validationEntry, error) {
	msg, err := v.streamer.GetMessage(pos)
	if err != nil {
		return nil, err
	}
	result, err := v.streamer.ResultAtCount(pos + 1)
	if err != nil {
		return nil, err
	}
	var prevDelayed uint64
	if pos > 0 {
		prev, err := v.streamer.GetMessage(pos - 1)
		if err != nil {
			return nil, err
		}
		prevDelayed = prev.DelayedMessagesRead
	}
	prevResult, err := v.streamer.ResultAtCount(pos)
	if err != nil {
		return nil, err
	}
	startPos, endPos, err := v.GlobalStatePositionsAtCount(pos + 1)
	if err != nil {
		return nil, fmt.Errorf("failed calculating position for validation: %w", err)
	}
	start := buildGlobalState(*prevResult, startPos)
	end := buildGlobalState(*result, endPos)
	seqMsg, err := v.inboxReader.GetSequencerMessageBytes(ctx, startPos.BatchNumber)
	if err != nil {
		return nil, err
	}
	entry, err := newValidationEntry(pos, start, end, msg, seqMsg, prevDelayed)
	if err != nil {
		return nil, err
	}
	err = v.ValidationEntryRecord(ctx, entry)
	if err != nil {
		return nil, err
	}

	return entry, nil
}

func (v *StatelessBlockValidator) ValidateResult(
	ctx context.Context, pos arbutil.MessageIndex, useExec bool, moduleRoot common.Hash,
) (bool, *validator.GoGlobalState, error) {
	entry, err := v.CreateReadyValidationEntry(ctx, pos)
	if err != nil {
		return false, nil, err
	}
	input, err := entry.ToInput()
	if err != nil {
		return false, nil, err
	}
	var spawners []validator.ValidationSpawner
	if useExec {
		spawners = append(spawners, v.execSpawner)
	} else {
		spawners = v.validationSpawners
	}
	if len(spawners) == 0 {
		return false, &entry.End, errors.New("no validation defined")
	}
	var runs []validator.ValidationRun
	for _, spawner := range spawners {
		run := spawner.Launch(input, moduleRoot)
		runs = append(runs, run)
	}
	defer func() {
		for _, run := range runs {
			run.Cancel()
		}
	}()
	for _, run := range runs {
		gsEnd, err := run.Await(ctx)
		if err != nil || gsEnd != entry.End {
			return false, &gsEnd, err
		}
	}
	return true, &entry.End, nil
}

func (v *StatelessBlockValidator) OverrideRecorder(t *testing.T, recorder execution.ExecutionRecorder) {
	v.recorder = recorder
}

func (v *StatelessBlockValidator) Start(ctx_in context.Context) error {
	err := v.execSpawner.Start(ctx_in)
	if err != nil {
		return err
	}
	for _, spawner := range v.validationSpawners {
		if err := spawner.Start(ctx_in); err != nil {
			return err
		}
	}
	if v.config.PendingUpgradeModuleRoot != "" {
		if v.config.PendingUpgradeModuleRoot == "latest" {
			latest, err := v.execSpawner.LatestWasmModuleRoot().Await(ctx_in)
			if err != nil {
				return err
			}
			v.pendingWasmModuleRoot = latest
		} else {
			valid, _ := regexp.MatchString("(0x)?[0-9a-fA-F]{64}", v.config.PendingUpgradeModuleRoot)
			v.pendingWasmModuleRoot = common.HexToHash(v.config.PendingUpgradeModuleRoot)
			if (!valid || v.pendingWasmModuleRoot == common.Hash{}) {
				return errors.New("pending-upgrade-module-root config value illegal")
			}
		}
	}
	return nil
}

func (v *StatelessBlockValidator) Stop() {
	v.execSpawner.Stop()
	for _, spawner := range v.validationSpawners {
		spawner.Stop()
	}
}

'''
'''--- staker/txbuilder/builder.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package txbuilder

import (
	"context"
	"math/big"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbutil"
)

type ValidatorWalletInterface interface {
	// Address must be able to be called concurrently with other functions
	Address() *common.Address
	L1Client() arbutil.L1Interface
	TestTransactions(context.Context, []*types.Transaction) error
	ExecuteTransactions(context.Context, *Builder, common.Address) (*types.Transaction, error)
	AuthIfEoa() *bind.TransactOpts
}

// Builder combines any transactions sent to it via SendTransaction into one batch,
// which is then sent to the validator wallet.
// This lets the validator make multiple atomic transactions.
// This inherits from an eth client so it can be used as an L1Interface,
// where it transparently intercepts calls to SendTransaction and queues them for the next batch.
type Builder struct {
	arbutil.L1Interface
	transactions []*types.Transaction
	builderAuth  *bind.TransactOpts
	isAuthFake   bool
	wallet       ValidatorWalletInterface
}

func NewBuilder(wallet ValidatorWalletInterface) (*Builder, error) {
	randKey, err := crypto.GenerateKey()
	if err != nil {
		return nil, err
	}
	builderAuth := wallet.AuthIfEoa()
	var isAuthFake bool
	if builderAuth == nil {
		// Make a fake auth so we have txs to give to the smart contract wallet
		builderAuth, err = bind.NewKeyedTransactorWithChainID(randKey, big.NewInt(9999999))
		if err != nil {
			return nil, err
		}
		isAuthFake = true
	}
	return &Builder{
		builderAuth: builderAuth,
		wallet:      wallet,
		L1Interface: wallet.L1Client(),
		isAuthFake:  isAuthFake,
	}, nil
}

func (b *Builder) BuildingTransactionCount() int {
	return len(b.transactions)
}

func (b *Builder) ClearTransactions() {
	b.transactions = nil
}

func (b *Builder) EstimateGas(ctx context.Context, call ethereum.CallMsg) (gas uint64, err error) {
	if len(b.transactions) == 0 && !b.isAuthFake {
		return b.L1Interface.EstimateGas(ctx, call)
	}
	return 0, nil
}

func (b *Builder) SendTransaction(ctx context.Context, tx *types.Transaction) error {
	b.transactions = append(b.transactions, tx)
	err := b.wallet.TestTransactions(ctx, b.transactions)
	if err != nil {
		// Remove the bad tx
		b.transactions = b.transactions[:len(b.transactions)-1]
		return err
	}
	return nil
}

// While this is not currently required, it's recommended not to reuse the returned auth for multiple transactions,
// as for an EOA this has the nonce in it. However, the EOA wwallet currently will only publish the first created tx,
// which is why that doesn't really matter.
func (b *Builder) AuthWithAmount(ctx context.Context, amount *big.Int) (*bind.TransactOpts, error) {
	nonce, err := b.NonceAt(ctx, b.builderAuth.From, nil)
	if err != nil {
		return nil, err
	}
	return &bind.TransactOpts{
		From:     b.builderAuth.From,
		Nonce:    new(big.Int).SetUint64(nonce),
		Signer:   b.builderAuth.Signer,
		Value:    amount,
		GasPrice: b.builderAuth.GasPrice,
		GasLimit: b.builderAuth.GasLimit,
		Context:  ctx,
	}, nil
}

// Auth is the same as AuthWithAmount with a 0 amount specified.
// See AuthWithAmount docs for important details.
func (b *Builder) Auth(ctx context.Context) (*bind.TransactOpts, error) {
	return b.AuthWithAmount(ctx, common.Big0)
}

func (b *Builder) Transactions() []*types.Transaction {
	return b.transactions
}

// Auth is the same as AuthWithAmount with a 0 amount specified.
// See AuthWithAmount docs for important details.
func (b *Builder) BuilderAuth() *bind.TransactOpts {
	return b.builderAuth
}

func (b *Builder) WalletAddress() *common.Address {
	return b.wallet.Address()
}

'''
'''--- staker/validatorwallet/contract.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package validatorwallet

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"strings"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/staker/txbuilder"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
)

var validatorABI abi.ABI
var walletCreatedID common.Hash

func init() {
	parsedValidator, err := abi.JSON(strings.NewReader(rollupgen.ValidatorWalletABI))
	if err != nil {
		panic(err)
	}
	validatorABI = parsedValidator

	parsedValidatorWalletCreator, err := abi.JSON(strings.NewReader(rollupgen.ValidatorWalletCreatorABI))
	if err != nil {
		panic(err)
	}
	walletCreatedID = parsedValidatorWalletCreator.Events["WalletCreated"].ID
}

type Contract struct {
	con                     *rollupgen.ValidatorWallet
	address                 atomic.Pointer[common.Address]
	onWalletCreated         func(common.Address)
	l1Reader                *headerreader.HeaderReader
	auth                    *bind.TransactOpts
	walletFactoryAddr       common.Address
	rollupFromBlock         int64
	rollup                  *rollupgen.RollupUserLogic
	rollupAddress           common.Address
	challengeManagerAddress common.Address
	dataPoster              *dataposter.DataPoster
	getExtraGas             func() uint64
}

func NewContract(dp *dataposter.DataPoster, address *common.Address, walletFactoryAddr, rollupAddress common.Address, l1Reader *headerreader.HeaderReader, auth *bind.TransactOpts, rollupFromBlock int64, onWalletCreated func(common.Address),
	getExtraGas func() uint64) (*Contract, error) {
	var con *rollupgen.ValidatorWallet
	if address != nil {
		var err error
		con, err = rollupgen.NewValidatorWallet(*address, l1Reader.Client())
		if err != nil {
			return nil, err
		}
	}
	rollup, err := rollupgen.NewRollupUserLogic(rollupAddress, l1Reader.Client())
	if err != nil {
		return nil, err
	}
	wallet := &Contract{
		con:               con,
		onWalletCreated:   onWalletCreated,
		l1Reader:          l1Reader,
		auth:              auth,
		walletFactoryAddr: walletFactoryAddr,
		rollupAddress:     rollupAddress,
		rollup:            rollup,
		rollupFromBlock:   rollupFromBlock,
		dataPoster:        dp,
		getExtraGas:       getExtraGas,
	}
	// Go complains if we make an address variable before wallet and copy it in
	wallet.address.Store(address)
	return wallet, nil
}

func (v *Contract) validateWallet(ctx context.Context) error {
	if v.con == nil || v.auth == nil {
		return nil
	}
	callOpts := &bind.CallOpts{Context: ctx}
	owner, err := v.con.Owner(callOpts)
	if err != nil {
		return err
	}
	isExecutor, err := v.con.Executors(callOpts, v.auth.From)
	if err != nil {
		return err
	}
	if v.auth.From != owner && !isExecutor {
		return errors.New("specified unauthorized smart contract wallet")
	}
	return nil
}

func (v *Contract) Initialize(ctx context.Context) error {
	err := v.populateWallet(ctx, false)
	if err != nil {
		return err
	}
	err = v.validateWallet(ctx)
	if err != nil {
		return err
	}
	callOpts := &bind.CallOpts{Context: ctx}
	v.challengeManagerAddress, err = v.rollup.ChallengeManager(callOpts)
	return err
}

// May be the nil if the wallet hasn't been deployed yet
func (v *Contract) Address() *common.Address {
	return v.address.Load()
}

// May be zero if the wallet hasn't been deployed yet
func (v *Contract) AddressOrZero() common.Address {
	addr := v.address.Load()
	if addr == nil {
		return common.Address{}
	}
	return *addr
}

func (v *Contract) TxSenderAddress() *common.Address {
	if v.auth == nil {
		return nil
	}
	return &v.auth.From
}

func (v *Contract) From() common.Address {
	if v.auth == nil {
		return common.Address{}
	}
	return v.auth.From
}

// nil value == 0 value
func (v *Contract) getAuth(ctx context.Context, value *big.Int) (*bind.TransactOpts, error) {
	newAuth := *v.auth
	newAuth.Context = ctx
	newAuth.Value = value
	nonce, err := v.L1Client().NonceAt(ctx, v.auth.From, nil)
	if err != nil {
		return nil, err
	}
	newAuth.Nonce = new(big.Int).SetUint64(nonce)
	return &newAuth, nil
}

func (v *Contract) executeTransaction(ctx context.Context, tx *types.Transaction, gasRefunder common.Address) (*types.Transaction, error) {
	auth, err := v.getAuth(ctx, tx.Value())
	if err != nil {
		return nil, err
	}
	data, err := validatorABI.Pack("executeTransactionWithGasRefunder", gasRefunder, tx.Data(), *tx.To(), tx.Value())
	if err != nil {
		return nil, fmt.Errorf("packing arguments for executeTransactionWithGasRefunder: %w", err)
	}
	gas, err := v.gasForTxData(ctx, auth, data)
	if err != nil {
		return nil, fmt.Errorf("getting gas for tx data: %w", err)
	}
	return v.dataPoster.PostTransaction(ctx, time.Now(), auth.Nonce.Uint64(), nil, *v.Address(), data, gas, auth.Value, nil)
}

func (v *Contract) populateWallet(ctx context.Context, createIfMissing bool) error {
	if v.con != nil {
		return nil
	}
	if v.auth == nil {
		if createIfMissing {
			return errors.New("cannot create validator smart contract wallet without key wallet")
		}
		return nil
	}
	if v.address.Load() == nil {
		auth, err := v.getAuth(ctx, nil)
		if err != nil {
			return err
		}
		addr, err := GetValidatorWalletContract(ctx, v.walletFactoryAddr, v.rollupFromBlock, auth, v.l1Reader, createIfMissing)
		if err != nil {
			return err
		}
		if addr == nil {
			return nil
		}
		v.address.Store(addr)
		if v.onWalletCreated != nil {
			v.onWalletCreated(*addr)
		}
	}
	con, err := rollupgen.NewValidatorWallet(*v.Address(), v.l1Reader.Client())
	if err != nil {
		return err
	}
	v.con = con

	if err := v.validateWallet(ctx); err != nil {
		return err
	}
	return nil
}

func combineTxes(txes []*types.Transaction) ([][]byte, []common.Address, []*big.Int, *big.Int) {
	totalAmount := big.NewInt(0)
	data := make([][]byte, 0, len(txes))
	dest := make([]common.Address, 0, len(txes))
	amount := make([]*big.Int, 0, len(txes))

	for _, tx := range txes {
		data = append(data, tx.Data())
		dest = append(dest, *tx.To())
		amount = append(amount, tx.Value())
		totalAmount = totalAmount.Add(totalAmount, tx.Value())
	}
	return data, dest, amount, totalAmount
}

// Not thread safe! Don't call this from multiple threads at the same time.
func (v *Contract) ExecuteTransactions(ctx context.Context, builder *txbuilder.Builder, gasRefunder common.Address) (*types.Transaction, error) {
	txes := builder.Transactions()
	if len(txes) == 0 {
		return nil, nil
	}

	err := v.populateWallet(ctx, true)
	if err != nil {
		return nil, err
	}

	if len(txes) == 1 {
		arbTx, err := v.executeTransaction(ctx, txes[0], gasRefunder)
		if err != nil {
			return nil, err
		}
		builder.ClearTransactions()
		return arbTx, nil
	}

	totalAmount := big.NewInt(0)
	data := make([][]byte, 0, len(txes))
	dest := make([]common.Address, 0, len(txes))
	amount := make([]*big.Int, 0, len(txes))

	for _, tx := range txes {
		data = append(data, tx.Data())
		dest = append(dest, *tx.To())
		amount = append(amount, tx.Value())
		totalAmount = totalAmount.Add(totalAmount, tx.Value())
	}

	balanceInContract, err := v.l1Reader.Client().BalanceAt(ctx, *v.Address(), nil)
	if err != nil {
		return nil, err
	}

	callValue := new(big.Int).Sub(totalAmount, balanceInContract)
	if callValue.Sign() < 0 {
		callValue.SetInt64(0)
	}
	auth, err := v.getAuth(ctx, callValue)
	if err != nil {
		return nil, err
	}
	txData, err := validatorABI.Pack("executeTransactionsWithGasRefunder", gasRefunder, data, dest, amount)
	if err != nil {
		return nil, fmt.Errorf("packing arguments for executeTransactionWithGasRefunder: %w", err)
	}
	gas, err := v.gasForTxData(ctx, auth, txData)
	if err != nil {
		return nil, fmt.Errorf("getting gas for tx data: %w", err)
	}
	arbTx, err := v.dataPoster.PostTransaction(ctx, time.Now(), auth.Nonce.Uint64(), nil, *v.Address(), txData, gas, auth.Value, nil)
	if err != nil {
		return nil, err
	}
	builder.ClearTransactions()
	return arbTx, nil
}

func (v *Contract) estimateGas(ctx context.Context, value *big.Int, data []byte) (uint64, error) {
	h, err := v.l1Reader.LastHeader(ctx)
	if err != nil {
		return 0, fmt.Errorf("getting the last header: %w", err)
	}
	gasFeeCap := new(big.Int).Mul(h.BaseFee, big.NewInt(2))
	gasFeeCap = arbmath.BigMax(gasFeeCap, arbmath.FloatToBig(params.GWei))

	gasTipCap, err := v.l1Reader.Client().SuggestGasTipCap(ctx)
	if err != nil {
		return 0, fmt.Errorf("getting suggested gas tip cap: %w", err)
	}
	g, err := v.l1Reader.Client().EstimateGas(
		ctx,
		ethereum.CallMsg{
			From:      v.auth.From,
			To:        v.Address(),
			Value:     value,
			Data:      data,
			GasFeeCap: gasFeeCap,
			GasTipCap: gasTipCap,
		},
	)
	if err != nil {
		return 0, fmt.Errorf("estimating gas: %w", err)
	}
	return g + v.getExtraGas(), nil
}

func (v *Contract) TimeoutChallenges(ctx context.Context, challenges []uint64) (*types.Transaction, error) {
	auth, err := v.getAuth(ctx, nil)
	if err != nil {
		return nil, err
	}
	data, err := validatorABI.Pack("timeoutChallenges", v.challengeManagerAddress, challenges)
	if err != nil {
		return nil, fmt.Errorf("packing arguments for timeoutChallenges: %w", err)
	}
	gas, err := v.gasForTxData(ctx, auth, data)
	if err != nil {
		return nil, fmt.Errorf("getting gas for tx data: %w", err)
	}
	return v.dataPoster.PostTransaction(ctx, time.Now(), auth.Nonce.Uint64(), nil, *v.Address(), data, gas, auth.Value, nil)
}

// gasForTxData returns auth.GasLimit if it's nonzero, otherwise returns estimate.
func (v *Contract) gasForTxData(ctx context.Context, auth *bind.TransactOpts, data []byte) (uint64, error) {
	if auth.GasLimit != 0 {
		return auth.GasLimit, nil
	}
	return v.estimateGas(ctx, auth.Value, data)
}

func (v *Contract) L1Client() arbutil.L1Interface {
	return v.l1Reader.Client()
}

func (v *Contract) RollupAddress() common.Address {
	return v.rollupAddress
}

func (v *Contract) ChallengeManagerAddress() common.Address {
	return v.challengeManagerAddress
}

func (v *Contract) TestTransactions(ctx context.Context, txs []*types.Transaction) error {
	if v.Address() == nil {
		return nil
	}
	data, dest, amount, totalAmount := combineTxes(txs)
	realData, err := validatorABI.Pack("executeTransactions", data, dest, amount)
	if err != nil {
		return err
	}
	msg := ethereum.CallMsg{
		From:  v.From(),
		To:    v.Address(),
		Value: totalAmount,
		Data:  realData,
	}
	_, err = v.L1Client().PendingCallContract(ctx, msg)
	return err
}

func (v *Contract) CanBatchTxs() bool {
	return true
}

func (v *Contract) AuthIfEoa() *bind.TransactOpts {
	return nil
}

func (w *Contract) Start(ctx context.Context) {
	w.dataPoster.Start(ctx)
}

func (b *Contract) StopAndWait() {
	b.dataPoster.StopAndWait()
}

func (b *Contract) DataPoster() *dataposter.DataPoster {
	return b.dataPoster
}

func GetValidatorWalletContract(
	ctx context.Context,
	validatorWalletFactoryAddr common.Address,
	fromBlock int64,
	transactAuth *bind.TransactOpts,
	l1Reader *headerreader.HeaderReader,
	createIfMissing bool,
) (*common.Address, error) {
	client := l1Reader.Client()

	// TODO: If we just save a mapping in the wallet creator we won't need log search
	walletCreator, err := rollupgen.NewValidatorWalletCreator(validatorWalletFactoryAddr, client)
	if err != nil {
		return nil, err
	}
	query := ethereum.FilterQuery{
		BlockHash: nil,
		FromBlock: big.NewInt(fromBlock),
		ToBlock:   nil,
		Addresses: []common.Address{validatorWalletFactoryAddr},
		Topics:    [][]common.Hash{{walletCreatedID}, nil, {transactAuth.From.Hash()}},
	}
	logs, err := client.FilterLogs(ctx, query)
	if err != nil {
		return nil, err
	}
	if len(logs) > 1 {
		return nil, errors.New("more than one validator wallet created for address")
	}
	if len(logs) == 1 {
		rawLog := logs[0]
		parsed, err := walletCreator.ParseWalletCreated(rawLog)
		if err != nil {
			return nil, err
		}
		log.Info("found validator smart contract wallet", "address", parsed.WalletAddress)
		return &parsed.WalletAddress, err
	}

	if !createIfMissing {
		return nil, nil
	}

	var initialExecutorAllowedDests []common.Address
	tx, err := walletCreator.CreateWallet(transactAuth, initialExecutorAllowedDests)
	if err != nil {
		return nil, err
	}

	receipt, err := l1Reader.WaitForTxApproval(ctx, tx)
	if err != nil {
		return nil, err
	}
	ev, err := walletCreator.ParseWalletCreated(*receipt.Logs[len(receipt.Logs)-1])
	if err != nil {
		return nil, err
	}
	log.Info("created validator smart contract wallet", "address", ev.WalletAddress)
	return &ev.WalletAddress, nil
}

'''
'''--- staker/validatorwallet/eoa.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package validatorwallet

import (
	"context"
	"fmt"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/staker/txbuilder"
)

type EOA struct {
	auth                    *bind.TransactOpts
	client                  arbutil.L1Interface
	rollupAddress           common.Address
	challengeManager        *challengegen.ChallengeManager
	challengeManagerAddress common.Address
	dataPoster              *dataposter.DataPoster
	getExtraGas             func() uint64
}

func NewEOA(dataPoster *dataposter.DataPoster, rollupAddress common.Address, l1Client arbutil.L1Interface, auth *bind.TransactOpts, getExtraGas func() uint64) (*EOA, error) {
	return &EOA{
		auth:          auth,
		client:        l1Client,
		rollupAddress: rollupAddress,
		dataPoster:    dataPoster,
		getExtraGas:   getExtraGas,
	}, nil
}

func (w *EOA) Initialize(ctx context.Context) error {
	rollup, err := rollupgen.NewRollupUserLogic(w.rollupAddress, w.client)
	if err != nil {
		return err
	}
	callOpts := &bind.CallOpts{Context: ctx}
	w.challengeManagerAddress, err = rollup.ChallengeManager(callOpts)
	if err != nil {
		return err
	}
	w.challengeManager, err = challengegen.NewChallengeManager(w.challengeManagerAddress, w.client)
	return err
}

func (w *EOA) Address() *common.Address {
	return &w.auth.From
}

func (w *EOA) AddressOrZero() common.Address {
	return w.auth.From
}

func (w *EOA) TxSenderAddress() *common.Address {
	return &w.auth.From
}

func (w *EOA) L1Client() arbutil.L1Interface {
	return w.client
}

func (w *EOA) RollupAddress() common.Address {
	return w.rollupAddress
}

func (w *EOA) ChallengeManagerAddress() common.Address {
	return w.challengeManagerAddress
}

func (w *EOA) TestTransactions(context.Context, []*types.Transaction) error {
	// We only use the first tx which is checked implicitly by gas estimation
	return nil
}

func (w *EOA) ExecuteTransactions(ctx context.Context, builder *txbuilder.Builder, _ common.Address) (*types.Transaction, error) {
	if len(builder.Transactions()) == 0 {
		return nil, nil
	}
	tx := builder.Transactions()[0] // we ignore future txs and only execute the first
	return w.postTransaction(ctx, tx)
}

func (w *EOA) postTransaction(ctx context.Context, baseTx *types.Transaction) (*types.Transaction, error) {
	nonce, err := w.L1Client().NonceAt(ctx, w.auth.From, nil)
	if err != nil {
		return nil, err
	}
	gas := baseTx.Gas() + w.getExtraGas()
	newTx, err := w.dataPoster.PostTransaction(ctx, time.Now(), nonce, nil, *baseTx.To(), baseTx.Data(), gas, baseTx.Value(), nil)
	if err != nil {
		return nil, fmt.Errorf("post transaction: %w", err)
	}
	return newTx, nil
}

func (w *EOA) TimeoutChallenges(ctx context.Context, timeouts []uint64) (*types.Transaction, error) {
	if len(timeouts) == 0 {
		return nil, nil
	}
	auth := *w.auth
	auth.Context = ctx
	auth.NoSend = true
	tx, err := w.challengeManager.Timeout(&auth, timeouts[0])
	if err != nil {
		return nil, err
	}
	return w.postTransaction(ctx, tx)
}

func (w *EOA) CanBatchTxs() bool {
	return false
}

func (w *EOA) AuthIfEoa() *bind.TransactOpts {
	return w.auth
}

func (w *EOA) Start(ctx context.Context) {
	w.dataPoster.Start(ctx)
}

func (b *EOA) StopAndWait() {
	b.dataPoster.StopAndWait()
}

func (b *EOA) DataPoster() *dataposter.DataPoster {
	return b.dataPoster
}

'''
'''--- staker/validatorwallet/noop.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package validatorwallet

import (
	"context"
	"errors"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbnode/dataposter"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/staker/txbuilder"
)

// NoOp validator wallet is used for watchtower mode.
type NoOp struct {
	l1Client      arbutil.L1Interface
	rollupAddress common.Address
}

func NewNoOp(l1Client arbutil.L1Interface, rollupAddress common.Address) *NoOp {
	return &NoOp{
		l1Client:      l1Client,
		rollupAddress: rollupAddress,
	}
}

func (*NoOp) Initialize(context.Context) error { return nil }

func (*NoOp) Address() *common.Address { return nil }

func (*NoOp) AddressOrZero() common.Address { return common.Address{} }

func (*NoOp) TxSenderAddress() *common.Address { return nil }

func (*NoOp) From() common.Address { return common.Address{} }

func (*NoOp) ExecuteTransactions(context.Context, *txbuilder.Builder, common.Address) (*types.Transaction, error) {
	return nil, errors.New("no op validator wallet cannot execute transactions")
}

func (*NoOp) TimeoutChallenges(ctx context.Context, challenges []uint64) (*types.Transaction, error) {
	return nil, errors.New("no op validator wallet cannot timeout challenges")
}

func (n *NoOp) L1Client() arbutil.L1Interface { return n.l1Client }

func (n *NoOp) RollupAddress() common.Address { return n.rollupAddress }

func (*NoOp) ChallengeManagerAddress() common.Address { return common.Address{} }

func (*NoOp) TestTransactions(ctx context.Context, txs []*types.Transaction) error {
	return nil
}

func (*NoOp) CanBatchTxs() bool { return false }

func (*NoOp) AuthIfEoa() *bind.TransactOpts { return nil }

func (w *NoOp) Start(ctx context.Context) {}

func (b *NoOp) StopAndWait() {}

func (b *NoOp) DataPoster() *dataposter.DataPoster { return nil }

'''
'''--- statetransfer/data.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package statetransfer

import (
	"math/big"

	"github.com/ethereum/go-ethereum/common"
)

type ArbosInitializationInfo struct {
	NextBlockNumber      uint64
	AddressTableContents []common.Address
	RetryableData        []InitializationDataForRetryable
	Accounts             []AccountInitializationInfo
}

type InitializationDataForRetryable struct {
	Id          common.Hash
	Timeout     uint64
	From        common.Address
	To          common.Address
	Callvalue   *big.Int
	Beneficiary common.Address
	Calldata    []byte
}

type AccountInitializationInfo struct {
	Addr            common.Address
	Nonce           uint64
	EthBalance      *big.Int
	ContractInfo    *AccountInitContractInfo
	AggregatorInfo  *AccountInitAggregatorInfo
	AggregatorToPay *common.Address
	ClassicHash     common.Hash
}

type AccountInitContractInfo struct {
	Code            []byte
	ContractStorage map[common.Hash]common.Hash
}

type AccountInitAggregatorInfo struct {
	FeeCollector common.Address
	BaseFeeL1Gas *big.Int // This is unused in Nitro, so its value will be ignored.
}

'''
'''--- statetransfer/interface.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package statetransfer

import (
	"errors"

	"github.com/ethereum/go-ethereum/common"
)

var errNoMore = errors.New("no more elements")

type InitDataReader interface {
	Close() error
	GetAddressTableReader() (AddressReader, error)
	GetNextBlockNumber() (uint64, error)
	GetRetryableDataReader() (RetryableDataReader, error)
	GetAccountDataReader() (AccountDataReader, error)
}

type ListReader interface {
	More() bool
	Close() error
}

type AddressReader interface {
	ListReader
	GetNext() (*common.Address, error)
}

type RetryableDataReader interface {
	ListReader
	GetNext() (*InitializationDataForRetryable, error)
}

type AccountDataReader interface {
	ListReader
	GetNext() (*AccountInitializationInfo, error)
}

'''
'''--- statetransfer/jsondatareader.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package statetransfer

import (
	"encoding/json"
	"fmt"
	"math/big"
	"os"
	"path"

	"github.com/ethereum/go-ethereum/common"
)

type ArbosInitFileContents struct {
	NextBlockNumber          uint64 `json:"NextBlockNumber"`
	AddressTableContentsPath string `json:"AddressTableContentsPath"`
	RetryableDataPath        string `json:"RetryableDataPath"`
	AccountsPath             string `json:"AccountsPath"`
}

type JsonInitDataReader struct {
	basePath string
	data     ArbosInitFileContents
}

func (r *JsonInitDataReader) GetNextBlockNumber() (uint64, error) {
	return r.data.NextBlockNumber, nil
}

type JsonListReader struct {
	input *json.Decoder
	file  *os.File
}

func (l *JsonListReader) More() bool {
	if l.input == nil {
		return false
	}
	return l.input.More()
}

func (l *JsonListReader) Close() error {
	l.input = nil
	if l.file != nil {
		if err := l.file.Close(); err != nil {
			return err
		}
		l.file = nil
	}
	return nil
}

func (r *JsonInitDataReader) getListReader(fileName string) (JsonListReader, error) {
	if fileName == "" {
		return JsonListReader{}, nil
	}
	filePath := path.Join(r.basePath, fileName)
	inboundFile, err := os.OpenFile(filePath, os.O_RDONLY, 0664)
	if err != nil {
		return JsonListReader{}, err
	}
	res := JsonListReader{
		file:  inboundFile,
		input: json.NewDecoder(inboundFile),
	}
	return res, nil
}

func NewJsonInitDataReader(filepath string) (InitDataReader, error) {
	data, err := os.ReadFile(filepath)
	if err != nil {
		return nil, err
	}
	reader := JsonInitDataReader{
		basePath: path.Dir(filepath),
	}
	if err := json.Unmarshal(data, &reader.data); err != nil {
		return nil, err
	}
	return &reader, nil
}

func (r *JsonInitDataReader) Close() error {
	return nil
}

type JsonRetryableDataReader struct {
	JsonListReader
}

type InitializationDataForRetryableJson struct {
	Id          common.Hash
	Timeout     uint64
	From        common.Address
	To          common.Address
	Callvalue   string
	Beneficiary common.Address
	Calldata    []byte
}

func stringToBig(input string) (*big.Int, error) {
	output, success := new(big.Int).SetString(input, 0)
	if !success {
		return nil, fmt.Errorf("%s cannot be parsed as big.Int", input)
	}
	return output, nil
}

func (r *JsonRetryableDataReader) GetNext() (*InitializationDataForRetryable, error) {
	if !r.More() {
		return nil, errNoMore
	}
	var elem InitializationDataForRetryableJson
	if err := r.input.Decode(&elem); err != nil {
		return nil, fmt.Errorf("decoding retryable: %w", err)
	}
	callValueBig, err := stringToBig(elem.Callvalue)
	if err != nil {
		return nil, err
	}
	return &InitializationDataForRetryable{
		Id:          elem.Id,
		Timeout:     elem.Timeout,
		From:        elem.From,
		To:          elem.To,
		Callvalue:   callValueBig,
		Beneficiary: elem.Beneficiary,
		Calldata:    elem.Calldata,
	}, nil
}

func (r *JsonInitDataReader) GetRetryableDataReader() (RetryableDataReader, error) {
	listreader, err := r.getListReader(r.data.RetryableDataPath)
	if err != nil {
		return nil, err
	}
	return &JsonRetryableDataReader{
		JsonListReader: listreader,
	}, nil
}

type JsonAddressReader struct {
	JsonListReader
}

func (r *JsonAddressReader) GetNext() (*common.Address, error) {
	if !r.More() {
		return nil, errNoMore
	}
	var elem common.Address
	if err := r.input.Decode(&elem); err != nil {
		return nil, err
	}
	return &elem, nil
}

func (r *JsonInitDataReader) GetAddressTableReader() (AddressReader, error) {
	listreader, err := r.getListReader(r.data.AddressTableContentsPath)
	if err != nil {
		return nil, err
	}
	return &JsonAddressReader{
		JsonListReader: listreader,
	}, nil
}

type JsonAccountDataReaderr struct {
	JsonListReader
}

type AccountInitializationInfoJson struct {
	Addr         common.Address
	Nonce        uint64
	Balance      string
	ContractInfo *AccountInitContractInfo
	ClassicHash  common.Hash
}

func (r *JsonAccountDataReaderr) GetNext() (*AccountInitializationInfo, error) {
	if !r.More() {
		return nil, errNoMore
	}
	var elem AccountInitializationInfoJson
	if err := r.input.Decode(&elem); err != nil {
		return nil, err
	}
	balanceBig, err := stringToBig(elem.Balance)
	if err != nil {
		return nil, err
	}
	return &AccountInitializationInfo{
		Addr:            elem.Addr,
		Nonce:           elem.Nonce,
		EthBalance:      balanceBig,
		ContractInfo:    elem.ContractInfo,
		AggregatorInfo:  nil,
		AggregatorToPay: nil,
		ClassicHash:     elem.ClassicHash,
	}, nil
}

func (r *JsonInitDataReader) GetAccountDataReader() (AccountDataReader, error) {
	listreader, err := r.getListReader(r.data.AccountsPath)
	if err != nil {
		return nil, err
	}
	return &JsonAccountDataReaderr{
		JsonListReader: listreader,
	}, nil
}

'''
'''--- statetransfer/memdatareader.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package statetransfer

import (
	"github.com/ethereum/go-ethereum/common"
)

type MemoryInitDataReader struct {
	d *ArbosInitializationInfo
}

func NewMemoryInitDataReader(data *ArbosInitializationInfo) InitDataReader {
	return &MemoryInitDataReader{
		d: data,
	}
}

func (r *MemoryInitDataReader) GetNextBlockNumber() (uint64, error) {
	return r.d.NextBlockNumber, nil
}

type FieldReader struct {
	m      *MemoryInitDataReader
	count  int
	length int
}

func (f *FieldReader) More() bool {
	return f.count < f.length
}

func (f *FieldReader) Close() error {
	f.count = f.length
	return nil
}

type MemoryRetryableDataReader struct {
	FieldReader
}

func (r *MemoryRetryableDataReader) GetNext() (*InitializationDataForRetryable, error) {
	if !r.More() {
		return nil, errNoMore
	}
	r.count++
	return &r.m.d.RetryableData[r.count-1], nil
}

func (r *MemoryInitDataReader) GetRetryableDataReader() (RetryableDataReader, error) {
	return &MemoryRetryableDataReader{
		FieldReader: FieldReader{
			m:      r,
			length: len(r.d.RetryableData),
		},
	}, nil
}

type MemoryAddressReader struct {
	FieldReader
}

func (r *MemoryAddressReader) GetNext() (*common.Address, error) {
	if !r.More() {
		return nil, errNoMore
	}
	r.count++
	return &r.m.d.AddressTableContents[r.count-1], nil
}

func (r *MemoryInitDataReader) GetAddressTableReader() (AddressReader, error) {
	return &MemoryAddressReader{
		FieldReader: FieldReader{
			m:      r,
			length: len(r.d.AddressTableContents),
		},
	}, nil
}

type MemoryAccountDataReaderr struct {
	FieldReader
}

func (r *MemoryAccountDataReaderr) GetNext() (*AccountInitializationInfo, error) {
	if !r.More() {
		return nil, errNoMore
	}
	r.count++
	return &r.m.d.Accounts[r.count-1], nil
}

func (r *MemoryInitDataReader) GetAccountDataReader() (AccountDataReader, error) {
	return &MemoryAccountDataReaderr{
		FieldReader: FieldReader{
			m:      r,
			length: len(r.d.Accounts),
		},
	}, nil
}

func (r *MemoryInitDataReader) Close() error {
	return nil
}

'''
'''--- system_tests/aliasing_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"strings"
	"testing"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
)

func TestAliasing(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	user := builder.L1Info.GetDefaultTransactOpts("User", ctx)
	builder.L2.TransferBalanceTo(t, "Owner", util.RemapL1Address(user.From), big.NewInt(1e18), builder.L2Info)

	simpleAddr, simple := builder.L2.DeploySimple(t, auth)
	simpleContract, err := abi.JSON(strings.NewReader(mocksgen.SimpleABI))
	Require(t, err)

	// Test direct calls
	arbsys, err := precompilesgen.NewArbSys(types.ArbSysAddress, builder.L2.Client)
	Require(t, err)
	top, err := arbsys.IsTopLevelCall(nil)
	Require(t, err)
	was, err := arbsys.WasMyCallersAddressAliased(nil)
	Require(t, err)
	alias, err := arbsys.MyCallersAddressWithoutAliasing(nil)
	Require(t, err)
	if !top {
		Fatal(t, "direct call is not top level")
	}
	if was || alias != (common.Address{}) {
		Fatal(t, "direct call has an alias", was, alias)
	}

	testL2Signed := func(top, direct, static, delegate, callcode, call bool) {
		t.Helper()

		// check via L2
		tx, err := simple.CheckCalls(&auth, top, direct, static, delegate, callcode, call)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)

		// check signed txes via L1
		data, err := simpleContract.Pack("checkCalls", top, direct, static, delegate, callcode, call)
		Require(t, err)
		tx = builder.L2Info.PrepareTxTo("Owner", &simpleAddr, 500000, big.NewInt(0), data)
		builder.L1.SendSignedTx(t, builder.L2.Client, tx, builder.L1Info)
	}

	testUnsigned := func(top, direct, static, delegate, callcode, call bool) {
		t.Helper()

		// check unsigned txes via L1
		data, err := simpleContract.Pack("checkCalls", top, direct, static, delegate, callcode, call)
		Require(t, err)
		tx := builder.L2Info.PrepareTxTo("Owner", &simpleAddr, 500000, big.NewInt(0), data)
		builder.L1.SendUnsignedTx(t, builder.L2.Client, tx, builder.L1Info)
	}

	testL2Signed(true, true, false, false, false, false)
	testL2Signed(false, false, false, false, false, false)
	testUnsigned(true, true, false, false, false, false)
	testUnsigned(false, true, false, true, false, false)
}

'''
'''--- system_tests/arbtrace_test.go ---
package arbtest

import (
	"context"
	"encoding/json"
	"errors"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/rpc"
)

type callTxArgs struct {
	From       *common.Address `json:"from"`
	To         *common.Address `json:"to"`
	Gas        *hexutil.Uint64 `json:"gas"`
	GasPrice   *hexutil.Big    `json:"gasPrice"`
	Value      *hexutil.Big    `json:"value"`
	Data       *hexutil.Bytes  `json:"data"`
	Aggregator *common.Address `json:"aggregator"`
}
type traceAction struct {
	CallType string          `json:"callType,omitempty"`
	From     common.Address  `json:"from"`
	Gas      hexutil.Uint64  `json:"gas"`
	Input    *hexutil.Bytes  `json:"input,omitempty"`
	Init     hexutil.Bytes   `json:"init,omitempty"`
	To       *common.Address `json:"to,omitempty"`
	Value    *hexutil.Big    `json:"value"`
}

type traceCallResult struct {
	Address *common.Address `json:"address,omitempty"`
	Code    *hexutil.Bytes  `json:"code,omitempty"`
	GasUsed hexutil.Uint64  `json:"gasUsed"`
	Output  *hexutil.Bytes  `json:"output,omitempty"`
}

type traceFrame struct {
	Action              traceAction      `json:"action"`
	BlockHash           *hexutil.Bytes   `json:"blockHash,omitempty"`
	BlockNumber         *uint64          `json:"blockNumber,omitempty"`
	Result              *traceCallResult `json:"result,omitempty"`
	Error               *string          `json:"error,omitempty"`
	Subtraces           int              `json:"subtraces"`
	TraceAddress        []int            `json:"traceAddress"`
	TransactionHash     *hexutil.Bytes   `json:"transactionHash,omitempty"`
	TransactionPosition *uint64          `json:"transactionPosition,omitempty"`
	Type                string           `json:"type"`
}

type traceResult struct {
	Output             hexutil.Bytes     `json:"output"`
	StateDiff          *int              `json:"stateDiff"`
	Trace              []traceFrame      `json:"trace"`
	VmTrace            *int              `json:"vmTrace"`
	DestroyedContracts *[]common.Address `json:"destroyedContracts"`
}

type callTraceRequest struct {
	callArgs   callTxArgs
	traceTypes []string
}

func (at *callTraceRequest) UnmarshalJSON(b []byte) error {
	fields := []interface{}{&at.callArgs, &at.traceTypes}
	if err := json.Unmarshal(b, &fields); err != nil {
		return err
	}
	if len(fields) != 2 {
		return errors.New("expected two arguments per call")
	}
	return nil
}

func (at *callTraceRequest) MarshalJSON() ([]byte, error) {
	fields := []interface{}{&at.callArgs, &at.traceTypes}
	data, err := json.Marshal(&fields)
	return data, err
}

type filterRequest struct {
	FromBlock   *rpc.BlockNumberOrHash `json:"fromBlock"`
	ToBlock     *rpc.BlockNumberOrHash `json:"toBlock"`
	FromAddress *[]common.Address      `json:"fromAddress"`
	ToAddress   *[]common.Address      `json:"toAddress"`
	After       *uint64                `json:"after"`
	Count       *uint64                `json:"count"`
}

type ArbTraceAPIStub struct {
	t *testing.T
}

func (s *ArbTraceAPIStub) Call(ctx context.Context, callArgs callTxArgs, traceTypes []string, blockNum rpc.BlockNumberOrHash) (*traceResult, error) {
	return &traceResult{}, nil
}

func (s *ArbTraceAPIStub) CallMany(ctx context.Context, calls []*callTraceRequest, blockNum rpc.BlockNumberOrHash) ([]*traceResult, error) {
	return []*traceResult{{}}, nil
}

func (s *ArbTraceAPIStub) ReplayBlockTransactions(ctx context.Context, blockNum rpc.BlockNumberOrHash, traceTypes []string) ([]*traceResult, error) {
	return []*traceResult{{}}, nil
}

func (s *ArbTraceAPIStub) ReplayTransaction(ctx context.Context, txHash hexutil.Bytes, traceTypes []string) (*traceResult, error) {
	return &traceResult{}, nil
}

func (s *ArbTraceAPIStub) Transaction(ctx context.Context, txHash hexutil.Bytes) ([]traceFrame, error) {
	return []traceFrame{{}}, nil
}

func (s *ArbTraceAPIStub) Get(ctx context.Context, txHash hexutil.Bytes, path []hexutil.Uint64) (*traceFrame, error) {
	return &traceFrame{}, nil
}

func (s *ArbTraceAPIStub) Block(ctx context.Context, blockNum rpc.BlockNumberOrHash) ([]traceFrame, error) {
	return []traceFrame{{}}, nil
}

func (s *ArbTraceAPIStub) Filter(ctx context.Context, filter *filterRequest) ([]traceFrame, error) {
	return []traceFrame{{}}, nil
}

func TestArbTraceForwarding(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ipcPath := tmpPath(t, "redirect.ipc")
	var apis []rpc.API
	apis = append(apis, rpc.API{
		Namespace: "arbtrace",
		Version:   "1.0",
		Service:   &ArbTraceAPIStub{t: t},
		Public:    false,
	})
	listener, srv, err := rpc.StartIPCEndpoint(ipcPath, apis)
	Require(t, err)
	defer srv.Stop()
	defer listener.Close()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.execConfig.RPC.ClassicRedirect = ipcPath
	builder.execConfig.RPC.ClassicRedirectTimeout = time.Second
	cleanup := builder.Build(t)
	defer cleanup()

	l2rpc := builder.L2.Stack.Attach()
	txArgs := callTxArgs{}
	traceTypes := []string{"trace"}
	blockNum := rpc.BlockNumberOrHash{}
	traceRequests := make([]*callTraceRequest, 1)
	traceRequests[0] = &callTraceRequest{callArgs: callTxArgs{}, traceTypes: traceTypes}
	txHash := hexutil.Bytes{}
	path := []hexutil.Uint64{}
	filter := filterRequest{}
	var result traceResult
	err = l2rpc.CallContext(ctx, &result, "arbtrace_call", txArgs, traceTypes, blockNum)
	Require(t, err)
	var results []*traceResult
	err = l2rpc.CallContext(ctx, &results, "arbtrace_callMany", traceRequests, blockNum)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &results, "arbtrace_replayBlockTransactions", blockNum, traceTypes)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &result, "arbtrace_replayTransaction", txHash, traceTypes)
	Require(t, err)
	var frames []traceFrame
	err = l2rpc.CallContext(ctx, &frames, "arbtrace_transaction", txHash)
	Require(t, err)
	var frame traceFrame
	err = l2rpc.CallContext(ctx, &frame, "arbtrace_get", txHash, path)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &frames, "arbtrace_block", blockNum)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &frames, "arbtrace_filter", filter)
	Require(t, err)
}

'''
'''--- system_tests/batch_poster_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"crypto/rand"
	"fmt"
	"math/big"
	"testing"
	"time"

	"github.com/andybalholm/brotli"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/util/redisutil"
)

func TestBatchPosterParallel(t *testing.T) {
	testBatchPosterParallel(t, false)
}

func TestRedisBatchPosterParallel(t *testing.T) {
	testBatchPosterParallel(t, true)
}

func testBatchPosterParallel(t *testing.T, useRedis bool) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	var redisUrl string
	if useRedis {
		redisUrl = redisutil.CreateTestRedis(ctx, t)
	}
	parallelBatchPosters := 1
	if redisUrl != "" {
		client, err := redisutil.RedisClientFromURL(redisUrl)
		Require(t, err)
		err = client.Del(ctx, "data-poster.queue").Err()
		Require(t, err)
		parallelBatchPosters = 4
	}

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.BatchPoster.Enable = false
	builder.nodeConfig.BatchPoster.RedisUrl = redisUrl
	cleanup := builder.Build(t)
	defer cleanup()

	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{})
	defer cleanupB()

	builder.L2Info.GenerateAccount("User2")

	var txs []*types.Transaction

	for i := 0; i < 100; i++ {
		tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, common.Big1, nil)
		txs = append(txs, tx)

		err := builder.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)
	}

	for _, tx := range txs {
		_, err := builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}

	firstTxData, err := txs[0].MarshalBinary()
	Require(t, err)
	seqTxOpts := builder.L1Info.GetDefaultTransactOpts("Sequencer", ctx)
	builder.nodeConfig.BatchPoster.Enable = true
	builder.nodeConfig.BatchPoster.MaxSize = len(firstTxData) * 2
	startL1Block, err := builder.L1.Client.BlockNumber(ctx)
	Require(t, err)
	for i := 0; i < parallelBatchPosters; i++ {
		// Make a copy of the batch poster config so NewBatchPoster calling Validate() on it doesn't race
		batchPosterConfig := builder.nodeConfig.BatchPoster
		batchPoster, err := arbnode.NewBatchPoster(ctx,
			&arbnode.BatchPosterOpts{
				DataPosterDB: nil,
				L1Reader:     builder.L2.ConsensusNode.L1Reader,
				Inbox:        builder.L2.ConsensusNode.InboxTracker,
				Streamer:     builder.L2.ConsensusNode.TxStreamer,
				SyncMonitor:  builder.L2.ConsensusNode.SyncMonitor,
				Config:       func() *arbnode.BatchPosterConfig { return &batchPosterConfig },
				DeployInfo:   builder.L2.ConsensusNode.DeployInfo,
				TransactOpts: &seqTxOpts,
				DAWriter:     nil,
			},
		)
		Require(t, err)
		batchPoster.Start(ctx)
		defer batchPoster.StopAndWait()
	}

	lastTxHash := txs[len(txs)-1].Hash()
	for i := 90; i > 0; i-- {
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
		time.Sleep(500 * time.Millisecond)
		_, err := testClientB.Client.TransactionReceipt(ctx, lastTxHash)
		if err == nil {
			break
		}
		if i == 0 {
			Require(t, err)
		}
	}

	// TODO: factor this out in separate test case and skip it or delete this
	// code entirely.
	// I've locally confirmed that this passes when the clique period is set to 1.
	// However, setting the clique period to 1 slows everything else (including the L1 deployment for this test) down to a crawl.
	if false {
		// Make sure the batch poster is able to post multiple batches in one block
		endL1Block, err := builder.L1.Client.BlockNumber(ctx)
		Require(t, err)
		seqInbox, err := arbnode.NewSequencerInbox(builder.L1.Client, builder.L2.ConsensusNode.DeployInfo.SequencerInbox, 0)
		Require(t, err)
		batches, err := seqInbox.LookupBatchesInRange(ctx, new(big.Int).SetUint64(startL1Block), new(big.Int).SetUint64(endL1Block))
		Require(t, err)
		var foundMultipleInBlock bool
		for i := range batches {
			if i == 0 {
				continue
			}
			if batches[i-1].ParentChainBlockNumber == batches[i].ParentChainBlockNumber {
				foundMultipleInBlock = true
				break
			}
		}

		if !foundMultipleInBlock {
			Fatal(t, "only found one batch per block")
		}
	}

	l2balance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)

	if l2balance.Sign() == 0 {
		Fatal(t, "Unexpected zero balance")
	}
}

func TestBatchPosterLargeTx(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.execConfig.Sequencer.MaxTxDataSize = 110000
	cleanup := builder.Build(t)
	defer cleanup()

	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{})
	defer cleanupB()

	data := make([]byte, 100000)
	_, err := rand.Read(data)
	Require(t, err)
	faucetAddr := builder.L2Info.GetAddress("Faucet")
	gas := builder.L2Info.TransferGas + 20000*uint64(len(data))
	tx := builder.L2Info.PrepareTxTo("Faucet", &faucetAddr, gas, common.Big0, data)
	err = builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)
	receiptA, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	receiptB, err := testClientB.EnsureTxSucceededWithTimeout(tx, time.Second*30)
	Require(t, err)
	if receiptA.BlockHash != receiptB.BlockHash {
		Fatal(t, "receipt A block hash", receiptA.BlockHash, "does not equal receipt B block hash", receiptB.BlockHash)
	}
}

func TestBatchPosterKeepsUp(t *testing.T) {
	t.Skip("This test is for manual inspection and would be unreliable in CI even if automated")
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.BatchPoster.CompressionLevel = brotli.BestCompression
	builder.nodeConfig.BatchPoster.MaxDelay = time.Hour
	builder.execConfig.RPC.RPCTxFeeCap = 1000.
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GasPrice = big.NewInt(100e9)

	go func() {
		data := make([]byte, 90000)
		_, err := rand.Read(data)
		Require(t, err)
		for {
			gas := builder.L2Info.TransferGas + 20000*uint64(len(data))
			tx := builder.L2Info.PrepareTx("Faucet", "Faucet", gas, common.Big0, data)
			err = builder.L2.Client.SendTransaction(ctx, tx)
			Require(t, err)
			_, err := builder.L2.EnsureTxSucceeded(tx)
			Require(t, err)
		}
	}()

	start := time.Now()
	for {
		time.Sleep(time.Second)
		batches, err := builder.L2.ConsensusNode.InboxTracker.GetBatchCount()
		Require(t, err)
		postedMessages, err := builder.L2.ConsensusNode.InboxTracker.GetBatchMessageCount(batches - 1)
		Require(t, err)
		haveMessages, err := builder.L2.ConsensusNode.TxStreamer.GetMessageCount()
		Require(t, err)
		duration := time.Since(start)
		fmt.Printf("batches posted: %v over %v (%.2f batches/second)\n", batches, duration, float64(batches)/(float64(duration)/float64(time.Second)))
		fmt.Printf("backlog: %v message\n", haveMessages-postedMessages)
	}
}

'''
'''--- system_tests/block_hash_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"testing"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
)

func TestBlockHash(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Even though we don't use the L1, we need to create this node on L1 to get accurate L1 block numbers
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)

	_, _, simple, err := mocksgen.DeploySimple(&auth, builder.L2.Client)
	Require(t, err)

	_, err = simple.CheckBlockHashes(&bind.CallOpts{Context: ctx})
	Require(t, err)
}

'''
'''--- system_tests/block_validator_bench_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build block_validator_bench
// +build block_validator_bench

package arbtest

import (
	"testing"
)

func TestBlockValidatorBenchmark(t *testing.T) {
	testBlockValidatorSimple(t, "onchain", 1, depleteGas, true)
}

'''
'''--- system_tests/block_validator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/core/vm"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
)

type workloadType uint

const (
	ethSend workloadType = iota
	smallContract
	depleteGas
	upgradeArbOs
)

func testBlockValidatorSimple(t *testing.T, dasModeString string, workloadLoops int, workload workloadType, arbitrator bool) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	chainConfig, l1NodeConfigA, lifecycleManager, _, dasSignerKey := setupConfigWithDAS(t, ctx, dasModeString)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	if workload == upgradeArbOs {
		chainConfig.ArbitrumChainParams.InitialArbOSVersion = 10
	}

	var delayEvery int
	if workloadLoops > 1 {
		l1NodeConfigA.BatchPoster.MaxDelay = time.Millisecond * 500
		delayEvery = workloadLoops / 3
	}

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig = l1NodeConfigA
	builder.chainConfig = chainConfig
	builder.L2Info = nil
	cleanup := builder.Build(t)
	defer cleanup()

	authorizeDASKeyset(t, ctx, dasSignerKey, builder.L1Info, builder.L1.Client)

	validatorConfig := arbnode.ConfigDefaultL1NonSequencerTest()
	validatorConfig.BlockValidator.Enable = true
	validatorConfig.DataAvailability = l1NodeConfigA.DataAvailability
	validatorConfig.DataAvailability.RPCAggregator.Enable = false
	AddDefaultValNode(t, ctx, validatorConfig, !arbitrator)
	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: validatorConfig})
	defer cleanupB()
	builder.L2Info.GenerateAccount("User2")

	perTransfer := big.NewInt(1e12)

	if workload != upgradeArbOs {
		for i := 0; i < workloadLoops; i++ {
			var tx *types.Transaction

			if workload == ethSend {
				tx = builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, perTransfer, nil)
			} else {
				var contractCode []byte
				var gas uint64

				if workload == smallContract {
					contractCode = []byte{byte(vm.PUSH0)}
					contractCode = append(contractCode, byte(vm.PUSH0))
					contractCode = append(contractCode, byte(vm.PUSH1))
					contractCode = append(contractCode, 8) // the prelude length
					contractCode = append(contractCode, byte(vm.PUSH0))
					contractCode = append(contractCode, byte(vm.CODECOPY))
					contractCode = append(contractCode, byte(vm.PUSH0))
					contractCode = append(contractCode, byte(vm.RETURN))
					basefee := builder.L2.GetBaseFee(t)
					var err error
					gas, err = builder.L2.Client.EstimateGas(ctx, ethereum.CallMsg{
						From:     builder.L2Info.GetAddress("Owner"),
						GasPrice: basefee,
						Value:    big.NewInt(0),
						Data:     contractCode,
					})
					Require(t, err)
				} else {
					contractCode = []byte{0x5b} // JUMPDEST
					for i := 0; i < 20; i++ {
						contractCode = append(contractCode, 0x60, 0x00, 0x60, 0x00, 0x52) // PUSH1 0 MSTORE
					}
					contractCode = append(contractCode, 0x60, 0x00, 0x56) // JUMP
					gas = builder.L2Info.TransferGas*2 + l2pricing.InitialPerBlockGasLimitV6
				}
				tx = builder.L2Info.PrepareTxTo("Owner", nil, gas, common.Big0, contractCode)
			}

			err := builder.L2.Client.SendTransaction(ctx, tx)
			Require(t, err)
			_, err = builder.L2.EnsureTxSucceeded(tx)
			if workload != depleteGas {
				Require(t, err)
			}
			if delayEvery > 0 && i%delayEvery == (delayEvery-1) {
				<-time.After(time.Second)
			}
		}
	} else {
		auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
		// make auth a chain owner
		arbDebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
		Require(t, err)
		tx, err := arbDebug.BecomeChainOwner(&auth)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
		arbOwner, err := precompilesgen.NewArbOwner(common.HexToAddress("0x70"), builder.L2.Client)
		Require(t, err)
		tx, err = arbOwner.ScheduleArbOSUpgrade(&auth, 11, 0)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)

		tx = builder.L2Info.PrepareTxTo("Owner", nil, builder.L2Info.TransferGas, perTransfer, []byte{byte(vm.PUSH0)})
		err = builder.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}

	if workload != depleteGas {
		delayedTx := builder.L2Info.PrepareTx("Owner", "User2", 30002, perTransfer, nil)
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			WrapL2ForDelayed(t, delayedTx, builder.L1Info, "User", 100000),
		})
		// give the inbox reader a bit of time to pick up the delayed message
		time.Sleep(time.Millisecond * 500)

		// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
		for i := 0; i < 30; i++ {
			builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
				builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
			})
		}

		_, err := WaitForTx(ctx, testClientB.Client, delayedTx.Hash(), time.Second*30)
		Require(t, err)
	}

	if workload == ethSend {
		l2balance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
		Require(t, err)

		expectedBalance := new(big.Int).Mul(perTransfer, big.NewInt(int64(workloadLoops+1)))
		if l2balance.Cmp(expectedBalance) != 0 {
			Fatal(t, "Unexpected balance:", l2balance)
		}
	}

	lastBlock, err := testClientB.Client.BlockByNumber(ctx, nil)
	Require(t, err)
	for {
		usefulBlock := false
		for _, tx := range lastBlock.Transactions() {
			if tx.Type() != types.ArbitrumInternalTxType {
				usefulBlock = true
				break
			}
		}
		if usefulBlock {
			break
		}
		lastBlock, err = testClientB.Client.BlockByHash(ctx, lastBlock.ParentHash())
		Require(t, err)
	}
	t.Log("waiting for block: ", lastBlock.NumberU64())
	timeout := getDeadlineTimeout(t, time.Minute*10)
	// messageindex is same as block number here
	if !testClientB.ConsensusNode.BlockValidator.WaitForPos(t, ctx, arbutil.MessageIndex(lastBlock.NumberU64()), timeout) {
		Fatal(t, "did not validate all blocks")
	}
	gethExec, ok := testClientB.ConsensusNode.Execution.(*gethexec.ExecutionNode)
	if !ok {
		t.Fail()
	}
	gethExec.Recorder.TrimAllPrepared(t)
	finalRefCount := gethExec.Recorder.RecordingDBReferenceCount()
	lastBlockNow, err := testClientB.Client.BlockByNumber(ctx, nil)
	Require(t, err)
	// up to 3 extra references: awaiting validation, recently valid, lastValidatedHeader
	largestRefCount := lastBlockNow.NumberU64() - lastBlock.NumberU64() + 3
	if finalRefCount < 0 || finalRefCount > int64(largestRefCount) {
		Fatal(t, "unexpected refcount:", finalRefCount)
	}
}

func TestBlockValidatorSimpleOnchainUpgradeArbOs(t *testing.T) {
	testBlockValidatorSimple(t, "onchain", 1, upgradeArbOs, true)
}

func TestBlockValidatorSimpleOnchain(t *testing.T) {
	testBlockValidatorSimple(t, "onchain", 1, ethSend, true)
}

func TestBlockValidatorSimpleLocalDAS(t *testing.T) {
	testBlockValidatorSimple(t, "files", 1, ethSend, true)
}

func TestBlockValidatorSimpleJITOnchain(t *testing.T) {
	testBlockValidatorSimple(t, "files", 8, smallContract, false)
}

'''
'''--- system_tests/bloom_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
)

func TestBloom(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.execConfig.RPC.BloomBitsBlocks = 256
	builder.execConfig.RPC.BloomConfirms = 1
	builder.takeOwnership = false
	cleanup := builder.Build(t)

	defer cleanup()

	builder.L2Info.GenerateAccount("User2")

	ownerTxOpts := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	ownerTxOpts.Context = ctx
	_, simple := builder.L2.DeploySimple(t, ownerTxOpts)
	simpleABI, err := mocksgen.SimpleMetaData.GetAbi()
	Require(t, err)

	countsNum := 800
	eventsNum := 20
	nullEventsNum := 50

	eventCounts := make(map[uint64]struct{})
	nullEventCounts := make(map[uint64]struct{})

	for i := 0; i < eventsNum; i++ {
		count := uint64(rand.Int() % countsNum)
		eventCounts[count] = struct{}{}
	}

	for i := 0; i < nullEventsNum; i++ {
		count := uint64(rand.Int() % countsNum)
		nullEventCounts[count] = struct{}{}
	}

	for i := 0; i <= countsNum; i++ {
		var tx *types.Transaction
		var err error
		_, sendNullEvent := nullEventCounts[uint64(i)]
		if sendNullEvent {
			tx, err = simple.EmitNullEvent(&ownerTxOpts)
			Require(t, err)
			_, err = builder.L2.EnsureTxSucceeded(tx)
			Require(t, err)
		}

		_, sendEvent := eventCounts[uint64(i)]
		if sendEvent {
			tx, err = simple.IncrementEmit(&ownerTxOpts)
		} else {
			tx, err = simple.Increment(&ownerTxOpts)
		}
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
		if i%100 == 0 {
			t.Log("counts: ", i, "/", countsNum)
		}
	}
	for {
		sectionSize, sectionNum := builder.L2.ExecNode.Backend.APIBackend().BloomStatus()
		if sectionSize != 256 {
			Fatal(t, "unexpected section size: ", sectionSize)
		}
		t.Log("sections: ", sectionNum, "/", uint64(countsNum)/sectionSize)
		if sectionSize*(sectionNum+1) > uint64(countsNum) && sectionNum > 1 {
			break
		}
		<-time.After(time.Second)
	}
	lastHeader, err := builder.L2.Client.HeaderByNumber(ctx, nil)
	Require(t, err)
	nullEventQuery := ethereum.FilterQuery{
		FromBlock: big.NewInt(0),
		ToBlock:   lastHeader.Number,
		Topics:    [][]common.Hash{{simpleABI.Events["NullEvent"].ID}},
	}
	logs, err := builder.L2.Client.FilterLogs(ctx, nullEventQuery)
	Require(t, err)
	if len(logs) != len(nullEventCounts) {
		Fatal(t, "expected ", len(nullEventCounts), " logs, got ", len(logs))
	}
	incrementEventQuery := ethereum.FilterQuery{
		Topics: [][]common.Hash{{simpleABI.Events["CounterEvent"].ID}},
	}
	logs, err = builder.L2.Client.FilterLogs(ctx, incrementEventQuery)
	Require(t, err)
	if len(logs) != len(eventCounts) {
		Fatal(t, "expected ", len(eventCounts), " logs, got ", len(logs))
	}
	for _, log := range logs {
		parsedLog, err := simple.ParseCounterEvent(log)
		Require(t, err)
		_, expected := eventCounts[parsedLog.Count-1]
		if !expected {
			Fatal(t, "unxpected count in logs: ", parsedLog.Count)
		}
	}
}

'''
'''--- system_tests/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"bytes"
	"context"
	"encoding/hex"
	"encoding/json"
	"math/big"
	"net"
	"os"
	"strconv"
	"strings"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/cmd/chaininfo"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/deploy"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/validator/server_api"
	"github.com/offchainlabs/nitro/validator/server_common"
	"github.com/offchainlabs/nitro/validator/valnode"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/accounts/keystore"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/eth"
	"github.com/ethereum/go-ethereum/eth/catalyst"
	"github.com/ethereum/go-ethereum/eth/downloader"
	"github.com/ethereum/go-ethereum/eth/ethconfig"
	"github.com/ethereum/go-ethereum/eth/filters"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbutil"
	_ "github.com/offchainlabs/nitro/nodeInterface"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/solgen/go/upgrade_executorgen"
	"github.com/offchainlabs/nitro/statetransfer"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

type info = *BlockchainTestInfo
type client = arbutil.L1Interface

type SecondNodeParams struct {
	nodeConfig  *arbnode.Config
	execConfig  *gethexec.Config
	stackConfig *node.Config
	dasConfig   *das.DataAvailabilityConfig
	initData    *statetransfer.ArbosInitializationInfo
}

type TestClient struct {
	ctx           context.Context
	Client        *ethclient.Client
	L1Backend     *eth.Ethereum
	Stack         *node.Node
	ConsensusNode *arbnode.Node
	ExecNode      *gethexec.ExecutionNode

	// having cleanup() field makes cleanup customizable from default cleanup methods after calling build
	cleanup func()
}

func NewTestClient(ctx context.Context) *TestClient {
	return &TestClient{ctx: ctx}
}

func (tc *TestClient) SendSignedTx(t *testing.T, l2Client *ethclient.Client, transaction *types.Transaction, lInfo info) *types.Receipt {
	return SendSignedTxViaL1(t, tc.ctx, lInfo, tc.Client, l2Client, transaction)
}

func (tc *TestClient) SendUnsignedTx(t *testing.T, l2Client *ethclient.Client, transaction *types.Transaction, lInfo info) *types.Receipt {
	return SendUnsignedTxViaL1(t, tc.ctx, lInfo, tc.Client, l2Client, transaction)
}

func (tc *TestClient) TransferBalance(t *testing.T, from string, to string, amount *big.Int, lInfo info) (*types.Transaction, *types.Receipt) {
	return TransferBalanceTo(t, from, lInfo.GetAddress(to), amount, lInfo, tc.Client, tc.ctx)
}

func (tc *TestClient) TransferBalanceTo(t *testing.T, from string, to common.Address, amount *big.Int, lInfo info) (*types.Transaction, *types.Receipt) {
	return TransferBalanceTo(t, from, to, amount, lInfo, tc.Client, tc.ctx)
}

func (tc *TestClient) GetBalance(t *testing.T, account common.Address) *big.Int {
	return GetBalance(t, tc.ctx, tc.Client, account)
}

func (tc *TestClient) GetBaseFee(t *testing.T) *big.Int {
	return GetBaseFee(t, tc.Client, tc.ctx)
}

func (tc *TestClient) GetBaseFeeAt(t *testing.T, blockNum *big.Int) *big.Int {
	return GetBaseFeeAt(t, tc.Client, tc.ctx, blockNum)
}

func (tc *TestClient) SendWaitTestTransactions(t *testing.T, txs []*types.Transaction) {
	SendWaitTestTransactions(t, tc.ctx, tc.Client, txs)
}

func (tc *TestClient) DeploySimple(t *testing.T, auth bind.TransactOpts) (common.Address, *mocksgen.Simple) {
	return deploySimple(t, tc.ctx, auth, tc.Client)
}

func (tc *TestClient) EnsureTxSucceeded(transaction *types.Transaction) (*types.Receipt, error) {
	return tc.EnsureTxSucceededWithTimeout(transaction, time.Second*5)
}

func (tc *TestClient) EnsureTxSucceededWithTimeout(transaction *types.Transaction, timeout time.Duration) (*types.Receipt, error) {
	return EnsureTxSucceededWithTimeout(tc.ctx, tc.Client, transaction, timeout)
}

type NodeBuilder struct {
	// NodeBuilder configuration
	ctx           context.Context
	chainConfig   *params.ChainConfig
	nodeConfig    *arbnode.Config
	execConfig    *gethexec.Config
	l1StackConfig *node.Config
	l2StackConfig *node.Config
	L1Info        info
	L2Info        info

	// L1, L2 Node parameters
	dataDir       string
	isSequencer   bool
	takeOwnership bool
	withL1        bool

	// Created nodes
	L1 *TestClient
	L2 *TestClient
}

func NewNodeBuilder(ctx context.Context) *NodeBuilder {
	return &NodeBuilder{ctx: ctx}
}

func (b *NodeBuilder) DefaultConfig(t *testing.T, withL1 bool) *NodeBuilder {
	// most used values across current tests are set here as default
	b.withL1 = withL1
	if withL1 {
		b.isSequencer = true
		b.nodeConfig = arbnode.ConfigDefaultL1Test()
	} else {
		b.takeOwnership = true
		b.nodeConfig = arbnode.ConfigDefaultL2Test()
	}
	b.chainConfig = params.ArbitrumDevTestChainConfig()
	b.L1Info = NewL1TestInfo(t)
	b.L2Info = NewArbTestInfo(t, b.chainConfig.ChainID)
	b.dataDir = t.TempDir()
	b.l1StackConfig = createStackConfigForTest(b.dataDir)
	b.l2StackConfig = createStackConfigForTest(b.dataDir)
	b.execConfig = gethexec.ConfigDefaultTest()
	return b
}

func (b *NodeBuilder) Build(t *testing.T) func() {
	if b.withL1 {
		l1, l2 := NewTestClient(b.ctx), NewTestClient(b.ctx)
		b.L2Info, l2.ConsensusNode, l2.Client, l2.Stack, b.L1Info, l1.L1Backend, l1.Client, l1.Stack =
			createTestNodeWithL1(t, b.ctx, b.isSequencer, b.nodeConfig, b.execConfig, b.chainConfig, b.l2StackConfig, b.L2Info)
		b.L1, b.L2 = l1, l2
		b.L1.cleanup = func() { requireClose(t, b.L1.Stack) }
	} else {
		l2 := NewTestClient(b.ctx)
		b.L2Info, l2.ConsensusNode, l2.Client =
			createTestNode(t, b.ctx, b.L2Info, b.nodeConfig, b.execConfig, b.takeOwnership)
		b.L2 = l2
	}
	b.L2.ExecNode = getExecNode(t, b.L2.ConsensusNode)
	b.L2.cleanup = func() { b.L2.ConsensusNode.StopAndWait() }
	return func() {
		b.L2.cleanup()
		if b.L1 != nil && b.L1.cleanup != nil {
			b.L1.cleanup()
		}
	}
}

func (b *NodeBuilder) Build2ndNode(t *testing.T, params *SecondNodeParams) (*TestClient, func()) {
	if b.L2 == nil {
		t.Fatal("builder did not previously build a L2 Node")
	}
	if b.withL1 && b.L1 == nil {
		t.Fatal("builder did not previously build a L1 Node")
	}
	if params.nodeConfig == nil {
		params.nodeConfig = arbnode.ConfigDefaultL1NonSequencerTest()
	}
	if params.dasConfig != nil {
		params.nodeConfig.DataAvailability = *params.dasConfig
	}
	if params.stackConfig == nil {
		params.stackConfig = b.l2StackConfig
		// should use different dataDir from the previously used ones
		params.stackConfig.DataDir = t.TempDir()
	}
	if params.initData == nil {
		params.initData = &b.L2Info.ArbInitData
	}
	if params.execConfig == nil {
		params.execConfig = b.execConfig
	}

	l2 := NewTestClient(b.ctx)
	l2.Client, l2.ConsensusNode =
		Create2ndNodeWithConfig(t, b.ctx, b.L2.ConsensusNode, b.L1.Stack, b.L1Info, params.initData, params.nodeConfig, params.execConfig, params.stackConfig)
	l2.ExecNode = getExecNode(t, l2.ConsensusNode)
	l2.cleanup = func() { l2.ConsensusNode.StopAndWait() }
	return l2, func() { l2.cleanup() }
}

func (b *NodeBuilder) BridgeBalance(t *testing.T, account string, amount *big.Int) (*types.Transaction, *types.Receipt) {
	return BridgeBalance(t, account, amount, b.L1Info, b.L2Info, b.L1.Client, b.L2.Client, b.ctx)
}

func SendWaitTestTransactions(t *testing.T, ctx context.Context, client client, txs []*types.Transaction) {
	t.Helper()
	for _, tx := range txs {
		Require(t, client.SendTransaction(ctx, tx))
	}
	for _, tx := range txs {
		_, err := EnsureTxSucceeded(ctx, client, tx)
		Require(t, err)
	}
}

func TransferBalance(
	t *testing.T, from, to string, amount *big.Int, l2info info, client client, ctx context.Context,
) (*types.Transaction, *types.Receipt) {
	t.Helper()
	return TransferBalanceTo(t, from, l2info.GetAddress(to), amount, l2info, client, ctx)
}

func TransferBalanceTo(
	t *testing.T, from string, to common.Address, amount *big.Int, l2info info, client client, ctx context.Context,
) (*types.Transaction, *types.Receipt) {
	t.Helper()
	tx := l2info.PrepareTxTo(from, &to, l2info.TransferGas, amount, nil)
	err := client.SendTransaction(ctx, tx)
	Require(t, err)
	res, err := EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	return tx, res
}

// if l2client is not nil - will wait until balance appears in l2
func BridgeBalance(
	t *testing.T, account string, amount *big.Int, l1info info, l2info info, l1client client, l2client client, ctx context.Context,
) (*types.Transaction, *types.Receipt) {
	t.Helper()

	// setup or validate the same account on l2info
	l1acct := l1info.GetInfoWithPrivKey(account)
	if l2info.Accounts[account] == nil {
		l2info.SetFullAccountInfo(account, &AccountInfo{
			Address:    l1acct.Address,
			PrivateKey: l1acct.PrivateKey,
			Nonce:      0,
		})
	} else {
		l2acct := l2info.GetInfoWithPrivKey(account)
		if l2acct.PrivateKey.X.Cmp(l1acct.PrivateKey.X) != 0 ||
			l2acct.PrivateKey.Y.Cmp(l1acct.PrivateKey.Y) != 0 {
			Fatal(t, "l2 account already exists and not compatible to l1")
		}
	}

	// check previous balance
	var l2Balance *big.Int
	var err error
	if l2client != nil {
		l2Balance, err = l2client.BalanceAt(ctx, l2info.GetAddress("Faucet"), nil)
		Require(t, err)
	}

	// send transaction
	data, err := hex.DecodeString("0f4d14e9000000000000000000000000000000000000000000000000000082f79cd90000")
	Require(t, err)
	tx := l1info.PrepareTx(account, "Inbox", l1info.TransferGas*100, amount, data)
	err = l1client.SendTransaction(ctx, tx)
	Require(t, err)
	res, err := EnsureTxSucceeded(ctx, l1client, tx)
	Require(t, err)

	// wait for balance to appear in l2
	if l2client != nil {
		l2Balance.Add(l2Balance, amount)
		for i := 0; true; i++ {
			balance, err := l2client.BalanceAt(ctx, l2info.GetAddress("Faucet"), nil)
			Require(t, err)
			if balance.Cmp(l2Balance) >= 0 {
				break
			}
			TransferBalance(t, "Faucet", "User", big.NewInt(1), l1info, l1client, ctx)
			if i > 20 {
				Fatal(t, "bridging failed")
			}
			<-time.After(time.Millisecond * 100)
		}
	}

	return tx, res
}

func SendSignedTxViaL1(
	t *testing.T,
	ctx context.Context,
	l1info *BlockchainTestInfo,
	l1client arbutil.L1Interface,
	l2client arbutil.L1Interface,
	delayedTx *types.Transaction,
) *types.Receipt {
	delayedInboxContract, err := bridgegen.NewInbox(l1info.GetAddress("Inbox"), l1client)
	Require(t, err)
	usertxopts := l1info.GetDefaultTransactOpts("User", ctx)

	txbytes, err := delayedTx.MarshalBinary()
	Require(t, err)
	txwrapped := append([]byte{arbos.L2MessageKind_SignedTx}, txbytes...)
	l1tx, err := delayedInboxContract.SendL2Message(&usertxopts, txwrapped)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1client, l1tx)
	Require(t, err)

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		SendWaitTestTransactions(t, ctx, l1client, []*types.Transaction{
			l1info.PrepareTx("Faucet", "Faucet", 30000, big.NewInt(1e12), nil),
		})
	}
	receipt, err := EnsureTxSucceeded(ctx, l2client, delayedTx)
	Require(t, err)
	return receipt
}

func SendUnsignedTxViaL1(
	t *testing.T,
	ctx context.Context,
	l1info *BlockchainTestInfo,
	l1client arbutil.L1Interface,
	l2client arbutil.L1Interface,
	templateTx *types.Transaction,
) *types.Receipt {
	delayedInboxContract, err := bridgegen.NewInbox(l1info.GetAddress("Inbox"), l1client)
	Require(t, err)

	usertxopts := l1info.GetDefaultTransactOpts("User", ctx)
	remapped := util.RemapL1Address(usertxopts.From)
	nonce, err := l2client.NonceAt(ctx, remapped, nil)
	Require(t, err)

	unsignedTx := types.NewTx(&types.ArbitrumUnsignedTx{
		ChainId:   templateTx.ChainId(),
		From:      remapped,
		Nonce:     nonce,
		GasFeeCap: templateTx.GasFeeCap(),
		Gas:       templateTx.Gas(),
		To:        templateTx.To(),
		Value:     templateTx.Value(),
		Data:      templateTx.Data(),
	})

	l1tx, err := delayedInboxContract.SendUnsignedTransaction(
		&usertxopts,
		arbmath.UintToBig(unsignedTx.Gas()),
		unsignedTx.GasFeeCap(),
		arbmath.UintToBig(unsignedTx.Nonce()),
		*unsignedTx.To(),
		unsignedTx.Value(),
		unsignedTx.Data(),
	)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1client, l1tx)
	Require(t, err)

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		SendWaitTestTransactions(t, ctx, l1client, []*types.Transaction{
			l1info.PrepareTx("Faucet", "Faucet", 30000, big.NewInt(1e12), nil),
		})
	}
	receipt, err := EnsureTxSucceeded(ctx, l2client, unsignedTx)
	Require(t, err)
	return receipt
}

func GetBaseFee(t *testing.T, client client, ctx context.Context) *big.Int {
	header, err := client.HeaderByNumber(ctx, nil)
	Require(t, err)
	return header.BaseFee
}

func GetBaseFeeAt(t *testing.T, client client, ctx context.Context, blockNum *big.Int) *big.Int {
	header, err := client.HeaderByNumber(ctx, blockNum)
	Require(t, err)
	return header.BaseFee
}

type lifecycle struct {
	start func() error
	stop  func() error
}

func (l *lifecycle) Start() error {
	if l.start != nil {
		return l.start()
	}
	return nil
}

func (l *lifecycle) Stop() error {
	if l.start != nil {
		return l.stop()
	}
	return nil
}

type staticNodeConfigFetcher struct {
	config *arbnode.Config
}

func NewFetcherFromConfig(c *arbnode.Config) *staticNodeConfigFetcher {
	err := c.Validate()
	if err != nil {
		panic("invalid static config: " + err.Error())
	}
	return &staticNodeConfigFetcher{c}
}

func (c *staticNodeConfigFetcher) Get() *arbnode.Config {
	return c.config
}

func (c *staticNodeConfigFetcher) Start(context.Context) {}

func (c *staticNodeConfigFetcher) StopAndWait() {}

func (c *staticNodeConfigFetcher) Started() bool {
	return true
}

func createTestL1BlockChain(t *testing.T, l1info info) (info, *ethclient.Client, *eth.Ethereum, *node.Node) {
	return createTestL1BlockChainWithConfig(t, l1info, nil)
}

func createStackConfigForTest(dataDir string) *node.Config {
	stackConf := node.DefaultConfig
	stackConf.DataDir = dataDir
	stackConf.UseLightweightKDF = true
	stackConf.WSPort = 0
	stackConf.HTTPPort = 0
	stackConf.HTTPHost = ""
	stackConf.HTTPModules = append(stackConf.HTTPModules, "eth")
	stackConf.P2P.NoDiscovery = true
	stackConf.P2P.NoDial = true
	stackConf.P2P.ListenAddr = ""
	stackConf.P2P.NAT = nil
	stackConf.DBEngine = "leveldb" // TODO Try pebble again in future once iterator race condition issues are fixed
	return &stackConf
}

func createTestValidationNode(t *testing.T, ctx context.Context, config *valnode.Config) (*valnode.ValidationNode, *node.Node) {
	stackConf := node.DefaultConfig
	stackConf.HTTPPort = 0
	stackConf.DataDir = ""
	stackConf.WSHost = "127.0.0.1"
	stackConf.WSPort = 0
	stackConf.WSModules = []string{server_api.Namespace}
	stackConf.P2P.NoDiscovery = true
	stackConf.P2P.ListenAddr = ""
	stackConf.DBEngine = "leveldb" // TODO Try pebble again in future once iterator race condition issues are fixed

	valnode.EnsureValidationExposedViaAuthRPC(&stackConf)

	stack, err := node.New(&stackConf)
	Require(t, err)

	configFetcher := func() *valnode.Config { return config }
	valnode, err := valnode.CreateValidationNode(configFetcher, stack, nil)
	Require(t, err)

	err = stack.Start()
	Require(t, err)

	err = valnode.Start(ctx)
	Require(t, err)

	go func() {
		<-ctx.Done()
		stack.Close()
	}()

	return valnode, stack
}

type validated interface {
	Validate() error
}

func StaticFetcherFrom[T any](t *testing.T, config *T) func() *T {
	t.Helper()
	tCopy := *config
	asEmptyIf := interface{}(&tCopy)
	if asValidtedIf, ok := asEmptyIf.(validated); ok {
		err := asValidtedIf.Validate()
		if err != nil {
			Fatal(t, err)
		}
	}
	return func() *T { return &tCopy }
}

func configByValidationNode(t *testing.T, clientConfig *arbnode.Config, valStack *node.Node) {
	clientConfig.BlockValidator.ValidationServer.URL = valStack.WSEndpoint()
	clientConfig.BlockValidator.ValidationServer.JWTSecret = ""
}

func AddDefaultValNode(t *testing.T, ctx context.Context, nodeConfig *arbnode.Config, useJit bool) {
	if !nodeConfig.ValidatorRequired() {
		return
	}
	if nodeConfig.BlockValidator.ValidationServer.URL != "" {
		return
	}
	conf := valnode.TestValidationConfig
	conf.UseJit = useJit
	_, valStack := createTestValidationNode(t, ctx, &conf)
	configByValidationNode(t, nodeConfig, valStack)
}

func createTestL1BlockChainWithConfig(t *testing.T, l1info info, stackConfig *node.Config) (info, *ethclient.Client, *eth.Ethereum, *node.Node) {
	if l1info == nil {
		l1info = NewL1TestInfo(t)
	}
	if stackConfig == nil {
		stackConfig = createStackConfigForTest(t.TempDir())
	}
	l1info.GenerateAccount("Faucet")

	chainConfig := params.ArbitrumDevTestChainConfig()
	chainConfig.ArbitrumChainParams = params.ArbitrumChainParams{}

	stack, err := node.New(stackConfig)
	Require(t, err)

	nodeConf := ethconfig.Defaults
	nodeConf.NetworkId = chainConfig.ChainID.Uint64()
	l1Genesis := core.DeveloperGenesisBlock(15_000_000, l1info.GetAddress("Faucet"))
	infoGenesis := l1info.GetGenesisAlloc()
	for acct, info := range infoGenesis {
		l1Genesis.Alloc[acct] = info
	}
	l1Genesis.BaseFee = big.NewInt(50 * params.GWei)
	nodeConf.Genesis = l1Genesis
	nodeConf.Miner.Etherbase = l1info.GetAddress("Faucet")
	nodeConf.SyncMode = downloader.FullSync

	l1backend, err := eth.New(stack, &nodeConf)
	Require(t, err)

	simBeacon, err := catalyst.NewSimulatedBeacon(0, l1backend)
	Require(t, err)
	catalyst.RegisterSimulatedBeaconAPIs(stack, simBeacon)
	stack.RegisterLifecycle(simBeacon)

	tempKeyStore := keystore.NewPlaintextKeyStore(t.TempDir())
	faucetAccount, err := tempKeyStore.ImportECDSA(l1info.Accounts["Faucet"].PrivateKey, "passphrase")
	Require(t, err)
	Require(t, tempKeyStore.Unlock(faucetAccount, "passphrase"))
	l1backend.AccountManager().AddBackend(tempKeyStore)
	l1backend.SetEtherbase(l1info.GetAddress("Faucet"))

	stack.RegisterLifecycle(&lifecycle{stop: func() error {
		l1backend.StopMining()
		return nil
	}})

	stack.RegisterAPIs([]rpc.API{{
		Namespace: "eth",
		Service:   filters.NewFilterAPI(filters.NewFilterSystem(l1backend.APIBackend, filters.Config{}), false),
	}})

	Require(t, stack.Start())
	Require(t, l1backend.StartMining())

	rpcClient := stack.Attach()

	l1Client := ethclient.NewClient(rpcClient)

	return l1info, l1Client, l1backend, stack
}

func getInitMessage(ctx context.Context, t *testing.T, l1client client, addresses *chaininfo.RollupAddresses) *arbostypes.ParsedInitMessage {
	bridge, err := arbnode.NewDelayedBridge(l1client, addresses.Bridge, addresses.DeployedAt)
	Require(t, err)
	deployedAtBig := arbmath.UintToBig(addresses.DeployedAt)
	messages, err := bridge.LookupMessagesInRange(ctx, deployedAtBig, deployedAtBig, nil)
	Require(t, err)
	if len(messages) == 0 {
		Fatal(t, "No delayed messages found at rollup creation block")
	}
	initMessage, err := messages[0].Message.ParseInitMessage()
	Require(t, err, "Failed to parse rollup init message")

	return initMessage
}

func DeployOnTestL1(
	t *testing.T, ctx context.Context, l1info info, l1client client, chainConfig *params.ChainConfig,
) (*chaininfo.RollupAddresses, *arbostypes.ParsedInitMessage) {
	l1info.GenerateAccount("RollupOwner")
	l1info.GenerateAccount("Sequencer")
	l1info.GenerateAccount("User")

	SendWaitTestTransactions(t, ctx, l1client, []*types.Transaction{
		l1info.PrepareTx("Faucet", "RollupOwner", 30000, big.NewInt(9223372036854775807), nil),
		l1info.PrepareTx("Faucet", "Sequencer", 30000, big.NewInt(9223372036854775807), nil),
		l1info.PrepareTx("Faucet", "User", 30000, big.NewInt(9223372036854775807), nil)})

	l1TransactionOpts := l1info.GetDefaultTransactOpts("RollupOwner", ctx)
	locator, err := server_common.NewMachineLocator("")
	Require(t, err)
	serializedChainConfig, err := json.Marshal(chainConfig)
	Require(t, err)

	arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1client)
	l1Reader, err := headerreader.New(ctx, l1client, func() *headerreader.Config { return &headerreader.TestConfig }, arbSys)
	Require(t, err)
	l1Reader.Start(ctx)
	defer l1Reader.StopAndWait()

	nativeToken := common.Address{}
	maxDataSize := big.NewInt(117964)
	addresses, err := deploy.DeployOnL1(
		ctx,
		l1Reader,
		&l1TransactionOpts,
		l1info.GetAddress("Sequencer"),
		0,
		arbnode.GenerateRollupConfig(false, locator.LatestWasmModuleRoot(), l1info.GetAddress("RollupOwner"), chainConfig, serializedChainConfig, common.Address{}),
		nativeToken,
		maxDataSize,
	)
	Require(t, err)
	l1info.SetContract("Bridge", addresses.Bridge)
	l1info.SetContract("SequencerInbox", addresses.SequencerInbox)
	l1info.SetContract("Inbox", addresses.Inbox)
	l1info.SetContract("UpgradeExecutor", addresses.UpgradeExecutor)
	initMessage := getInitMessage(ctx, t, l1client, addresses)
	return addresses, initMessage
}

func createL2BlockChain(
	t *testing.T, l2info *BlockchainTestInfo, dataDir string, chainConfig *params.ChainConfig, cacheConfig *gethexec.CachingConfig,
) (*BlockchainTestInfo, *node.Node, ethdb.Database, ethdb.Database, *core.BlockChain) {
	return createL2BlockChainWithStackConfig(t, l2info, dataDir, chainConfig, nil, nil, cacheConfig)
}

func createL2BlockChainWithStackConfig(
	t *testing.T, l2info *BlockchainTestInfo, dataDir string, chainConfig *params.ChainConfig, initMessage *arbostypes.ParsedInitMessage, stackConfig *node.Config, cacheConfig *gethexec.CachingConfig,
) (*BlockchainTestInfo, *node.Node, ethdb.Database, ethdb.Database, *core.BlockChain) {
	if l2info == nil {
		l2info = NewArbTestInfo(t, chainConfig.ChainID)
	}
	var stack *node.Node
	var err error
	if stackConfig == nil {
		stackConfig = createStackConfigForTest(dataDir)
	}
	stack, err = node.New(stackConfig)
	Require(t, err)

	chainDb, err := stack.OpenDatabase("chaindb", 0, 0, "", false)
	Require(t, err)
	arbDb, err := stack.OpenDatabase("arbdb", 0, 0, "", false)
	Require(t, err)

	initReader := statetransfer.NewMemoryInitDataReader(&l2info.ArbInitData)
	if initMessage == nil {
		serializedChainConfig, err := json.Marshal(chainConfig)
		Require(t, err)
		initMessage = &arbostypes.ParsedInitMessage{
			ChainId:               chainConfig.ChainID,
			InitialL1BaseFee:      arbostypes.DefaultInitialL1BaseFee,
			ChainConfig:           chainConfig,
			SerializedChainConfig: serializedChainConfig,
		}
	}
	var coreCacheConfig *core.CacheConfig
	if cacheConfig != nil {
		coreCacheConfig = gethexec.DefaultCacheConfigFor(stack, cacheConfig)
	}
	blockchain, err := gethexec.WriteOrTestBlockChain(chainDb, coreCacheConfig, initReader, chainConfig, initMessage, gethexec.ConfigDefaultTest().TxLookupLimit, 0)
	Require(t, err)

	return l2info, stack, chainDb, arbDb, blockchain
}

func ClientForStack(t *testing.T, backend *node.Node) *ethclient.Client {
	rpcClient := backend.Attach()
	return ethclient.NewClient(rpcClient)
}

// Create and deploy L1 and arbnode for L2
func createTestNodeWithL1(
	t *testing.T,
	ctx context.Context,
	isSequencer bool,
	nodeConfig *arbnode.Config,
	execConfig *gethexec.Config,
	chainConfig *params.ChainConfig,
	stackConfig *node.Config,
	l2info_in info,
) (
	l2info info, currentNode *arbnode.Node, l2client *ethclient.Client, l2stack *node.Node,
	l1info info, l1backend *eth.Ethereum, l1client *ethclient.Client, l1stack *node.Node,
) {
	if nodeConfig == nil {
		nodeConfig = arbnode.ConfigDefaultL1Test()
	}
	if execConfig == nil {
		execConfig = gethexec.ConfigDefaultTest()
	}
	if chainConfig == nil {
		chainConfig = params.ArbitrumDevTestChainConfig()
	}
	fatalErrChan := make(chan error, 10)
	l1info, l1client, l1backend, l1stack = createTestL1BlockChain(t, nil)
	var l2chainDb ethdb.Database
	var l2arbDb ethdb.Database
	var l2blockchain *core.BlockChain
	l2info = l2info_in
	if l2info == nil {
		l2info = NewArbTestInfo(t, chainConfig.ChainID)
	}
	addresses, initMessage := DeployOnTestL1(t, ctx, l1info, l1client, chainConfig)
	_, l2stack, l2chainDb, l2arbDb, l2blockchain = createL2BlockChainWithStackConfig(t, l2info, "", chainConfig, initMessage, stackConfig, &execConfig.Caching)
	var sequencerTxOptsPtr *bind.TransactOpts
	var dataSigner signature.DataSignerFunc
	if isSequencer {
		sequencerTxOpts := l1info.GetDefaultTransactOpts("Sequencer", ctx)
		sequencerTxOptsPtr = &sequencerTxOpts
		dataSigner = signature.DataSignerFromPrivateKey(l1info.GetInfoWithPrivKey("Sequencer").PrivateKey)
	}

	if !isSequencer {
		nodeConfig.BatchPoster.Enable = false
		nodeConfig.Sequencer = false
		nodeConfig.DelayedSequencer.Enable = false
		execConfig.Sequencer.Enable = false
	}

	AddDefaultValNode(t, ctx, nodeConfig, true)

	Require(t, execConfig.Validate())
	execConfigFetcher := func() *gethexec.Config { return execConfig }
	execNode, err := gethexec.CreateExecutionNode(ctx, l2stack, l2chainDb, l2blockchain, l1client, execConfigFetcher)
	Require(t, err)

	currentNode, err = arbnode.CreateNode(
		ctx, l2stack, execNode, l2arbDb, NewFetcherFromConfig(nodeConfig), l2blockchain.Config(), l1client,
		addresses, sequencerTxOptsPtr, sequencerTxOptsPtr, dataSigner, fatalErrChan,
	)
	Require(t, err)

	Require(t, currentNode.Start(ctx))

	l2client = ClientForStack(t, l2stack)

	StartWatchChanErr(t, ctx, fatalErrChan, currentNode)

	return
}

// L2 -Only. Enough for tests that needs no interface to L1
// Requires precompiles.AllowDebugPrecompiles = true
func createTestNode(
	t *testing.T, ctx context.Context, l2Info *BlockchainTestInfo, nodeConfig *arbnode.Config, execConfig *gethexec.Config, takeOwnership bool,
) (*BlockchainTestInfo, *arbnode.Node, *ethclient.Client) {
	if nodeConfig == nil {
		nodeConfig = arbnode.ConfigDefaultL2Test()
	}
	if execConfig == nil {
		execConfig = gethexec.ConfigDefaultTest()
	}

	feedErrChan := make(chan error, 10)

	AddDefaultValNode(t, ctx, nodeConfig, true)

	l2info, stack, chainDb, arbDb, blockchain := createL2BlockChain(t, l2Info, "", params.ArbitrumDevTestChainConfig(), &execConfig.Caching)

	Require(t, execConfig.Validate())
	execConfigFetcher := func() *gethexec.Config { return execConfig }
	execNode, err := gethexec.CreateExecutionNode(ctx, stack, chainDb, blockchain, nil, execConfigFetcher)
	Require(t, err)

	currentNode, err := arbnode.CreateNode(ctx, stack, execNode, arbDb, NewFetcherFromConfig(nodeConfig), blockchain.Config(), nil, nil, nil, nil, nil, feedErrChan)
	Require(t, err)

	// Give the node an init message
	err = currentNode.TxStreamer.AddFakeInitMessage()
	Require(t, err)

	Require(t, currentNode.Start(ctx))
	client := ClientForStack(t, stack)

	if takeOwnership {
		debugAuth := l2info.GetDefaultTransactOpts("Owner", ctx)

		// make auth a chain owner
		arbdebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), client)
		Require(t, err, "failed to deploy ArbDebug")

		tx, err := arbdebug.BecomeChainOwner(&debugAuth)
		Require(t, err, "failed to deploy ArbDebug")

		_, err = EnsureTxSucceeded(ctx, client, tx)
		Require(t, err)
	}

	StartWatchChanErr(t, ctx, feedErrChan, currentNode)

	return l2info, currentNode, client
}

func StartWatchChanErr(t *testing.T, ctx context.Context, feedErrChan chan error, node *arbnode.Node) {
	go func() {
		select {
		case <-ctx.Done():
			return
		case err := <-feedErrChan:
			t.Errorf("error occurred: %v", err)
			if node != nil {
				node.StopAndWait()
			}
		}
	}()
}

func Require(t *testing.T, err error, text ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, text...)
}

func Fatal(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

func Create2ndNodeWithConfig(
	t *testing.T,
	ctx context.Context,
	first *arbnode.Node,
	l1stack *node.Node,
	l1info *BlockchainTestInfo,
	l2InitData *statetransfer.ArbosInitializationInfo,
	nodeConfig *arbnode.Config,
	execConfig *gethexec.Config,
	stackConfig *node.Config,
) (*ethclient.Client, *arbnode.Node) {
	if nodeConfig == nil {
		nodeConfig = arbnode.ConfigDefaultL1NonSequencerTest()
	}
	if execConfig == nil {
		execConfig = gethexec.ConfigDefaultNonSequencerTest()
	}
	feedErrChan := make(chan error, 10)
	l1rpcClient := l1stack.Attach()
	l1client := ethclient.NewClient(l1rpcClient)

	if stackConfig == nil {
		stackConfig = createStackConfigForTest(t.TempDir())
	}
	l2stack, err := node.New(stackConfig)
	Require(t, err)

	l2chainDb, err := l2stack.OpenDatabase("chaindb", 0, 0, "", false)
	Require(t, err)
	l2arbDb, err := l2stack.OpenDatabase("arbdb", 0, 0, "", false)
	Require(t, err)
	initReader := statetransfer.NewMemoryInitDataReader(l2InitData)

	dataSigner := signature.DataSignerFromPrivateKey(l1info.GetInfoWithPrivKey("Sequencer").PrivateKey)
	txOpts := l1info.GetDefaultTransactOpts("Sequencer", ctx)
	firstExec := getExecNode(t, first)

	chainConfig := firstExec.ArbInterface.BlockChain().Config()
	initMessage := getInitMessage(ctx, t, l1client, first.DeployInfo)

	coreCacheConfig := gethexec.DefaultCacheConfigFor(l2stack, &execConfig.Caching)
	l2blockchain, err := gethexec.WriteOrTestBlockChain(l2chainDb, coreCacheConfig, initReader, chainConfig, initMessage, gethexec.ConfigDefaultTest().TxLookupLimit, 0)
	Require(t, err)

	AddDefaultValNode(t, ctx, nodeConfig, true)

	Require(t, execConfig.Validate())
	configFetcher := func() *gethexec.Config { return execConfig }
	currentExec, err := gethexec.CreateExecutionNode(ctx, l2stack, l2chainDb, l2blockchain, l1client, configFetcher)
	Require(t, err)

	currentNode, err := arbnode.CreateNode(ctx, l2stack, currentExec, l2arbDb, NewFetcherFromConfig(nodeConfig), l2blockchain.Config(), l1client, first.DeployInfo, &txOpts, &txOpts, dataSigner, feedErrChan)
	Require(t, err)

	err = currentNode.Start(ctx)
	Require(t, err)
	l2client := ClientForStack(t, l2stack)

	StartWatchChanErr(t, ctx, feedErrChan, currentNode)

	return l2client, currentNode
}

func GetBalance(t *testing.T, ctx context.Context, client *ethclient.Client, account common.Address) *big.Int {
	t.Helper()
	balance, err := client.BalanceAt(ctx, account, nil)
	Require(t, err, "could not get balance")
	return balance
}

func requireClose(t *testing.T, s *node.Node, text ...interface{}) {
	t.Helper()
	Require(t, s.Close(), text...)
}

func authorizeDASKeyset(
	t *testing.T,
	ctx context.Context,
	dasSignerKey *blsSignatures.PublicKey,
	l1info info,
	l1client arbutil.L1Interface,
) {
	if dasSignerKey == nil {
		return
	}
	keyset := &arbstate.DataAvailabilityKeyset{
		AssumedHonest: 1,
		PubKeys:       []blsSignatures.PublicKey{*dasSignerKey},
	}
	wr := bytes.NewBuffer([]byte{})
	err := keyset.Serialize(wr)
	Require(t, err, "unable to serialize DAS keyset")
	keysetBytes := wr.Bytes()

	sequencerInboxABI, err := abi.JSON(strings.NewReader(bridgegen.SequencerInboxABI))
	Require(t, err, "unable to parse sequencer inbox ABI")
	setKeysetCalldata, err := sequencerInboxABI.Pack("setValidKeyset", keysetBytes)
	Require(t, err, "unable to generate calldata")

	upgradeExecutor, err := upgrade_executorgen.NewUpgradeExecutor(l1info.Accounts["UpgradeExecutor"].Address, l1client)
	Require(t, err, "unable to bind upgrade executor")

	trOps := l1info.GetDefaultTransactOpts("RollupOwner", ctx)
	tx, err := upgradeExecutor.ExecuteCall(&trOps, l1info.Accounts["SequencerInbox"].Address, setKeysetCalldata)
	Require(t, err, "unable to set valid keyset")

	_, err = EnsureTxSucceeded(ctx, l1client, tx)
	Require(t, err, "unable to ensure transaction success for setting valid keyset")
}

func setupConfigWithDAS(
	t *testing.T, ctx context.Context, dasModeString string,
) (*params.ChainConfig, *arbnode.Config, *das.LifecycleManager, string, *blsSignatures.PublicKey) {
	l1NodeConfigA := arbnode.ConfigDefaultL1Test()
	chainConfig := params.ArbitrumDevTestChainConfig()
	var dbPath string
	var err error

	enableFileStorage, enableDbStorage, enableDas := false, false, true
	switch dasModeString {
	case "db":
		enableDbStorage = true
		chainConfig = params.ArbitrumDevTestDASChainConfig()
	case "files":
		enableFileStorage = true
		chainConfig = params.ArbitrumDevTestDASChainConfig()
	case "onchain":
		enableDas = false
	default:
		Fatal(t, "unknown storage type")
	}
	dbPath = t.TempDir()
	dasSignerKey, _, err := das.GenerateAndStoreKeys(dbPath)
	Require(t, err)

	dasConfig := &das.DataAvailabilityConfig{
		Enable: enableDas,
		Key: das.KeyConfig{
			KeyDir: dbPath,
		},
		LocalFileStorage: das.LocalFileStorageConfig{
			Enable:  enableFileStorage,
			DataDir: dbPath,
		},
		LocalDBStorage: das.LocalDBStorageConfig{
			Enable:  enableDbStorage,
			DataDir: dbPath,
		},
		RequestTimeout:           5 * time.Second,
		ParentChainNodeURL:       "none",
		SequencerInboxAddress:    "none",
		PanicOnError:             true,
		DisableSignatureChecking: true,
	}

	l1NodeConfigA.DataAvailability = das.DefaultDataAvailabilityConfig
	var lifecycleManager *das.LifecycleManager
	var daReader das.DataAvailabilityServiceReader
	var daWriter das.DataAvailabilityServiceWriter
	var daHealthChecker das.DataAvailabilityServiceHealthChecker
	if dasModeString != "onchain" {
		daReader, daWriter, daHealthChecker, lifecycleManager, err = das.CreateDAComponentsForDaserver(ctx, dasConfig, nil, nil)

		Require(t, err)
		rpcLis, err := net.Listen("tcp", "localhost:0")
		Require(t, err)
		restLis, err := net.Listen("tcp", "localhost:0")
		Require(t, err)
		_, err = das.StartDASRPCServerOnListener(ctx, rpcLis, genericconf.HTTPServerTimeoutConfigDefault, daReader, daWriter, daHealthChecker)
		Require(t, err)
		_, err = das.NewRestfulDasServerOnListener(restLis, genericconf.HTTPServerTimeoutConfigDefault, daReader, daHealthChecker)
		Require(t, err)

		beConfigA := das.BackendConfig{
			URL:                 "http://" + rpcLis.Addr().String(),
			PubKeyBase64Encoded: blsPubToBase64(dasSignerKey),
			SignerMask:          1,
		}
		l1NodeConfigA.DataAvailability.RPCAggregator = aggConfigForBackend(t, beConfigA)
		l1NodeConfigA.DataAvailability.Enable = true
		l1NodeConfigA.DataAvailability.RestAggregator = das.DefaultRestfulClientAggregatorConfig
		l1NodeConfigA.DataAvailability.RestAggregator.Enable = true
		l1NodeConfigA.DataAvailability.RestAggregator.Urls = []string{"http://" + restLis.Addr().String()}
		l1NodeConfigA.DataAvailability.ParentChainNodeURL = "none"
	}

	return chainConfig, l1NodeConfigA, lifecycleManager, dbPath, dasSignerKey
}

func getDeadlineTimeout(t *testing.T, defaultTimeout time.Duration) time.Duration {
	testDeadLine, deadlineExist := t.Deadline()
	var timeout time.Duration
	if deadlineExist {
		timeout = time.Until(testDeadLine) - (time.Second * 10)
		if timeout > time.Second*10 {
			timeout = timeout - (time.Second * 10)
		}
	} else {
		timeout = defaultTimeout
	}

	return timeout
}

func deploySimple(
	t *testing.T, ctx context.Context, auth bind.TransactOpts, client *ethclient.Client,
) (common.Address, *mocksgen.Simple) {
	addr, tx, simple, err := mocksgen.DeploySimple(&auth, client)
	Require(t, err, "could not deploy Simple.sol contract")
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	return addr, simple
}

func TestMain(m *testing.M) {
	logLevelEnv := os.Getenv("TEST_LOGLEVEL")
	if logLevelEnv != "" {
		logLevel, err := strconv.ParseUint(logLevelEnv, 10, 32)
		if err != nil || logLevel > uint64(log.LvlTrace) {
			log.Warn("TEST_LOGLEVEL exists but out of bound, ignoring", "logLevel", logLevelEnv, "max", log.LvlTrace)
		}
		glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
		glogger.Verbosity(log.Lvl(logLevel))
		log.Root().SetHandler(glogger)
	}
	code := m.Run()
	os.Exit(code)
}

func getExecNode(t *testing.T, node *arbnode.Node) *gethexec.ExecutionNode {
	t.Helper()
	gethExec, ok := node.Execution.(*gethexec.ExecutionNode)
	if !ok {
		t.Fatal("failed to get exec node from arbnode")
	}
	return gethExec
}

'''
'''--- system_tests/conditionaltx_test.go ---
package arbtest

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"math/big"
	"strings"
	"sync"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/arbitrum_types"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
)

func getStorageRootHash(t *testing.T, execNode *gethexec.ExecutionNode, address common.Address) common.Hash {
	t.Helper()
	statedb, err := execNode.Backend.ArbInterface().BlockChain().State()
	Require(t, err)
	trie, err := statedb.StorageTrie(address)
	Require(t, err)
	return trie.Hash()
}

func getStorageSlotValue(t *testing.T, execNode *gethexec.ExecutionNode, address common.Address) map[common.Hash]common.Hash {
	t.Helper()
	statedb, err := execNode.Backend.ArbInterface().BlockChain().State()
	Require(t, err)
	slotValue := make(map[common.Hash]common.Hash)
	Require(t, err)
	err = statedb.ForEachStorage(address, func(key, value common.Hash) bool {
		slotValue[key] = value
		return true
	})
	Require(t, err)
	return slotValue
}

func testConditionalTxThatShouldSucceed(t *testing.T, ctx context.Context, idx int, l2info info, rpcClient *rpc.Client, options *arbitrum_types.ConditionalOptions) {
	t.Helper()
	tx := l2info.PrepareTx("Owner", "User2", l2info.TransferGas, big.NewInt(1e12), nil)
	err := arbitrum.SendConditionalTransactionRPC(ctx, rpcClient, tx, options)
	if err != nil {
		Fatal(t, "SendConditionalTransactionRPC failed, idx:", idx, "err:", err)
	}
}

func testConditionalTxThatShouldFail(t *testing.T, ctx context.Context, idx int, l2info info, rpcClient *rpc.Client, options *arbitrum_types.ConditionalOptions, expectedErrorCode int) {
	t.Helper()
	accountInfo := l2info.GetInfoWithPrivKey("Owner")
	nonce := accountInfo.Nonce
	tx := l2info.PrepareTx("Owner", "User2", l2info.TransferGas, big.NewInt(1e12), nil)
	err := arbitrum.SendConditionalTransactionRPC(ctx, rpcClient, tx, options)
	if err == nil {
		if options == nil {
			Fatal(t, "SendConditionalTransactionRPC didn't fail as expected, idx:", idx, "options:", options)
		} else {
			Fatal(t, "SendConditionalTransactionRPC didn't fail as expected, idx:", idx, "options:", *options)
		}
	} else {
		var rErr rpc.Error
		if errors.As(err, &rErr) {
			if rErr.ErrorCode() != expectedErrorCode {
				Fatal(t, "unexpected error code, have:", rErr.ErrorCode(), "want:", expectedErrorCode)
			}
		} else {
			Fatal(t, "unexpected error type, err:", err)
		}
	}
	accountInfo.Nonce = nonce // revert nonce as the tx failed
}

func getEmptyOptions(address common.Address) []*arbitrum_types.ConditionalOptions {
	return []*arbitrum_types.ConditionalOptions{
		{},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {SlotValue: map[common.Hash]common.Hash{}}}},
	}
}

func getOptions(address common.Address, rootHash common.Hash, slotValueMap map[common.Hash]common.Hash) []*arbitrum_types.ConditionalOptions {
	return []*arbitrum_types.ConditionalOptions{
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {RootHash: &rootHash}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {RootHash: &rootHash}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {SlotValue: slotValueMap}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {SlotValue: slotValueMap}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {RootHash: &rootHash}}},
		{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{address: {SlotValue: slotValueMap}}},
	}
}

func getFulfillableBlockTimeLimits(t *testing.T, blockNumber uint64, timestamp uint64) []*arbitrum_types.ConditionalOptions {
	future := math.HexOrDecimal64(timestamp + 40)
	past := math.HexOrDecimal64(timestamp - 1)
	futureBlockNumber := math.HexOrDecimal64(blockNumber + 1000)
	currentBlockNumber := math.HexOrDecimal64(blockNumber)
	return getBlockTimeLimits(t, currentBlockNumber, futureBlockNumber, past, future)
}

func getUnfulfillableBlockTimeLimits(t *testing.T, blockNumber uint64, timestamp uint64) []*arbitrum_types.ConditionalOptions {
	future := math.HexOrDecimal64(timestamp + 30)
	past := math.HexOrDecimal64(timestamp - 1)
	futureBlockNumber := math.HexOrDecimal64(blockNumber + 1000)
	previousBlockNumber := math.HexOrDecimal64(blockNumber - 1)
	// skip first empty options
	return getBlockTimeLimits(t, futureBlockNumber, previousBlockNumber, future, past)[1:]
}

func getBlockTimeLimits(t *testing.T, blockMin, blockMax math.HexOrDecimal64, timeMin, timeMax math.HexOrDecimal64) []*arbitrum_types.ConditionalOptions {
	basic := []*arbitrum_types.ConditionalOptions{
		{},
		{TimestampMin: &timeMin},
		{TimestampMax: &timeMax},
		{BlockNumberMin: &blockMin},
		{BlockNumberMax: &blockMax},
	}
	power := []*arbitrum_types.ConditionalOptions{
		{},
	}
	for range basic {
		power = optionsProduct(power, basic)
	}
	return dedupOptions(t, power)
}

func optionsDedupProduct(t *testing.T, optionsA, optionsB []*arbitrum_types.ConditionalOptions) []*arbitrum_types.ConditionalOptions {
	return dedupOptions(t, optionsProduct(optionsA, optionsB))
}

// Product of options slices, where each element from optionsA is merged with element of optionsB
// The merge involves:
// * merging KnownAccounts maps, where in case of key collision the value is taken from optionsB element
// * assigning new block and timestamp limits preferring values from optionsB element
func optionsProduct(optionsA, optionsB []*arbitrum_types.ConditionalOptions) []*arbitrum_types.ConditionalOptions {
	var optionsC []*arbitrum_types.ConditionalOptions
	for _, a := range optionsA {
		for _, b := range optionsB {
			var c arbitrum_types.ConditionalOptions
			c.KnownAccounts = make(map[common.Address]arbitrum_types.RootHashOrSlots)
			for k, v := range a.KnownAccounts {
				c.KnownAccounts[k] = v
			}
			for k, v := range b.KnownAccounts {
				c.KnownAccounts[k] = v
			}
			limitTriples := []struct {
				a *math.HexOrDecimal64
				b *math.HexOrDecimal64
				c **math.HexOrDecimal64
			}{
				{a.BlockNumberMin, b.BlockNumberMin, &c.BlockNumberMin},
				{a.BlockNumberMax, b.BlockNumberMax, &c.BlockNumberMax},
				{a.TimestampMin, b.TimestampMin, &c.TimestampMin},
				{a.TimestampMax, b.TimestampMax, &c.TimestampMax},
			}
			for _, tripple := range limitTriples {
				if tripple.b != nil {
					value := math.HexOrDecimal64(*tripple.b)
					*tripple.c = &value
				} else if tripple.a != nil {
					value := math.HexOrDecimal64(*tripple.a)
					*tripple.c = &value
				} else {
					*tripple.c = nil
				}
			}
			optionsC = append(optionsC, &c)
		}
	}
	return optionsC
}

func dedupOptions(t *testing.T, options []*arbitrum_types.ConditionalOptions) []*arbitrum_types.ConditionalOptions {
	var result []*arbitrum_types.ConditionalOptions
	seenBefore := make(map[common.Hash]struct{})
	for _, opt := range options {
		data, err := json.Marshal(opt)
		Require(t, err)
		dataHash := crypto.Keccak256Hash(data)
		_, seen := seenBefore[dataHash]
		if !seen {
			result = append(result, opt)
			seenBefore[dataHash] = struct{}{}
		}
	}
	return result
}

func TestSendRawTransactionConditionalBasic(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	contractAddress1, simple1 := builder.L2.DeploySimple(t, auth)
	tx, err := simple1.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	contractAddress2, simple2 := builder.L2.DeploySimple(t, auth)
	tx, err = simple2.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	tx, err = simple2.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	currentRootHash1 := getStorageRootHash(t, builder.L2.ExecNode, contractAddress1)
	currentSlotValueMap1 := getStorageSlotValue(t, builder.L2.ExecNode, contractAddress1)
	currentRootHash2 := getStorageRootHash(t, builder.L2.ExecNode, contractAddress2)
	currentSlotValueMap2 := getStorageSlotValue(t, builder.L2.ExecNode, contractAddress2)

	rpcClient := builder.L2.ConsensusNode.Stack.Attach()
	builder.L2Info.GenerateAccount("User2")

	testConditionalTxThatShouldSucceed(t, ctx, -1, builder.L2Info, rpcClient, nil)
	for i, options := range getEmptyOptions(contractAddress1) {
		testConditionalTxThatShouldSucceed(t, ctx, i, builder.L2Info, rpcClient, options)
	}

	block, err := builder.L1.Client.BlockByNumber(ctx, nil)
	Require(t, err)
	blockNumber := block.NumberU64()

	currentL2BlockTime := func() uint64 {
		l2Block, err := builder.L2.Client.BlockByNumber(ctx, nil)
		Require(t, err)
		return l2Block.Time()
	}

	optionsA := getOptions(contractAddress1, currentRootHash1, currentSlotValueMap1)
	optionsB := getOptions(contractAddress2, currentRootHash2, currentSlotValueMap2)
	optionsAB := optionsProduct(optionsA, optionsB)
	options1 := dedupOptions(t, append(append(optionsAB, optionsA...), optionsB...))
	options1 = optionsDedupProduct(t, options1, getFulfillableBlockTimeLimits(t, blockNumber, currentL2BlockTime()))
	for i, options := range options1 {
		testConditionalTxThatShouldSucceed(t, ctx, i, builder.L2Info, rpcClient, options)
	}

	tx, err = simple1.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	tx, err = simple2.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	previousStorageRootHash1 := currentRootHash1
	currentRootHash1 = getStorageRootHash(t, builder.L2.ExecNode, contractAddress1)
	if bytes.Equal(previousStorageRootHash1.Bytes(), currentRootHash1.Bytes()) {
		Fatal(t, "storage root hash didn't change as expected")
	}
	currentSlotValueMap1 = getStorageSlotValue(t, builder.L2.ExecNode, contractAddress1)

	previousStorageRootHash2 := currentRootHash2
	currentRootHash2 = getStorageRootHash(t, builder.L2.ExecNode, contractAddress2)
	if bytes.Equal(previousStorageRootHash2.Bytes(), currentRootHash2.Bytes()) {
		Fatal(t, "storage root hash didn't change as expected")
	}
	currentSlotValueMap2 = getStorageSlotValue(t, builder.L2.ExecNode, contractAddress2)

	block, err = builder.L1.Client.BlockByNumber(ctx, nil)
	Require(t, err)
	blockNumber = block.NumberU64()

	optionsC := getOptions(contractAddress1, currentRootHash1, currentSlotValueMap1)
	optionsD := getOptions(contractAddress2, currentRootHash2, currentSlotValueMap2)
	optionsCD := optionsProduct(optionsC, optionsD)
	options2 := dedupOptions(t, append(append(optionsCD, optionsC...), optionsD...))
	options2 = optionsDedupProduct(t, options2, getFulfillableBlockTimeLimits(t, blockNumber, currentL2BlockTime()))
	for i, options := range options2 {
		testConditionalTxThatShouldSucceed(t, ctx, i, builder.L2Info, rpcClient, options)
	}
	for i, options := range options1 {
		testConditionalTxThatShouldFail(t, ctx, i, builder.L2Info, rpcClient, options, -32003)
	}
	block, err = builder.L1.Client.BlockByNumber(ctx, nil)
	Require(t, err)
	blockNumber = block.NumberU64()
	options3 := optionsDedupProduct(t, options2, getUnfulfillableBlockTimeLimits(t, blockNumber, currentL2BlockTime()))
	for i, options := range options3 {
		testConditionalTxThatShouldFail(t, ctx, i, builder.L2Info, rpcClient, options, -32003)
	}
	options4 := optionsDedupProduct(t, options2, options1)
	for i, options := range options4 {
		testConditionalTxThatShouldFail(t, ctx, i, builder.L2Info, rpcClient, options, -32003)
	}
}

func TestSendRawTransactionConditionalMultiRoutine(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	rpcClient := builder.L2.ConsensusNode.Stack.Attach()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	contractAddress, simple := builder.L2.DeploySimple(t, auth)

	simpleContract, err := abi.JSON(strings.NewReader(mocksgen.SimpleABI))
	Require(t, err)

	numTxes := 200
	expectedSuccesses := numTxes / 20
	var txes types.Transactions
	var options []*arbitrum_types.ConditionalOptions
	for i := 0; i < numTxes; i++ {
		account := fmt.Sprintf("User%v", i)
		builder.L2Info.GenerateAccount(account)
		tx := builder.L2Info.PrepareTx("Owner", account, builder.L2Info.TransferGas, big.NewInt(1e16), nil)
		err := builder.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}
	for i := numTxes - 1; i >= 0; i-- {
		expected := i % expectedSuccesses
		data, err := simpleContract.Pack("logAndIncrement", big.NewInt(int64(expected)))
		Require(t, err)
		account := fmt.Sprintf("User%v", i)
		txes = append(txes, builder.L2Info.PrepareTxTo(account, &contractAddress, builder.L2Info.TransferGas, big.NewInt(0), data))
		options = append(options, &arbitrum_types.ConditionalOptions{KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{contractAddress: {SlotValue: map[common.Hash]common.Hash{{0}: common.BigToHash(big.NewInt(int64(expected)))}}}})
	}
	ctxWithTimeout, cancelCtxWithTimeout := context.WithTimeout(ctx, 5*time.Second)
	success := make(chan struct{}, len(txes))
	wg := sync.WaitGroup{}
	for i := 0; i < len(txes); i++ {
		wg.Add(1)
		tx := txes[i]
		opts := options[i]
		go func() {
			defer wg.Done()
			for ctxWithTimeout.Err() == nil {
				err := arbitrum.SendConditionalTransactionRPC(ctxWithTimeout, rpcClient, tx, opts)
				if err == nil {
					success <- struct{}{}
					break
				}
			}
		}()
	}
	for i := 0; i < expectedSuccesses; i++ {
		select {
		case <-success:
		case <-ctxWithTimeout.Done():
			Fatal(t, "test timeouted")
		}
	}
	cancelCtxWithTimeout()
	wg.Wait()
	bc := builder.L2.ExecNode.Backend.ArbInterface().BlockChain()
	genesis := bc.Config().ArbitrumChainParams.GenesisBlockNum

	var receipts types.Receipts
	header := bc.GetHeaderByNumber(genesis)
	for i := genesis + 1; header != nil; i++ {
		blockReceipts := bc.GetReceiptsByHash(header.Hash())
		if blockReceipts == nil {
			Fatal(t, "Failed to get block receipts, block number:", header.Number)
		}
		receipts = append(receipts, blockReceipts...)
		header = bc.GetHeaderByNumber(i)
	}

	succeeded := 0
	for _, receipt := range receipts {
		if receipt.Status == types.ReceiptStatusSuccessful && len(receipt.Logs) == 1 {
			parsed, err := simple.ParseLogAndIncrementCalled(*receipt.Logs[0])
			Require(t, err)
			if parsed.Expected.Int64() != parsed.Have.Int64() {
				Fatal(t, "Got invalid log, log.Expected:", parsed.Expected, "log.Have:", parsed.Have)
			} else {
				succeeded++
			}
		}
	}
	if succeeded != expectedSuccesses {
		Fatal(t, "Unexpected number of successful txes, want:", numTxes, "have:", succeeded)
	}
}

func TestSendRawTransactionConditionalPreCheck(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.execConfig.Sequencer.MaxBlockSpeed = 0
	builder.execConfig.TxPreChecker.Strictness = gethexec.TxPreCheckerStrictnessLikelyCompatible
	builder.execConfig.TxPreChecker.RequiredStateAge = 1
	builder.execConfig.TxPreChecker.RequiredStateMaxBlocks = 2
	cleanup := builder.Build(t)
	defer cleanup()

	rpcClient := builder.L2.ConsensusNode.Stack.Attach()

	builder.L2Info.GenerateAccount("User2")

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	start := time.Now().Unix()
	contractAddress, simple := builder.L2.DeploySimple(t, auth)
	if time.Since(time.Unix(start, 0)) > 200*time.Millisecond {
		start++
		time.Sleep(time.Until(time.Unix(start, 0)))
	}
	tx, err := simple.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	currentRootHash := getStorageRootHash(t, builder.L2.ExecNode, contractAddress)
	options := &arbitrum_types.ConditionalOptions{
		KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{
			contractAddress: {RootHash: &currentRootHash},
		},
	}
	testConditionalTxThatShouldFail(t, ctx, 0, builder.L2Info, rpcClient, options, -32003)
	time.Sleep(time.Until(time.Unix(start+1, 0)))
	testConditionalTxThatShouldSucceed(t, ctx, 1, builder.L2Info, rpcClient, options)

	start = time.Now().Unix()
	if time.Since(time.Unix(start, 0)) > 200*time.Millisecond {
		start++
		time.Sleep(time.Until(time.Unix(start, 0)))
	}
	tx, err = simple.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	currentRootHash = getStorageRootHash(t, builder.L2.ExecNode, contractAddress)
	options = &arbitrum_types.ConditionalOptions{
		KnownAccounts: map[common.Address]arbitrum_types.RootHashOrSlots{
			contractAddress: {RootHash: &currentRootHash},
		},
	}
	testConditionalTxThatShouldFail(t, ctx, 2, builder.L2Info, rpcClient, options, -32003)
	tx = builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)
	Require(t, builder.L2.Client.SendTransaction(ctx, tx))
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	testConditionalTxThatShouldFail(t, ctx, 3, builder.L2Info, rpcClient, options, -32003)
	tx = builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)
	Require(t, builder.L2.Client.SendTransaction(ctx, tx))
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	testConditionalTxThatShouldSucceed(t, ctx, 4, builder.L2Info, rpcClient, options)
}

'''
'''--- system_tests/contract_tx_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"bytes"
	"context"
	"encoding/hex"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestContractTxDeploy(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	cleanup := builder.Build(t)
	defer cleanup()

	from := common.HexToAddress("0x123412341234")
	builder.L2.TransferBalanceTo(t, "Faucet", from, big.NewInt(1e18), builder.L2Info)

	for stateNonce := uint64(0); stateNonce < 2; stateNonce++ {
		pos, err := builder.L2.ConsensusNode.TxStreamer.GetMessageCount()
		Require(t, err)
		var delayedMessagesRead uint64
		if pos > 0 {
			lastMessage, err := builder.L2.ConsensusNode.TxStreamer.GetMessage(pos - 1)
			Require(t, err)
			delayedMessagesRead = lastMessage.DelayedMessagesRead
		}
		// Deploys a single 0xFE (INVALID) byte as a smart contract
		deployCode := []byte{
			0x60, 0xFE, // PUSH1 0xFE
			0x60, 0x00, // PUSH1 0
			0x53,       // MSTORE8
			0x60, 0x01, // PUSH1 1
			0x60, 0x00, // PUSH1 0
			0xF3, // RETURN
		}
		var requestId common.Hash
		requestId[0] = uint8(stateNonce)
		contractTx := &types.ArbitrumContractTx{
			ChainId:   params.ArbitrumDevTestChainConfig().ChainID,
			RequestId: requestId,
			From:      from,
			GasFeeCap: big.NewInt(1e9),
			Gas:       1e6,
			To:        nil,
			Value:     big.NewInt(0),
			Data:      deployCode,
		}
		l2Msg := []byte{arbos.L2MessageKind_ContractTx}
		l2Msg = append(l2Msg, math.U256Bytes(arbmath.UintToBig(contractTx.Gas))...)
		l2Msg = append(l2Msg, math.U256Bytes(contractTx.GasFeeCap)...)
		l2Msg = append(l2Msg, common.Hash{}.Bytes()...) // to is zero, translated into nil
		l2Msg = append(l2Msg, math.U256Bytes(contractTx.Value)...)
		l2Msg = append(l2Msg, contractTx.Data...)

		err = builder.L2.ConsensusNode.TxStreamer.AddMessages(pos, true, []arbostypes.MessageWithMetadata{
			{
				Message: &arbostypes.L1IncomingMessage{
					Header: &arbostypes.L1IncomingMessageHeader{
						Kind:        arbostypes.L1MessageType_L2Message,
						Poster:      from,
						BlockNumber: 0,
						Timestamp:   0,
						RequestId:   &contractTx.RequestId,
						L1BaseFee:   &big.Int{},
					},
					L2msg:        l2Msg,
					BatchGasCost: new(uint64),
				},
				DelayedMessagesRead: delayedMessagesRead,
			},
		})
		Require(t, err)

		txHash := types.NewTx(contractTx).Hash()
		t.Log("made contract tx", contractTx, "with hash", txHash)
		receipt, err := WaitForTx(ctx, builder.L2.Client, txHash, time.Second*10)
		Require(t, err)
		if receipt.Status != types.ReceiptStatusSuccessful {
			Fatal(t, "Receipt has non-successful status", receipt.Status)
		}

		expectedAddr := crypto.CreateAddress(from, stateNonce)
		if receipt.ContractAddress != expectedAddr {
			Fatal(t, "expected address", from, "nonce", stateNonce, "to deploy to", expectedAddr, "but got", receipt.ContractAddress)
		}
		t.Log("deployed contract", receipt.ContractAddress, "from address", from, "with nonce", stateNonce)
		stateNonce++

		code, err := builder.L2.Client.CodeAt(ctx, receipt.ContractAddress, nil)
		Require(t, err)
		if !bytes.Equal(code, []byte{0xFE}) {
			Fatal(t, "expected contract", receipt.ContractAddress, "code of 0xFE but got", hex.EncodeToString(code))
		}
	}
}

'''
'''--- system_tests/das_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"encoding/base64"
	"encoding/json"
	"math/big"
	"net"
	"net/http"
	"os"
	"strconv"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/blsSignatures"
	"github.com/offchainlabs/nitro/cmd/genericconf"
	"github.com/offchainlabs/nitro/das"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/headerreader"
	"github.com/offchainlabs/nitro/util/signature"
)

func startLocalDASServer(
	t *testing.T,
	ctx context.Context,
	dataDir string,
	l1client arbutil.L1Interface,
	seqInboxAddress common.Address,
) (*http.Server, *blsSignatures.PublicKey, das.BackendConfig, *das.RestfulDasServer, string) {
	keyDir := t.TempDir()
	pubkey, _, err := das.GenerateAndStoreKeys(keyDir)
	Require(t, err)

	config := das.DataAvailabilityConfig{
		Enable: true,
		Key: das.KeyConfig{
			KeyDir: keyDir,
		},
		LocalFileStorage: das.LocalFileStorageConfig{
			Enable:  true,
			DataDir: dataDir,
		},
		ParentChainNodeURL: "none",
		RequestTimeout:     5 * time.Second,
	}

	var syncFromStorageServices []*das.IterableStorageService
	var syncToStorageServices []das.StorageService
	storageService, lifecycleManager, err := das.CreatePersistentStorageService(ctx, &config, &syncFromStorageServices, &syncToStorageServices)
	defer lifecycleManager.StopAndWaitUntil(time.Second)

	Require(t, err)
	seqInboxCaller, err := bridgegen.NewSequencerInboxCaller(seqInboxAddress, l1client)
	Require(t, err)
	privKey, err := config.Key.BLSPrivKey()
	Require(t, err)
	daWriter, err := das.NewSignAfterStoreDASWriterWithSeqInboxCaller(privKey, seqInboxCaller, storageService, "")
	Require(t, err)
	rpcLis, err := net.Listen("tcp", "localhost:0")
	Require(t, err)
	rpcServer, err := das.StartDASRPCServerOnListener(ctx, rpcLis, genericconf.HTTPServerTimeoutConfigDefault, storageService, daWriter, storageService)
	Require(t, err)
	restLis, err := net.Listen("tcp", "localhost:0")
	Require(t, err)
	restServer, err := das.NewRestfulDasServerOnListener(restLis, genericconf.HTTPServerTimeoutConfigDefault, storageService, storageService)
	Require(t, err)
	beConfig := das.BackendConfig{
		URL:                 "http://" + rpcLis.Addr().String(),
		PubKeyBase64Encoded: blsPubToBase64(pubkey),
		SignerMask:          1,
	}
	return rpcServer, pubkey, beConfig, restServer, "http://" + restLis.Addr().String()
}

func blsPubToBase64(pubkey *blsSignatures.PublicKey) string {
	pubkeyBytes := blsSignatures.PublicKeyToBytes(*pubkey)
	encodedPubkey := make([]byte, base64.StdEncoding.EncodedLen(len(pubkeyBytes)))
	base64.StdEncoding.Encode(encodedPubkey, pubkeyBytes)
	return string(encodedPubkey)
}

func aggConfigForBackend(t *testing.T, backendConfig das.BackendConfig) das.AggregatorConfig {
	backendsJsonByte, err := json.Marshal([]das.BackendConfig{backendConfig})
	Require(t, err)
	return das.AggregatorConfig{
		Enable:        true,
		AssumedHonest: 1,
		Backends:      string(backendsJsonByte),
	}
}

func TestDASRekey(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Setup L1 chain and contracts
	chainConfig := params.ArbitrumDevTestDASChainConfig()
	l1info, l1client, _, l1stack := createTestL1BlockChain(t, nil)
	defer requireClose(t, l1stack)
	feedErrChan := make(chan error, 10)
	addresses, initMessage := DeployOnTestL1(t, ctx, l1info, l1client, chainConfig)

	// Setup DAS servers
	dasDataDir := t.TempDir()
	nodeDir := t.TempDir()
	dasRpcServerA, pubkeyA, backendConfigA, _, restServerUrlA := startLocalDASServer(t, ctx, dasDataDir, l1client, addresses.SequencerInbox)
	l2info := NewArbTestInfo(t, chainConfig.ChainID)
	l1NodeConfigA := arbnode.ConfigDefaultL1Test()
	l1NodeConfigB := arbnode.ConfigDefaultL1NonSequencerTest()
	sequencerTxOpts := l1info.GetDefaultTransactOpts("Sequencer", ctx)
	sequencerTxOptsPtr := &sequencerTxOpts

	{
		authorizeDASKeyset(t, ctx, pubkeyA, l1info, l1client)

		// Setup L2 chain
		_, l2stackA, l2chainDb, l2arbDb, l2blockchain := createL2BlockChainWithStackConfig(t, l2info, nodeDir, chainConfig, initMessage, nil, nil)
		l2info.GenerateAccount("User2")

		// Setup DAS config

		l1NodeConfigA.DataAvailability.Enable = true
		l1NodeConfigA.DataAvailability.RPCAggregator = aggConfigForBackend(t, backendConfigA)
		l1NodeConfigA.DataAvailability.RestAggregator = das.DefaultRestfulClientAggregatorConfig
		l1NodeConfigA.DataAvailability.RestAggregator.Enable = true
		l1NodeConfigA.DataAvailability.RestAggregator.Urls = []string{restServerUrlA}
		l1NodeConfigA.DataAvailability.ParentChainNodeURL = "none"
		execA, err := gethexec.CreateExecutionNode(ctx, l2stackA, l2chainDb, l2blockchain, l1client, gethexec.ConfigDefaultTest)
		Require(t, err)

		nodeA, err := arbnode.CreateNode(ctx, l2stackA, execA, l2arbDb, NewFetcherFromConfig(l1NodeConfigA), l2blockchain.Config(), l1client, addresses, sequencerTxOptsPtr, sequencerTxOptsPtr, nil, feedErrChan)
		Require(t, err)
		Require(t, nodeA.Start(ctx))
		l2clientA := ClientForStack(t, l2stackA)

		l1NodeConfigB.BlockValidator.Enable = false
		l1NodeConfigB.DataAvailability.Enable = true
		l1NodeConfigB.DataAvailability.RestAggregator = das.DefaultRestfulClientAggregatorConfig
		l1NodeConfigB.DataAvailability.RestAggregator.Enable = true
		l1NodeConfigB.DataAvailability.RestAggregator.Urls = []string{restServerUrlA}

		l1NodeConfigB.DataAvailability.ParentChainNodeURL = "none"

		l2clientB, nodeB := Create2ndNodeWithConfig(t, ctx, nodeA, l1stack, l1info, &l2info.ArbInitData, l1NodeConfigB, nil, nil)
		checkBatchPosting(t, ctx, l1client, l2clientA, l1info, l2info, big.NewInt(1e12), l2clientB)
		nodeA.StopAndWait()
		nodeB.StopAndWait()
	}

	err := dasRpcServerA.Shutdown(ctx)
	Require(t, err)
	dasRpcServerB, pubkeyB, backendConfigB, _, _ := startLocalDASServer(t, ctx, dasDataDir, l1client, addresses.SequencerInbox)
	defer func() {
		err = dasRpcServerB.Shutdown(ctx)
		Require(t, err)
	}()
	authorizeDASKeyset(t, ctx, pubkeyB, l1info, l1client)

	// Restart the node on the new keyset against the new DAS server running on the same disk as the first with new keys

	stackConfig := createStackConfigForTest(nodeDir)
	l2stackA, err := node.New(stackConfig)
	Require(t, err)

	l2chainDb, err := l2stackA.OpenDatabase("chaindb", 0, 0, "", false)
	Require(t, err)

	l2arbDb, err := l2stackA.OpenDatabase("arbdb", 0, 0, "", false)
	Require(t, err)

	l2blockchain, err := gethexec.GetBlockChain(l2chainDb, nil, chainConfig, gethexec.ConfigDefaultTest().TxLookupLimit)
	Require(t, err)

	execA, err := gethexec.CreateExecutionNode(ctx, l2stackA, l2chainDb, l2blockchain, l1client, gethexec.ConfigDefaultTest)
	Require(t, err)

	l1NodeConfigA.DataAvailability.RPCAggregator = aggConfigForBackend(t, backendConfigB)
	nodeA, err := arbnode.CreateNode(ctx, l2stackA, execA, l2arbDb, NewFetcherFromConfig(l1NodeConfigA), l2blockchain.Config(), l1client, addresses, sequencerTxOptsPtr, sequencerTxOptsPtr, nil, feedErrChan)
	Require(t, err)
	Require(t, nodeA.Start(ctx))
	l2clientA := ClientForStack(t, l2stackA)

	l2clientB, nodeB := Create2ndNodeWithConfig(t, ctx, nodeA, l1stack, l1info, &l2info.ArbInitData, l1NodeConfigB, nil, nil)
	checkBatchPosting(t, ctx, l1client, l2clientA, l1info, l2info, big.NewInt(2e12), l2clientB)

	nodeA.StopAndWait()
	nodeB.StopAndWait()
}

func checkBatchPosting(t *testing.T, ctx context.Context, l1client, l2clientA *ethclient.Client, l1info, l2info info, expectedBalance *big.Int, l2ClientsToCheck ...*ethclient.Client) {
	tx := l2info.PrepareTx("Owner", "User2", l2info.TransferGas, big.NewInt(1e12), nil)
	err := l2clientA.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = EnsureTxSucceeded(ctx, l2clientA, tx)
	Require(t, err)

	// give the inbox reader a bit of time to pick up the delayed message
	time.Sleep(time.Millisecond * 100)

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		SendWaitTestTransactions(t, ctx, l1client, []*types.Transaction{
			l1info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
	}

	for _, client := range l2ClientsToCheck {
		_, err = WaitForTx(ctx, client, tx.Hash(), time.Second*30)
		Require(t, err)

		l2balance, err := client.BalanceAt(ctx, l2info.GetAddress("User2"), nil)
		Require(t, err)

		if l2balance.Cmp(expectedBalance) != 0 {
			Fatal(t, "Unexpected balance:", l2balance)
		}

	}
}

func TestDASComplexConfigAndRestMirror(t *testing.T) {
	initTest(t)
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Setup L1 chain and contracts
	chainConfig := params.ArbitrumDevTestDASChainConfig()
	l1info, l1client, _, l1stack := createTestL1BlockChain(t, nil)
	defer requireClose(t, l1stack)
	arbSys, _ := precompilesgen.NewArbSys(types.ArbSysAddress, l1client)
	l1Reader, err := headerreader.New(ctx, l1client, func() *headerreader.Config { return &headerreader.TestConfig }, arbSys)
	Require(t, err)
	l1Reader.Start(ctx)
	defer l1Reader.StopAndWait()
	feedErrChan := make(chan error, 10)
	addresses, initMessage := DeployOnTestL1(t, ctx, l1info, l1client, chainConfig)

	keyDir, fileDataDir, dbDataDir := t.TempDir(), t.TempDir(), t.TempDir()
	pubkey, _, err := das.GenerateAndStoreKeys(keyDir)
	Require(t, err)

	serverConfig := das.DataAvailabilityConfig{
		Enable: true,

		LocalCache: das.TestBigCacheConfig,

		LocalFileStorage: das.LocalFileStorageConfig{
			Enable:  true,
			DataDir: fileDataDir,
		},
		LocalDBStorage: das.LocalDBStorageConfig{
			Enable:  true,
			DataDir: dbDataDir,
		},

		Key: das.KeyConfig{
			KeyDir: keyDir,
		},

		RequestTimeout: 5 * time.Second,
		// L1NodeURL: normally we would have to set this but we are passing in the already constructed client and addresses to the factory
	}

	daReader, daWriter, daHealthChecker, lifecycleManager, err := das.CreateDAComponentsForDaserver(ctx, &serverConfig, l1Reader, &addresses.SequencerInbox)
	Require(t, err)
	defer lifecycleManager.StopAndWaitUntil(time.Second)
	rpcLis, err := net.Listen("tcp", "localhost:0")
	Require(t, err)
	_, err = das.StartDASRPCServerOnListener(ctx, rpcLis, genericconf.HTTPServerTimeoutConfigDefault, daReader, daWriter, daHealthChecker)
	Require(t, err)
	restLis, err := net.Listen("tcp", "localhost:0")
	Require(t, err)
	restServer, err := das.NewRestfulDasServerOnListener(restLis, genericconf.HTTPServerTimeoutConfigDefault, daReader, daHealthChecker)

	pubkeyA := pubkey
	authorizeDASKeyset(t, ctx, pubkeyA, l1info, l1client)

	//
	l1NodeConfigA := arbnode.ConfigDefaultL1Test()
	l1NodeConfigA.DataAvailability = das.DataAvailabilityConfig{
		Enable: true,

		// AggregatorConfig set up below
		RequestTimeout: 5 * time.Second,
	}
	beConfigA := das.BackendConfig{
		URL:                 "http://" + rpcLis.Addr().String(),
		PubKeyBase64Encoded: blsPubToBase64(pubkey),
		SignerMask:          1,
	}
	l1NodeConfigA.DataAvailability.RPCAggregator = aggConfigForBackend(t, beConfigA)
	l1NodeConfigA.DataAvailability.RestAggregator = das.DefaultRestfulClientAggregatorConfig
	l1NodeConfigA.DataAvailability.RestAggregator.Enable = true
	l1NodeConfigA.DataAvailability.RestAggregator.Urls = []string{"http://" + restLis.Addr().String()}
	l1NodeConfigA.DataAvailability.ParentChainNodeURL = "none"

	dataSigner := signature.DataSignerFromPrivateKey(l1info.Accounts["Sequencer"].PrivateKey)

	Require(t, err)

	// Setup L2 chain
	l2info, l2stackA, l2chainDb, l2arbDb, l2blockchain := createL2BlockChainWithStackConfig(t, nil, "", chainConfig, initMessage, nil, nil)
	l2info.GenerateAccount("User2")

	execA, err := gethexec.CreateExecutionNode(ctx, l2stackA, l2chainDb, l2blockchain, l1client, gethexec.ConfigDefaultTest)
	Require(t, err)

	sequencerTxOpts := l1info.GetDefaultTransactOpts("Sequencer", ctx)
	sequencerTxOptsPtr := &sequencerTxOpts
	nodeA, err := arbnode.CreateNode(ctx, l2stackA, execA, l2arbDb, NewFetcherFromConfig(l1NodeConfigA), l2blockchain.Config(), l1client, addresses, sequencerTxOptsPtr, sequencerTxOptsPtr, dataSigner, feedErrChan)
	Require(t, err)
	Require(t, nodeA.Start(ctx))
	l2clientA := ClientForStack(t, l2stackA)

	// Create node to sync from chain
	l1NodeConfigB := arbnode.ConfigDefaultL1NonSequencerTest()
	l1NodeConfigB.DataAvailability = das.DataAvailabilityConfig{
		Enable: true,

		// AggregatorConfig set up below

		ParentChainNodeURL: "none",
		RequestTimeout:     5 * time.Second,
	}

	l1NodeConfigB.BlockValidator.Enable = false
	l1NodeConfigB.DataAvailability.Enable = true
	l1NodeConfigB.DataAvailability.RestAggregator = das.DefaultRestfulClientAggregatorConfig
	l1NodeConfigB.DataAvailability.RestAggregator.Enable = true
	l1NodeConfigB.DataAvailability.RestAggregator.Urls = []string{"http://" + restLis.Addr().String()}
	l1NodeConfigB.DataAvailability.ParentChainNodeURL = "none"
	l2clientB, nodeB := Create2ndNodeWithConfig(t, ctx, nodeA, l1stack, l1info, &l2info.ArbInitData, l1NodeConfigB, nil, nil)

	checkBatchPosting(t, ctx, l1client, l2clientA, l1info, l2info, big.NewInt(1e12), l2clientB)

	nodeA.StopAndWait()
	nodeB.StopAndWait()

	err = restServer.Shutdown()
	Require(t, err)
}

func enableLogging(logLvl int) {
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.Lvl(logLvl))
	log.Root().SetHandler(glogger)
}

func initTest(t *testing.T) {
	t.Parallel()
	loggingStr := os.Getenv("LOGGING")
	if len(loggingStr) > 0 {
		var err error
		logLvl, err := strconv.Atoi(loggingStr)
		Require(t, err, "Failed to parse string")
		enableLogging(logLvl)
	}
}

'''
'''--- system_tests/debugapi_test.go ---
package arbtest

import (
	"context"
	"testing"

	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/eth"
	"github.com/ethereum/go-ethereum/rpc"
)

func TestDebugAPI(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	l2rpc := builder.L2.Stack.Attach()

	var dump state.Dump
	err := l2rpc.CallContext(ctx, &dump, "debug_dumpBlock", rpc.LatestBlockNumber)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &dump, "debug_dumpBlock", rpc.PendingBlockNumber)
	Require(t, err)

	var badBlocks []eth.BadBlockArgs
	err = l2rpc.CallContext(ctx, &badBlocks, "debug_getBadBlocks")
	Require(t, err)

	var dumpIt state.IteratorDump
	err = l2rpc.CallContext(ctx, &dumpIt, "debug_accountRange", rpc.LatestBlockNumber, hexutil.Bytes{}, 10, true, true, false)
	Require(t, err)
	err = l2rpc.CallContext(ctx, &dumpIt, "debug_accountRange", rpc.PendingBlockNumber, hexutil.Bytes{}, 10, true, true, false)
	Require(t, err)

}

'''
'''--- system_tests/delayedinbox_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"strings"
	"testing"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

var inboxABI abi.ABI

func init() {
	var err error
	inboxABI, err = abi.JSON(strings.NewReader(bridgegen.InboxABI))
	if err != nil {
		panic(err)
	}
}

func WrapL2ForDelayed(t *testing.T, l2Tx *types.Transaction, l1info *BlockchainTestInfo, delayedSender string, gas uint64) *types.Transaction {
	txbytes, err := l2Tx.MarshalBinary()
	Require(t, err)
	txwrapped := append([]byte{arbos.L2MessageKind_SignedTx}, txbytes...)
	delayedInboxTxData, err := inboxABI.Pack("sendL2Message", txwrapped)
	Require(t, err)
	return l1info.PrepareTx(delayedSender, "Inbox", gas, big.NewInt(0), delayedInboxTxData)
}

func TestDelayInboxSimple(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User2")

	delayedTx := builder.L2Info.PrepareTx("Owner", "User2", 50001, big.NewInt(1e6), nil)
	builder.L1.SendSignedTx(t, builder.L2.Client, delayedTx, builder.L1Info)

	l2balance, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)
	if l2balance.Cmp(big.NewInt(1e6)) != 0 {
		Fatal(t, "Unexpected balance:", l2balance)
	}
}

'''
'''--- system_tests/delayedinboxlong_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/core/txpool"
	"github.com/ethereum/go-ethereum/core/types"
)

func TestDelayInboxLong(t *testing.T) {
	t.Parallel()
	addLocalLoops := 3
	messagesPerAddLocal := 1000
	messagesPerDelayed := 10

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User2")

	fundsPerDelayed := int64(1000000)
	delayedMessages := int64(0)

	var lastDelayedMessage *types.Transaction

	for i := 0; i < addLocalLoops; i++ {
		wrappedL1Txs := make([]*txpool.Transaction, 0, messagesPerAddLocal)
		for len(wrappedL1Txs) < messagesPerAddLocal {
			randNum := rand.Int() % messagesPerDelayed
			var l1tx *types.Transaction
			if randNum == 0 {
				delayedTx := builder.L2Info.PrepareTx("Owner", "User2", 50001, big.NewInt(fundsPerDelayed), nil)
				l1tx = WrapL2ForDelayed(t, delayedTx, builder.L1Info, "User", 100000)
				lastDelayedMessage = delayedTx
				delayedMessages++
			} else {
				l1tx = builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil)
			}
			wrappedL1Txs = append(wrappedL1Txs, &txpool.Transaction{Tx: l1tx})
		}

		// adding multiple messages in the same Add with local=true to get them in the same L1 block
		errs := builder.L1.L1Backend.TxPool().Add(wrappedL1Txs, true, false)
		for _, err := range errs {
			Require(t, err)
		}
		// Checking every tx is expensive, so we just check the last, assuming that the others succeeded too
		confirmLatestBlock(ctx, t, builder.L1Info, builder.L1.Client)
		_, err := builder.L1.EnsureTxSucceeded(wrappedL1Txs[len(wrappedL1Txs)-1].Tx)
		Require(t, err)
	}

	t.Log("Done sending", delayedMessages, "delayedMessages")
	if delayedMessages == 0 {
		Fatal(t, "No delayed messages sent!")
	}

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 100; i++ {
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
	}

	_, err := WaitForTx(ctx, builder.L2.Client, lastDelayedMessage.Hash(), time.Second*5)
	Require(t, err)
	l2balance, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)
	if l2balance.Cmp(big.NewInt(fundsPerDelayed*delayedMessages)) != 0 {
		Fatal(t, "Unexpected balance:", "balance", l2balance, "expected", fundsPerDelayed*delayedMessages)
	}
}

'''
'''--- system_tests/estimation_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestDeploy(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	auth.GasMargin = 0 // don't adjust, we want to see if the estimate alone is sufficient

	_, simple := builder.L2.DeploySimple(t, auth)

	tx, err := simple.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	counter, err := simple.Counter(&bind.CallOpts{})
	Require(t, err, "failed to get counter")

	if counter != 1 {
		Fatal(t, "Unexpected counter value", counter)
	}
}

func TestEstimate(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	auth.GasMargin = 0 // don't adjust, we want to see if the estimate alone is sufficient

	gasPrice := big.NewInt(params.GWei / 10)

	// set the gas price
	arbOwner, err := precompilesgen.NewArbOwner(common.HexToAddress("0x70"), builder.L2.Client)
	Require(t, err, "could not deploy ArbOwner contract")
	tx, err := arbOwner.SetMinimumL2BaseFee(&auth, gasPrice)
	Require(t, err, "could not set L2 gas price")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	// connect to arbGasInfo precompile
	arbGasInfo, err := precompilesgen.NewArbGasInfo(common.HexToAddress("0x6c"), builder.L2.Client)
	Require(t, err, "could not deploy contract")

	// wait for price to come to equilibrium
	equilibrated := false
	numTriesLeft := 20
	for !equilibrated && numTriesLeft > 0 {
		// make an empty block to let the gas price update
		builder.L2Info.GasPrice = new(big.Int).Mul(builder.L2Info.GasPrice, big.NewInt(2))
		builder.L2.TransferBalance(t, "Owner", "Owner", common.Big0, builder.L2Info)

		// check if the price has equilibrated
		_, _, _, _, _, setPrice, err := arbGasInfo.GetPricesInWei(&bind.CallOpts{})
		Require(t, err, "could not get L2 gas price")
		if gasPrice.Cmp(setPrice) == 0 {
			equilibrated = true
		}
		numTriesLeft--
	}
	if !equilibrated {
		Fatal(t, "L2 gas price did not converge", gasPrice)
	}

	initialBalance, err := builder.L2.Client.BalanceAt(ctx, auth.From, nil)
	Require(t, err, "could not get balance")

	// deploy a test contract
	_, tx, simple, err := mocksgen.DeploySimple(&auth, builder.L2.Client)
	Require(t, err, "could not deploy contract")
	receipt, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	header, err := builder.L2.Client.HeaderByNumber(ctx, receipt.BlockNumber)
	Require(t, err, "could not get header")
	if header.BaseFee.Cmp(gasPrice) != 0 {
		Fatal(t, "Header has wrong basefee", header.BaseFee, gasPrice)
	}

	balance, err := builder.L2.Client.BalanceAt(ctx, auth.From, nil)
	Require(t, err, "could not get balance")
	expectedCost := receipt.GasUsed * gasPrice.Uint64()
	observedCost := initialBalance.Uint64() - balance.Uint64()
	if expectedCost != observedCost {
		Fatal(t, "Expected deployment to cost", expectedCost, "instead of", observedCost)
	}

	tx, err = simple.Increment(&auth)
	Require(t, err, "failed to call Increment()")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	counter, err := simple.Counter(&bind.CallOpts{})
	Require(t, err, "failed to get counter")

	if counter != 1 {
		Fatal(t, "Unexpected counter value", counter)
	}
}

func TestComponentEstimate(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	l1BaseFee := new(big.Int).Set(arbostypes.DefaultInitialL1BaseFee)
	l2BaseFee := builder.L2.GetBaseFee(t)

	colors.PrintGrey("l1 basefee ", l1BaseFee)
	colors.PrintGrey("l2 basefee ", l2BaseFee)

	userBalance := big.NewInt(1e16)
	maxPriorityFeePerGas := big.NewInt(0)
	maxFeePerGas := arbmath.BigMulByUfrac(l2BaseFee, 3, 2)

	builder.L2Info.GenerateAccount("User")
	builder.L2.TransferBalance(t, "Owner", "User", userBalance, builder.L2Info)

	from := builder.L2Info.GetAddress("User")
	to := testhelpers.RandomAddress()
	gas := uint64(100000000)
	calldata := []byte{0x00, 0x12}
	value := big.NewInt(4096)

	nodeAbi, err := node_interfacegen.NodeInterfaceMetaData.GetAbi()
	Require(t, err)

	nodeMethod := nodeAbi.Methods["gasEstimateComponents"]
	estimateCalldata := append([]byte{}, nodeMethod.ID...)
	packed, err := nodeMethod.Inputs.Pack(to, false, calldata)
	Require(t, err)
	estimateCalldata = append(estimateCalldata, packed...)

	msg := ethereum.CallMsg{
		From:      from,
		To:        &types.NodeInterfaceAddress,
		Gas:       gas,
		GasFeeCap: maxFeePerGas,
		GasTipCap: maxPriorityFeePerGas,
		Value:     value,
		Data:      estimateCalldata,
	}
	returnData, err := builder.L2.Client.CallContract(ctx, msg, nil)
	Require(t, err)

	outputs, err := nodeMethod.Outputs.Unpack(returnData)
	Require(t, err)
	if len(outputs) != 4 {
		Fatal(t, "expected 4 outputs from gasEstimateComponents, got", len(outputs))
	}

	gasEstimate, _ := outputs[0].(uint64)
	gasEstimateForL1, _ := outputs[1].(uint64)
	baseFee, _ := outputs[2].(*big.Int)
	l1BaseFeeEstimate, _ := outputs[3].(*big.Int)

	tx := builder.L2Info.SignTxAs("User", &types.DynamicFeeTx{
		ChainID:   builder.L2.ExecNode.ArbInterface.BlockChain().Config().ChainID,
		Nonce:     0,
		GasTipCap: maxPriorityFeePerGas,
		GasFeeCap: maxFeePerGas,
		Gas:       gasEstimate,
		To:        &to,
		Value:     value,
		Data:      calldata,
	})

	l2Estimate := gasEstimate - gasEstimateForL1

	colors.PrintBlue("Est. ", gasEstimate, " - ", gasEstimateForL1, " = ", l2Estimate)

	if !arbmath.BigEquals(l1BaseFeeEstimate, l1BaseFee) {
		Fatal(t, l1BaseFeeEstimate, l1BaseFee)
	}
	if !arbmath.BigEquals(baseFee, l2BaseFee) {
		Fatal(t, baseFee, l2BaseFee.Uint64())
	}

	Require(t, builder.L2.Client.SendTransaction(ctx, tx))
	receipt, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	l2Used := receipt.GasUsed - receipt.GasUsedForL1
	colors.PrintMint("True ", receipt.GasUsed, " - ", receipt.GasUsedForL1, " = ", l2Used)

	if l2Estimate != l2Used {
		Fatal(t, l2Estimate, l2Used)
	}
}

func TestDisableL1Charging(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()
	addr := common.HexToAddress("0x12345678")

	gasWithL1Charging, err := builder.L2.Client.EstimateGas(ctx, ethereum.CallMsg{To: &addr})
	Require(t, err)

	gasWithoutL1Charging, err := builder.L2.Client.EstimateGas(ctx, ethereum.CallMsg{To: &addr, SkipL1Charging: true})
	Require(t, err)

	if gasWithL1Charging <= gasWithoutL1Charging {
		Fatal(t, "SkipL1Charging didn't disable L1 charging")
	}
	if gasWithoutL1Charging != params.TxGas {
		Fatal(t, "Incorrect gas estimate with disabled L1 charging")
	}

	_, err = builder.L2.Client.CallContract(ctx, ethereum.CallMsg{To: &addr, Gas: gasWithL1Charging}, nil)
	Require(t, err)

	_, err = builder.L2.Client.CallContract(ctx, ethereum.CallMsg{To: &addr, Gas: gasWithoutL1Charging}, nil)
	if err == nil {
		Fatal(t, "CallContract passed with insufficient gas")
	}

	_, err = builder.L2.Client.CallContract(ctx, ethereum.CallMsg{To: &addr, Gas: gasWithoutL1Charging, SkipL1Charging: true}, nil)
	Require(t, err)
}

'''
'''--- system_tests/fees_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// these tests seems to consume too much memory with race detection
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"os"
	"path/filepath"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbos/l1pricing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
)

func TestSequencerFeePaid(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	version := builder.L2.ExecNode.ArbInterface.BlockChain().Config().ArbitrumChainParams.InitialArbOSVersion
	callOpts := builder.L2Info.GetDefaultCallOpts("Owner", ctx)

	// get the network fee account
	arbOwnerPublic, err := precompilesgen.NewArbOwnerPublic(common.HexToAddress("0x6b"), builder.L2.Client)
	Require(t, err, "failed to deploy contract")
	arbGasInfo, err := precompilesgen.NewArbGasInfo(common.HexToAddress("0x6c"), builder.L2.Client)
	Require(t, err, "failed to deploy contract")
	arbDebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
	Require(t, err, "failed to deploy contract")
	networkFeeAccount, err := arbOwnerPublic.GetNetworkFeeAccount(callOpts)
	Require(t, err, "could not get the network fee account")

	l1Estimate, err := arbGasInfo.GetL1BaseFeeEstimate(callOpts)
	Require(t, err)

	baseFee := builder.L2.GetBaseFee(t)
	builder.L2Info.GasPrice = baseFee

	testFees := func(tip uint64) (*big.Int, *big.Int) {
		tipCap := arbmath.BigMulByUint(baseFee, tip)
		txOpts := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
		txOpts.GasTipCap = tipCap
		gasPrice := arbmath.BigAdd(baseFee, tipCap)

		networkBefore := builder.L2.GetBalance(t, networkFeeAccount)

		tx, err := arbDebug.Events(&txOpts, true, [32]byte{})
		Require(t, err)
		receipt, err := builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)

		networkAfter := builder.L2.GetBalance(t, networkFeeAccount)
		l1Charge := arbmath.BigMulByUint(builder.L2Info.GasPrice, receipt.GasUsedForL1)

		// the network should receive
		//     1. compute costs
		//     2. tip on the compute costs
		//     3. tip on the data costs
		networkRevenue := arbmath.BigSub(networkAfter, networkBefore)
		gasUsedForL2 := receipt.GasUsed - receipt.GasUsedForL1
		feePaidForL2 := arbmath.BigMulByUint(gasPrice, gasUsedForL2)
		tipPaidToNet := arbmath.BigMulByUint(tipCap, receipt.GasUsedForL1)
		gotTip := arbmath.BigEquals(networkRevenue, arbmath.BigAdd(feePaidForL2, tipPaidToNet))
		if !gotTip && version == 9 {
			Fatal(t, "network didn't receive expected payment", networkRevenue, feePaidForL2, tipPaidToNet)
		}
		if gotTip && version != 9 {
			Fatal(t, "tips are somehow enabled")
		}

		txSize := compressedTxSize(t, tx)
		l1GasBought := arbmath.BigDiv(l1Charge, l1Estimate).Uint64()
		l1ChargeExpected := arbmath.BigMulByUint(l1Estimate, txSize*params.TxDataNonZeroGasEIP2028)
		// L1 gas can only be charged in terms of L2 gas, so subtract off any rounding error from the expected value
		l1ChargeExpected.Sub(l1ChargeExpected, new(big.Int).Mod(l1ChargeExpected, builder.L2Info.GasPrice))

		colors.PrintBlue("bytes ", l1GasBought/params.TxDataNonZeroGasEIP2028, txSize)

		if !arbmath.BigEquals(l1Charge, l1ChargeExpected) {
			Fatal(t, "the sequencer's future revenue does not match its costs", l1Charge, l1ChargeExpected)
		}
		return networkRevenue, tipPaidToNet
	}

	if version != 9 {
		testFees(3)
		return
	}

	net0, tip0 := testFees(0)
	net2, tip2 := testFees(2)

	if tip0.Sign() != 0 {
		Fatal(t, "nonzero tip")
	}
	if arbmath.BigEquals(arbmath.BigSub(net2, tip2), net0) {
		Fatal(t, "a tip of 2 should yield a total of 3")
	}
}

func testSequencerPriceAdjustsFrom(t *testing.T, initialEstimate uint64) {
	t.Parallel()

	_ = os.Mkdir("test-data", 0766)
	path := filepath.Join("test-data", fmt.Sprintf("testSequencerPriceAdjustsFrom%v.csv", initialEstimate))

	f, err := os.Create(path)
	Require(t, err)
	defer func() { Require(t, f.Close()) }()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.DelayedSequencer.FinalizeDistance = 1
	cleanup := builder.Build(t)
	defer cleanup()

	// SimulatedBeacon running in OnDemand block production mode
	// produces blocks in the future so we need this to avoid the batch poster
	// not posting because the txs appear to be in the future.
	builder.nodeConfig.BatchPoster.MaxDelay = -time.Hour

	ownerAuth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)

	// make ownerAuth a chain owner
	arbdebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
	Require(t, err)
	tx, err := arbdebug.BecomeChainOwner(&ownerAuth)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)

	// use ownerAuth to set the L1 price per unit
	Require(t, err)
	arbOwner, err := precompilesgen.NewArbOwner(common.HexToAddress("0x70"), builder.L2.Client)
	Require(t, err)
	tx, err = arbOwner.SetL1PricePerUnit(&ownerAuth, arbmath.UintToBig(initialEstimate))
	Require(t, err)
	_, err = WaitForTx(ctx, builder.L2.Client, tx.Hash(), time.Second*5)
	Require(t, err)

	arbGasInfo, err := precompilesgen.NewArbGasInfo(common.HexToAddress("0x6c"), builder.L2.Client)
	Require(t, err)
	lastEstimate, err := arbGasInfo.GetL1BaseFeeEstimate(&bind.CallOpts{Context: ctx})
	Require(t, err)
	lastBatchCount, err := builder.L2.ConsensusNode.InboxTracker.GetBatchCount()
	Require(t, err)
	l1Header, err := builder.L1.Client.HeaderByNumber(ctx, nil)
	Require(t, err)

	rewardRecipientBalanceBefore := builder.L2.GetBalance(t, l1pricing.BatchPosterAddress)
	timesPriceAdjusted := 0

	colors.PrintBlue("Initial values")
	colors.PrintBlue("    L1 base fee ", l1Header.BaseFee)
	colors.PrintBlue("    L1 estimate ", lastEstimate)

	numRetrogradeMoves := 0
	for i := 0; i < 256; i++ {
		tx, receipt := builder.L2.TransferBalance(t, "Owner", "Owner", common.Big1, builder.L2Info)
		header, err := builder.L2.Client.HeaderByHash(ctx, receipt.BlockHash)
		Require(t, err)

		builder.L1.TransferBalance(t, "Faucet", "Faucet", common.Big1, builder.L1Info) // generate l1 traffic

		units := compressedTxSize(t, tx) * params.TxDataNonZeroGasEIP2028
		estimatedL1FeePerUnit := arbmath.BigDivByUint(arbmath.BigMulByUint(header.BaseFee, receipt.GasUsedForL1), units)

		if !arbmath.BigEquals(lastEstimate, estimatedL1FeePerUnit) {
			l1Header, err = builder.L1.Client.HeaderByNumber(ctx, nil)
			Require(t, err)

			callOpts := &bind.CallOpts{Context: ctx, BlockNumber: receipt.BlockNumber}
			actualL1FeePerUnit, err := arbGasInfo.GetL1BaseFeeEstimate(callOpts)
			Require(t, err)
			surplus, err := arbGasInfo.GetL1PricingSurplus(callOpts)
			Require(t, err)

			colors.PrintGrey("ArbOS updated its L1 estimate")
			colors.PrintGrey("    L1 base fee ", l1Header.BaseFee)
			colors.PrintGrey("    L1 estimate ", lastEstimate, " âž¤ ", estimatedL1FeePerUnit, " = ", actualL1FeePerUnit)
			colors.PrintGrey("    Surplus ", surplus)
			fmt.Fprintf(
				f, "%v, %v, %v, %v, %v, %v\n", i, l1Header.BaseFee, lastEstimate,
				estimatedL1FeePerUnit, actualL1FeePerUnit, surplus,
			)

			oldDiff := arbmath.BigAbs(arbmath.BigSub(lastEstimate, l1Header.BaseFee))
			newDiff := arbmath.BigAbs(arbmath.BigSub(actualL1FeePerUnit, l1Header.BaseFee))
			cmpDiff := arbmath.BigGreaterThan(newDiff, oldDiff)
			signums := surplus.Sign() == arbmath.BigSub(actualL1FeePerUnit, l1Header.BaseFee).Sign()

			if timesPriceAdjusted > 0 && cmpDiff && signums {
				numRetrogradeMoves++
				if numRetrogradeMoves > 1 {
					colors.PrintRed(timesPriceAdjusted, newDiff, oldDiff, lastEstimate, surplus)
					colors.PrintRed(estimatedL1FeePerUnit, l1Header.BaseFee, actualL1FeePerUnit)
					Fatal(t, "L1 gas price estimate should tend toward the basefee")
				}
			} else {
				numRetrogradeMoves = 0
			}
			diff := arbmath.BigAbs(arbmath.BigSub(actualL1FeePerUnit, estimatedL1FeePerUnit))
			maxDiffToAllow := arbmath.BigDivByUint(actualL1FeePerUnit, 100)
			if arbmath.BigLessThan(maxDiffToAllow, diff) { // verify that estimates is within 1% of actual
				Fatal(t, "New L1 estimate differs too much from receipt")
			}
			if arbmath.BigEquals(actualL1FeePerUnit, common.Big0) {
				Fatal(t, "Estimate is zero", i)
			}
			lastEstimate = actualL1FeePerUnit
			timesPriceAdjusted++
		}

		if i%16 == 0 {
			// see that the inbox advances

			for j := 16; j > 0; j-- {
				newBatchCount, err := builder.L2.ConsensusNode.InboxTracker.GetBatchCount()
				Require(t, err)
				if newBatchCount > lastBatchCount {
					colors.PrintGrey("posted new batch ", newBatchCount)
					lastBatchCount = newBatchCount
					break
				}
				if j == 1 {
					Fatal(t, "batch count didn't update in time")
				}
				time.Sleep(time.Millisecond * 100)
			}
		}
	}

	rewardRecipientBalanceAfter := builder.L2.GetBalance(t, builder.chainConfig.ArbitrumChainParams.InitialChainOwner)
	colors.PrintMint("reward recipient balance ", rewardRecipientBalanceBefore, " âž¤ ", rewardRecipientBalanceAfter)
	colors.PrintMint("price changes     ", timesPriceAdjusted)

	if timesPriceAdjusted == 0 {
		Fatal(t, "L1 gas price estimate never adjusted")
	}
	if !arbmath.BigGreaterThan(rewardRecipientBalanceAfter, rewardRecipientBalanceBefore) {
		Fatal(t, "reward recipient didn't get paid")
	}

	arbAggregator, err := precompilesgen.NewArbAggregator(common.HexToAddress("0x6d"), builder.L2.Client)
	Require(t, err)
	batchPosterAddresses, err := arbAggregator.GetBatchPosters(&bind.CallOpts{Context: ctx})
	Require(t, err)
	numReimbursed := 0
	for _, bpAddr := range batchPosterAddresses {
		if bpAddr != l1pricing.BatchPosterAddress && bpAddr != l1pricing.L1PricerFundsPoolAddress {
			numReimbursed++
			bal, err := builder.L1.Client.BalanceAt(ctx, bpAddr, nil)
			Require(t, err)
			if bal.Sign() == 0 {
				Fatal(t, "Batch poster balance is zero for", bpAddr)
			}
		}
	}
	if numReimbursed != 1 {
		Fatal(t, "Wrong number of batch posters were reimbursed", numReimbursed)
	}
}

func TestSequencerPriceAdjustsFrom1Gwei(t *testing.T) {
	testSequencerPriceAdjustsFrom(t, params.GWei)
}

func TestSequencerPriceAdjustsFrom2Gwei(t *testing.T) {
	testSequencerPriceAdjustsFrom(t, 2*params.GWei)
}

func TestSequencerPriceAdjustsFrom5Gwei(t *testing.T) {
	testSequencerPriceAdjustsFrom(t, 5*params.GWei)
}

func TestSequencerPriceAdjustsFrom10Gwei(t *testing.T) {
	testSequencerPriceAdjustsFrom(t, 10*params.GWei)
}

func TestSequencerPriceAdjustsFrom25Gwei(t *testing.T) {
	testSequencerPriceAdjustsFrom(t, 25*params.GWei)
}

func compressedTxSize(t *testing.T, tx *types.Transaction) uint64 {
	txBin, err := tx.MarshalBinary()
	Require(t, err)
	compressed, err := arbcompress.CompressLevel(txBin, 0)
	Require(t, err)
	return uint64(len(compressed))
}

'''
'''--- system_tests/forwarder_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"testing"
	"time"

	"github.com/alicebob/miniredis/v2"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/util/redisutil"
)

var transferAmount = big.NewInt(1e12) // amount of ether to use for transactions in tests

const nodesCount = 5 // number of testnodes to create in tests

func TestStaticForwarder(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	ipcPath := tmpPath(t, "test.ipc")

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.BatchPoster.Enable = false
	builder.l2StackConfig.IPCPath = ipcPath
	cleanupA := builder.Build(t)
	defer cleanupA()

	clientA := builder.L2.Client

	nodeConfigB := arbnode.ConfigDefaultL1Test()
	execConfigB := gethexec.ConfigDefaultTest()
	execConfigB.Sequencer.Enable = false
	nodeConfigB.Sequencer = false
	nodeConfigB.DelayedSequencer.Enable = false
	execConfigB.Forwarder.RedisUrl = ""
	execConfigB.ForwardingTarget = ipcPath
	nodeConfigB.BatchPoster.Enable = false

	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{
		nodeConfig: nodeConfigB,
		execConfig: execConfigB,
	})
	defer cleanupB()
	clientB := testClientB.Client

	builder.L2Info.GenerateAccount("User2")
	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, transferAmount, nil)
	err := clientB.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	l2balance, err := clientA.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)

	if l2balance.Cmp(transferAmount) != 0 {
		Fatal(t, "Unexpected balance:", l2balance)
	}
}

func initRedis(ctx context.Context, t *testing.T, nodeNames []string) (*miniredis.Miniredis, string) {
	t.Helper()

	redisServer, err := miniredis.Run()
	Require(t, err)

	redisUrl := fmt.Sprintf("redis://%s/0", redisServer.Addr())
	redisClient, err := redisutil.RedisClientFromURL(redisUrl)
	Require(t, err)
	defer redisClient.Close()

	priorities := strings.Join(nodeNames, ",")

	Require(t, redisClient.Set(ctx, redisutil.PRIORITIES_KEY, priorities, time.Duration(0)).Err())
	return redisServer, redisUrl
}

type fallbackSequencerOpts struct {
	ipcPath              string
	redisUrl             string
	enableSecCoordinator bool
}

func fallbackSequencer(ctx context.Context, t *testing.T, opts *fallbackSequencerOpts) *NodeBuilder {
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.l2StackConfig.IPCPath = opts.ipcPath
	builder.nodeConfig.SeqCoordinator.Enable = opts.enableSecCoordinator
	builder.nodeConfig.SeqCoordinator.RedisUrl = opts.redisUrl
	builder.nodeConfig.SeqCoordinator.MyUrl = opts.ipcPath
	return builder
}

func createForwardingNode(t *testing.T, builder *NodeBuilder, ipcPath string, redisUrl string, fallbackPath string) (*TestClient, func()) {
	if ipcPath != "" {
		builder.l2StackConfig.IPCPath = ipcPath
	}
	nodeConfig := arbnode.ConfigDefaultL1Test()
	nodeConfig.Sequencer = false
	nodeConfig.DelayedSequencer.Enable = false
	nodeConfig.BatchPoster.Enable = false
	execConfig := gethexec.ConfigDefaultTest()
	execConfig.Sequencer.Enable = false
	execConfig.Forwarder.RedisUrl = redisUrl
	execConfig.ForwardingTarget = fallbackPath
	//	nodeConfig.Feed.Output.Enable = false

	return builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: nodeConfig, execConfig: execConfig})
}

func createSequencer(t *testing.T, builder *NodeBuilder, ipcPath string, redisUrl string) (*TestClient, func()) {
	builder.l2StackConfig.IPCPath = ipcPath
	nodeConfig := arbnode.ConfigDefaultL1Test()
	nodeConfig.BatchPoster.Enable = false
	nodeConfig.SeqCoordinator.Enable = true
	nodeConfig.SeqCoordinator.RedisUrl = redisUrl
	nodeConfig.SeqCoordinator.MyUrl = ipcPath

	return builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: nodeConfig})
}

// tmpPath returns file path with specified filename from temporary directory of the test.
func tmpPath(t *testing.T, filename string) string {
	t.Helper()
	// create a unique, maximum 10 characters-long temporary directory {name} with path as $TMPDIR/{name}
	tmpDir, err := os.MkdirTemp("", "")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	t.Cleanup(func() {
		if err = os.RemoveAll(tmpDir); err != nil {
			t.Errorf("Failed to cleanup temp dir: %v", err)
		}
	})
	return filepath.Join(tmpDir, filename)
}

// testNodes creates specified number of paths for ipc from temporary directory of the test.
// e.g. /tmp/TestRedisForwarder689063006/003/0.ipc, /tmp/TestRedisForwarder689063006/007/1.ipc and so on.
func testNodes(t *testing.T, n int) []string {
	var paths []string
	for i := 0; i < n; i++ {
		paths = append(paths, tmpPath(t, fmt.Sprintf("%d.ipc", i)))
	}
	return paths
}

// waitForSequencerLockout blocks and waits until there is some sequencer chosen for specified duration.
// Errors out after timeout.
func waitForSequencerLockout(ctx context.Context, node *arbnode.Node, duration time.Duration) error {
	if node == nil {
		return fmt.Errorf("node is nil")
	}
	if node.SeqCoordinator == nil {
		return fmt.Errorf("sequence coordinator in the node is nil")
	}
	// TODO: implement exponential backoff retry mechanism and use it instead.
	for {
		select {
		case <-time.After(duration):
			return fmt.Errorf("no sequencer was chosen")
		default:
			if c, err := node.SeqCoordinator.CurrentChosenSequencer(ctx); err == nil && c != "" {
				return nil
			}
			time.Sleep(100 * time.Millisecond)
		}
	}
}

// stopNodes blocks and waits until all nodes are stopped.
func stopNodes(nodes []*arbnode.Node) {
	var wg sync.WaitGroup
	for _, node := range nodes {
		if node != nil {
			wg.Add(1)
			n := node
			go func() {
				n.StopAndWait()
				wg.Done()
			}()
		}
	}
	wg.Wait()
}

func user(suffix string, idx int) string {
	return fmt.Sprintf("User%s_%d", suffix, idx)
}

// tryWithTimeout calls function f() repeatedly foruntil it succeeds.
func tryWithTimeout(ctx context.Context, f func() error, duration time.Duration) error {
	for {
		select {
		case <-time.After(duration):
			return fmt.Errorf("timeout expired")
		default:
			if err := f(); err == nil {
				return nil
			}
		}
	}
}

func TestRedisForwarder(t *testing.T) {
	ctx := context.Background()

	nodePaths := testNodes(t, nodesCount)
	fbNodePath := tmpPath(t, "fallback.ipc") // fallback node path
	redisServer, redisUrl := initRedis(ctx, t, append(nodePaths, fbNodePath))
	defer redisServer.Close()

	builder := fallbackSequencer(ctx, t,
		&fallbackSequencerOpts{
			ipcPath:              fbNodePath,
			redisUrl:             redisUrl,
			enableSecCoordinator: true,
		})
	cleanup := builder.Build(t)
	defer cleanup()
	fallbackNode, fallbackClient := builder.L2.ConsensusNode, builder.L2.Client

	TestClientForwarding, cleanupForwarding := createForwardingNode(t, builder, "", redisUrl, fbNodePath)
	defer cleanupForwarding()
	forwardingClient := TestClientForwarding.Client

	var seqNodes []*arbnode.Node
	var seqClients []*ethclient.Client
	for _, path := range nodePaths {
		testClientSeq, _ := createSequencer(t, builder, path, redisUrl)
		seqNodes = append(seqNodes, testClientSeq.ConsensusNode)
		seqClients = append(seqClients, testClientSeq.Client)
	}
	defer stopNodes(seqNodes)

	for i := range seqClients {
		userA := user("A", i)
		builder.L2Info.GenerateAccount(userA)
		tx := builder.L2Info.PrepareTx("Owner", userA, builder.L2Info.TransferGas, big.NewInt(1e12+int64(builder.L2Info.TransferGas)*builder.L2Info.GasPrice.Int64()), nil)
		err := fallbackClient.SendTransaction(ctx, tx)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}

	for i := range seqClients {
		if err := waitForSequencerLockout(ctx, fallbackNode, 2*time.Second); err != nil {
			t.Fatalf("Error waiting for lockout: %v", err)
		}
		userA := user("A", i)
		userB := user("B", i)
		builder.L2Info.GenerateAccount(userB)
		tx := builder.L2Info.PrepareTx(userA, userB, builder.L2Info.TransferGas, transferAmount, nil)

		sendFunc := func() error { return forwardingClient.SendTransaction(ctx, tx) }
		if err := tryWithTimeout(ctx, sendFunc, gethexec.DefaultTestForwarderConfig.UpdateInterval*10); err != nil {
			t.Fatalf("Client: %v, error sending transaction: %v", i, err)
		}
		_, err := EnsureTxSucceeded(ctx, seqClients[i], tx)
		Require(t, err)

		l2balance, err := seqClients[i].BalanceAt(ctx, builder.L2Info.GetAddress(userB), nil)
		Require(t, err)

		if l2balance.Cmp(transferAmount) != 0 {
			Fatal(t, "Unexpected balance:", l2balance)
		}
		if i < len(seqNodes) {
			seqNodes[i].StopAndWait()
			seqNodes[i] = nil
		}
	}
}

func TestRedisForwarderFallbackNoRedis(t *testing.T) {
	ctx := context.Background()

	fallbackIpcPath := tmpPath(t, "fallback.ipc")
	nodePaths := testNodes(t, nodesCount)
	redisServer, redisUrl := initRedis(ctx, t, nodePaths)
	redisServer.Close()

	builder := fallbackSequencer(ctx, t,
		&fallbackSequencerOpts{
			ipcPath:              fallbackIpcPath,
			redisUrl:             redisUrl,
			enableSecCoordinator: false,
		})
	cleanup := builder.Build(t)
	defer cleanup()
	fallbackClient := builder.L2.Client

	TestClientForwarding, cleanupForwarding := createForwardingNode(t, builder, "", redisUrl, fallbackIpcPath)
	defer cleanupForwarding()
	forwardingClient := TestClientForwarding.Client

	user := "User2"
	builder.L2Info.GenerateAccount(user)
	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, transferAmount, nil)
	sendFunc := func() error { return forwardingClient.SendTransaction(ctx, tx) }
	err := tryWithTimeout(ctx, sendFunc, gethexec.DefaultTestForwarderConfig.UpdateInterval*10)
	Require(t, err)

	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	l2balance, err := fallbackClient.BalanceAt(ctx, builder.L2Info.GetAddress(user), nil)
	Require(t, err)

	if l2balance.Cmp(transferAmount) != 0 {
		t.Errorf("Got balance: %v, want: %v", l2balance, transferAmount)
	}
}

'''
'''--- system_tests/full_challenge_impl_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"bytes"
	"context"
	"io"
	"math/big"
	"os"
	"strings"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/ospgen"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_common"
	"github.com/offchainlabs/nitro/validator/valnode"
)

func DeployOneStepProofEntry(t *testing.T, ctx context.Context, auth *bind.TransactOpts, client *ethclient.Client) common.Address {
	osp0, tx, _, err := ospgen.DeployOneStepProver0(auth, client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)

	ospMem, tx, _, err := ospgen.DeployOneStepProverMemory(auth, client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)

	ospMath, tx, _, err := ospgen.DeployOneStepProverMath(auth, client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)

	ospHostIo, tx, _, err := ospgen.DeployOneStepProverHostIo(auth, client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)

	ospEntry, tx, _, err := ospgen.DeployOneStepProofEntry(auth, client, osp0, ospMem, ospMath, ospHostIo)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)

	return ospEntry
}

func CreateChallenge(
	t *testing.T,
	ctx context.Context,
	auth *bind.TransactOpts,
	client *ethclient.Client,
	ospEntry common.Address,
	sequencerInbox common.Address,
	delayedBridge common.Address,
	wasmModuleRoot common.Hash,
	startGlobalState validator.GoGlobalState,
	endGlobalState validator.GoGlobalState,
	numBlocks uint64,
	asserter common.Address,
	challenger common.Address,
) (*mocksgen.MockResultReceiver, common.Address) {
	challengeManagerLogic, tx, _, err := challengegen.DeployChallengeManager(auth, client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	challengeManagerAddr, tx, _, err := mocksgen.DeploySimpleProxy(auth, client, challengeManagerLogic)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	challengeManager, err := challengegen.NewChallengeManager(challengeManagerAddr, client)
	Require(t, err)

	resultReceiverAddr, _, resultReceiver, err := mocksgen.DeployMockResultReceiver(auth, client, challengeManagerAddr)
	Require(t, err)
	tx, err = challengeManager.Initialize(auth, resultReceiverAddr, sequencerInbox, delayedBridge, ospEntry)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	tx, err = resultReceiver.CreateChallenge(
		auth,
		wasmModuleRoot,
		[2]uint8{
			staker.StatusFinished,
			staker.StatusFinished,
		},
		[2]mocksgen.GlobalState{
			{
				Bytes32Vals: [2][32]byte{startGlobalState.BlockHash, startGlobalState.SendRoot},
				U64Vals:     [2]uint64{startGlobalState.Batch, startGlobalState.PosInBatch},
			},
			{
				Bytes32Vals: [2][32]byte{endGlobalState.BlockHash, endGlobalState.SendRoot},
				U64Vals:     [2]uint64{endGlobalState.Batch, endGlobalState.PosInBatch},
			},
		},
		numBlocks,
		asserter,
		challenger,
		big.NewInt(100000),
		big.NewInt(100000),
	)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, client, tx)
	Require(t, err)
	return resultReceiver, challengeManagerAddr
}

func writeTxToBatch(writer io.Writer, tx *types.Transaction) error {
	txData, err := tx.MarshalBinary()
	if err != nil {
		return err
	}
	var segment []byte
	segment = append(segment, arbstate.BatchSegmentKindL2Message)
	segment = append(segment, arbos.L2MessageKind_SignedTx)
	segment = append(segment, txData...)
	err = rlp.Encode(writer, segment)
	return err
}

const makeBatch_MsgsPerBatch = int64(5)

func makeBatch(t *testing.T, l2Node *arbnode.Node, l2Info *BlockchainTestInfo, backend *ethclient.Client, sequencer *bind.TransactOpts, seqInbox *mocksgen.SequencerInboxStub, seqInboxAddr common.Address, modStep int64) {
	ctx := context.Background()

	batchBuffer := bytes.NewBuffer([]byte{})
	for i := int64(0); i < makeBatch_MsgsPerBatch; i++ {
		value := i
		if i == modStep {
			value++
		}
		err := writeTxToBatch(batchBuffer, l2Info.PrepareTx("Owner", "Destination", 1000000, big.NewInt(value), []byte{}))
		Require(t, err)
	}
	compressed, err := arbcompress.CompressWell(batchBuffer.Bytes())
	Require(t, err)
	message := append([]byte{0}, compressed...)

	seqNum := new(big.Int).Lsh(common.Big1, 256)
	seqNum.Sub(seqNum, common.Big1)
	tx, err := seqInbox.AddSequencerL2BatchFromOrigin0(sequencer, seqNum, message, big.NewInt(1), common.Address{}, big.NewInt(0), big.NewInt(0))
	Require(t, err)
	receipt, err := EnsureTxSucceeded(ctx, backend, tx)
	Require(t, err)

	nodeSeqInbox, err := arbnode.NewSequencerInbox(backend, seqInboxAddr, 0)
	Require(t, err)
	batches, err := nodeSeqInbox.LookupBatchesInRange(ctx, receipt.BlockNumber, receipt.BlockNumber)
	Require(t, err)
	if len(batches) == 0 {
		Fatal(t, "batch not found after AddSequencerL2BatchFromOrigin")
	}
	err = l2Node.InboxTracker.AddSequencerBatches(ctx, backend, batches)
	Require(t, err)
	_, err = l2Node.InboxTracker.GetBatchMetadata(0)
	Require(t, err, "failed to get batch metadata after adding batch:")
}

func confirmLatestBlock(ctx context.Context, t *testing.T, l1Info *BlockchainTestInfo, backend arbutil.L1Interface) {
	// With SimulatedBeacon running in on-demand block production mode, the
	// finalized block is considered to be be the nearest multiple of 32 less
	// than or equal to the block number.
	for i := 0; i < 32; i++ {
		SendWaitTestTransactions(t, ctx, backend, []*types.Transaction{
			l1Info.PrepareTx("Faucet", "Faucet", 30000, big.NewInt(1e12), nil),
		})
	}
}

func setupSequencerInboxStub(ctx context.Context, t *testing.T, l1Info *BlockchainTestInfo, l1Client arbutil.L1Interface, chainConfig *params.ChainConfig) (common.Address, *mocksgen.SequencerInboxStub, common.Address) {
	txOpts := l1Info.GetDefaultTransactOpts("deployer", ctx)
	bridgeAddr, tx, bridge, err := mocksgen.DeployBridgeUnproxied(&txOpts, l1Client)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1Client, tx)
	Require(t, err)
	timeBounds := mocksgen.ISequencerInboxMaxTimeVariation{
		DelayBlocks:   big.NewInt(10000),
		FutureBlocks:  big.NewInt(10000),
		DelaySeconds:  big.NewInt(10000),
		FutureSeconds: big.NewInt(10000),
	}
	seqInboxAddr, tx, seqInbox, err := mocksgen.DeploySequencerInboxStub(
		&txOpts,
		l1Client,
		bridgeAddr,
		l1Info.GetAddress("sequencer"),
		timeBounds,
		big.NewInt(117964),
	)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1Client, tx)
	Require(t, err)
	tx, err = bridge.SetSequencerInbox(&txOpts, seqInboxAddr)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1Client, tx)
	Require(t, err)
	tx, err = bridge.SetDelayedInbox(&txOpts, seqInboxAddr, true)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1Client, tx)
	Require(t, err)
	tx, err = seqInbox.AddInitMessage(&txOpts, chainConfig.ChainID)
	Require(t, err)
	_, err = EnsureTxSucceeded(ctx, l1Client, tx)
	Require(t, err)
	return bridgeAddr, seqInbox, seqInboxAddr
}

func RunChallengeTest(t *testing.T, asserterIsCorrect bool, useStubs bool, challengeMsgIdx int64) {
	glogger := log.NewGlogHandler(log.StreamHandler(os.Stderr, log.TerminalFormat(false)))
	glogger.Verbosity(log.LvlInfo)
	log.Root().SetHandler(glogger)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	initialBalance := new(big.Int).Lsh(big.NewInt(1), 200)
	l1Info := NewL1TestInfo(t)
	l1Info.GenerateGenesisAccount("deployer", initialBalance)
	l1Info.GenerateGenesisAccount("asserter", initialBalance)
	l1Info.GenerateGenesisAccount("challenger", initialBalance)
	l1Info.GenerateGenesisAccount("sequencer", initialBalance)

	chainConfig := params.ArbitrumDevTestChainConfig()
	l1Info, l1Backend, _, _ := createTestL1BlockChain(t, l1Info)
	conf := arbnode.ConfigDefaultL1Test()
	conf.BlockValidator.Enable = false
	conf.BatchPoster.Enable = false
	conf.InboxReader.CheckDelay = time.Second

	var valStack *node.Node
	var mockSpawn *mockSpawner
	if useStubs {
		mockSpawn, valStack = createMockValidationNode(t, ctx, &valnode.TestValidationConfig.Arbitrator)
	} else {
		_, valStack = createTestValidationNode(t, ctx, &valnode.TestValidationConfig)
	}
	configByValidationNode(t, conf, valStack)

	fatalErrChan := make(chan error, 10)
	asserterRollupAddresses, initMessage := DeployOnTestL1(t, ctx, l1Info, l1Backend, chainConfig)

	deployerTxOpts := l1Info.GetDefaultTransactOpts("deployer", ctx)
	sequencerTxOpts := l1Info.GetDefaultTransactOpts("sequencer", ctx)
	asserterTxOpts := l1Info.GetDefaultTransactOpts("asserter", ctx)
	challengerTxOpts := l1Info.GetDefaultTransactOpts("challenger", ctx)

	asserterBridgeAddr, asserterSeqInbox, asserterSeqInboxAddr := setupSequencerInboxStub(ctx, t, l1Info, l1Backend, chainConfig)
	challengerBridgeAddr, challengerSeqInbox, challengerSeqInboxAddr := setupSequencerInboxStub(ctx, t, l1Info, l1Backend, chainConfig)

	asserterL2Info, asserterL2Stack, asserterL2ChainDb, asserterL2ArbDb, asserterL2Blockchain := createL2BlockChainWithStackConfig(t, nil, "", chainConfig, initMessage, nil, nil)
	asserterRollupAddresses.Bridge = asserterBridgeAddr
	asserterRollupAddresses.SequencerInbox = asserterSeqInboxAddr
	asserterExec, err := gethexec.CreateExecutionNode(ctx, asserterL2Stack, asserterL2ChainDb, asserterL2Blockchain, l1Backend, gethexec.ConfigDefaultTest)
	Require(t, err)
	asserterL2, err := arbnode.CreateNode(ctx, asserterL2Stack, asserterExec, asserterL2ArbDb, NewFetcherFromConfig(conf), chainConfig, l1Backend, asserterRollupAddresses, nil, nil, nil, fatalErrChan)
	Require(t, err)
	err = asserterL2.Start(ctx)
	Require(t, err)

	challengerL2Info, challengerL2Stack, challengerL2ChainDb, challengerL2ArbDb, challengerL2Blockchain := createL2BlockChainWithStackConfig(t, nil, "", chainConfig, initMessage, nil, nil)
	challengerRollupAddresses := *asserterRollupAddresses
	challengerRollupAddresses.Bridge = challengerBridgeAddr
	challengerRollupAddresses.SequencerInbox = challengerSeqInboxAddr
	challengerExec, err := gethexec.CreateExecutionNode(ctx, challengerL2Stack, challengerL2ChainDb, challengerL2Blockchain, l1Backend, gethexec.ConfigDefaultTest)
	Require(t, err)
	challengerL2, err := arbnode.CreateNode(ctx, challengerL2Stack, challengerExec, challengerL2ArbDb, NewFetcherFromConfig(conf), chainConfig, l1Backend, &challengerRollupAddresses, nil, nil, nil, fatalErrChan)
	Require(t, err)
	err = challengerL2.Start(ctx)
	Require(t, err)

	asserterL2Info.GenerateAccount("Destination")
	challengerL2Info.SetFullAccountInfo("Destination", asserterL2Info.GetInfoWithPrivKey("Destination"))

	if challengeMsgIdx < 1 || challengeMsgIdx > 3*makeBatch_MsgsPerBatch {
		Fatal(t, "challengeMsgIdx illegal")
	}

	// seqNum := common.Big2
	makeBatch(t, asserterL2, asserterL2Info, l1Backend, &sequencerTxOpts, asserterSeqInbox, asserterSeqInboxAddr, -1)
	makeBatch(t, challengerL2, challengerL2Info, l1Backend, &sequencerTxOpts, challengerSeqInbox, challengerSeqInboxAddr, challengeMsgIdx-1)

	// seqNum.Add(seqNum, common.Big1)
	makeBatch(t, asserterL2, asserterL2Info, l1Backend, &sequencerTxOpts, asserterSeqInbox, asserterSeqInboxAddr, -1)
	makeBatch(t, challengerL2, challengerL2Info, l1Backend, &sequencerTxOpts, challengerSeqInbox, challengerSeqInboxAddr, challengeMsgIdx-makeBatch_MsgsPerBatch-1)

	// seqNum.Add(seqNum, common.Big1)
	makeBatch(t, asserterL2, asserterL2Info, l1Backend, &sequencerTxOpts, asserterSeqInbox, asserterSeqInboxAddr, -1)
	makeBatch(t, challengerL2, challengerL2Info, l1Backend, &sequencerTxOpts, challengerSeqInbox, challengerSeqInboxAddr, challengeMsgIdx-makeBatch_MsgsPerBatch*2-1)

	trueSeqInboxAddr := challengerSeqInboxAddr
	trueDelayedBridge := challengerBridgeAddr
	expectedWinner := l1Info.GetAddress("challenger")
	if asserterIsCorrect {
		trueSeqInboxAddr = asserterSeqInboxAddr
		trueDelayedBridge = asserterBridgeAddr
		expectedWinner = l1Info.GetAddress("asserter")
	}
	ospEntry := DeployOneStepProofEntry(t, ctx, &deployerTxOpts, l1Backend)

	locator, err := server_common.NewMachineLocator("")
	if err != nil {
		Fatal(t, err)
	}
	var wasmModuleRoot common.Hash
	if useStubs {
		wasmModuleRoot = mockWasmModuleRoot
	} else {
		wasmModuleRoot = locator.LatestWasmModuleRoot()
		if (wasmModuleRoot == common.Hash{}) {
			Fatal(t, "latest machine not found")
		}
	}

	asserterGenesis := asserterExec.ArbInterface.BlockChain().Genesis()
	challengerGenesis := challengerExec.ArbInterface.BlockChain().Genesis()
	if asserterGenesis.Hash() != challengerGenesis.Hash() {
		Fatal(t, "asserter and challenger have different genesis hashes")
	}
	asserterLatestBlock := asserterExec.ArbInterface.BlockChain().CurrentBlock()
	challengerLatestBlock := challengerExec.ArbInterface.BlockChain().CurrentBlock()
	if asserterLatestBlock.Hash() == challengerLatestBlock.Hash() {
		Fatal(t, "asserter and challenger have the same end block")
	}

	asserterStartGlobalState := validator.GoGlobalState{
		BlockHash:  asserterGenesis.Hash(),
		Batch:      1,
		PosInBatch: 0,
	}
	asserterEndGlobalState := validator.GoGlobalState{
		BlockHash:  asserterLatestBlock.Hash(),
		Batch:      4,
		PosInBatch: 0,
	}
	numBlocks := asserterLatestBlock.Number.Uint64() - asserterGenesis.NumberU64()

	resultReceiver, challengeManagerAddr := CreateChallenge(
		t,
		ctx,
		&deployerTxOpts,
		l1Backend,
		ospEntry,
		trueSeqInboxAddr,
		trueDelayedBridge,
		wasmModuleRoot,
		asserterStartGlobalState,
		asserterEndGlobalState,
		numBlocks,
		l1Info.GetAddress("asserter"),
		l1Info.GetAddress("challenger"),
	)

	confirmLatestBlock(ctx, t, l1Info, l1Backend)

	asserterValidator, err := staker.NewStatelessBlockValidator(asserterL2.InboxReader, asserterL2.InboxTracker, asserterL2.TxStreamer, asserterExec.Recorder, asserterL2ArbDb, nil, StaticFetcherFrom(t, &conf.BlockValidator), valStack)
	if err != nil {
		Fatal(t, err)
	}
	if useStubs {
		asserterRecorder := newMockRecorder(asserterValidator, asserterL2.TxStreamer)
		asserterValidator.OverrideRecorder(t, asserterRecorder)
	}
	err = asserterValidator.Start(ctx)
	if err != nil {
		Fatal(t, err)
	}
	defer asserterValidator.Stop()
	asserterManager, err := staker.NewChallengeManager(ctx, l1Backend, &asserterTxOpts, asserterTxOpts.From, challengeManagerAddr, 1, asserterValidator, 0, 0)
	if err != nil {
		Fatal(t, err)
	}
	challengerValidator, err := staker.NewStatelessBlockValidator(challengerL2.InboxReader, challengerL2.InboxTracker, challengerL2.TxStreamer, challengerExec.Recorder, challengerL2ArbDb, nil, StaticFetcherFrom(t, &conf.BlockValidator), valStack)
	if err != nil {
		Fatal(t, err)
	}
	if useStubs {
		challengerRecorder := newMockRecorder(challengerValidator, challengerL2.TxStreamer)
		challengerValidator.OverrideRecorder(t, challengerRecorder)
	}
	err = challengerValidator.Start(ctx)
	if err != nil {
		Fatal(t, err)
	}
	defer challengerValidator.Stop()
	challengerManager, err := staker.NewChallengeManager(ctx, l1Backend, &challengerTxOpts, challengerTxOpts.From, challengeManagerAddr, 1, challengerValidator, 0, 0)
	if err != nil {
		Fatal(t, err)
	}

	confirmLatestBlock(ctx, t, l1Info, l1Backend)

	for i := 0; i < 100; i++ {
		var tx *types.Transaction
		var currentCorrect bool
		// Gas cost is slightly reduced if done in the same timestamp or block as previous call.
		// This might make gas estimation undersestimate next move.
		// Invoke a new L1 block, with a new timestamp, before estimating.
		time.Sleep(time.Second)
		SendWaitTestTransactions(t, ctx, l1Backend, []*types.Transaction{
			l1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})

		if i%2 == 0 {
			currentCorrect = !asserterIsCorrect
			tx, err = challengerManager.Act(ctx)
		} else {
			currentCorrect = asserterIsCorrect
			tx, err = asserterManager.Act(ctx)
		}
		if err != nil {
			if !currentCorrect && (strings.Contains(err.Error(), "lost challenge") ||
				strings.Contains(err.Error(), "SAME_OSP_END") ||
				strings.Contains(err.Error(), "BAD_SEQINBOX_MESSAGE")) {
				t.Log("challenge completed! asserter hit expected error:", err)
				return
			}
			Fatal(t, "challenge step", i, "hit error:", err)
		}
		if tx == nil {
			Fatal(t, "no move")
		}

		if useStubs {
			if len(mockSpawn.ExecSpawned) != 0 {
				if len(mockSpawn.ExecSpawned) != 1 {
					Fatal(t, "bad number of spawned execRuns: ", len(mockSpawn.ExecSpawned))
				}
				if mockSpawn.ExecSpawned[0] != uint64(challengeMsgIdx) {
					Fatal(t, "wrong spawned execRuns: ", mockSpawn.ExecSpawned[0], " expected: ", challengeMsgIdx)
				}
				return
			}
		}

		_, err = EnsureTxSucceeded(ctx, l1Backend, tx)
		if err != nil {
			if !currentCorrect && strings.Contains(err.Error(), "BAD_SEQINBOX_MESSAGE") {
				t.Log("challenge complete! Tx failed as expected:", err)
				return
			}
			Fatal(t, err)
		}

		confirmLatestBlock(ctx, t, l1Info, l1Backend)

		winner, err := resultReceiver.Winner(&bind.CallOpts{})
		if err != nil {
			Fatal(t, err)
		}
		if winner == (common.Address{}) {
			continue
		}
		if winner != expectedWinner {
			Fatal(t, "wrong party won challenge")
		}
	}

	Fatal(t, "challenge timed out without winner")
}

func TestMockChallengeManagerAsserterIncorrect(t *testing.T) {
	t.Parallel()
	for i := int64(1); i <= makeBatch_MsgsPerBatch*3; i++ {
		RunChallengeTest(t, false, true, i)
	}
}

func TestMockChallengeManagerAsserterCorrect(t *testing.T) {
	t.Parallel()
	for i := int64(1); i <= makeBatch_MsgsPerBatch*3; i++ {
		RunChallengeTest(t, true, true, i)
	}
}

'''
'''--- system_tests/full_challenge_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build challengetest
// +build challengetest

//
// Copyright 2021-2022, Offchain Labs, Inc. All rights reserved.
//

package arbtest

import (
	"testing"
)

func TestChallengeManagerFullAsserterIncorrect(t *testing.T) {
	t.Parallel()
	RunChallengeTest(t, false, false, makeBatch_MsgsPerBatch+1)
}

func TestChallengeManagerFullAsserterCorrect(t *testing.T) {
	t.Parallel()
	RunChallengeTest(t, true, false, makeBatch_MsgsPerBatch+2)
}

'''
'''--- system_tests/infra_fee_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestInfraFee(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User2")

	ownerTxOpts := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	ownerTxOpts.Context = ctx
	ownerCallOpts := builder.L2Info.GetDefaultCallOpts("Owner", ctx)

	arbowner, err := precompilesgen.NewArbOwner(common.HexToAddress("70"), builder.L2.Client)
	Require(t, err)
	arbownerPublic, err := precompilesgen.NewArbOwnerPublic(common.HexToAddress("6b"), builder.L2.Client)
	Require(t, err)
	networkFeeAddr, err := arbownerPublic.GetNetworkFeeAccount(ownerCallOpts)
	Require(t, err)
	infraFeeAddr := common.BytesToAddress(crypto.Keccak256([]byte{3, 2, 6}))
	tx, err := arbowner.SetInfraFeeAccount(&ownerTxOpts, infraFeeAddr)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	_, simple := builder.L2.DeploySimple(t, ownerTxOpts)

	netFeeBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)
	infraFeeBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)

	tx, err = simple.Increment(&ownerTxOpts)
	Require(t, err)
	receipt, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	l2GasUsed := receipt.GasUsed - receipt.GasUsedForL1
	expectedFunds := arbmath.BigMulByUint(arbmath.UintToBig(l2pricing.InitialBaseFeeWei), l2GasUsed)
	expectedBalanceAfter := arbmath.BigAdd(infraFeeBalanceBefore, expectedFunds)

	netFeeBalanceAfter, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)
	infraFeeBalanceAfter, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)

	if !arbmath.BigEquals(netFeeBalanceBefore, netFeeBalanceAfter) {
		Fatal(t, netFeeBalanceBefore, netFeeBalanceAfter)
	}
	if !arbmath.BigEquals(infraFeeBalanceAfter, expectedBalanceAfter) {
		Fatal(t, infraFeeBalanceBefore, expectedFunds, infraFeeBalanceAfter, expectedBalanceAfter)
	}
}

'''
'''--- system_tests/initialization_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/statetransfer"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

// Each contract gets a set of storage cells with values, and code that returns a sum of their cell
// Getting expectedsum proves both code and storage cells are correct
func InitOneContract(prand *testhelpers.PseudoRandomDataSource) (*statetransfer.AccountInitContractInfo, *big.Int) {
	storageMap := make(map[common.Hash]common.Hash)
	code := []byte{0x60, 0x0} // PUSH1 0
	sum := big.NewInt(0)
	numCells := int(prand.GetUint64() % 1000)
	for i := 0; i < numCells; i++ {
		storageAddr := prand.GetHash()
		storageVal := prand.GetAddress().Hash() // 20 bytes so sum won't overflow
		code = append(code, 0x7f)               // PUSH32
		code = append(code, storageAddr[:]...)  // storageAdr
		code = append(code, 0x54)               // SLOAD
		code = append(code, 0x01)               // ADD
		storageMap[storageAddr] = storageVal
		sum.Add(sum, storageVal.Big())
	}
	code = append(code, 0x60, 0x00) // PUSH1 0
	code = append(code, 0x52)       // MSTORE
	code = append(code, 0x60, 0x20) // PUSH1 32
	code = append(code, 0x60, 0x00) // PUSH1 0
	code = append(code, 0xf3)       // RETURN
	return &statetransfer.AccountInitContractInfo{
		ContractStorage: storageMap,
		Code:            code,
	}, sum
}

func TestInitContract(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	expectedSums := make(map[common.Address]*big.Int)
	prand := testhelpers.NewPseudoRandomDataSource(t, 1)
	l2info := NewArbTestInfo(t, params.ArbitrumDevTestChainConfig().ChainID)
	for i := 0; i < 50; i++ {
		contractData, sum := InitOneContract(prand)
		accountAddress := prand.GetAddress()
		accountInfo := statetransfer.AccountInitializationInfo{
			Addr:         accountAddress,
			EthBalance:   big.NewInt(0),
			Nonce:        1,
			ContractInfo: contractData,
		}
		l2info.ArbInitData.Accounts = append(l2info.ArbInitData.Accounts, accountInfo)
		expectedSums[accountAddress] = sum
	}
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.L2Info = l2info
	cleanup := builder.Build(t)
	defer cleanup()

	for accountAddress, sum := range expectedSums {
		msg := ethereum.CallMsg{
			To: &accountAddress,
		}
		res, err := builder.L2.Client.CallContract(ctx, msg, big.NewInt(0))
		Require(t, err)
		resBig := new(big.Int).SetBytes(res)
		if resBig.Cmp(sum) != 0 {
			t.Fatal("address {} exp {} got {}", accountAddress, sum, resBig)
		}
	}
}

'''
'''--- system_tests/ipc_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"path/filepath"
	"testing"

	"github.com/ethereum/go-ethereum/ethclient"
)

func TestIpcRpc(t *testing.T) {
	ipcPath := filepath.Join(t.TempDir(), "test.ipc")

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.l2StackConfig.IPCPath = ipcPath
	cleanup := builder.Build(t)
	defer cleanup()

	_, err := ethclient.Dial(ipcPath)
	Require(t, err)
}

'''
'''--- system_tests/log_subscription_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"reflect"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
)

func TestLogSubscription(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	arbSys, err := precompilesgen.NewArbSys(types.ArbSysAddress, builder.L2.Client)
	Require(t, err)

	logChan := make(chan types.Log, 128)
	subscription, err := builder.L2.Client.SubscribeFilterLogs(ctx, ethereum.FilterQuery{}, logChan)
	Require(t, err)
	defer subscription.Unsubscribe()

	tx, err := arbSys.WithdrawEth(&auth, common.Address{})
	Require(t, err)
	receipt, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	if len(receipt.Logs) != 1 {
		Fatal(t, "Unexpected number of logs", len(receipt.Logs))
	}

	var receiptLog types.Log = *receipt.Logs[0]
	var subscriptionLog types.Log
	timer := time.NewTimer(time.Second * 5)
	defer timer.Stop()
	select {
	case <-timer.C:
		Fatal(t, "Hit timeout waiting for log from subscription")
	case subscriptionLog = <-logChan:
	}
	if !reflect.DeepEqual(receiptLog, subscriptionLog) {
		Fatal(t, "Receipt log", receiptLog, "is different than subscription log", subscriptionLog)
	}
	_, err = builder.L2.Client.BlockByHash(ctx, subscriptionLog.BlockHash)
	Require(t, err)
}

'''
'''--- system_tests/meaningless_reorg_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

func TestMeaninglessBatchReorg(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.BatchPoster.Enable = false
	cleanup := builder.Build(t)
	defer cleanup()

	seqInbox, err := bridgegen.NewSequencerInbox(builder.L1Info.GetAddress("SequencerInbox"), builder.L1.Client)
	Require(t, err)
	seqOpts := builder.L1Info.GetDefaultTransactOpts("Sequencer", ctx)

	tx, err := seqInbox.AddSequencerL2BatchFromOrigin(&seqOpts, big.NewInt(1), nil, big.NewInt(1), common.Address{})
	Require(t, err)
	batchReceipt, err := builder.L1.EnsureTxSucceeded(tx)
	Require(t, err)

	for i := 0; ; i++ {
		if i >= 500 {
			Fatal(t, "Failed to read batch from L1")
		}
		msgNum, err := builder.L2.ExecNode.ExecEngine.HeadMessageNumber()
		Require(t, err)
		if msgNum == 1 {
			break
		} else if msgNum > 1 {
			Fatal(t, "More than two batches in test?")
		}
		time.Sleep(10 * time.Millisecond)
	}
	metadata, err := builder.L2.ConsensusNode.InboxTracker.GetBatchMetadata(1)
	Require(t, err)
	originalBatchBlock := batchReceipt.BlockNumber.Uint64()
	if metadata.ParentChainBlock != originalBatchBlock {
		Fatal(t, "Posted batch in block", originalBatchBlock, "but metadata says L1 block was", metadata.ParentChainBlock)
	}

	_, l2Receipt := builder.L2.TransferBalance(t, "Owner", "Owner", common.Big1, builder.L2Info)

	// Make the reorg larger to force the miner to discard transactions.
	// The miner usually collects transactions from deleted blocks and puts them in the mempool.
	// However, this code doesn't run on reorgs larger than 64 blocks for performance reasons.
	// Therefore, we make a bunch of small blocks to prevent the code from running.
	for j := uint64(0); j < 70; j++ {
		builder.L1.TransferBalance(t, "Faucet", "Faucet", common.Big1, builder.L1Info)
	}

	parentBlock := builder.L1.L1Backend.BlockChain().GetBlockByNumber(batchReceipt.BlockNumber.Uint64() - 1)
	err = builder.L1.L1Backend.BlockChain().ReorgToOldBlock(parentBlock)
	Require(t, err)

	// Produce a new l1Block so that the batch ends up in a different l1Block than before
	builder.L1.TransferBalance(t, "User", "User", common.Big1, builder.L1Info)

	tx, err = seqInbox.AddSequencerL2BatchFromOrigin(&seqOpts, big.NewInt(1), nil, big.NewInt(1), common.Address{})
	Require(t, err)
	newBatchReceipt, err := builder.L1.EnsureTxSucceeded(tx)
	Require(t, err)

	newBatchBlock := newBatchReceipt.BlockNumber.Uint64()
	if newBatchBlock == originalBatchBlock {
		Fatal(t, "Attempted to change L1 block number in batch reorg, but it ended up in the same block", newBatchBlock)
	} else {
		t.Log("Batch successfully moved in reorg from L1 block", originalBatchBlock, "to L1 block", newBatchBlock)
	}

	for i := 0; ; i++ {
		if i >= 500 {
			Fatal(t, "Failed to read batch reorg from L1")
		}
		metadata, err = builder.L2.ConsensusNode.InboxTracker.GetBatchMetadata(1)
		Require(t, err)
		if metadata.ParentChainBlock == newBatchBlock {
			break
		} else if metadata.ParentChainBlock != originalBatchBlock {
			Fatal(t, "Batch L1 block changed from", originalBatchBlock, "to", metadata.ParentChainBlock, "instead of expected", metadata.ParentChainBlock)
		}
		time.Sleep(10 * time.Millisecond)
	}

	_, err = builder.L2.ConsensusNode.InboxReader.GetSequencerMessageBytes(ctx, 1)
	Require(t, err)

	l2Header, err := builder.L2.Client.HeaderByNumber(ctx, l2Receipt.BlockNumber)
	Require(t, err)

	if l2Header.Hash() != l2Receipt.BlockHash {
		Fatal(t, "L2 block hash changed")
	}
}

'''
'''--- system_tests/nodeinterface_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
)

func TestL2BlockRangeForL1(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()
	user := builder.L1Info.GetDefaultTransactOpts("User", ctx)

	numTransactions := 200
	for i := 0; i < numTransactions; i++ {
		builder.L2.TransferBalanceTo(t, "Owner", util.RemapL1Address(user.From), big.NewInt(1e18), builder.L2Info)
	}

	nodeInterface, err := node_interfacegen.NewNodeInterface(types.NodeInterfaceAddress, builder.L2.Client)
	if err != nil {
		t.Fatalf("Error creating node interface: %v", err)
	}

	l1BlockNums := map[uint64]*[2]uint64{}
	latestL2, err := builder.L2.Client.BlockNumber(ctx)
	if err != nil {
		t.Fatalf("Error querying most recent l2 block: %v", err)
	}
	for l2BlockNum := uint64(0); l2BlockNum <= latestL2; l2BlockNum++ {
		l1BlockNum, err := nodeInterface.BlockL1Num(&bind.CallOpts{}, l2BlockNum)
		if err != nil {
			t.Fatalf("Error quering l1 block number for l2 block: %d, error: %v", l2BlockNum, err)
		}
		if _, ok := l1BlockNums[l1BlockNum]; !ok {
			l1BlockNums[l1BlockNum] = &[2]uint64{l2BlockNum, l2BlockNum}
		}
		l1BlockNums[l1BlockNum][1] = l2BlockNum
	}

	// Test success.
	for l1BlockNum := range l1BlockNums {
		rng, err := nodeInterface.L2BlockRangeForL1(&bind.CallOpts{}, l1BlockNum)
		if err != nil {
			t.Fatalf("Error getting l2 block range for l1 block: %d, error: %v", l1BlockNum, err)
		}
		expected := l1BlockNums[l1BlockNum]
		if rng.FirstBlock != expected[0] || rng.LastBlock != expected[1] {
			unexpectedL1BlockNum, err := nodeInterface.BlockL1Num(&bind.CallOpts{}, rng.LastBlock)
			if err != nil {
				t.Fatalf("Error quering l1 block number for l2 block: %d, error: %v", rng.LastBlock, err)
			}
			// Handle the edge case when new l2 blocks are produced between latestL2 was last calculated and now.
			if unexpectedL1BlockNum != l1BlockNum || rng.LastBlock < expected[1] || rng.FirstBlock != expected[0] {
				t.Errorf("L2BlockRangeForL1(%d) = (%d %d) want (%d %d)", l1BlockNum, rng.FirstBlock, rng.LastBlock, expected[0], expected[1])
			}
		}
	}
	// Test invalid case.
	if _, err := nodeInterface.L2BlockRangeForL1(&bind.CallOpts{}, 1e5); err == nil {
		t.Fatalf("GetL2BlockRangeForL1 didn't fail for an invalid input")
	}
}

'''
'''--- system_tests/outbox_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"encoding/hex"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/gethhook"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/merkletree"
)

func TestOutboxProofs(t *testing.T) {
	t.Parallel()
	gethhook.RequireHookedGeth()
	rand.Seed(time.Now().UTC().UnixNano())
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	arbSysAbi, err := precompilesgen.ArbSysMetaData.GetAbi()
	Require(t, err, "failed to get abi")
	withdrawTopic := arbSysAbi.Events["L2ToL1Tx"].ID
	merkleTopic := arbSysAbi.Events["SendMerkleUpdate"].ID

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)

	arbSys, err := precompilesgen.NewArbSys(types.ArbSysAddress, builder.L2.Client)
	Require(t, err)
	nodeInterface, err := node_interfacegen.NewNodeInterface(types.NodeInterfaceAddress, builder.L2.Client)
	Require(t, err)

	txnCount := int64(1 + rand.Intn(16))

	// represents a send we should be able to prove exists
	type proofPair struct {
		hash common.Hash
		leaf uint64
	}

	// represents a historical root we'll prove against
	type proofRoot struct {
		root common.Hash
		size uint64
	}

	provables := make([]proofPair, 0)
	roots := make([]proofRoot, 0)
	txns := []common.Hash{}

	for i := int64(0); i < txnCount; i++ {
		auth.Value = big.NewInt(i * 1000000000)
		auth.Nonce = big.NewInt(i + 1)
		tx, err := arbSys.WithdrawEth(&auth, common.Address{})
		Require(t, err, "ArbSys failed")
		txns = append(txns, tx.Hash())

		time.Sleep(4 * time.Millisecond) // Geth takes a few ms for the receipt to show up
		_, err = builder.L2.Client.TransactionReceipt(ctx, tx.Hash())
		if err == nil {
			merkleState, err := arbSys.SendMerkleTreeState(&bind.CallOpts{})
			Require(t, err, "could not get merkle root")

			root := proofRoot{
				root: merkleState.Root,          // we assume the user knows the root and size
				size: merkleState.Size.Uint64(), //
			}
			roots = append(roots, root)
		}
	}

	for _, tx := range txns {
		var receipt *types.Receipt
		receipt, err = builder.L2.Client.TransactionReceipt(ctx, tx)
		Require(t, err, "No receipt for txn")

		if receipt.Status != types.ReceiptStatusSuccessful {
			Fatal(t, "Tx failed with status code:", receipt)
		}
		if len(receipt.Logs) == 0 {
			Fatal(t, "Tx didn't emit any logs")
		}

		for _, log := range receipt.Logs {

			if log.Topics[0] == withdrawTopic {
				parsedLog, err := arbSys.ParseL2ToL1Tx(*log)
				Require(t, err, "Failed to parse log")

				provables = append(provables, proofPair{
					hash: common.BigToHash(parsedLog.Hash),
					leaf: parsedLog.Position.Uint64(),
				})
			}
		}
	}

	t.Log("Proving against", len(roots), "historical roots among the", txnCount, "ever")
	t.Log("Will query against topics\n\tmerkle:   ", merkleTopic, "\n\twithdraw: ", withdrawTopic)

	for _, root := range roots {
		rootHash := root.root
		treeSize := root.size

		balanced := treeSize == arbmath.NextPowerOf2(treeSize)/2
		treeLevels := int(arbmath.Log2ceil(treeSize)) // the # of levels in the tree
		proofLevels := treeLevels - 1                 // the # of levels where a hash is needed (all but root)
		walkLevels := treeLevels                      // the # of levels we need to consider when building walks
		if balanced {
			walkLevels -= 1 // skip the root
		}

		t.Log("Tree has", treeSize, "leaves and", treeLevels, "levels")
		t.Log("Root hash", hex.EncodeToString(rootHash[:]))
		t.Log("Balanced:", balanced)

		// using only the root and position, we'll prove the send hash exists for each leaf
		for _, provable := range provables {
			if provable.leaf >= treeSize {
				continue
			}

			t.Log("Proving leaf", provable.leaf)

			// find which nodes we'll want in our proof up to a partial
			query := make([]common.Hash, 0)             // the nodes we'll query for
			nodes := make([]merkletree.LevelAndLeaf, 0) // the nodes needed (might not be found from query)
			which := uint64(1)                          // which bit to flip & set
			place := provable.leaf                      // where we are in the tree
			for level := 0; level < walkLevels; level++ {
				sibling := place ^ which

				position := merkletree.LevelAndLeaf{
					Level: uint64(level),
					Leaf:  sibling,
				}

				if sibling < treeSize {
					// the sibling must not be newer than the root
					query = append(query, common.BigToHash(position.ToBigInt()))
				}
				nodes = append(nodes, position)
				place |= which // set the bit so that we approach from the right
				which <<= 1    // advance to the next bit
			}

			// find all the partials
			partials := make(map[merkletree.LevelAndLeaf]common.Hash)
			if !balanced {
				power := uint64(1) << proofLevels
				total := uint64(0)
				for level := proofLevels; level >= 0; level-- {

					if (power & treeSize) > 0 { // the partials map to the binary representation of the tree size

						total += power    // The actual leaf for a given partial is the sum of the powers of 2
						leaf := total - 1 // preceding it. We subtract 1 since we count from 0

						partial := merkletree.LevelAndLeaf{
							Level: uint64(level),
							Leaf:  leaf,
						}

						query = append(query, common.BigToHash(partial.ToBigInt()))
						partials[partial] = common.Hash{}
					}
					power >>= 1
				}
			}
			t.Log("Found", len(partials), "partials")

			// in one lookup, query geth for all the data we need to construct a proof
			var logs []types.Log
			if len(query) > 0 {
				logs, err = builder.L2.Client.FilterLogs(ctx, ethereum.FilterQuery{
					Addresses: []common.Address{
						types.ArbSysAddress,
					},
					Topics: [][]common.Hash{
						{merkleTopic, withdrawTopic},
						nil,
						nil,
						query,
					},
				})
				Require(t, err, "couldn't get logs")
			}

			t.Log("Querried for", len(query), "positions", query)
			t.Log("Found", len(logs), "logs for proof", provable.leaf, "of", treeSize)

			known := make(map[merkletree.LevelAndLeaf]common.Hash) // all values in the tree we know
			partialsByLevel := make(map[uint64]common.Hash)        // maps for each level the partial it may have
			var minPartialPlace *merkletree.LevelAndLeaf           // the lowest-level partial

			for _, log := range logs {

				hash := log.Topics[2]
				position := log.Topics[3]

				level := new(big.Int).SetBytes(position[:8]).Uint64()
				leaf := new(big.Int).SetBytes(position[8:]).Uint64()

				if level == 0 {
					hash = crypto.Keccak256Hash(hash.Bytes())
				}

				place := merkletree.LevelAndLeaf{
					Level: level,
					Leaf:  leaf,
				}

				t.Log("Log:\n\tposition: level", level, "leaf", leaf, "\n\thash:    ", hash)
				known[place] = hash

				if zero, ok := partials[place]; ok {
					if zero != (common.Hash{}) {
						Fatal(t, "Somehow got 2 partials for the same level\n\t1st:", zero, "\n\t2nd:", hash)
					}
					partials[place] = hash
					partialsByLevel[level] = hash
					if minPartialPlace == nil || level < minPartialPlace.Level {
						minPartialPlace = &place
					}
				}
			}

			for place, hash := range known {
				t.Log("known  ", place.Level, hash, "@", place)
			}
			t.Log(len(known), "values are known\n")

			for place, hash := range partials {
				t.Log("partial", place.Level, hash, "@", place)
			}
			t.Log("resolving frontiers\n")

			if !balanced {
				// This tree isn't balanced, so we'll need to use the partials to recover the missing info.
				// To do this, we'll walk the boundry of what's known, computing hashes along the way

				zero := common.Hash{}

				step := *minPartialPlace
				step.Leaf += 1 << step.Level // we start on the min partial's zero-hash sibling
				known[step] = zero

				for step.Level < uint64(treeLevels) {

					curr, ok := known[step]
					if !ok {
						Fatal(t, "We should know the current node's value")
					}

					left := curr
					right := curr

					if _, ok := partialsByLevel[step.Level]; ok {
						// a partial on the frontier can only appear on the left
						// moving leftward for a level l skips 2^l leaves
						step.Leaf -= 1 << step.Level
						partial, ok := known[step]
						if !ok {
							Fatal(t, "There should be a partial here")
						}
						left = partial
					} else {
						// getting to the next partial means covering its mirror subtree, so we look right
						// moving rightward for a level l skips 2^l leaves
						step.Leaf += 1 << step.Level
						known[step] = zero
						right = zero
					}

					// move to the parent
					step.Level += 1
					step.Leaf |= 1 << (step.Level - 1)
					known[step] = crypto.Keccak256Hash(left.Bytes(), right.Bytes())
				}

				if known[step] != rootHash {
					// a correct walk of the frontier should end with resolving the root
					t.Log("Walking up the tree didn't re-create the root", known[step], "vs", rootHash)
				}

				for place, hash := range known {
					t.Log("known", place, hash)
				}
			}

			t.Log("Complete proof of leaf", provable.leaf)

			hashes := make([]common.Hash, len(nodes))
			for i, place := range nodes {
				hash, ok := known[place]
				if !ok {
					Fatal(t, "We're missing data for the node at position", place)
				}
				hashes[i] = hash
				t.Log("node", place, hash)
			}

			proof := merkletree.MerkleProof{
				RootHash:  rootHash,
				LeafHash:  crypto.Keccak256Hash(provable.hash.Bytes()),
				LeafIndex: provable.leaf,
				Proof:     hashes,
			}

			if !proof.IsCorrect() {
				Fatal(t, "Proof is wrong")
			}

			// Check NodeInterface.sol produces equivalent proofs
			outboxProof, err := nodeInterface.ConstructOutboxProof(
				&bind.CallOpts{}, treeSize, provable.leaf,
			)
			Require(t, err, "failed to construct outbox proof using NodeInterface.sol")
			nodeRoot := common.Hash(outboxProof.Root)
			nodeProof := outboxProof.Proof
			nodeSend := outboxProof.Send

			if nodeRoot != rootHash {
				Fatal(t, "NodeInterface root differs\n", nodeRoot, "\n", rootHash)
			}
			if len(hashes) != len(nodeProof) {
				Fatal(t, "NodeInterface proof is the wrong size", len(nodeProof), len(hashes))
			}
			for i, correct := range hashes {
				if nodeProof[i] != correct {
					t.Error("NodeInterface proof differs", i, correct, nodeProof[i])
				}
			}
			if nodeSend != provable.hash {
				Fatal(t, "NodeInterface send differs\n", nodeSend, "\n", provable.hash)
			}
		}
	}
}

'''
'''--- system_tests/precompile_fuzz_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/vm"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/gethhook"
	"github.com/offchainlabs/nitro/precompiles"
)

const fuzzGas uint64 = 1200000

func FuzzPrecompiles(f *testing.F) {
	gethhook.RequireHookedGeth()

	f.Fuzz(func(t *testing.T, precompileSelector byte, methodSelector byte, input []byte) {
		// Create a StateDB
		sdb, err := state.New(common.Hash{}, state.NewDatabase(rawdb.NewMemoryDatabase()), nil)
		if err != nil {
			panic(err)
		}
		burner := burn.NewSystemBurner(nil, false)
		chainConfig := params.ArbitrumDevTestChainConfig()
		_, err = arbosState.InitializeArbosState(sdb, burner, chainConfig, arbostypes.TestInitMessage)
		if err != nil {
			panic(err)
		}

		// Create an EVM
		gp := core.GasPool(fuzzGas)
		txContext := vm.TxContext{
			GasPrice: common.Big1,
		}
		blockContext := vm.BlockContext{
			CanTransfer: core.CanTransfer,
			Transfer:    core.Transfer,
			GetHash:     nil,
			Coinbase:    common.Address{},
			BlockNumber: new(big.Int),
			Time:        0,
			Difficulty:  new(big.Int),
			GasLimit:    fuzzGas,
			BaseFee:     common.Big1,
		}
		evm := vm.NewEVM(blockContext, txContext, sdb, chainConfig, vm.Config{})

		// Pick a precompile address based on the first byte of the input
		var addr common.Address
		addr[19] = precompileSelector

		// Pick a precompile method based on the second byte of the input
		if precompile := precompiles.Precompiles()[addr]; precompile != nil {
			sigs := precompile.Precompile().Get4ByteMethodSignatures()
			if int(methodSelector) < len(sigs) {
				newInput := make([]byte, 4)
				copy(newInput, sigs[methodSelector][:])
				newInput = append(newInput, input...)
				input = newInput
			}
		}

		// Create and apply a message
		msg := &core.Message{
			From:       common.Address{},
			To:         &addr,
			Nonce:      0,
			Value:      new(big.Int),
			GasLimit:   fuzzGas,
			GasPrice:   new(big.Int),
			GasFeeCap:  new(big.Int),
			GasTipCap:  new(big.Int),
			Data:       input,
			AccessList: nil,
		}
		_, _ = core.ApplyMessage(evm, msg, &gp)
	})
}

'''
'''--- system_tests/precompile_test.go ---
// Copyright 2021-2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestPurePrecompileMethodCalls(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	arbSys, err := precompilesgen.NewArbSys(common.HexToAddress("0x64"), builder.L2.Client)
	Require(t, err, "could not deploy ArbSys contract")
	chainId, err := arbSys.ArbChainID(&bind.CallOpts{})
	Require(t, err, "failed to get the ChainID")
	if chainId.Uint64() != params.ArbitrumDevTestChainConfig().ChainID.Uint64() {
		Fatal(t, "Wrong ChainID", chainId.Uint64())
	}
}

func TestViewLogReverts(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	arbDebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
	Require(t, err, "could not deploy ArbSys contract")

	err = arbDebug.EventsView(nil)
	if err == nil {
		Fatal(t, "unexpected success")
	}
}

func TestCustomSolidityErrors(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	callOpts := &bind.CallOpts{Context: ctx}
	arbDebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
	Require(t, err, "could not bind ArbDebug contract")
	customError := arbDebug.CustomRevert(callOpts, 1024)
	if customError == nil {
		Fatal(t, "customRevert call should have errored")
	}
	observedMessage := customError.Error()
	expectedMessage := "execution reverted: error Custom(1024, This spider family wards off bugs: /\\oo/\\ //\\(oo)/\\ /\\oo/\\, true)"
	if observedMessage != expectedMessage {
		Fatal(t, observedMessage)
	}

	arbSys, err := precompilesgen.NewArbSys(arbos.ArbSysAddress, builder.L2.Client)
	Require(t, err, "could not bind ArbSys contract")
	_, customError = arbSys.ArbBlockHash(callOpts, big.NewInt(1e9))
	if customError == nil {
		Fatal(t, "out of range ArbBlockHash call should have errored")
	}
	observedMessage = customError.Error()
	expectedMessage = "execution reverted: error InvalidBlockNumber(1000000000, 1)"
	if observedMessage != expectedMessage {
		Fatal(t, observedMessage)
	}
}

func TestPrecompileErrorGasLeft(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
	_, _, simple, err := mocksgen.DeploySimple(&auth, builder.L2.Client)
	Require(t, err)

	assertNotAllGasConsumed := func(to common.Address, input []byte) {
		gas, err := simple.CheckGasUsed(&bind.CallOpts{Context: ctx}, to, input)
		Require(t, err, "Failed to call CheckGasUsed to precompile", to)
		maxGas := big.NewInt(100_000)
		if arbmath.BigGreaterThan(gas, maxGas) {
			Fatal(t, "Precompile", to, "used", gas, "gas reverting, greater than max expected", maxGas)
		}
	}

	arbSys, err := precompilesgen.ArbSysMetaData.GetAbi()
	Require(t, err)

	arbBlockHash := arbSys.Methods["arbBlockHash"]
	data, err := arbBlockHash.Inputs.Pack(big.NewInt(1e9))
	Require(t, err)
	input := append([]byte{}, arbBlockHash.ID...)
	input = append(input, data...)
	assertNotAllGasConsumed(arbos.ArbSysAddress, input)

	arbDebug, err := precompilesgen.ArbDebugMetaData.GetAbi()
	Require(t, err)
	assertNotAllGasConsumed(common.HexToAddress("0xff"), arbDebug.Methods["legacyError"].ID)
}

'''
'''--- system_tests/recreatestate_rpc_test.go ---
package arbtest

import (
	"context"
	"errors"
	"math/big"
	"strings"
	"testing"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/trie"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/util"
)

func prepareNodeWithHistory(t *testing.T, ctx context.Context, execConfig *gethexec.Config, txCount uint64) (node *arbnode.Node, executionNode *gethexec.ExecutionNode, l2client *ethclient.Client, cancel func()) {
	t.Helper()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.execConfig = execConfig
	cleanup := builder.Build(t)
	builder.L2Info.GenerateAccount("User2")
	var txs []*types.Transaction
	for i := uint64(0); i < txCount; i++ {
		tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, common.Big1, nil)
		txs = append(txs, tx)
		err := builder.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)
	}
	for _, tx := range txs {
		_, err := builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}

	return builder.L2.ConsensusNode, builder.L2.ExecNode, builder.L2.Client, cleanup
}

func fillHeaderCache(t *testing.T, bc *core.BlockChain, from, to uint64) {
	t.Helper()
	for i := from; i <= to; i++ {
		header := bc.GetHeaderByNumber(i)
		if header == nil {
			Fatal(t, "internal test error - failed to get header while trying to fill headerCache, header:", i)
		}
	}
}

func fillBlockCache(t *testing.T, bc *core.BlockChain, from, to uint64) {
	t.Helper()
	for i := from; i <= to; i++ {
		block := bc.GetBlockByNumber(i)
		if block == nil {
			Fatal(t, "internal test error - failed to get block while trying to fill blockCache, block:", i)
		}
	}
}

func removeStatesFromDb(t *testing.T, bc *core.BlockChain, db ethdb.Database, from, to uint64) {
	t.Helper()
	for i := from; i <= to; i++ {
		header := bc.GetHeaderByNumber(i)
		if header == nil {
			Fatal(t, "failed to get last block header")
		}
		hash := header.Root
		err := db.Delete(hash.Bytes())
		Require(t, err)
	}
	for i := from; i <= to; i++ {
		header := bc.GetHeaderByNumber(i)
		_, err := bc.StateAt(header.Root)
		if err == nil {
			Fatal(t, "internal test error - failed to remove state from db")
		}
		expectedErr := &trie.MissingNodeError{}
		if !errors.As(err, &expectedErr) {
			Fatal(t, "internal test error - failed to remove state from db, err: ", err)
		}
	}
}

func TestRecreateStateForRPCNoDepthLimit(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = arbitrum.InfiniteMaxRecreateStateDepth
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0
	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, 32)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)
	middleBlock := lastBlock / 2

	expectedBalance, err := l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)

	removeStatesFromDb(t, bc, db, middleBlock, lastBlock)

	balance, err := l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)
	if balance.Cmp(expectedBalance) != 0 {
		Fatal(t, "unexpected balance result for last block, want: ", expectedBalance, " have: ", balance)
	}
}

func TestRecreateStateForRPCBigEnoughDepthLimit(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	depthGasLimit := int64(256 * util.NormalizeL2GasForL1GasInitial(800_000, params.GWei))
	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = depthGasLimit
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0
	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, 32)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)
	middleBlock := lastBlock / 2

	expectedBalance, err := l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)

	removeStatesFromDb(t, bc, db, middleBlock, lastBlock)

	balance, err := l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)
	if balance.Cmp(expectedBalance) != 0 {
		Fatal(t, "unexpected balance result for last block, want: ", expectedBalance, " have: ", balance)
	}

}

func TestRecreateStateForRPCDepthLimitExceeded(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = int64(200)
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0
	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, 32)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)
	middleBlock := lastBlock / 2

	removeStatesFromDb(t, bc, db, middleBlock, lastBlock)

	_, err = l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	if err == nil {
		Fatal(t, "Didn't fail as expected")
	}
	if err.Error() != arbitrum.ErrDepthLimitExceeded.Error() {
		Fatal(t, "Failed with unexpected error:", err)
	}
}

func TestRecreateStateForRPCMissingBlockParent(t *testing.T) {
	// HeaderChain.headerCache size limit is currently core.headerCacheLimit = 512
	var headerCacheLimit uint64 = 512
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = arbitrum.InfiniteMaxRecreateStateDepth
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0
	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, headerCacheLimit+5)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)
	if lastBlock < headerCacheLimit+4 {
		Fatal(t, "Internal test error - not enough blocks produced during preparation, want:", headerCacheLimit, "have:", lastBlock)
	}

	removeStatesFromDb(t, bc, db, lastBlock-4, lastBlock)

	headerToRemove := lastBlock - 4
	hash := rawdb.ReadCanonicalHash(db, headerToRemove)
	rawdb.DeleteHeader(db, hash, headerToRemove)

	firstBlock := lastBlock - headerCacheLimit - 5
	fillHeaderCache(t, bc, firstBlock, firstBlock+headerCacheLimit)

	for i := lastBlock; i > lastBlock-3; i-- {
		_, err = l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(i))
		if err == nil {
			hash := rawdb.ReadCanonicalHash(db, i)
			Fatal(t, "Didn't fail to get balance at block:", i, " with hash:", hash, ", lastBlock:", lastBlock)
		}
		if !strings.Contains(err.Error(), "chain doesn't contain parent of block") {
			Fatal(t, "Failed with unexpected error: \"", err, "\", at block:", i, "lastBlock:", lastBlock)
		}
	}
}

func TestRecreateStateForRPCBeyondGenesis(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = arbitrum.InfiniteMaxRecreateStateDepth
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0
	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, 32)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)

	genesis := bc.Config().ArbitrumChainParams.GenesisBlockNum
	removeStatesFromDb(t, bc, db, genesis, lastBlock)

	_, err = l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	if err == nil {
		hash := rawdb.ReadCanonicalHash(db, lastBlock)
		Fatal(t, "Didn't fail to get balance at block:", lastBlock, " with hash:", hash, ", lastBlock:", lastBlock)
	}
	if !strings.Contains(err.Error(), "moved beyond genesis") {
		Fatal(t, "Failed with unexpected error: \"", err, "\", at block:", lastBlock, "lastBlock:", lastBlock)
	}
}

func TestRecreateStateForRPCBlockNotFoundWhileRecreating(t *testing.T) {
	// BlockChain.blockCache size limit is currently core.blockCacheLimit = 256
	var blockCacheLimit uint64 = 256
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	nodeConfig := gethexec.ConfigDefaultTest()
	nodeConfig.RPC.MaxRecreateStateDepth = arbitrum.InfiniteMaxRecreateStateDepth
	nodeConfig.Sequencer.MaxBlockSpeed = 0
	nodeConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	nodeConfig.Caching.Archive = true
	// disable trie/Database.cleans cache, so as states removed from ChainDb won't be cached there
	nodeConfig.Caching.TrieCleanCache = 0

	nodeConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 0
	nodeConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	_, execNode, l2client, cancelNode := prepareNodeWithHistory(t, ctx, nodeConfig, blockCacheLimit+4)
	defer cancelNode()
	bc := execNode.Backend.ArbInterface().BlockChain()
	db := execNode.Backend.ChainDb()

	lastBlock, err := l2client.BlockNumber(ctx)
	Require(t, err)
	if lastBlock < blockCacheLimit+4 {
		Fatal(t, "Internal test error - not enough blocks produced during preparation, want:", blockCacheLimit, "have:", lastBlock)
	}

	removeStatesFromDb(t, bc, db, lastBlock-4, lastBlock)

	blockBodyToRemove := lastBlock - 1
	hash := rawdb.ReadCanonicalHash(db, blockBodyToRemove)
	rawdb.DeleteBody(db, hash, blockBodyToRemove)

	firstBlock := lastBlock - blockCacheLimit - 4
	fillBlockCache(t, bc, firstBlock, firstBlock+blockCacheLimit)

	_, err = l2client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	if err == nil {
		hash := rawdb.ReadCanonicalHash(db, lastBlock)
		Fatal(t, "Didn't fail to get balance at block:", lastBlock, " with hash:", hash, ", lastBlock:", lastBlock)
	}
	if !strings.Contains(err.Error(), "block not found while recreating") {
		Fatal(t, "Failed with unexpected error: \"", err, "\", at block:", lastBlock, "lastBlock:", lastBlock)
	}
}

func testSkippingSavingStateAndRecreatingAfterRestart(t *testing.T, cacheConfig *gethexec.CachingConfig, txCount int) {
	maxRecreateStateDepth := int64(30 * 1000 * 1000)
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	ctx1, cancel1 := context.WithCancel(ctx)
	execConfig := gethexec.ConfigDefaultTest()
	execConfig.RPC.MaxRecreateStateDepth = maxRecreateStateDepth
	execConfig.Sequencer.MaxBlockSpeed = 0
	execConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	execConfig.Caching = *cacheConfig

	skipBlocks := execConfig.Caching.MaxNumberOfBlocksToSkipStateSaving
	skipGas := execConfig.Caching.MaxAmountOfGasToSkipStateSaving

	feedErrChan := make(chan error, 10)
	l2info, stack, chainDb, arbDb, blockchain := createL2BlockChain(t, nil, t.TempDir(), params.ArbitrumDevTestChainConfig(), &execConfig.Caching)

	Require(t, execConfig.Validate())
	execConfigFetcher := func() *gethexec.Config { return execConfig }
	execNode, err := gethexec.CreateExecutionNode(ctx1, stack, chainDb, blockchain, nil, execConfigFetcher)
	Require(t, err)

	node, err := arbnode.CreateNode(ctx1, stack, execNode, arbDb, NewFetcherFromConfig(arbnode.ConfigDefaultL2Test()), blockchain.Config(), nil, nil, nil, nil, nil, feedErrChan)
	Require(t, err)
	err = node.TxStreamer.AddFakeInitMessage()
	Require(t, err)
	Require(t, node.Start(ctx1))
	client := ClientForStack(t, stack)

	StartWatchChanErr(t, ctx, feedErrChan, node)
	dataDir := node.Stack.DataDir()

	l2info.GenerateAccount("User2")
	var txs []*types.Transaction
	for i := 0; i < txCount; i++ {
		tx := l2info.PrepareTx("Owner", "User2", l2info.TransferGas, common.Big1, nil)
		txs = append(txs, tx)
		err := client.SendTransaction(ctx, tx)
		Require(t, err)
		receipt, err := EnsureTxSucceeded(ctx, client, tx)
		Require(t, err)
		if have, want := receipt.BlockNumber.Uint64(), uint64(i)+1; have != want {
			Fatal(t, "internal test error - tx got included in unexpected block number, have:", have, "want:", want)
		}
	}
	genesis := uint64(0)
	lastBlock, err := client.BlockNumber(ctx)
	Require(t, err)
	if want := genesis + uint64(txCount); lastBlock < want {
		Fatal(t, "internal test error - not enough blocks produced during preparation, want:", want, "have:", lastBlock)
	}
	expectedBalance, err := client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)

	node.StopAndWait()
	cancel1()
	t.Log("stopped first node")

	l2info, stack, chainDb, arbDb, blockchain = createL2BlockChain(t, l2info, dataDir, params.ArbitrumDevTestChainConfig(), &execConfig.Caching)

	execNode, err = gethexec.CreateExecutionNode(ctx1, stack, chainDb, blockchain, nil, execConfigFetcher)
	Require(t, err)

	node, err = arbnode.CreateNode(ctx, stack, execNode, arbDb, NewFetcherFromConfig(arbnode.ConfigDefaultL2Test()), blockchain.Config(), nil, node.DeployInfo, nil, nil, nil, feedErrChan)
	Require(t, err)
	Require(t, node.Start(ctx))
	client = ClientForStack(t, stack)
	defer node.StopAndWait()
	bc := execNode.Backend.ArbInterface().BlockChain()
	gas := skipGas
	blocks := skipBlocks
	for i := genesis + 1; i <= genesis+uint64(txCount); i++ {
		block := bc.GetBlockByNumber(i)
		if block == nil {
			Fatal(t, "header not found for block number:", i)
			continue
		}
		gas += block.GasUsed()
		blocks++
		_, err := bc.StateAt(block.Root())
		if (skipBlocks == 0 && skipGas == 0) || (skipBlocks != 0 && blocks > skipBlocks) || (skipGas != 0 && gas > skipGas) {
			if err != nil {
				t.Log("blocks:", blocks, "skipBlocks:", skipBlocks, "gas:", gas, "skipGas:", skipGas)
			}
			Require(t, err, "state not found, root:", block.Root(), "blockNumber:", i, "blockHash", block.Hash(), "err:", err)
			gas = 0
			blocks = 0
		} else {
			if err == nil {
				t.Log("blocks:", blocks, "skipBlocks:", skipBlocks, "gas:", gas, "skipGas:", skipGas)
				Fatal(t, "state shouldn't be available, root:", block.Root(), "blockNumber:", i, "blockHash", block.Hash())
			}
			expectedErr := &trie.MissingNodeError{}
			if !errors.As(err, &expectedErr) {
				Fatal(t, "getting state failed with unexpected error, root:", block.Root(), "blockNumber:", i, "blockHash", block.Hash())
			}
		}
	}
	for i := genesis + 1; i <= genesis+uint64(txCount); i += i % 10 {
		_, err = client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(i))
		if err != nil {
			t.Log("skipBlocks:", skipBlocks, "skipGas:", skipGas)
		}
		Require(t, err)
	}

	balance, err := client.BalanceAt(ctx, GetTestAddressForAccountName(t, "User2"), new(big.Int).SetUint64(lastBlock))
	Require(t, err)
	if balance.Cmp(expectedBalance) != 0 {
		Fatal(t, "unexpected balance result for last block, want: ", expectedBalance, " have: ", balance)
	}
}

func TestSkippingSavingStateAndRecreatingAfterRestart(t *testing.T) {
	cacheConfig := gethexec.DefaultCachingConfig
	cacheConfig.Archive = true
	//// test defaults
	testSkippingSavingStateAndRecreatingAfterRestart(t, &cacheConfig, 512)

	cacheConfig.MaxNumberOfBlocksToSkipStateSaving = 127
	cacheConfig.MaxAmountOfGasToSkipStateSaving = 0
	testSkippingSavingStateAndRecreatingAfterRestart(t, &cacheConfig, 512)

	cacheConfig.MaxNumberOfBlocksToSkipStateSaving = 0
	cacheConfig.MaxAmountOfGasToSkipStateSaving = 15 * 1000 * 1000
	testSkippingSavingStateAndRecreatingAfterRestart(t, &cacheConfig, 512)

	cacheConfig.MaxNumberOfBlocksToSkipStateSaving = 127
	cacheConfig.MaxAmountOfGasToSkipStateSaving = 15 * 1000 * 1000
	testSkippingSavingStateAndRecreatingAfterRestart(t, &cacheConfig, 512)

	// one test block ~ 925000 gas
	testBlockGas := uint64(925000)
	skipBlockValues := []uint64{0, 1, 2, 3, 5, 21, 51, 100, 101}
	var skipGasValues []uint64
	for _, i := range skipBlockValues {
		skipGasValues = append(skipGasValues, i*testBlockGas)
	}
	for _, skipGas := range skipGasValues {
		for _, skipBlocks := range skipBlockValues[:len(skipBlockValues)-2] {
			cacheConfig.MaxAmountOfGasToSkipStateSaving = skipGas
			cacheConfig.MaxNumberOfBlocksToSkipStateSaving = uint32(skipBlocks)
			testSkippingSavingStateAndRecreatingAfterRestart(t, &cacheConfig, 100)
		}
	}
}

'''
'''--- system_tests/reorg_resequencing_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/math"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
)

func TestReorgResequencing(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	startMsgCount, err := builder.L2.ConsensusNode.TxStreamer.GetMessageCount()
	Require(t, err)

	builder.L2Info.GenerateAccount("Intermediate")
	builder.L2Info.GenerateAccount("User1")
	builder.L2Info.GenerateAccount("User2")
	builder.L2Info.GenerateAccount("User3")
	builder.L2Info.GenerateAccount("User4")
	builder.L2.TransferBalance(t, "Owner", "User1", big.NewInt(params.Ether), builder.L2Info)
	builder.L2.TransferBalance(t, "Owner", "Intermediate", big.NewInt(params.Ether*3), builder.L2Info)
	builder.L2.TransferBalance(t, "Intermediate", "User2", big.NewInt(params.Ether), builder.L2Info)
	builder.L2.TransferBalance(t, "Intermediate", "User3", big.NewInt(params.Ether), builder.L2Info)

	// Intermediate does not have exactly 1 ether because of fees
	accountsWithBalance := []string{"User1", "User2", "User3"}
	verifyBalances := func(scenario string) {
		for _, account := range accountsWithBalance {
			balance, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress(account), nil)
			Require(t, err)
			if balance.Int64() != params.Ether {
				Fatal(t, "expected account", account, "to have a balance of 1 ether but instead it has", balance, "wei "+scenario)
			}
		}
	}
	verifyBalances("before reorg")

	err = builder.L2.ConsensusNode.TxStreamer.ReorgTo(startMsgCount)
	Require(t, err)

	_, err = builder.L2.ExecNode.ExecEngine.HeadMessageNumberSync(t)
	Require(t, err)

	verifyBalances("after empty reorg")

	prevMessage, err := builder.L2.ConsensusNode.TxStreamer.GetMessage(startMsgCount - 1)
	Require(t, err)
	delayedIndexHash := common.BigToHash(big.NewInt(int64(prevMessage.DelayedMessagesRead)))
	newMessage := &arbostypes.L1IncomingMessage{
		Header: &arbostypes.L1IncomingMessageHeader{
			Kind:        arbostypes.L1MessageType_EthDeposit,
			Poster:      [20]byte{},
			BlockNumber: 0,
			Timestamp:   0,
			RequestId:   &delayedIndexHash,
			L1BaseFee:   common.Big0,
		},
		L2msg: append(builder.L2Info.GetAddress("User4").Bytes(), math.U256Bytes(big.NewInt(params.Ether))...),
	}
	err = builder.L2.ConsensusNode.TxStreamer.AddMessages(startMsgCount, true, []arbostypes.MessageWithMetadata{{
		Message:             newMessage,
		DelayedMessagesRead: prevMessage.DelayedMessagesRead + 1,
	}})
	Require(t, err)

	_, err = builder.L2.ExecNode.ExecEngine.HeadMessageNumberSync(t)
	Require(t, err)

	accountsWithBalance = append(accountsWithBalance, "User4")
	verifyBalances("after reorg with new deposit")

	err = builder.L2.ConsensusNode.TxStreamer.ReorgTo(startMsgCount)
	Require(t, err)

	_, err = builder.L2.ExecNode.ExecEngine.HeadMessageNumberSync(t)
	Require(t, err)

	verifyBalances("after second empty reorg")
}

'''
'''--- system_tests/retryable_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"strings"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbos/retryables"
	"github.com/offchainlabs/nitro/arbos/util"
	"github.com/offchainlabs/nitro/execution/gethexec"

	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/node_interfacegen"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
)

func retryableSetup(t *testing.T) (
	*NodeBuilder,
	*bridgegen.Inbox,
	func(*types.Receipt) *types.Transaction,
	context.Context,
	func(),
) {
	ctx, cancel := context.WithCancel(context.Background())
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.Build(t)

	builder.L2Info.GenerateAccount("User2")
	builder.L2Info.GenerateAccount("Beneficiary")
	builder.L2Info.GenerateAccount("Burn")

	delayedInbox, err := bridgegen.NewInbox(builder.L1Info.GetAddress("Inbox"), builder.L1.Client)
	Require(t, err)
	delayedBridge, err := arbnode.NewDelayedBridge(builder.L1.Client, builder.L1Info.GetAddress("Bridge"), 0)
	Require(t, err)

	lookupL2Tx := func(l1Receipt *types.Receipt) *types.Transaction {
		messages, err := delayedBridge.LookupMessagesInRange(ctx, l1Receipt.BlockNumber, l1Receipt.BlockNumber, nil)
		Require(t, err)
		if len(messages) == 0 {
			Fatal(t, "didn't find message for submission")
		}
		var submissionTxs []*types.Transaction
		msgTypes := map[uint8]bool{
			arbostypes.L1MessageType_SubmitRetryable: true,
			arbostypes.L1MessageType_EthDeposit:      true,
			arbostypes.L1MessageType_L2Message:       true,
		}
		txTypes := map[uint8]bool{
			types.ArbitrumSubmitRetryableTxType: true,
			types.ArbitrumDepositTxType:         true,
			types.ArbitrumContractTxType:        true,
		}
		for _, message := range messages {
			if !msgTypes[message.Message.Header.Kind] {
				continue
			}
			txs, err := arbos.ParseL2Transactions(message.Message, params.ArbitrumDevTestChainConfig().ChainID, nil)
			Require(t, err)
			for _, tx := range txs {
				if txTypes[tx.Type()] {
					submissionTxs = append(submissionTxs, tx)
				}
			}
		}
		if len(submissionTxs) != 1 {
			Fatal(t, "expected 1 tx from submission, found", len(submissionTxs))
		}
		return submissionTxs[0]
	}

	// burn some gas so that the faucet's Callvalue + Balance never exceeds a uint256
	discard := arbmath.BigMul(big.NewInt(1e12), big.NewInt(1e12))
	builder.L2.TransferBalance(t, "Faucet", "Burn", discard, builder.L2Info)

	teardown := func() {

		// check the integrity of the RPC
		blockNum, err := builder.L2.Client.BlockNumber(ctx)
		Require(t, err, "failed to get L2 block number")
		for number := uint64(0); number < blockNum; number++ {
			block, err := builder.L2.Client.BlockByNumber(ctx, arbmath.UintToBig(number))
			Require(t, err, "failed to get L2 block", number, "of", blockNum)
			if block.Number().Uint64() != number {
				Fatal(t, "block number mismatch", number, block.Number().Uint64())
			}
		}

		cancel()

		builder.L2.ConsensusNode.StopAndWait()
		requireClose(t, builder.L1.Stack)
	}
	return builder, delayedInbox, lookupL2Tx, ctx, teardown
}

func TestRetryableNoExist(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	arbRetryableTx, err := precompilesgen.NewArbRetryableTx(common.HexToAddress("6e"), builder.L2.Client)
	Require(t, err)
	_, err = arbRetryableTx.GetTimeout(&bind.CallOpts{}, common.Hash{})
	if err.Error() != "execution reverted: error NoTicketWithID()" {
		Fatal(t, "didn't get expected NoTicketWithID error")
	}
}

func TestSubmitRetryableImmediateSuccess(t *testing.T) {
	t.Parallel()
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()

	user2Address := builder.L2Info.GetAddress("User2")
	beneficiaryAddress := builder.L2Info.GetAddress("Beneficiary")

	deposit := arbmath.BigMul(big.NewInt(1e12), big.NewInt(1e12))
	callValue := big.NewInt(1e6)

	nodeInterface, err := node_interfacegen.NewNodeInterface(types.NodeInterfaceAddress, builder.L2.Client)
	Require(t, err, "failed to deploy NodeInterface")

	// estimate the gas needed to auto redeem the retryable
	usertxoptsL2 := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
	usertxoptsL2.NoSend = true
	usertxoptsL2.GasMargin = 0
	tx, err := nodeInterface.EstimateRetryableTicket(
		&usertxoptsL2,
		usertxoptsL2.From,
		deposit,
		user2Address,
		callValue,
		beneficiaryAddress,
		beneficiaryAddress,
		[]byte{0x32, 0x42, 0x32, 0x88}, // increase the cost to beyond that of params.TxGas
	)
	Require(t, err, "failed to estimate retryable submission")
	estimate := tx.Gas()
	colors.PrintBlue("estimate: ", estimate)

	// submit & auto redeem the retryable using the gas estimate
	usertxoptsL1 := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	usertxoptsL1.Value = deposit
	l1tx, err := delayedInbox.CreateRetryableTicket(
		&usertxoptsL1,
		user2Address,
		callValue,
		big.NewInt(1e16),
		beneficiaryAddress,
		beneficiaryAddress,
		arbmath.UintToBig(estimate),
		big.NewInt(l2pricing.InitialBaseFeeWei*2),
		[]byte{0x32, 0x42, 0x32, 0x88},
	)
	Require(t, err)

	l1Receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	Require(t, err)
	if l1Receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "l1Receipt indicated failure")
	}

	waitForL1DelayBlocks(t, ctx, builder)

	receipt, err := builder.L2.EnsureTxSucceeded(lookupL2Tx(l1Receipt))
	Require(t, err)
	if receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t)
	}

	l2balance, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)

	if !arbmath.BigEquals(l2balance, big.NewInt(1e6)) {
		Fatal(t, "Unexpected balance:", l2balance)
	}
}

func TestSubmitRetryableFailThenRetry(t *testing.T) {
	t.Parallel()
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()

	ownerTxOpts := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	usertxopts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	usertxopts.Value = arbmath.BigMul(big.NewInt(1e12), big.NewInt(1e12))

	simpleAddr, simple := builder.L2.DeploySimple(t, ownerTxOpts)
	simpleABI, err := mocksgen.SimpleMetaData.GetAbi()
	Require(t, err)

	beneficiaryAddress := builder.L2Info.GetAddress("Beneficiary")
	l1tx, err := delayedInbox.CreateRetryableTicket(
		&usertxopts,
		simpleAddr,
		common.Big0,
		big.NewInt(1e16),
		beneficiaryAddress,
		beneficiaryAddress,
		// send enough L2 gas for intrinsic but not compute
		big.NewInt(int64(params.TxGas+params.TxDataNonZeroGasEIP2028*4)),
		big.NewInt(l2pricing.InitialBaseFeeWei*2),
		simpleABI.Methods["incrementRedeem"].ID,
	)
	Require(t, err)

	l1Receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	Require(t, err)
	if l1Receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "l1Receipt indicated failure")
	}

	waitForL1DelayBlocks(t, ctx, builder)

	receipt, err := builder.L2.EnsureTxSucceeded(lookupL2Tx(l1Receipt))
	Require(t, err)
	if len(receipt.Logs) != 2 {
		Fatal(t, len(receipt.Logs))
	}
	ticketId := receipt.Logs[0].Topics[1]
	firstRetryTxId := receipt.Logs[1].Topics[2]

	// get receipt for the auto redeem, make sure it failed
	receipt, err = WaitForTx(ctx, builder.L2.Client, firstRetryTxId, time.Second*5)
	Require(t, err)
	if receipt.Status != types.ReceiptStatusFailed {
		Fatal(t, receipt.GasUsed)
	}

	arbRetryableTx, err := precompilesgen.NewArbRetryableTx(common.HexToAddress("6e"), builder.L2.Client)
	Require(t, err)
	tx, err := arbRetryableTx.Redeem(&ownerTxOpts, ticketId)
	Require(t, err)
	receipt, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	retryTxId := receipt.Logs[0].Topics[2]

	// check the receipt for the retry
	receipt, err = WaitForTx(ctx, builder.L2.Client, retryTxId, time.Second*1)
	Require(t, err)
	if receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, receipt.Status)
	}

	// verify that the increment happened, so we know the retry succeeded
	counter, err := simple.Counter(&bind.CallOpts{})
	Require(t, err)

	if counter != 1 {
		Fatal(t, "Unexpected counter:", counter)
	}

	if len(receipt.Logs) != 1 {
		Fatal(t, "Unexpected log count:", len(receipt.Logs))
	}
	parsed, err := simple.ParseRedeemedEvent(*receipt.Logs[0])
	Require(t, err)
	aliasedSender := util.RemapL1Address(usertxopts.From)
	if parsed.Caller != aliasedSender {
		Fatal(t, "Unexpected caller", parsed.Caller, "expected", aliasedSender)
	}
	if parsed.Redeemer != ownerTxOpts.From {
		Fatal(t, "Unexpected redeemer", parsed.Redeemer, "expected", ownerTxOpts.From)
	}
}

func TestSubmissionGasCosts(t *testing.T) {
	t.Parallel()
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()
	infraFeeAddr, networkFeeAddr := setupFeeAddresses(t, ctx, builder)
	elevateL2Basefee(t, ctx, builder)

	usertxopts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	usertxopts.Value = arbmath.BigMul(big.NewInt(1e12), big.NewInt(1e12))

	builder.L2Info.GenerateAccount("Refund")
	builder.L2Info.GenerateAccount("Receive")
	faucetAddress := util.RemapL1Address(builder.L1Info.GetAddress("Faucet"))
	beneficiaryAddress := builder.L2Info.GetAddress("Beneficiary")
	feeRefundAddress := builder.L2Info.GetAddress("Refund")
	receiveAddress := builder.L2Info.GetAddress("Receive")

	colors.PrintBlue("Faucet      ", faucetAddress)
	colors.PrintBlue("Receive     ", receiveAddress)
	colors.PrintBlue("Beneficiary ", beneficiaryAddress)
	colors.PrintBlue("Fee Refund  ", feeRefundAddress)

	fundsBeforeSubmit, err := builder.L2.Client.BalanceAt(ctx, faucetAddress, nil)
	Require(t, err)

	infraBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)
	networkBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)

	usefulGas := params.TxGas
	excessGasLimit := uint64(808)

	maxSubmissionFee := big.NewInt(1e14)
	retryableGas := arbmath.UintToBig(usefulGas + excessGasLimit) // will only burn the intrinsic cost
	retryableL2CallValue := big.NewInt(1e4)
	retryableCallData := []byte{}
	gasFeeCap := big.NewInt(l2pricing.InitialBaseFeeWei * 2)
	l1tx, err := delayedInbox.CreateRetryableTicket(
		&usertxopts,
		receiveAddress,
		retryableL2CallValue,
		maxSubmissionFee,
		feeRefundAddress,
		beneficiaryAddress,
		retryableGas,
		gasFeeCap,
		retryableCallData,
	)
	Require(t, err)

	l1Receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	Require(t, err)
	if l1Receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "l1Receipt indicated failure")
	}

	waitForL1DelayBlocks(t, ctx, builder)

	submissionTxOuter := lookupL2Tx(l1Receipt)
	submissionReceipt, err := builder.L2.EnsureTxSucceeded(submissionTxOuter)
	Require(t, err)
	if len(submissionReceipt.Logs) != 2 {
		Fatal(t, "Unexpected number of logs:", len(submissionReceipt.Logs))
	}
	firstRetryTxId := submissionReceipt.Logs[1].Topics[2]
	// get receipt for the auto redeem
	redeemReceipt, err := WaitForTx(ctx, builder.L2.Client, firstRetryTxId, time.Second*5)
	Require(t, err)
	if redeemReceipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "first retry tx failed")
	}
	redeemBlock, err := builder.L2.Client.HeaderByNumber(ctx, redeemReceipt.BlockNumber)
	Require(t, err)

	l2BaseFee := redeemBlock.BaseFee
	excessGasPrice := arbmath.BigSub(gasFeeCap, l2BaseFee)
	excessWei := arbmath.BigMulByUint(l2BaseFee, excessGasLimit)
	excessWei.Add(excessWei, arbmath.BigMul(excessGasPrice, retryableGas))

	fundsAfterSubmit, err := builder.L2.Client.BalanceAt(ctx, faucetAddress, nil)
	Require(t, err)
	beneficiaryFunds, err := builder.L2.Client.BalanceAt(ctx, beneficiaryAddress, nil)
	Require(t, err)
	refundFunds, err := builder.L2.Client.BalanceAt(ctx, feeRefundAddress, nil)
	Require(t, err)
	receiveFunds, err := builder.L2.Client.BalanceAt(ctx, receiveAddress, nil)
	Require(t, err)

	infraBalanceAfter, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)
	networkBalanceAfter, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)

	colors.PrintBlue("CallGas    ", retryableGas)
	colors.PrintMint("Gas cost   ", arbmath.BigMul(retryableGas, l2BaseFee))
	colors.PrintBlue("Payment    ", usertxopts.Value)

	colors.PrintMint("Faucet before ", fundsAfterSubmit)
	colors.PrintMint("Faucet after  ", fundsAfterSubmit)

	// the retryable should pay the receiver the supplied callvalue
	colors.PrintMint("Receive       ", receiveFunds)
	colors.PrintBlue("L2 Call Value ", retryableL2CallValue)
	if !arbmath.BigEquals(receiveFunds, retryableL2CallValue) {
		Fatal(t, "Recipient didn't receive the right funds")
	}

	// the beneficiary should receive nothing
	colors.PrintMint("Beneficiary   ", beneficiaryFunds)
	if beneficiaryFunds.Sign() != 0 {
		Fatal(t, "The beneficiary shouldn't have received funds")
	}

	// the fee refund address should recieve the excess gas
	colors.PrintBlue("Base Fee         ", l2BaseFee)
	colors.PrintBlue("Excess Gas Price ", excessGasPrice)
	colors.PrintBlue("Excess Gas       ", excessGasLimit)
	colors.PrintBlue("Excess Wei       ", excessWei)
	colors.PrintMint("Fee Refund       ", refundFunds)
	if !arbmath.BigEquals(refundFunds, arbmath.BigAdd(excessWei, maxSubmissionFee)) {
		Fatal(t, "The Fee Refund Address didn't receive the right funds")
	}

	// the faucet must pay for both the gas used and the call value supplied
	expectedGasChange := arbmath.BigMul(gasFeeCap, retryableGas)
	expectedGasChange = arbmath.BigSub(expectedGasChange, usertxopts.Value) // the user is credited this
	expectedGasChange = arbmath.BigAdd(expectedGasChange, maxSubmissionFee)
	expectedGasChange = arbmath.BigAdd(expectedGasChange, retryableL2CallValue)

	if !arbmath.BigEquals(fundsBeforeSubmit, arbmath.BigAdd(fundsAfterSubmit, expectedGasChange)) {
		diff := arbmath.BigSub(fundsBeforeSubmit, fundsAfterSubmit)
		colors.PrintRed("Expected ", expectedGasChange)
		colors.PrintRed("Observed ", diff)
		colors.PrintRed("Off by   ", arbmath.BigSub(expectedGasChange, diff))
		Fatal(t, "Supplied gas was improperly deducted\n", fundsBeforeSubmit, "\n", fundsAfterSubmit)
	}

	arbGasInfo, err := precompilesgen.NewArbGasInfo(common.HexToAddress("0x6c"), builder.L2.Client)
	Require(t, err)
	minimumBaseFee, err := arbGasInfo.GetMinimumGasPrice(&bind.CallOpts{Context: ctx})
	Require(t, err)

	expectedFee := arbmath.BigMulByUint(l2BaseFee, usefulGas)
	expectedInfraFee := arbmath.BigMulByUint(minimumBaseFee, usefulGas)
	expectedNetworkFee := arbmath.BigSub(expectedFee, expectedInfraFee)

	infraFee := arbmath.BigSub(infraBalanceAfter, infraBalanceBefore)
	networkFee := arbmath.BigSub(networkBalanceAfter, networkBalanceBefore)
	fee := arbmath.BigAdd(infraFee, networkFee)

	colors.PrintMint("paid infra fee:      ", infraFee)
	colors.PrintMint("paid network fee:    ", networkFee)
	colors.PrintMint("paid fee:            ", fee)

	if !arbmath.BigEquals(infraFee, expectedInfraFee) {
		Fatal(t, "Unexpected infra fee paid, want:", expectedInfraFee, "have:", infraFee)
	}
	if !arbmath.BigEquals(networkFee, expectedNetworkFee) {
		Fatal(t, "Unexpected network fee paid, want:", expectedNetworkFee, "have:", networkFee)
	}
}

func waitForL1DelayBlocks(t *testing.T, ctx context.Context, builder *NodeBuilder) {
	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
	}
}

func TestDepositETH(t *testing.T) {
	t.Parallel()
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()

	faucetAddr := builder.L1Info.GetAddress("Faucet")

	oldBalance, err := builder.L2.Client.BalanceAt(ctx, faucetAddr, nil)
	if err != nil {
		t.Fatalf("BalanceAt(%v) unexpected error: %v", faucetAddr, err)
	}

	txOpts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	txOpts.Value = big.NewInt(13)

	l1tx, err := delayedInbox.DepositEth0(&txOpts)
	if err != nil {
		t.Fatalf("DepositEth0() unexected error: %v", err)
	}

	l1Receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	if err != nil {
		t.Fatalf("EnsureTxSucceeded() unexpected error: %v", err)
	}
	if l1Receipt.Status != types.ReceiptStatusSuccessful {
		t.Errorf("Got transaction status: %v, want: %v", l1Receipt.Status, types.ReceiptStatusSuccessful)
	}
	waitForL1DelayBlocks(t, ctx, builder)

	l2Receipt, err := builder.L2.EnsureTxSucceeded(lookupL2Tx(l1Receipt))
	if err != nil {
		t.Fatalf("EnsureTxSucceeded unexpected error: %v", err)
	}
	newBalance, err := builder.L2.Client.BalanceAt(ctx, faucetAddr, l2Receipt.BlockNumber)
	if err != nil {
		t.Fatalf("BalanceAt(%v) unexpected error: %v", faucetAddr, err)
	}
	if got := new(big.Int); got.Sub(newBalance, oldBalance).Cmp(txOpts.Value) != 0 {
		t.Errorf("Got transferred: %v, want: %v", got, txOpts.Value)
	}
}

func TestArbitrumContractTx(t *testing.T) {
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()
	faucetL2Addr := util.RemapL1Address(builder.L1Info.GetAddress("Faucet"))
	builder.L2.TransferBalanceTo(t, "Faucet", faucetL2Addr, big.NewInt(1e18), builder.L2Info)

	l2TxOpts := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
	l2ContractAddr, _ := builder.L2.DeploySimple(t, l2TxOpts)
	l2ContractABI, err := abi.JSON(strings.NewReader(mocksgen.SimpleABI))
	if err != nil {
		t.Fatalf("Error parsing contract ABI: %v", err)
	}
	data, err := l2ContractABI.Pack("checkCalls", true, true, false, false, false, false)
	if err != nil {
		t.Fatalf("Error packing method's call data: %v", err)
	}
	unsignedTx := types.NewTx(&types.ArbitrumContractTx{
		ChainId:   builder.L2Info.Signer.ChainID(),
		From:      faucetL2Addr,
		GasFeeCap: builder.L2Info.GasPrice.Mul(builder.L2Info.GasPrice, big.NewInt(2)),
		Gas:       1e6,
		To:        &l2ContractAddr,
		Value:     common.Big0,
		Data:      data,
	})
	txOpts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	l1tx, err := delayedInbox.SendContractTransaction(
		&txOpts,
		arbmath.UintToBig(unsignedTx.Gas()),
		unsignedTx.GasFeeCap(),
		*unsignedTx.To(),
		unsignedTx.Value(),
		unsignedTx.Data(),
	)
	if err != nil {
		t.Fatalf("Error sending unsigned transaction: %v", err)
	}
	receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	if err != nil {
		t.Fatalf("EnsureTxSucceeded(%v) unexpected error: %v", l1tx.Hash(), err)
	}
	if receipt.Status != types.ReceiptStatusSuccessful {
		t.Errorf("L1 transaction: %v has failed", l1tx.Hash())
	}
	waitForL1DelayBlocks(t, ctx, builder)
	_, err = builder.L2.EnsureTxSucceeded(lookupL2Tx(receipt))
	if err != nil {
		t.Fatalf("EnsureTxSucceeded(%v) unexpected error: %v", unsignedTx.Hash(), err)
	}
}

func TestL1FundedUnsignedTransaction(t *testing.T) {
	t.Parallel()
	ctx := context.Background()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	faucetL2Addr := util.RemapL1Address(builder.L1Info.GetAddress("Faucet"))
	// Transfer balance to Faucet's corresponding L2 address, so that there is
	// enough balance on its' account for executing L2 transaction.
	builder.L2.TransferBalanceTo(t, "Faucet", faucetL2Addr, big.NewInt(1e18), builder.L2Info)

	l2TxOpts := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
	contractAddr, _ := builder.L2.DeploySimple(t, l2TxOpts)
	contractABI, err := abi.JSON(strings.NewReader(mocksgen.SimpleABI))
	if err != nil {
		t.Fatalf("Error parsing contract ABI: %v", err)
	}
	data, err := contractABI.Pack("checkCalls", true, true, false, false, false, false)
	if err != nil {
		t.Fatalf("Error packing method's call data: %v", err)
	}
	nonce, err := builder.L2.Client.NonceAt(ctx, faucetL2Addr, nil)
	if err != nil {
		t.Fatalf("Error getting nonce at address: %v, error: %v", faucetL2Addr, err)
	}
	unsignedTx := types.NewTx(&types.ArbitrumUnsignedTx{
		ChainId:   builder.L2Info.Signer.ChainID(),
		From:      faucetL2Addr,
		Nonce:     nonce,
		GasFeeCap: builder.L2Info.GasPrice,
		Gas:       1e6,
		To:        &contractAddr,
		Value:     common.Big0,
		Data:      data,
	})

	delayedInbox, err := bridgegen.NewInbox(builder.L1Info.GetAddress("Inbox"), builder.L1.Client)
	if err != nil {
		t.Fatalf("Error getting Go binding of L1 Inbox contract: %v", err)
	}

	txOpts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	l1tx, err := delayedInbox.SendUnsignedTransaction(
		&txOpts,
		arbmath.UintToBig(unsignedTx.Gas()),
		unsignedTx.GasFeeCap(),
		arbmath.UintToBig(unsignedTx.Nonce()),
		*unsignedTx.To(),
		unsignedTx.Value(),
		unsignedTx.Data(),
	)
	if err != nil {
		t.Fatalf("Error sending unsigned transaction: %v", err)
	}
	receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	if err != nil {
		t.Fatalf("EnsureTxSucceeded(%v) unexpected error: %v", l1tx.Hash(), err)
	}
	if receipt.Status != types.ReceiptStatusSuccessful {
		t.Errorf("L1 transaction: %v has failed", l1tx.Hash())
	}
	waitForL1DelayBlocks(t, ctx, builder)
	receipt, err = builder.L2.EnsureTxSucceeded(unsignedTx)
	if err != nil {
		t.Fatalf("EnsureTxSucceeded(%v) unexpected error: %v", unsignedTx.Hash(), err)
	}
	if receipt.Status != types.ReceiptStatusSuccessful {
		t.Errorf("L2 transaction: %v has failed", receipt.TxHash)
	}
}

func TestRetryableSubmissionAndRedeemFees(t *testing.T) {
	builder, delayedInbox, lookupL2Tx, ctx, teardown := retryableSetup(t)
	defer teardown()
	infraFeeAddr, networkFeeAddr := setupFeeAddresses(t, ctx, builder)

	ownerTxOpts := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	simpleAddr, simple := builder.L2.DeploySimple(t, ownerTxOpts)
	simpleABI, err := mocksgen.SimpleMetaData.GetAbi()
	Require(t, err)

	elevateL2Basefee(t, ctx, builder)

	infraBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)
	networkBalanceBefore, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)

	beneficiaryAddress := builder.L2Info.GetAddress("Beneficiary")
	deposit := arbmath.BigMul(big.NewInt(1e12), big.NewInt(1e12))
	callValue := common.Big0
	usertxoptsL1 := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	usertxoptsL1.Value = deposit
	baseFee := builder.L2.GetBaseFee(t)
	l1tx, err := delayedInbox.CreateRetryableTicket(
		&usertxoptsL1,
		simpleAddr,
		callValue,
		big.NewInt(1e16),
		beneficiaryAddress,
		beneficiaryAddress,
		// send enough L2 gas for intrinsic but not compute
		big.NewInt(int64(params.TxGas+params.TxDataNonZeroGasEIP2028*4)),
		big.NewInt(baseFee.Int64()*2),
		simpleABI.Methods["incrementRedeem"].ID,
	)
	Require(t, err)
	l1Receipt, err := builder.L1.EnsureTxSucceeded(l1tx)
	Require(t, err)
	if l1Receipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "l1Receipt indicated failure")
	}

	waitForL1DelayBlocks(t, ctx, builder)

	submissionTxOuter := lookupL2Tx(l1Receipt)
	submissionReceipt, err := builder.L2.EnsureTxSucceeded(submissionTxOuter)
	Require(t, err)
	if len(submissionReceipt.Logs) != 2 {
		Fatal(t, len(submissionReceipt.Logs))
	}
	ticketId := submissionReceipt.Logs[0].Topics[1]
	firstRetryTxId := submissionReceipt.Logs[1].Topics[2]
	// get receipt for the auto redeem, make sure it failed
	autoRedeemReceipt, err := WaitForTx(ctx, builder.L2.Client, firstRetryTxId, time.Second*5)
	Require(t, err)
	if autoRedeemReceipt.Status != types.ReceiptStatusFailed {
		Fatal(t, "first retry tx shouldn't have succeeded")
	}

	infraBalanceAfterSubmission, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)
	networkBalanceAfterSubmission, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)

	usertxoptsL2 := builder.L2Info.GetDefaultTransactOpts("Faucet", ctx)
	arbRetryableTx, err := precompilesgen.NewArbRetryableTx(common.HexToAddress("6e"), builder.L2.Client)
	Require(t, err)
	tx, err := arbRetryableTx.Redeem(&usertxoptsL2, ticketId)
	Require(t, err)
	redeemReceipt, err := builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	retryTxId := redeemReceipt.Logs[0].Topics[2]

	// check the receipt for the retry
	retryReceipt, err := WaitForTx(ctx, builder.L2.Client, retryTxId, time.Second*1)
	Require(t, err)
	if retryReceipt.Status != types.ReceiptStatusSuccessful {
		Fatal(t, "retry failed")
	}

	infraBalanceAfterRedeem, err := builder.L2.Client.BalanceAt(ctx, infraFeeAddr, nil)
	Require(t, err)
	networkBalanceAfterRedeem, err := builder.L2.Client.BalanceAt(ctx, networkFeeAddr, nil)
	Require(t, err)

	// verify that the increment happened, so we know the retry succeeded
	counter, err := simple.Counter(&bind.CallOpts{})
	Require(t, err)

	if counter != 1 {
		Fatal(t, "Unexpected counter:", counter)
	}

	if len(retryReceipt.Logs) != 1 {
		Fatal(t, "Unexpected log count:", len(retryReceipt.Logs))
	}
	parsed, err := simple.ParseRedeemedEvent(*retryReceipt.Logs[0])
	Require(t, err)
	aliasedSender := util.RemapL1Address(usertxoptsL1.From)
	if parsed.Caller != aliasedSender {
		Fatal(t, "Unexpected caller", parsed.Caller, "expected", aliasedSender)
	}
	if parsed.Redeemer != usertxoptsL2.From {
		Fatal(t, "Unexpected redeemer", parsed.Redeemer, "expected", usertxoptsL2.From)
	}

	infraSubmissionFee := arbmath.BigSub(infraBalanceAfterSubmission, infraBalanceBefore)
	networkSubmissionFee := arbmath.BigSub(networkBalanceAfterSubmission, networkBalanceBefore)
	infraRedeemFee := arbmath.BigSub(infraBalanceAfterRedeem, infraBalanceAfterSubmission)
	networkRedeemFee := arbmath.BigSub(networkBalanceAfterRedeem, networkBalanceAfterSubmission)

	arbGasInfo, err := precompilesgen.NewArbGasInfo(common.HexToAddress("0x6c"), builder.L2.Client)
	Require(t, err)
	minimumBaseFee, err := arbGasInfo.GetMinimumGasPrice(&bind.CallOpts{Context: ctx})
	Require(t, err)
	submissionBaseFee := builder.L2.GetBaseFeeAt(t, submissionReceipt.BlockNumber)
	submissionTx, ok := submissionTxOuter.GetInner().(*types.ArbitrumSubmitRetryableTx)
	if !ok {
		Fatal(t, "inner tx isn't ArbitrumSubmitRetryableTx")
	}
	// submission + auto redeemed retry expected fees
	retryableSubmissionFee := retryables.RetryableSubmissionFee(len(submissionTx.RetryData), submissionTx.L1BaseFee)
	expectedSubmissionFee := arbmath.BigMulByUint(submissionBaseFee, autoRedeemReceipt.GasUsed)
	expectedInfraSubmissionFee := arbmath.BigMulByUint(minimumBaseFee, autoRedeemReceipt.GasUsed)
	expectedNetworkSubmissionFee := arbmath.BigAdd(
		arbmath.BigSub(expectedSubmissionFee, expectedInfraSubmissionFee),
		retryableSubmissionFee,
	)

	retryTxOuter, _, err := builder.L2.Client.TransactionByHash(ctx, retryTxId)
	Require(t, err)
	retryTx, ok := retryTxOuter.GetInner().(*types.ArbitrumRetryTx)
	if !ok {
		Fatal(t, "inner tx isn't ArbitrumRetryTx")
	}
	redeemBaseFee := builder.L2.GetBaseFeeAt(t, redeemReceipt.BlockNumber)

	t.Log("redeem base fee:", redeemBaseFee)
	// redeem & retry expected fees
	redeemGasUsed := redeemReceipt.GasUsed - redeemReceipt.GasUsedForL1 - retryTx.Gas + retryReceipt.GasUsed
	expectedRedeemFee := arbmath.BigMulByUint(redeemBaseFee, redeemGasUsed)
	expectedInfraRedeemFee := arbmath.BigMulByUint(minimumBaseFee, redeemGasUsed)
	expectedNetworkRedeemFee := arbmath.BigSub(expectedRedeemFee, expectedInfraRedeemFee)

	t.Log("submission gas:         ", submissionReceipt.GasUsed)
	t.Log("auto redeemed retry gas:", autoRedeemReceipt.GasUsed)
	t.Log("redeem gas:             ", redeemReceipt.GasUsed)
	t.Log("retry gas:              ", retryReceipt.GasUsed)
	colors.PrintMint("submission and auto redeemed retry - paid infra fee:        ", infraSubmissionFee)
	colors.PrintBlue("submission and auto redeemed retry - expected infra fee:    ", expectedInfraSubmissionFee)
	colors.PrintMint("submission and auto redeemed retry - paid network fee:      ", networkSubmissionFee)
	colors.PrintBlue("submission and auto redeemed retry - expected network fee:  ", expectedNetworkSubmissionFee)
	colors.PrintMint("redeem and retry - paid infra fee:            ", infraRedeemFee)
	colors.PrintBlue("redeem and retry - expected infra fee:        ", expectedInfraRedeemFee)
	colors.PrintMint("redeem and retry - paid network fee:          ", networkRedeemFee)
	colors.PrintBlue("redeem and retry - expected network fee:      ", expectedNetworkRedeemFee)
	if !arbmath.BigEquals(infraSubmissionFee, expectedInfraSubmissionFee) {
		Fatal(t, "Unexpected infra fee paid by submission and auto redeem, want:", expectedInfraSubmissionFee, "have:", infraSubmissionFee)
	}
	if !arbmath.BigEquals(networkSubmissionFee, expectedNetworkSubmissionFee) {
		Fatal(t, "Unexpected network fee paid by submission and auto redeem, want:", expectedNetworkSubmissionFee, "have:", networkSubmissionFee)
	}
	if !arbmath.BigEquals(infraRedeemFee, expectedInfraRedeemFee) {
		Fatal(t, "Unexpected infra fee paid by redeem and retry, want:", expectedInfraRedeemFee, "have:", infraRedeemFee)
	}
	if !arbmath.BigEquals(networkRedeemFee, expectedNetworkRedeemFee) {
		Fatal(t, "Unexpected network fee paid by redeem and retry, want:", expectedNetworkRedeemFee, "have:", networkRedeemFee)
	}
}

// elevateL2Basefee by burning gas exceeding speed limit
func elevateL2Basefee(t *testing.T, ctx context.Context, builder *NodeBuilder) {
	baseFeeBefore := builder.L2.GetBaseFee(t)
	colors.PrintBlue("Elevating base fee...")
	arbostestabi, err := precompilesgen.ArbosTestMetaData.GetAbi()
	Require(t, err)
	_, err = precompilesgen.NewArbosTest(common.HexToAddress("0x69"), builder.L2.Client)
	Require(t, err, "failed to deploy ArbosTest")

	burnAmount := gethexec.ConfigDefaultTest().RPC.RPCGasCap
	burnTarget := uint64(5 * l2pricing.InitialSpeedLimitPerSecondV6 * l2pricing.InitialBacklogTolerance)
	for i := uint64(0); i < (burnTarget+burnAmount)/burnAmount; i++ {
		burnArbGas := arbostestabi.Methods["burnArbGas"]
		data, err := burnArbGas.Inputs.Pack(arbmath.UintToBig(burnAmount - builder.L2Info.TransferGas))
		Require(t, err)
		input := append([]byte{}, burnArbGas.ID...)
		input = append(input, data...)
		to := common.HexToAddress("0x69")
		tx := builder.L2Info.PrepareTxTo("Faucet", &to, burnAmount, big.NewInt(0), input)
		Require(t, builder.L2.Client.SendTransaction(ctx, tx))
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}
	baseFee := builder.L2.GetBaseFee(t)
	colors.PrintBlue("New base fee: ", baseFee, " diff:", baseFee.Uint64()-baseFeeBefore.Uint64())
}

func setupFeeAddresses(t *testing.T, ctx context.Context, builder *NodeBuilder) (common.Address, common.Address) {
	ownerTxOpts := builder.L2Info.GetDefaultTransactOpts("Owner", ctx)
	ownerCallOpts := builder.L2Info.GetDefaultCallOpts("Owner", ctx)
	// make "Owner" a chain owner
	arbdebug, err := precompilesgen.NewArbDebug(common.HexToAddress("0xff"), builder.L2.Client)
	Require(t, err, "failed to deploy ArbDebug")
	tx, err := arbdebug.BecomeChainOwner(&ownerTxOpts)
	Require(t, err, "failed to deploy ArbDebug")
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	arbowner, err := precompilesgen.NewArbOwner(common.HexToAddress("70"), builder.L2.Client)
	Require(t, err)
	arbownerPublic, err := precompilesgen.NewArbOwnerPublic(common.HexToAddress("6b"), builder.L2.Client)
	Require(t, err)
	builder.L2Info.GenerateAccount("InfraFee")
	builder.L2Info.GenerateAccount("NetworkFee")
	networkFeeAddr := builder.L2Info.GetAddress("NetworkFee")
	infraFeeAddr := builder.L2Info.GetAddress("InfraFee")
	tx, err = arbowner.SetNetworkFeeAccount(&ownerTxOpts, networkFeeAddr)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	networkFeeAccount, err := arbownerPublic.GetNetworkFeeAccount(ownerCallOpts)
	Require(t, err)
	tx, err = arbowner.SetInfraFeeAccount(&ownerTxOpts, infraFeeAddr)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)
	infraFeeAccount, err := arbownerPublic.GetInfraFeeAccount(ownerCallOpts)
	Require(t, err)
	t.Log("Infra fee account: ", infraFeeAccount)
	t.Log("Network fee account: ", networkFeeAccount)
	return infraFeeAddr, networkFeeAddr
}

'''
'''--- system_tests/seq_coordinator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"testing"
	"time"

	"github.com/go-redis/redis/v8"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/execution/gethexec"
	"github.com/offchainlabs/nitro/util/redisutil"
)

func initRedisForTest(t *testing.T, ctx context.Context, redisUrl string, nodeNames []string) {
	var priorities string

	redisClient, err := redisutil.RedisClientFromURL(redisUrl)
	Require(t, err)
	defer redisClient.Close()

	for _, name := range nodeNames {
		priorities = priorities + name + ","
		redisClient.Del(ctx, redisutil.WANTS_LOCKOUT_KEY_PREFIX+name)
	}
	priorities = priorities[:len(priorities)-1] // remove last ","
	Require(t, redisClient.Set(ctx, redisutil.PRIORITIES_KEY, priorities, time.Duration(0)).Err())
	for msg := 0; msg < 1000; msg++ {
		redisClient.Del(ctx, fmt.Sprintf("%s%d", redisutil.MESSAGE_KEY_PREFIX, msg))
	}
	redisClient.Del(ctx, redisutil.CHOSENSEQ_KEY, redisutil.MSG_COUNT_KEY)
}

func TestRedisSeqCoordinatorPriorities(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	builder.nodeConfig.SeqCoordinator.Enable = true
	builder.nodeConfig.SeqCoordinator.RedisUrl = redisutil.CreateTestRedis(ctx, t)

	l2Info := builder.L2Info

	// stdio protocol makes sure forwarder initialization doesn't fail
	nodeNames := []string{"stdio://A", "stdio://B", "stdio://C", "stdio://D", "stdio://E"}

	testNodes := make([]*TestClient, len(nodeNames))

	// init DB to known state
	initRedisForTest(t, ctx, builder.nodeConfig.SeqCoordinator.RedisUrl, nodeNames)

	createStartNode := func(nodeNum int) {
		builder.nodeConfig.SeqCoordinator.MyUrl = nodeNames[nodeNum]
		builder.L2Info = l2Info
		builder.Build(t)
		testNodes[nodeNum] = builder.L2
	}

	trySequencing := func(nodeNum int) bool {
		node := testNodes[nodeNum].ConsensusNode
		curMsgs, err := node.TxStreamer.GetMessageCountSync(t)
		Require(t, err)
		emptyMessage := arbostypes.MessageWithMetadata{
			Message: &arbostypes.L1IncomingMessage{
				Header: &arbostypes.L1IncomingMessageHeader{
					Kind:        0,
					Poster:      common.Address{},
					BlockNumber: 0,
					Timestamp:   0,
					RequestId:   &common.Hash{},
					L1BaseFee:   common.Big0,
				},
				L2msg: nil,
			},
			DelayedMessagesRead: 1,
		}
		err = node.SeqCoordinator.SequencingMessage(curMsgs, &emptyMessage)
		if errors.Is(err, execution.ErrRetrySequencer) {
			return false
		}
		Require(t, err)
		Require(t, node.TxStreamer.AddMessages(curMsgs, false, []arbostypes.MessageWithMetadata{emptyMessage}))
		return true
	}

	// node(n) has higher prio than node(n+1), so should be impossible for more than one to succeed
	trySequencingEverywhere := func() int {
		succeeded := -1
		for nodeNum, testNode := range testNodes {
			node := testNode.ConsensusNode
			if node == nil {
				continue
			}
			if trySequencing(nodeNum) {
				if succeeded >= 0 {
					t.Fatal("sequnced succeeded in parallel",
						"index1:", succeeded, "debug", testNodes[succeeded].ConsensusNode.SeqCoordinator.DebugPrint(),
						"index2:", nodeNum, "debug", node.SeqCoordinator.DebugPrint(),
						"now", time.Now().UnixMilli())
				}
				succeeded = nodeNum
			}
		}
		return succeeded
	}

	waitForMsgEverywhere := func(msgNum arbutil.MessageIndex) {
		for _, testNode := range testNodes {
			currentNode := testNode.ConsensusNode
			if currentNode == nil {
				continue
			}
			for attempts := 1; ; attempts++ {
				msgCount, err := currentNode.TxStreamer.GetMessageCountSync(t)
				Require(t, err)
				if msgCount >= msgNum {
					break
				}
				if attempts > 10 {
					Fatal(t, "timeout waiting for msg ", msgNum, " debug: ", currentNode.SeqCoordinator.DebugPrint())
				}
				<-time.After(builder.nodeConfig.SeqCoordinator.UpdateInterval / 3)
			}
		}
	}

	var needsStop []*arbnode.Node
	killNode := func(nodeNum int) {
		if nodeNum%3 == 0 {
			testNodes[nodeNum].ConsensusNode.SeqCoordinator.PrepareForShutdown()
			needsStop = append(needsStop, testNodes[nodeNum].ConsensusNode)
		} else {
			testNodes[nodeNum].ConsensusNode.StopAndWait()
		}
		testNodes[nodeNum].ConsensusNode = nil
	}

	nodeForwardTarget := func(nodeNum int) int {
		execNode := testNodes[nodeNum].ExecNode
		fwTarget := execNode.TxPublisher.(*gethexec.TxPreChecker).TransactionPublisher.(*gethexec.Sequencer).ForwardTarget()
		if fwTarget == "" {
			return -1
		}
		for cNum, name := range nodeNames {
			if name == fwTarget {
				return cNum
			}
		}
		t.Fatal("Bad FW target")
		return -2
	}

	messagesPerRound := arbutil.MessageIndex(10)
	currentSequencer := 0
	sequencedMesssages := arbutil.MessageIndex(1) // we start with 1 so messageCountKey will be written

	t.Log("Starting node 0")
	// give node 0 room to set himself primary
	createStartNode(0)

	for attempts := 1; !trySequencing(0); attempts++ {
		if attempts > 10 {
			t.Fatal("failed first sequencing")
		}
		time.Sleep(time.Millisecond * 200)
	}
	sequencedMesssages++

	t.Log("Starting other nodes")

	for i := 1; i < len(testNodes); i++ {
		createStartNode(i)
	}

	addNodes := false

	// remove sequencers one by one

	for {

		// all remaining nodes know which is the chosen one
		for i := currentSequencer + 1; i < len(testNodes); i++ {
			for attempts := 1; nodeForwardTarget(i) != currentSequencer; attempts++ {
				if attempts > 10 {
					t.Fatal("initial forward target not set")
				}
				time.Sleep(time.Millisecond * 100)
			}
		}

		// sequencing succeeds only on the leder
		for i := arbutil.MessageIndex(0); i < messagesPerRound; i++ {
			if sequencer := trySequencingEverywhere(); sequencer != currentSequencer {
				Fatal(t, "unexpected sequencer. expected: ", currentSequencer, " got ", sequencer)
			}
			sequencedMesssages++
		}

		if currentSequencer == len(testNodes)-1 {
			addNodes = true
		}
		if addNodes {
			if currentSequencer == 0 {
				break
			}
			t.Log("adding node")
			currentSequencer--
			createStartNode(currentSequencer)
		} else {
			t.Log("killing node")
			killNode(currentSequencer)
			currentSequencer++
		}

		// cannot sequence until up to date with all messages
		for attempts := 0; ; attempts++ {
			sequencer := trySequencingEverywhere()
			if sequencer == -1 && attempts > 15 {
				Fatal(t, "failed to sequence")
			}
			if sequencer != -1 {
				sequencedMesssages++
			}
			if sequencer == -1 ||
				(addNodes && (sequencer == currentSequencer+1)) {
				time.Sleep(builder.nodeConfig.SeqCoordinator.LockoutDuration / 5)
				continue
			}
			if sequencer == currentSequencer {
				break
			}
			Fatal(t, "unexpected sequencer", "expected", currentSequencer, "got", sequencer, "messages", sequencedMesssages)
		}

		// all nodes get messages
		waitForMsgEverywhere(sequencedMesssages)

		// can sequence after up to date
		for i := arbutil.MessageIndex(0); i < messagesPerRound; i++ {
			sequencer := trySequencingEverywhere()
			if sequencer != currentSequencer {
				Fatal(t, "unexpected sequencer", "expected", currentSequencer, "got", sequencer, "messages", sequencedMesssages)
			}
			sequencedMesssages++
		}

		// all nodes get messages
		waitForMsgEverywhere(sequencedMesssages)
	}

	for nodeNum := range testNodes {
		killNode(nodeNum)
	}
	for _, node := range needsStop {
		node.StopAndWait()
	}

}

func testCoordinatorMessageSync(t *testing.T, successCase bool) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.SeqCoordinator.Enable = true
	builder.nodeConfig.SeqCoordinator.RedisUrl = redisutil.CreateTestRedis(ctx, t)
	builder.nodeConfig.BatchPoster.Enable = false

	nodeNames := []string{"stdio://A", "stdio://B"}
	initRedisForTest(t, ctx, builder.nodeConfig.SeqCoordinator.RedisUrl, nodeNames)
	builder.nodeConfig.SeqCoordinator.MyUrl = nodeNames[0]

	cleanup := builder.Build(t)
	defer cleanup()

	redisClient, err := redisutil.RedisClientFromURL(builder.nodeConfig.SeqCoordinator.RedisUrl)
	Require(t, err)
	defer redisClient.Close()

	// wait for sequencerA to become master
	for {
		err := redisClient.Get(ctx, redisutil.CHOSENSEQ_KEY).Err()
		if errors.Is(err, redis.Nil) {
			time.Sleep(builder.nodeConfig.SeqCoordinator.UpdateInterval)
			continue
		}
		Require(t, err)
		break
	}

	builder.L2Info.GenerateAccount("User2")

	nodeConfigDup := *builder.nodeConfig
	builder.nodeConfig = &nodeConfigDup

	builder.nodeConfig.SeqCoordinator.MyUrl = nodeNames[1]
	if !successCase {
		builder.nodeConfig.SeqCoordinator.Signer.ECDSA.AcceptSequencer = false
		builder.nodeConfig.SeqCoordinator.Signer.ECDSA.AllowedAddresses = []string{builder.L2Info.GetAddress("User2").Hex()}
	}

	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: builder.nodeConfig})
	defer cleanupB()

	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)

	err = builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	if successCase {
		_, err = WaitForTx(ctx, testClientB.Client, tx.Hash(), time.Second*5)
		Require(t, err)
		l2balance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
		Require(t, err)
		if l2balance.Cmp(big.NewInt(1e12)) != 0 {
			t.Fatal("Unexpected balance:", l2balance)
		}
	} else {
		_, err = WaitForTx(ctx, testClientB.Client, tx.Hash(), time.Second)
		if err == nil {
			Fatal(t, "tx received by node with different seq coordinator signing key")
		}
	}
}

func TestRedisSeqCoordinatorMessageSync(t *testing.T) {
	testCoordinatorMessageSync(t, true)
}

func TestRedisSeqCoordinatorWrongKeyMessageSync(t *testing.T) {
	testCoordinatorMessageSync(t, false)
}

'''
'''--- system_tests/seq_nonce_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"math/rand"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/offchainlabs/nitro/util/arbmath"
)

func TestSequencerParallelNonces(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	builder.execConfig.Sequencer.NonceFailureCacheExpiry = time.Minute
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("Destination")

	wg := sync.WaitGroup{}
	for thread := 0; thread < 10; thread++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for i := 0; i < 10; i++ {
				tx := builder.L2Info.PrepareTx("Owner", "Destination", builder.L2Info.TransferGas, common.Big1, nil)
				// Sleep a random amount of time up to 20 milliseconds
				time.Sleep(time.Millisecond * time.Duration(rand.Intn(20)))
				t.Log("Submitting transaction with nonce", tx.Nonce())
				err := builder.L2.Client.SendTransaction(ctx, tx)
				Require(t, err)
				t.Log("Got response for transaction with nonce", tx.Nonce())
			}
		}()
	}
	wg.Wait()

	addr := builder.L2Info.GetAddress("Destination")
	balance, err := builder.L2.Client.BalanceAt(ctx, addr, nil)
	Require(t, err)
	if !arbmath.BigEquals(balance, big.NewInt(100)) {
		Fatal(t, "Unexpected user balance", balance)
	}
}

func TestSequencerNonceTooHigh(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GetInfoWithPrivKey("Owner").Nonce++

	before := time.Now()
	tx := builder.L2Info.PrepareTx("Owner", "Owner", builder.L2Info.TransferGas, common.Big0, nil)
	err := builder.L2.Client.SendTransaction(ctx, tx)
	if err == nil {
		Fatal(t, "No error when nonce was too high")
	}
	if !strings.Contains(err.Error(), core.ErrNonceTooHigh.Error()) {
		Fatal(t, "Unexpected transaction error", err)
	}
	elapsed := time.Since(before)
	if elapsed > 2*builder.execConfig.Sequencer.NonceFailureCacheExpiry {
		Fatal(t, "Sequencer took too long to respond with nonce too high")
	}
}

func TestSequencerNonceTooHighQueueFull(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	builder.execConfig.Sequencer.NonceFailureCacheSize = 5
	builder.execConfig.Sequencer.NonceFailureCacheExpiry = time.Minute
	cleanup := builder.Build(t)
	defer cleanup()

	count := 15
	var completed uint64
	for i := 0; i < count; i++ {
		builder.L2Info.GetInfoWithPrivKey("Owner").Nonce++
		tx := builder.L2Info.PrepareTx("Owner", "Owner", builder.L2Info.TransferGas, common.Big0, nil)
		go func() {
			err := builder.L2.Client.SendTransaction(ctx, tx)
			if err == nil {
				Fatal(t, "No error when nonce was too high")
			}
			atomic.AddUint64(&completed, 1)
		}()
	}

	for wait := 9; wait >= 0; wait-- {
		got := int(atomic.LoadUint64(&completed))
		expected := count - builder.execConfig.Sequencer.NonceFailureCacheSize
		if got == expected {
			break
		}
		if wait == 0 || got > expected {
			Fatal(t, "Wrong number of transaction responses; got", got, "but expected", expected)
		}
		time.Sleep(time.Millisecond * 100)
	}
}

'''
'''--- system_tests/seq_pause_test.go ---
package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/execution/gethexec"
)

func TestSequencerPause(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	const numUsers = 100

	prechecker, ok := builder.L2.ExecNode.TxPublisher.(*gethexec.TxPreChecker)
	if !ok {
		t.Error("prechecker not found on node")
	}
	sequencer, ok := prechecker.TransactionPublisher.(*gethexec.Sequencer)
	if !ok {
		t.Error("sequencer not found on node")
	}

	var users []string

	for num := 0; num < numUsers; num++ {
		userName := fmt.Sprintf("My_User_%d", num)
		builder.L2Info.GenerateAccount(userName)
		users = append(users, userName)
	}

	for _, userName := range users {
		tx := builder.L2Info.PrepareTx("Owner", userName, builder.L2Info.TransferGas, big.NewInt(1e16), nil)
		err := builder.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}

	sequencer.Pause()

	var txs types.Transactions

	for _, userName := range users {
		tx := builder.L2Info.PrepareTx(userName, "Owner", builder.L2Info.TransferGas, big.NewInt(2), nil)
		txs = append(txs, tx)
	}

	for _, tx := range txs {
		go func(ptx *types.Transaction) {
			err := sequencer.PublishTransaction(ctx, ptx, nil)
			Require(t, err)
		}(tx)
	}

	_, err := builder.L2.EnsureTxSucceededWithTimeout(txs[0], time.Second)
	if err == nil {
		t.Error("tx passed while sequencer paused")
	}

	sequencer.Activate()

	for _, tx := range txs {
		_, err := builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)
	}
}

'''
'''--- system_tests/seq_reject_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"net"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
)

func TestSequencerRejection(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	feedErrChan := make(chan error, 10)
	builderSeq := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builderSeq.nodeConfig.Feed.Output = *newBroadcasterConfigTest()
	cleanupSeq := builderSeq.Build(t)
	defer cleanupSeq()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.takeOwnership = false
	port := builderSeq.L2.ConsensusNode.BroadcastServer.ListenerAddr().(*net.TCPAddr).Port
	builder.nodeConfig.Feed.Input = *newBroadcastClientConfigTest(port)
	cleanup := builder.Build(t)
	defer cleanup()

	auth := builderSeq.L2Info.GetDefaultTransactOpts("Owner", ctx)
	simpleAddr, _ := builderSeq.L2.DeploySimple(t, auth)
	simpleAbi, err := mocksgen.SimpleMetaData.GetAbi()
	Require(t, err)
	noopId := simpleAbi.Methods["noop"].ID
	revertId := simpleAbi.Methods["pleaseRevert"].ID

	// Generate the accounts before hand to avoid races
	for user := 0; user < 9; user++ {
		name := fmt.Sprintf("User%v", user)
		builderSeq.L2Info.GenerateAccount(name)
	}

	wg := sync.WaitGroup{}
	var stopBackground int32
	for user := 0; user < 9; user++ {
		user := user
		name := fmt.Sprintf("User%v", user)
		tx := builderSeq.L2Info.PrepareTx("Owner", name, builderSeq.L2Info.TransferGas, big.NewInt(params.Ether), nil)

		err := builderSeq.L2.Client.SendTransaction(ctx, tx)
		Require(t, err)

		_, err = builderSeq.L2.EnsureTxSucceeded(tx)
		Require(t, err)
		_, err = builder.L2.EnsureTxSucceeded(tx)
		Require(t, err)

		wg.Add(1)
		go func() {
			defer wg.Done()
			info := builderSeq.L2Info.GetInfoWithPrivKey(name)
			txData := &types.DynamicFeeTx{
				To:        &simpleAddr,
				Gas:       builderSeq.L2Info.TransferGas + 10000,
				GasFeeCap: arbmath.BigMulByUint(builderSeq.L2Info.GasPrice, 100),
				Value:     common.Big0,
			}
			for atomic.LoadInt32(&stopBackground) == 0 {
				txData.Nonce = info.Nonce
				var expectedErr string
				if user%3 == 0 {
					txData.Data = noopId
					info.Nonce += 1
				} else if user%3 == 1 {
					txData.Data = revertId
					expectedErr = "execution reverted: SOLIDITY_REVERTING"
				} else {
					txData.Nonce = 1 << 32
					expectedErr = "nonce too high"
				}
				tx = builderSeq.L2Info.SignTxAs(name, txData)
				err = builderSeq.L2.Client.SendTransaction(ctx, tx)
				if err != nil && (expectedErr == "" || !strings.Contains(err.Error(), expectedErr)) {
					Require(t, err, "failed to send tx for user", user)
				}
			}
		}()
	}

	for i := 100; i >= 0; i-- {
		block, err := builderSeq.L2.Client.BlockNumber(ctx)
		Require(t, err)
		if block >= 200 {
			break
		}
		if i == 0 {
			Fatal(t, "failed to reach block 200, only reached block", block)
		}
		select {
		case err := <-feedErrChan:
			Fatal(t, "error: ", err)
		case <-time.After(time.Millisecond * 100):
		}
	}

	atomic.StoreInt32(&stopBackground, 1)
	wg.Wait()

	header1, err := builderSeq.L2.Client.HeaderByNumber(ctx, nil)
	Require(t, err)

	for i := 100; i >= 0; i-- {
		header2, err := builder.L2.Client.HeaderByNumber(ctx, header1.Number)
		if err != nil {
			select {
			case err := <-feedErrChan:
				Fatal(t, "error: ", err)
			case <-time.After(time.Millisecond * 100):
			}
			if i == 0 {
				client2Block, _ := builder.L2.Client.BlockNumber(ctx)
				Fatal(t, "client2 failed to reach client1 block ", header1.Number, ", only reached block", client2Block)
			}
			continue
		}
		if header1.Hash() == header2.Hash() {
			colors.PrintMint("client headers are equal")
			break
		} else {
			colors.PrintBlue("header 1:", header1)
			colors.PrintBlue("header 2:", header2)
			Fatal(t, "header 1 and header 2 have different hashes")
		}
	}
}

'''
'''--- system_tests/seq_whitelist_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/params"
)

func TestSequencerWhitelist(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.execConfig.Sequencer.SenderWhitelist = GetTestAddressForAccountName(t, "Owner").String() + "," + GetTestAddressForAccountName(t, "User").String()
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User")
	builder.L2Info.GenerateAccount("User2")

	// Owner is on the whitelist
	builder.L2.TransferBalance(t, "Owner", "User", big.NewInt(params.Ether), builder.L2Info)
	builder.L2.TransferBalance(t, "Owner", "User2", big.NewInt(params.Ether), builder.L2Info)

	// User is on the whitelist
	builder.L2.TransferBalance(t, "User", "User2", big.NewInt(params.Ether/10), builder.L2Info)

	// User2 is *not* on the whitelist, therefore this should fail
	tx := builder.L2Info.PrepareTx("User2", "User", builder.L2Info.TransferGas, big.NewInt(params.Ether/10), nil)
	err := builder.L2.Client.SendTransaction(ctx, tx)
	if err == nil {
		Fatal(t, "transaction from user not on whitelist accepted")
	}
}

'''
'''--- system_tests/seqcompensation_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/core/types"
	"github.com/offchainlabs/nitro/arbos/l1pricing"
)

// L1 Pricer pool address gets something when the sequencer posts batches
func TestSequencerCompensation(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	cleanup := builder.Build(t)
	defer cleanup()

	TestClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{})
	defer cleanupB()

	builder.L2Info.GenerateAccount("User2")

	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)
	err := builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	// give the inbox reader a bit of time to pick up the delayed message
	time.Sleep(time.Millisecond * 100)

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
	}

	_, err = WaitForTx(ctx, TestClientB.Client, tx.Hash(), time.Second*5)
	Require(t, err)

	// clientB sees balance means sequencer message was sent
	l2balance, err := TestClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)
	if l2balance.Cmp(big.NewInt(1e12)) != 0 {
		Fatal(t, "Unexpected balance:", l2balance)
	}

	initialSeqBalance, err := TestClientB.Client.BalanceAt(ctx, l1pricing.BatchPosterAddress, big.NewInt(0))
	Require(t, err)
	if initialSeqBalance.Sign() != 0 {
		Fatal(t, "Unexpected initial sequencer balance:", initialSeqBalance)
	}
}

'''
'''--- system_tests/seqfeed_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"net"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/broadcastclient"
	"github.com/offchainlabs/nitro/relay"
	"github.com/offchainlabs/nitro/util/signature"
	"github.com/offchainlabs/nitro/wsbroadcastserver"
)

func newBroadcasterConfigTest() *wsbroadcastserver.BroadcasterConfig {
	config := wsbroadcastserver.DefaultTestBroadcasterConfig
	config.Enable = true
	config.Port = "0"
	return &config
}

func newBroadcastClientConfigTest(port int) *broadcastclient.Config {
	return &broadcastclient.Config{
		URL:     []string{fmt.Sprintf("ws://localhost:%d/feed", port)},
		Timeout: 200 * time.Millisecond,
		Verify: signature.VerifierConfig{
			Dangerous: signature.DangerousVerifierConfig{
				AcceptMissing: true,
			},
		},
	}
}

func TestSequencerFeed(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builderSeq := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builderSeq.nodeConfig.Feed.Output = *newBroadcasterConfigTest()
	cleanupSeq := builderSeq.Build(t)
	defer cleanupSeq()
	seqInfo, seqNode, seqClient := builderSeq.L2Info, builderSeq.L2.ConsensusNode, builderSeq.L2.Client

	port := seqNode.BroadcastServer.ListenerAddr().(*net.TCPAddr).Port
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.nodeConfig.Feed.Input = *newBroadcastClientConfigTest(port)
	builder.takeOwnership = false
	cleanup := builder.Build(t)
	defer cleanup()
	client := builder.L2.Client

	seqInfo.GenerateAccount("User2")

	tx := seqInfo.PrepareTx("Owner", "User2", seqInfo.TransferGas, big.NewInt(1e12), nil)

	err := seqClient.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builderSeq.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	_, err = WaitForTx(ctx, client, tx.Hash(), time.Second*5)
	Require(t, err)
	l2balance, err := client.BalanceAt(ctx, seqInfo.GetAddress("User2"), nil)
	Require(t, err)
	if l2balance.Cmp(big.NewInt(1e12)) != 0 {
		t.Fatal("Unexpected balance:", l2balance)
	}
}

func TestRelayedSequencerFeed(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builderSeq := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builderSeq.nodeConfig.Feed.Output = *newBroadcasterConfigTest()
	cleanupSeq := builderSeq.Build(t)
	defer cleanupSeq()
	seqInfo, seqNode, seqClient := builderSeq.L2Info, builderSeq.L2.ConsensusNode, builderSeq.L2.Client

	bigChainId, err := seqClient.ChainID(ctx)
	Require(t, err)

	config := relay.ConfigDefault
	port := seqNode.BroadcastServer.ListenerAddr().(*net.TCPAddr).Port
	config.Node.Feed.Input = *newBroadcastClientConfigTest(port)
	config.Node.Feed.Output = *newBroadcasterConfigTest()
	config.Chain.ID = bigChainId.Uint64()

	feedErrChan := make(chan error, 10)
	currentRelay, err := relay.NewRelay(&config, feedErrChan)
	Require(t, err)
	err = currentRelay.Start(ctx)
	Require(t, err)
	defer currentRelay.StopAndWait()

	port = currentRelay.GetListenerAddr().(*net.TCPAddr).Port
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	builder.nodeConfig.Feed.Input = *newBroadcastClientConfigTest(port)
	builder.takeOwnership = false
	cleanup := builder.Build(t)
	defer cleanup()
	node, client := builder.L2.ConsensusNode, builder.L2.Client
	StartWatchChanErr(t, ctx, feedErrChan, node)

	seqInfo.GenerateAccount("User2")

	tx := seqInfo.PrepareTx("Owner", "User2", seqInfo.TransferGas, big.NewInt(1e12), nil)

	err = seqClient.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builderSeq.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	_, err = WaitForTx(ctx, client, tx.Hash(), time.Second*5)
	Require(t, err)
	l2balance, err := client.BalanceAt(ctx, seqInfo.GetAddress("User2"), nil)
	Require(t, err)
	if l2balance.Cmp(big.NewInt(1e12)) != 0 {
		t.Fatal("Unexpected balance:", l2balance)
	}
}

func testLyingSequencer(t *testing.T, dasModeStr string) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// The truthful sequencer
	chainConfig, nodeConfigA, lifecycleManager, _, dasSignerKey := setupConfigWithDAS(t, ctx, dasModeStr)
	defer lifecycleManager.StopAndWaitUntil(time.Second)

	nodeConfigA.BatchPoster.Enable = true
	nodeConfigA.Feed.Output.Enable = false
	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig = nodeConfigA
	builder.chainConfig = chainConfig
	builder.L2Info = nil
	cleanup := builder.Build(t)
	defer cleanup()

	l2clientA := builder.L2.Client

	authorizeDASKeyset(t, ctx, dasSignerKey, builder.L1Info, builder.L1.Client)

	// The lying sequencer
	nodeConfigC := arbnode.ConfigDefaultL1Test()
	nodeConfigC.BatchPoster.Enable = false
	nodeConfigC.DataAvailability = nodeConfigA.DataAvailability
	nodeConfigC.DataAvailability.RPCAggregator.Enable = false
	nodeConfigC.Feed.Output = *newBroadcasterConfigTest()
	testClientC, cleanupC := builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: nodeConfigC})
	defer cleanupC()
	l2clientC, nodeC := testClientC.Client, testClientC.ConsensusNode

	port := nodeC.BroadcastServer.ListenerAddr().(*net.TCPAddr).Port

	// The client node, connects to lying sequencer's feed
	nodeConfigB := arbnode.ConfigDefaultL1NonSequencerTest()
	nodeConfigB.Feed.Output.Enable = false
	nodeConfigB.Feed.Input = *newBroadcastClientConfigTest(port)
	nodeConfigB.DataAvailability = nodeConfigA.DataAvailability
	nodeConfigB.DataAvailability.RPCAggregator.Enable = false
	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: nodeConfigB})
	defer cleanupB()
	l2clientB := testClientB.Client

	builder.L2Info.GenerateAccount("FraudUser")
	builder.L2Info.GenerateAccount("RealUser")

	fraudTx := builder.L2Info.PrepareTx("Owner", "FraudUser", builder.L2Info.TransferGas, big.NewInt(1e12), nil)
	builder.L2Info.GetInfoWithPrivKey("Owner").Nonce -= 1 // Use same l2info object for different l2s
	realTx := builder.L2Info.PrepareTx("Owner", "RealUser", builder.L2Info.TransferGas, big.NewInt(1e12), nil)

	for i := 0; i < 10; i++ {
		err := l2clientC.SendTransaction(ctx, fraudTx)
		if err == nil {
			break
		}
		<-time.After(time.Millisecond * 10)
		if i == 9 {
			t.Fatal("error sending fraud transaction:", err)
		}
	}

	_, err := testClientC.EnsureTxSucceeded(fraudTx)
	if err != nil {
		t.Fatal("error ensuring fraud transaction succeeded:", err)
	}

	// Node B should get the transaction immediately from the sequencer feed
	_, err = WaitForTx(ctx, l2clientB, fraudTx.Hash(), time.Second*15)
	if err != nil {
		t.Fatal("error waiting for tx:", err)
	}
	l2balance, err := l2clientB.BalanceAt(ctx, builder.L2Info.GetAddress("FraudUser"), nil)
	if err != nil {
		t.Fatal("error getting balance:", err)
	}
	if l2balance.Cmp(big.NewInt(1e12)) != 0 {
		t.Fatal("Unexpected balance:", l2balance)
	}

	// Send the real transaction to client A
	err = l2clientA.SendTransaction(ctx, realTx)
	if err != nil {
		t.Fatal("error sending real transaction:", err)
	}

	_, err = builder.L2.EnsureTxSucceeded(realTx)
	if err != nil {
		t.Fatal("error ensuring real transaction succeeded:", err)
	}

	// Node B should get the transaction after NodeC posts a batch.
	_, err = WaitForTx(ctx, l2clientB, realTx.Hash(), time.Second*5)
	if err != nil {
		t.Fatal("error waiting for transaction to get to node b:", err)
	}
	l2balanceFraudAcct, err := l2clientB.BalanceAt(ctx, builder.L2Info.GetAddress("FraudUser"), nil)
	if err != nil {
		t.Fatal("error getting fraud balance:", err)
	}
	if l2balanceFraudAcct.Cmp(big.NewInt(0)) != 0 {
		t.Fatal("Unexpected balance (fraud acct should be empty) was:", l2balanceFraudAcct)
	}

	l2balanceRealAcct, err := l2clientB.BalanceAt(ctx, builder.L2Info.GetAddress("RealUser"), nil)
	if err != nil {
		t.Fatal("error getting real balance:", err)
	}
	if l2balanceRealAcct.Cmp(big.NewInt(1e12)) != 0 {
		t.Fatal("Unexpected balance of real account:", l2balanceRealAcct)
	}
}

func TestLyingSequencer(t *testing.T) {
	testLyingSequencer(t, "onchain")
}

func TestLyingSequencerLocalDAS(t *testing.T) {
	testLyingSequencer(t, "files")
}

'''
'''--- system_tests/seqinbox_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/ethclient/gethclient"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
	"github.com/offchainlabs/nitro/util"
)

type blockTestState struct {
	balances      map[common.Address]*big.Int
	nonces        map[common.Address]uint64
	accounts      []common.Address
	l2BlockNumber uint64
	l1BlockNumber uint64
}

const seqInboxTestIters = 40

func encodeAddBatch(seqABI *abi.ABI, seqNum *big.Int, message []byte, afterDelayedMsgRead *big.Int, gasRefunder common.Address) ([]byte, error) {
	method, ok := seqABI.Methods["addSequencerL2BatchFromOrigin0"]
	if !ok {
		return nil, errors.New("failed to find add addSequencerL2BatchFromOrigin0 method")
	}
	inputData, err := method.Inputs.Pack(
		seqNum,
		message,
		afterDelayedMsgRead,
		gasRefunder,
		new(big.Int).SetUint64(uint64(1)),
		new(big.Int).SetUint64(uint64(1)),
	)
	if err != nil {
		return nil, err
	}
	fullData := append([]byte{}, method.ID...)
	fullData = append(fullData, inputData...)
	return fullData, nil
}
func diffAccessList(accessed, al types.AccessList) string {
	m := make(map[common.Address]map[common.Hash]bool)
	for i := 0; i < len(al); i++ {
		if _, ok := m[al[i].Address]; !ok {
			m[al[i].Address] = make(map[common.Hash]bool)
		}
		for _, slot := range al[i].StorageKeys {
			m[al[i].Address][slot] = true
		}
	}

	diff := ""
	for i := 0; i < len(accessed); i++ {
		addr := accessed[i].Address
		if _, ok := m[addr]; !ok {
			diff += fmt.Sprintf("contract address: %q wasn't accessed\n", addr)
			continue
		}
		for j := 0; j < len(accessed[i].StorageKeys); j++ {
			slot := accessed[i].StorageKeys[j]
			if _, ok := m[addr][slot]; !ok {
				diff += fmt.Sprintf("storage slot: %v for contract: %v wasn't accessed\n", slot, addr)
			}
		}
	}
	return diff
}

func deployGasRefunder(ctx context.Context, t *testing.T, builder *NodeBuilder) common.Address {
	t.Helper()
	abi, err := bridgegen.GasRefunderMetaData.GetAbi()
	if err != nil {
		t.Fatalf("Error getting gas refunder abi: %v", err)
	}
	fauOpts := builder.L1Info.GetDefaultTransactOpts("Faucet", ctx)
	addr, tx, _, err := bind.DeployContract(&fauOpts, *abi, common.FromHex(bridgegen.GasRefunderBin), builder.L1.Client)
	if err != nil {
		t.Fatalf("Error getting gas refunder contract deployment transaction: %v", err)
	}
	if _, err := builder.L1.EnsureTxSucceeded(tx); err != nil {
		t.Fatalf("Error deploying gas refunder contract: %v", err)
	}
	tx = builder.L1Info.PrepareTxTo("Faucet", &addr, 30000, big.NewInt(9223372036854775807), nil)
	if err := builder.L1.Client.SendTransaction(ctx, tx); err != nil {
		t.Fatalf("Error sending gas refunder funding transaction")
	}
	if _, err := builder.L1.EnsureTxSucceeded(tx); err != nil {
		t.Fatalf("Error funding gas refunder")
	}
	contract, err := bridgegen.NewGasRefunder(addr, builder.L1.Client)
	if err != nil {
		t.Fatalf("Error getting gas refunder contract binding: %v", err)
	}
	tx, err = contract.AllowContracts(&fauOpts, []common.Address{builder.L1Info.GetAddress("SequencerInbox")})
	if err != nil {
		t.Fatalf("Error creating transaction for altering allowlist in refunder: %v", err)
	}
	if _, err := builder.L1.EnsureTxSucceeded(tx); err != nil {
		t.Fatalf("Error addting sequencer inbox in gas refunder allowlist: %v", err)
	}

	tx, err = contract.AllowRefundees(&fauOpts, []common.Address{builder.L1Info.GetAddress("Sequencer")})
	if err != nil {
		t.Fatalf("Error creating transaction for altering allowlist in refunder: %v", err)
	}
	if _, err := builder.L1.EnsureTxSucceeded(tx); err != nil {
		t.Fatalf("Error addting sequencer in gas refunder allowlist: %v", err)
	}
	return addr
}

func testSequencerInboxReaderImpl(t *testing.T, validator bool) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig.InboxReader.HardReorg = true
	if validator {
		builder.nodeConfig.BlockValidator.Enable = true
	}
	builder.isSequencer = false
	cleanup := builder.Build(t)
	defer cleanup()

	l2Backend := builder.L2.ExecNode.Backend

	l1BlockChain := builder.L1.L1Backend.BlockChain()

	rpcC := builder.L1.Stack.Attach()
	gethClient := gethclient.New(rpcC)

	seqInbox, err := bridgegen.NewSequencerInbox(builder.L1Info.GetAddress("SequencerInbox"), builder.L1.Client)
	Require(t, err)
	seqOpts := builder.L1Info.GetDefaultTransactOpts("Sequencer", ctx)

	gasRefunderAddr := deployGasRefunder(ctx, t, builder)

	ownerAddress := builder.L2Info.GetAddress("Owner")
	var startL2BlockNumber uint64 = 0

	startState, _, err := l2Backend.APIBackend().StateAndHeaderByNumber(ctx, rpc.LatestBlockNumber)
	Require(t, err)
	startOwnerBalance := startState.GetBalance(ownerAddress)
	startOwnerNonce := startState.GetNonce(ownerAddress)

	var blockStates []blockTestState
	blockStates = append(blockStates, blockTestState{
		balances: map[common.Address]*big.Int{
			ownerAddress: startOwnerBalance,
		},
		nonces: map[common.Address]uint64{
			ownerAddress: startOwnerNonce,
		},
		accounts:      []common.Address{ownerAddress},
		l2BlockNumber: startL2BlockNumber,
	})

	accountName := func(x int) string {
		if x == 0 {
			return "Owner"
		}
		return fmt.Sprintf("Account%v", x)
	}

	accounts := []string{"ReorgPadding"}
	for i := 1; i <= (seqInboxTestIters-1)/10; i++ {
		accounts = append(accounts, fmt.Sprintf("ReorgSacrifice%v", i))
	}
	var faucetTxs []*types.Transaction
	for _, acct := range accounts {
		builder.L1Info.GenerateAccount(acct)
		faucetTxs = append(faucetTxs, builder.L1Info.PrepareTx("Faucet", acct, 30000, big.NewInt(1e16), nil))
	}
	builder.L1.SendWaitTestTransactions(t, faucetTxs)

	seqABI, err := bridgegen.SequencerInboxMetaData.GetAbi()
	if err != nil {
		t.Fatalf("Error getting sequencer inbox abi: %v", err)
	}

	for i := 1; i < seqInboxTestIters; i++ {
		if i%10 == 0 {
			reorgTo := rand.Int() % len(blockStates)
			if reorgTo == 0 {
				reorgTo = 1
			}
			// Make the reorg larger to force the miner to discard transactions.
			// The miner usually collects transactions from deleted blocks and puts them in the mempool.
			// However, this code doesn't run on reorgs larger than 64 blocks for performance reasons.
			// Therefore, we make a bunch of small blocks to prevent the code from running.
			padAddr := builder.L1Info.GetAddress("ReorgPadding")
			for j := uint64(0); j < 70; j++ {
				rawTx := &types.DynamicFeeTx{
					To:        &padAddr,
					Gas:       21000,
					GasFeeCap: big.NewInt(params.GWei * 100),
					Value:     new(big.Int),
					Nonce:     j,
				}
				tx := builder.L1Info.SignTxAs("ReorgPadding", rawTx)
				Require(t, builder.L1.Client.SendTransaction(ctx, tx))
				_, _ = builder.L1.EnsureTxSucceeded(tx)
			}
			reorgTargetNumber := blockStates[reorgTo].l1BlockNumber
			currentHeader, err := builder.L1.Client.HeaderByNumber(ctx, nil)
			Require(t, err)
			if currentHeader.Number.Int64()-int64(reorgTargetNumber) < 65 {
				Fatal(t, "Less than 65 blocks of difference between current block", currentHeader.Number, "and target", reorgTargetNumber)
			}
			t.Logf("Reorganizing to L1 block %v", reorgTargetNumber)
			reorgTarget := l1BlockChain.GetBlockByNumber(reorgTargetNumber)
			err = l1BlockChain.ReorgToOldBlock(reorgTarget)
			Require(t, err)
			blockStates = blockStates[:(reorgTo + 1)]

			// Geth's miner's mempool might not immediately process the reorg.
			// Sometimes, this causes it to drop the next tx.
			// To work around this, we create a sacrificial tx, which may or may not succeed.
			// Whichever happens, by the end of this block, the miner will have processed the reorg.
			tx := builder.L1Info.PrepareTx(fmt.Sprintf("ReorgSacrifice%v", i/10), "Faucet", 30000, big.NewInt(0), nil)
			err = builder.L1.Client.SendTransaction(ctx, tx)
			Require(t, err)
			_, _ = WaitForTx(ctx, builder.L1.Client, tx.Hash(), time.Second)
		} else {
			state := blockStates[len(blockStates)-1]
			newBalances := make(map[common.Address]*big.Int)
			for k, v := range state.balances {
				newBalances[k] = new(big.Int).Set(v)
			}
			state.balances = newBalances
			newNonces := make(map[common.Address]uint64)
			for k, v := range state.nonces {
				newNonces[k] = v
			}
			state.nonces = newNonces

			batchBuffer := bytes.NewBuffer([]byte{})
			numMessages := 1 + rand.Int()%5
			for j := 0; j < numMessages; j++ {
				sourceNum := rand.Int() % len(state.accounts)
				source := state.accounts[sourceNum]
				amount := new(big.Int).SetUint64(uint64(rand.Int()) % state.balances[source].Uint64())
				reserveAmount := new(big.Int).SetUint64(l2pricing.InitialBaseFeeWei * 100000000)
				if state.balances[source].Cmp(new(big.Int).Add(amount, reserveAmount)) < 0 {
					// Leave enough funds for gas
					amount = big.NewInt(1)
				}
				var dest common.Address
				if j == 0 && amount.Cmp(reserveAmount) >= 0 {
					name := accountName(len(state.accounts))
					if !builder.L2Info.HasAccount(name) {
						builder.L2Info.GenerateAccount(name)
					}
					dest = builder.L2Info.GetAddress(name)
					state.accounts = append(state.accounts, dest)
					state.balances[dest] = big.NewInt(0)
				} else {
					dest = state.accounts[rand.Int()%len(state.accounts)]
				}

				rawTx := &types.DynamicFeeTx{
					To:        &dest,
					Gas:       util.NormalizeL2GasForL1GasInitial(210000, params.GWei),
					GasFeeCap: big.NewInt(l2pricing.InitialBaseFeeWei * 2),
					Value:     amount,
					Nonce:     state.nonces[source],
				}
				state.nonces[source]++
				tx := builder.L2Info.SignTxAs(accountName(sourceNum), rawTx)
				txData, err := tx.MarshalBinary()
				Require(t, err)
				var segment []byte
				segment = append(segment, arbstate.BatchSegmentKindL2Message)
				segment = append(segment, arbos.L2MessageKind_SignedTx)
				segment = append(segment, txData...)
				err = rlp.Encode(batchBuffer, segment)
				Require(t, err)

				state.balances[source].Sub(state.balances[source], amount)
				state.balances[dest].Add(state.balances[dest], amount)
			}

			compressed, err := arbcompress.CompressWell(batchBuffer.Bytes())
			Require(t, err)
			batchData := append([]byte{0}, compressed...)

			seqNonce := len(blockStates) - 1
			for j := 0; ; j++ {
				haveNonce, err := builder.L1.Client.PendingNonceAt(ctx, seqOpts.From)
				Require(t, err)
				if haveNonce == uint64(seqNonce) {
					break
				}
				if j >= 10 {
					t.Fatal("timed out with sequencer nonce", haveNonce, "waiting for expected nonce", seqNonce)
				}
				time.Sleep(time.Millisecond * 100)
			}
			seqOpts.Nonce = big.NewInt(int64(seqNonce))
			var tx *types.Transaction
			before, err := builder.L1.Client.BalanceAt(ctx, seqOpts.From, nil)
			if err != nil {
				t.Fatalf("BalanceAt(%v) unexpected error: %v", seqOpts.From, err)
			}

			data, err := encodeAddBatch(seqABI, big.NewInt(int64(len(blockStates))), batchData, big.NewInt(1), gasRefunderAddr)
			if err != nil {
				t.Fatalf("Error encoding batch data: %v", err)
			}
			si := builder.L1Info.GetAddress("SequencerInbox")
			wantAL, _, _, err := gethClient.CreateAccessList(ctx, ethereum.CallMsg{
				From: seqOpts.From,
				To:   &si,
				Data: data,
			})
			if err != nil {
				t.Fatalf("Error creating access list: %v", err)
			}
			accessed := arbnode.AccessList(&arbnode.AccessListOpts{
				SequencerInboxAddr:       builder.L1Info.GetAddress("SequencerInbox"),
				BridgeAddr:               builder.L1Info.GetAddress("Bridge"),
				DataPosterAddr:           seqOpts.From,
				GasRefunderAddr:          gasRefunderAddr,
				SequencerInboxAccs:       len(blockStates),
				AfterDelayedMessagesRead: 1,
			})
			if diff := diffAccessList(accessed, *wantAL); diff != "" {
				t.Errorf("Access list mistmatch:\n%s\n", diff)
			}
			if i%5 == 0 {
				tx, err = seqInbox.AddSequencerL2Batch(&seqOpts, big.NewInt(int64(len(blockStates))), batchData, big.NewInt(1), gasRefunderAddr, big.NewInt(0), big.NewInt(0))
			} else {
				tx, err = seqInbox.AddSequencerL2BatchFromOrigin(&seqOpts, big.NewInt(int64(len(blockStates))), batchData, big.NewInt(1), gasRefunderAddr)
			}
			Require(t, err)
			txRes, err := builder.L1.EnsureTxSucceeded(tx)
			if err != nil {
				// Geth's clique miner is finicky.
				// Unfortunately this is so rare that I haven't had an opportunity to test this workaround.
				// Specifically, I suspect there's a race where it thinks there's no txs to put in the new block,
				// if a new tx arrives at the same time as it tries to create a block.
				// Resubmit the transaction in an attempt to get the miner going again.
				_ = builder.L1.Client.SendTransaction(ctx, tx)
				txRes, err = builder.L1.EnsureTxSucceeded(tx)
				Require(t, err)
			}
			after, err := builder.L1.Client.BalanceAt(ctx, seqOpts.From, nil)
			if err != nil {
				t.Fatalf("BalanceAt(%v) unexpected error: %v", seqOpts.From, err)
			}
			txCost := txRes.EffectiveGasPrice.Uint64() * txRes.GasUsed
			if diff := before.Int64() - after.Int64(); diff >= int64(txCost) {
				t.Errorf("Transaction: %v was not refunded, balance diff: %v, cost: %v", tx.Hash(), diff, txCost)
			}

			state.l2BlockNumber += uint64(numMessages)
			state.l1BlockNumber = txRes.BlockNumber.Uint64()
			blockStates = append(blockStates, state)
		}

		t.Logf("Iteration %v: state %v block %v", i, len(blockStates)-1, blockStates[len(blockStates)-1].l2BlockNumber)

		for i := 0; ; i++ {
			batchCount, err := seqInbox.BatchCount(&bind.CallOpts{})
			if err != nil {
				Fatal(t, err)
			}
			if batchCount.Cmp(big.NewInt(int64(len(blockStates)))) == 0 {
				break
			} else if i >= 100 {
				Fatal(t, "timed out waiting for l1 batch count update; have", batchCount, "want", len(blockStates)-1)
			}
			time.Sleep(10 * time.Millisecond)
		}

		expectedBlockNumber := blockStates[len(blockStates)-1].l2BlockNumber
		for i := 0; ; i++ {
			blockNumber := l2Backend.APIBackend().CurrentHeader().Number.Uint64()
			if blockNumber == expectedBlockNumber {
				break
			} else if i >= 1000 {
				Fatal(t, "timed out waiting for l2 block update; have", blockNumber, "want", expectedBlockNumber)
			}
			time.Sleep(10 * time.Millisecond)
		}

		if validator && i%15 == 0 {
			for i := 0; ; i++ {
				expectedPos, err := builder.L2.ExecNode.ExecEngine.BlockNumberToMessageIndex(expectedBlockNumber)
				Require(t, err)
				lastValidated := builder.L2.ConsensusNode.BlockValidator.Validated(t)
				if lastValidated == expectedPos+1 {
					break
				} else if i >= 1000 {
					Fatal(t, "timed out waiting for block validator; have", lastValidated, "want", expectedPos+1)
				}
				time.Sleep(time.Second)
			}
		}

		for _, state := range blockStates {
			block, err := l2Backend.APIBackend().BlockByNumber(ctx, rpc.BlockNumber(state.l2BlockNumber))
			Require(t, err)
			if block == nil {
				Fatal(t, "missing state block", state.l2BlockNumber)
			}
			stateDb, _, err := l2Backend.APIBackend().StateAndHeaderByNumber(ctx, rpc.BlockNumber(state.l2BlockNumber))
			Require(t, err)
			for acct, expectedBalance := range state.balances {
				haveBalance := stateDb.GetBalance(acct)
				if expectedBalance.Cmp(haveBalance) < 0 {
					Fatal(t, "unexpected balance for account", acct, "; expected", expectedBalance, "got", haveBalance)
				}
			}
		}
	}
}

func TestSequencerInboxReader(t *testing.T) {
	testSequencerInboxReaderImpl(t, false)
}

'''
'''--- system_tests/staker_challenge_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build challengetest
// +build challengetest

package arbtest

import "testing"

func TestChallengeStakersFaultyHonestActive(t *testing.T) {
	stakerTestImpl(t, true, false)
}

func TestChallengeStakersFaultyHonestInactive(t *testing.T) {
	stakerTestImpl(t, true, true)
}

'''
'''--- system_tests/staker_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"strings"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"

	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbnode/dataposter/storage"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/solgen/go/mocksgen"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
	"github.com/offchainlabs/nitro/solgen/go/upgrade_executorgen"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/staker/validatorwallet"
	"github.com/offchainlabs/nitro/util"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/validator/valnode"
)

func makeBackgroundTxs(ctx context.Context, builder *NodeBuilder) error {
	for i := uint64(0); ctx.Err() == nil; i++ {
		builder.L2Info.Accounts["BackgroundUser"].Nonce = i
		tx := builder.L2Info.PrepareTx("BackgroundUser", "BackgroundUser", builder.L2Info.TransferGas, common.Big0, nil)
		err := builder.L2.Client.SendTransaction(ctx, tx)
		if err != nil {
			return err
		}
		_, err = builder.L2.EnsureTxSucceeded(tx)
		if err != nil {
			return err
		}
	}
	return nil
}

func stakerTestImpl(t *testing.T, faultyStaker bool, honestStakerInactive bool) {
	t.Parallel()
	ctx, cancelCtx := context.WithCancel(context.Background())
	defer cancelCtx()
	var transferGas = util.NormalizeL2GasForL1GasInitial(800_000, params.GWei) // include room for aggregator L1 costs

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.L2Info = NewBlockChainTestInfo(
		t,
		types.NewArbitrumSigner(types.NewLondonSigner(builder.chainConfig.ChainID)), big.NewInt(l2pricing.InitialBaseFeeWei*2),
		transferGas,
	)

	builder.nodeConfig.BatchPoster.MaxDelay = -1000 * time.Hour
	cleanupA := builder.Build(t)
	defer cleanupA()

	l2nodeA := builder.L2.ConsensusNode
	execNodeA := builder.L2.ExecNode

	if faultyStaker {
		builder.L2Info.GenerateGenesisAccount("FaultyAddr", common.Big1)
	}

	config := arbnode.ConfigDefaultL1Test()
	config.Sequencer = false
	config.DelayedSequencer.Enable = false
	config.BatchPoster.Enable = false
	builder.execConfig.Sequencer.Enable = false
	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{nodeConfig: config})
	defer cleanupB()

	l2nodeB := testClientB.ConsensusNode
	execNodeB := testClientB.ExecNode

	nodeAGenesis := execNodeA.Backend.APIBackend().CurrentHeader().Hash()
	nodeBGenesis := execNodeB.Backend.APIBackend().CurrentHeader().Hash()
	if faultyStaker {
		if nodeAGenesis == nodeBGenesis {
			Fatal(t, "node A L2 genesis hash", nodeAGenesis, "== node B L2 genesis hash", nodeBGenesis)
		}
	} else {
		if nodeAGenesis != nodeBGenesis {
			Fatal(t, "node A L2 genesis hash", nodeAGenesis, "!= node B L2 genesis hash", nodeBGenesis)
		}
	}

	builder.BridgeBalance(t, "Faucet", big.NewInt(1).Mul(big.NewInt(params.Ether), big.NewInt(10000)))

	deployAuth := builder.L1Info.GetDefaultTransactOpts("RollupOwner", ctx)

	balance := big.NewInt(params.Ether)
	balance.Mul(balance, big.NewInt(100))
	builder.L1Info.GenerateAccount("ValidatorA")
	builder.L1.TransferBalance(t, "Faucet", "ValidatorA", balance, builder.L1Info)
	l1authA := builder.L1Info.GetDefaultTransactOpts("ValidatorA", ctx)

	builder.L1Info.GenerateAccount("ValidatorB")
	builder.L1.TransferBalance(t, "Faucet", "ValidatorB", balance, builder.L1Info)
	l1authB := builder.L1Info.GetDefaultTransactOpts("ValidatorB", ctx)

	valWalletAddrAPtr, err := validatorwallet.GetValidatorWalletContract(ctx, l2nodeA.DeployInfo.ValidatorWalletCreator, 0, &l1authA, l2nodeA.L1Reader, true)
	Require(t, err)
	valWalletAddrA := *valWalletAddrAPtr
	valWalletAddrCheck, err := validatorwallet.GetValidatorWalletContract(ctx, l2nodeA.DeployInfo.ValidatorWalletCreator, 0, &l1authA, l2nodeA.L1Reader, true)
	Require(t, err)
	if valWalletAddrA == *valWalletAddrCheck {
		Require(t, err, "didn't cache validator wallet address", valWalletAddrA.String(), "vs", valWalletAddrCheck.String())
	}

	rollup, err := rollupgen.NewRollupAdminLogic(l2nodeA.DeployInfo.Rollup, builder.L1.Client)
	Require(t, err)

	upgradeExecutor, err := upgrade_executorgen.NewUpgradeExecutor(l2nodeA.DeployInfo.UpgradeExecutor, builder.L1.Client)
	Require(t, err, "unable to bind upgrade executor")
	rollupABI, err := abi.JSON(strings.NewReader(rollupgen.RollupAdminLogicABI))
	Require(t, err, "unable to parse rollup ABI")

	setValidatorCalldata, err := rollupABI.Pack("setValidator", []common.Address{valWalletAddrA, l1authB.From}, []bool{true, true})
	Require(t, err, "unable to generate setValidator calldata")
	tx, err := upgradeExecutor.ExecuteCall(&deployAuth, l2nodeA.DeployInfo.Rollup, setValidatorCalldata)
	Require(t, err, "unable to set validators")
	_, err = builder.L1.EnsureTxSucceeded(tx)
	Require(t, err)

	setMinAssertPeriodCalldata, err := rollupABI.Pack("setMinimumAssertionPeriod", big.NewInt(1))
	Require(t, err, "unable to generate setMinimumAssertionPeriod calldata")
	tx, err = upgradeExecutor.ExecuteCall(&deployAuth, l2nodeA.DeployInfo.Rollup, setMinAssertPeriodCalldata)
	Require(t, err, "unable to set minimum assertion period")
	_, err = builder.L1.EnsureTxSucceeded(tx)
	Require(t, err)

	validatorUtils, err := rollupgen.NewValidatorUtils(l2nodeA.DeployInfo.ValidatorUtils, builder.L1.Client)
	Require(t, err)

	valConfig := staker.TestL1ValidatorConfig

	dpA, err := arbnode.StakerDataposter(ctx, rawdb.NewTable(l2nodeB.ArbDB, storage.StakerPrefix), l2nodeA.L1Reader, &l1authA, NewFetcherFromConfig(arbnode.ConfigDefaultL1NonSequencerTest()), nil)
	if err != nil {
		t.Fatalf("Error creating validator dataposter: %v", err)
	}
	valWalletA, err := validatorwallet.NewContract(dpA, nil, l2nodeA.DeployInfo.ValidatorWalletCreator, l2nodeA.DeployInfo.Rollup, l2nodeA.L1Reader, &l1authA, 0, func(common.Address) {}, func() uint64 { return valConfig.ExtraGas })
	Require(t, err)
	if honestStakerInactive {
		valConfig.Strategy = "Defensive"
	} else {
		valConfig.Strategy = "MakeNodes"
	}

	_, valStack := createTestValidationNode(t, ctx, &valnode.TestValidationConfig)
	blockValidatorConfig := staker.TestBlockValidatorConfig

	statelessA, err := staker.NewStatelessBlockValidator(
		l2nodeA.InboxReader,
		l2nodeA.InboxTracker,
		l2nodeA.TxStreamer,
		execNodeA,
		l2nodeA.ArbDB,
		nil,
		StaticFetcherFrom(t, &blockValidatorConfig),
		valStack,
	)
	Require(t, err)
	err = statelessA.Start(ctx)
	Require(t, err)
	stakerA, err := staker.NewStaker(
		l2nodeA.L1Reader,
		valWalletA,
		bind.CallOpts{},
		valConfig,
		nil,
		statelessA,
		nil,
		nil,
		l2nodeA.DeployInfo.ValidatorUtils,
		nil,
	)
	Require(t, err)
	err = stakerA.Initialize(ctx)
	if stakerA.Strategy() != staker.WatchtowerStrategy {
		err = valWalletA.Initialize(ctx)
		Require(t, err)
	}
	Require(t, err)

	dpB, err := arbnode.StakerDataposter(ctx, rawdb.NewTable(l2nodeB.ArbDB, storage.StakerPrefix), l2nodeB.L1Reader, &l1authB, NewFetcherFromConfig(arbnode.ConfigDefaultL1NonSequencerTest()), nil)
	if err != nil {
		t.Fatalf("Error creating validator dataposter: %v", err)
	}
	valWalletB, err := validatorwallet.NewEOA(dpB, l2nodeB.DeployInfo.Rollup, l2nodeB.L1Reader.Client(), &l1authB, func() uint64 { return 0 })
	Require(t, err)
	valConfig.Strategy = "MakeNodes"
	statelessB, err := staker.NewStatelessBlockValidator(
		l2nodeB.InboxReader,
		l2nodeB.InboxTracker,
		l2nodeB.TxStreamer,
		execNodeB,
		l2nodeB.ArbDB,
		nil,
		StaticFetcherFrom(t, &blockValidatorConfig),
		valStack,
	)
	Require(t, err)
	err = statelessB.Start(ctx)
	Require(t, err)
	stakerB, err := staker.NewStaker(
		l2nodeB.L1Reader,
		valWalletB,
		bind.CallOpts{},
		valConfig,
		nil,
		statelessB,
		nil,
		nil,
		l2nodeB.DeployInfo.ValidatorUtils,
		nil,
	)
	Require(t, err)
	err = stakerB.Initialize(ctx)
	Require(t, err)
	if stakerB.Strategy() != staker.WatchtowerStrategy {
		err = valWalletB.Initialize(ctx)
		Require(t, err)
	}
	valWalletC := validatorwallet.NewNoOp(builder.L1.Client, l2nodeA.DeployInfo.Rollup)
	valConfig.Strategy = "Watchtower"
	stakerC, err := staker.NewStaker(
		l2nodeA.L1Reader,
		valWalletC,
		bind.CallOpts{},
		valConfig,
		nil,
		statelessA,
		nil,
		nil,
		l2nodeA.DeployInfo.ValidatorUtils,
		nil,
	)
	Require(t, err)
	if stakerC.Strategy() != staker.WatchtowerStrategy {
		err = valWalletC.Initialize(ctx)
		Require(t, err)
	}
	err = stakerC.Initialize(ctx)
	Require(t, err)

	builder.L2Info.GenerateAccount("BackgroundUser")
	tx = builder.L2Info.PrepareTx("Faucet", "BackgroundUser", builder.L2Info.TransferGas, balance, nil)
	err = builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)
	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	// Continually make L2 transactions in a background thread
	backgroundTxsCtx, cancelBackgroundTxs := context.WithCancel(ctx)
	backgroundTxsShutdownChan := make(chan struct{})
	defer (func() {
		cancelBackgroundTxs()
		<-backgroundTxsShutdownChan
	})()
	go (func() {
		defer close(backgroundTxsShutdownChan)
		err := makeBackgroundTxs(backgroundTxsCtx, builder)
		if !errors.Is(err, context.Canceled) {
			log.Warn("error making background txs", "err", err)
		}
	})()

	stakerATxs := 0
	stakerAWasStaked := false
	stakerBTxs := 0
	stakerBWasStaked := false
	sawStakerZombie := false
	challengeMangerTimedOut := false
	for i := 0; i < 100; i++ {
		var stakerName string
		if i%2 == 0 {
			stakerName = "A"
			fmt.Printf("staker A acting:\n")
			tx, err = stakerA.Act(ctx)
			if tx != nil {
				stakerATxs++
			}
		} else {
			stakerName = "B"
			fmt.Printf("staker B acting:\n")
			tx, err = stakerB.Act(ctx)
			if tx != nil {
				stakerBTxs++
			}
		}

		if err != nil && strings.Contains(err.Error(), "waiting") {
			colors.PrintRed("retrying ", err.Error(), i)
			time.Sleep(20 * time.Millisecond)
			i--
			continue
		}
		if err != nil && faultyStaker && i%2 == 1 {
			// Check if this is an expected error from the faulty staker.
			if strings.Contains(err.Error(), "agreed with entire challenge") || strings.Contains(err.Error(), "after msg 0 expected global state") {
				// Expected error upon realizing you're losing the challenge. Get ready for a timeout.
				if !challengeMangerTimedOut {
					// Upgrade the ChallengeManager contract to an implementation which says challenges are always timed out

					mockImpl, tx, _, err := mocksgen.DeployTimedOutChallengeManager(&deployAuth, builder.L1.Client)
					Require(t, err)
					_, err = builder.L1.EnsureTxSucceeded(tx)
					Require(t, err)

					managerAddr := valWalletA.ChallengeManagerAddress()
					// 0xb53127684a568b3173ae13b9f8a6016e243e63b6e8ee1178d6a717850b5d6103
					proxyAdminSlot := common.BigToHash(arbmath.BigSub(crypto.Keccak256Hash([]byte("eip1967.proxy.admin")).Big(), common.Big1))
					proxyAdminBytes, err := builder.L1.Client.StorageAt(ctx, managerAddr, proxyAdminSlot, nil)
					Require(t, err)
					proxyAdminAddr := common.BytesToAddress(proxyAdminBytes)
					if proxyAdminAddr == (common.Address{}) {
						Fatal(t, "failed to get challenge manager proxy admin")
					}

					proxyAdminABI, err := abi.JSON(strings.NewReader(mocksgen.ProxyAdminForBindingABI))
					Require(t, err)
					upgradeCalldata, err := proxyAdminABI.Pack("upgrade", managerAddr, mockImpl)
					Require(t, err)
					tx, err = upgradeExecutor.ExecuteCall(&deployAuth, proxyAdminAddr, upgradeCalldata)
					Require(t, err)
					_, err = builder.L1.EnsureTxSucceeded(tx)
					Require(t, err)

					challengeMangerTimedOut = true
				}
			} else if strings.Contains(err.Error(), "insufficient funds") && sawStakerZombie {
				// Expected error when trying to re-stake after losing initial stake.
			} else if strings.Contains(err.Error(), "start state not in chain") && sawStakerZombie {
				// Expected error when trying to re-stake after the challenger's nodes getting confirmed.
			} else if strings.Contains(err.Error(), "STAKER_IS_ZOMBIE") && sawStakerZombie {
				// Expected error when the staker is a zombie and thus can't advance its stake.
			} else {
				Require(t, err, "Faulty staker failed to act")
			}
			t.Log("got expected faulty staker error", err)
			err = nil
			tx = nil
		}
		Require(t, err, "Staker", stakerName, "failed to act")
		if tx != nil {
			_, err = builder.L1.EnsureTxSucceeded(tx)
			Require(t, err, "EnsureTxSucceeded failed for staker", stakerName, "tx")
		}
		if faultyStaker {
			conflictInfo, err := validatorUtils.FindStakerConflict(&bind.CallOpts{}, l2nodeA.DeployInfo.Rollup, l1authA.From, l1authB.From, big.NewInt(1024))
			Require(t, err)
			if staker.ConflictType(conflictInfo.Ty) == staker.CONFLICT_TYPE_FOUND {
				cancelBackgroundTxs()
			}
		}
		if faultyStaker && !sawStakerZombie {
			sawStakerZombie, err = rollup.IsZombie(&bind.CallOpts{}, l1authB.From)
			Require(t, err)
		}
		isHonestZombie, err := rollup.IsZombie(&bind.CallOpts{}, valWalletAddrA)
		Require(t, err)
		if isHonestZombie {
			Fatal(t, "staker A became a zombie")
		}
		fmt.Printf("watchtower staker acting:\n")
		watchTx, err := stakerC.Act(ctx)
		if err != nil && !strings.Contains(err.Error(), "catch up") {
			Require(t, err, "watchtower staker failed to act")
		}
		if watchTx != nil {
			Fatal(t, "watchtower staker made a transaction")
		}
		if !stakerAWasStaked {
			stakerAWasStaked, err = rollup.IsStaked(&bind.CallOpts{}, valWalletAddrA)
			Require(t, err)
		}
		if !stakerBWasStaked {
			stakerBWasStaked, err = rollup.IsStaked(&bind.CallOpts{}, l1authB.From)
			Require(t, err)
		}
		for j := 0; j < 5; j++ {
			builder.L1.TransferBalance(t, "Faucet", "Faucet", common.Big0, builder.L1Info)
		}
	}

	if stakerATxs == 0 || stakerBTxs == 0 {
		Fatal(t, "staker didn't make txs: staker A made", stakerATxs, "staker B made", stakerBTxs)
	}

	latestConfirmedNode, err := rollup.LatestConfirmed(&bind.CallOpts{})
	Require(t, err)

	if latestConfirmedNode <= 1 && !honestStakerInactive {
		latestCreatedNode, err := rollup.LatestNodeCreated(&bind.CallOpts{})
		Require(t, err)
		Fatal(t, "latest confirmed node didn't advance:", latestConfirmedNode, latestCreatedNode)
	}

	if faultyStaker && !sawStakerZombie {
		Fatal(t, "staker B didn't become a zombie despite being faulty")
	}

	if !stakerAWasStaked {
		Fatal(t, "staker A was never staked")
	}
	if !stakerBWasStaked {
		Fatal(t, "staker B was never staked")
	}
}

func TestStakersCooperative(t *testing.T) {
	stakerTestImpl(t, false, false)
}

'''
'''--- system_tests/state_fuzz_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"bytes"
	"context"
	"encoding/binary"
	"encoding/json"
	"errors"
	"fmt"
	"math/big"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/consensus"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/arbos"
	"github.com/offchainlabs/nitro/arbos/arbosState"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbstate"
	"github.com/offchainlabs/nitro/statetransfer"
)

func BuildBlock(
	statedb *state.StateDB,
	lastBlockHeader *types.Header,
	chainContext core.ChainContext,
	chainConfig *params.ChainConfig,
	inbox arbstate.InboxBackend,
	seqBatch []byte,
) (*types.Block, error) {
	var delayedMessagesRead uint64
	if lastBlockHeader != nil {
		delayedMessagesRead = lastBlockHeader.Nonce.Uint64()
	}
	inboxMultiplexer := arbstate.NewInboxMultiplexer(inbox, delayedMessagesRead, nil, arbstate.KeysetValidate)

	ctx := context.Background()
	message, err := inboxMultiplexer.Pop(ctx)
	if err != nil {
		return nil, err
	}

	delayedMessagesRead = inboxMultiplexer.DelayedMessagesRead()
	l1Message := message.Message

	batchFetcher := func(uint64) ([]byte, error) {
		return seqBatch, nil
	}
	block, _, err := arbos.ProduceBlock(
		l1Message, delayedMessagesRead, lastBlockHeader, statedb, chainContext, chainConfig, batchFetcher,
	)
	return block, err
}

// A simple mock inbox multiplexer backend
type inboxBackend struct {
	batchSeqNum           uint64
	batches               [][]byte
	positionWithinMessage uint64
	delayedMessages       [][]byte
}

func (b *inboxBackend) PeekSequencerInbox() ([]byte, error) {
	if len(b.batches) == 0 {
		return nil, errors.New("read past end of specified sequencer batches")
	}
	return b.batches[0], nil
}

func (b *inboxBackend) GetSequencerInboxPosition() uint64 {
	return b.batchSeqNum
}

func (b *inboxBackend) AdvanceSequencerInbox() {
	b.batchSeqNum++
	if len(b.batches) > 0 {
		b.batches = b.batches[1:]
	}
}

func (b *inboxBackend) GetPositionWithinMessage() uint64 {
	return b.positionWithinMessage
}

func (b *inboxBackend) SetPositionWithinMessage(pos uint64) {
	b.positionWithinMessage = pos
}

func (b *inboxBackend) ReadDelayedInbox(seqNum uint64) (*arbostypes.L1IncomingMessage, error) {
	if seqNum >= uint64(len(b.delayedMessages)) {
		return nil, errors.New("delayed inbox message out of bounds")
	}
	msg, err := arbostypes.ParseIncomingL1Message(bytes.NewReader(b.delayedMessages[seqNum]), nil)
	if err != nil {
		// The bridge won't generate an invalid L1 message,
		// so here we substitute it with a less invalid one for fuzzing.
		msg = &arbostypes.TestIncomingMessageWithRequestId
	}
	return msg, nil
}

// A chain context with no information
type noopChainContext struct{}

func (c noopChainContext) Engine() consensus.Engine {
	return nil
}

func (c noopChainContext) GetHeader(common.Hash, uint64) *types.Header {
	return nil
}

func FuzzStateTransition(f *testing.F) {
	f.Fuzz(func(t *testing.T, compressSeqMsg bool, seqMsg []byte, delayedMsg []byte) {
		chainDb := rawdb.NewMemoryDatabase()
		chainConfig := params.ArbitrumRollupGoerliTestnetChainConfig()
		serializedChainConfig, err := json.Marshal(chainConfig)
		if err != nil {
			panic(err)
		}
		initMessage := &arbostypes.ParsedInitMessage{
			ChainId:               chainConfig.ChainID,
			InitialL1BaseFee:      arbostypes.DefaultInitialL1BaseFee,
			ChainConfig:           chainConfig,
			SerializedChainConfig: serializedChainConfig,
		}
		stateRoot, err := arbosState.InitializeArbosInDatabase(
			chainDb,
			statetransfer.NewMemoryInitDataReader(&statetransfer.ArbosInitializationInfo{}),
			chainConfig,
			initMessage,
			0,
			0,
		)
		if err != nil {
			panic(err)
		}
		statedb, err := state.New(stateRoot, state.NewDatabase(chainDb), nil)
		if err != nil {
			panic(err)
		}
		genesis := &types.Header{
			Number:     new(big.Int),
			Nonce:      types.EncodeNonce(0),
			Time:       0,
			ParentHash: common.Hash{},
			Extra:      []byte("Arbitrum"),
			GasLimit:   l2pricing.GethBlockGasLimit,
			GasUsed:    0,
			BaseFee:    big.NewInt(l2pricing.InitialBaseFeeWei),
			Difficulty: big.NewInt(1),
			MixDigest:  common.Hash{},
			Coinbase:   common.Address{},
			Root:       stateRoot,
		}

		// Append a header to the input (this part is authenticated by L1).
		// The first 32 bytes encode timestamp and L1 block number bounds.
		// For simplicity, those are all set to 0.
		// The next 8 bytes encode the after delayed message count.
		delayedMessages := [][]byte{delayedMsg}
		seqBatch := make([]byte, 40)
		binary.BigEndian.PutUint64(seqBatch[8:16], ^uint64(0))
		binary.BigEndian.PutUint64(seqBatch[24:32], ^uint64(0))
		binary.BigEndian.PutUint64(seqBatch[32:40], uint64(len(delayedMessages)))
		if compressSeqMsg {
			seqBatch = append(seqBatch, arbstate.BrotliMessageHeaderByte)
			seqMsgCompressed, err := arbcompress.CompressLevel(seqMsg, 0)
			if err != nil {
				panic(fmt.Sprintf("failed to compress sequencer message: %v", err))
			}
			seqBatch = append(seqBatch, seqMsgCompressed...)
		} else {
			seqBatch = append(seqBatch, seqMsg...)
		}
		inbox := &inboxBackend{
			batchSeqNum:           0,
			batches:               [][]byte{seqBatch},
			positionWithinMessage: 0,
			delayedMessages:       delayedMessages,
		}
		_, err = BuildBlock(statedb, genesis, noopChainContext{}, params.ArbitrumOneChainConfig(), inbox, seqBatch)
		if err != nil {
			// With the fixed header it shouldn't be possible to read a delayed message,
			// and no other type of error should be possible.
			panic(err)
		}
	})
}

'''
'''--- system_tests/test_info.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"crypto/ecdsa"
	"errors"
	"math/big"
	"sync/atomic"
	"testing"

	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/util"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/params"
	"github.com/offchainlabs/nitro/statetransfer"
)

var simulatedChainID = big.NewInt(1337)

type AccountInfo struct {
	Address    common.Address
	PrivateKey *ecdsa.PrivateKey
	Nonce      uint64
}

type BlockchainTestInfo struct {
	T           *testing.T
	Signer      types.Signer
	Accounts    map[string]*AccountInfo
	ArbInitData statetransfer.ArbosInitializationInfo
	GasPrice    *big.Int
	// The amount of gas needed for a simple transfer tx.
	TransferGas uint64
}

func NewBlockChainTestInfo(t *testing.T, signer types.Signer, gasPrice *big.Int, transferGas uint64) *BlockchainTestInfo {
	return &BlockchainTestInfo{
		T:           t,
		Signer:      signer,
		Accounts:    make(map[string]*AccountInfo),
		GasPrice:    new(big.Int).Set(gasPrice),
		TransferGas: transferGas,
	}
}

func NewArbTestInfo(t *testing.T, chainId *big.Int) *BlockchainTestInfo {
	var transferGas = util.NormalizeL2GasForL1GasInitial(800_000, params.GWei) // include room for aggregator L1 costs
	arbinfo := NewBlockChainTestInfo(
		t,
		types.NewArbitrumSigner(types.NewLondonSigner(chainId)), big.NewInt(l2pricing.InitialBaseFeeWei*2),
		transferGas,
	)
	arbinfo.GenerateGenesisAccount("Owner", new(big.Int).Sub(new(big.Int).Lsh(big.NewInt(1), 256), big.NewInt(9)))
	arbinfo.GenerateGenesisAccount("Faucet", new(big.Int).Sub(new(big.Int).Lsh(big.NewInt(1), 256), big.NewInt(9)))
	return arbinfo
}

func NewL1TestInfo(t *testing.T) *BlockchainTestInfo {
	return NewBlockChainTestInfo(t, types.NewLondonSigner(simulatedChainID), big.NewInt(params.GWei*100), params.TxGas)
}

func GetTestKeyForAccountName(t *testing.T, name string) *ecdsa.PrivateKey {
	keyBytes := crypto.Keccak256([]byte(name))
	keyBytes[0] = 0
	privateKey, err := crypto.ToECDSA(keyBytes)
	if err != nil {
		t.Fatal(err)
	}
	return privateKey
}

func GetTestAddressForAccountName(t *testing.T, name string) common.Address {
	privateKey := GetTestKeyForAccountName(t, name)
	return crypto.PubkeyToAddress(privateKey.PublicKey)
}

func (b *BlockchainTestInfo) GenerateAccount(name string) {
	b.T.Helper()

	privateKey := GetTestKeyForAccountName(b.T, name)
	if b.Accounts[name] != nil {
		b.T.Fatal("account already exists")
	}
	b.Accounts[name] = &AccountInfo{
		PrivateKey: privateKey,
		Address:    crypto.PubkeyToAddress(privateKey.PublicKey),
		Nonce:      0,
	}
	log.Info("New Key ", "name", name, "Address", b.Accounts[name].Address)
}

func (b *BlockchainTestInfo) HasAccount(name string) bool {
	return b.Accounts[name] != nil
}

func (b *BlockchainTestInfo) GenerateGenesisAccount(name string, balance *big.Int) {
	b.GenerateAccount(name)
	b.ArbInitData.Accounts = append(b.ArbInitData.Accounts, statetransfer.AccountInitializationInfo{
		Addr:       b.Accounts[name].Address,
		EthBalance: new(big.Int).Set(balance),
	})
}

func (b *BlockchainTestInfo) GetGenesisAlloc() core.GenesisAlloc {
	alloc := make(core.GenesisAlloc)
	for _, info := range b.ArbInitData.Accounts {
		var contractCode []byte
		contractStorage := make(map[common.Hash]common.Hash)
		if info.ContractInfo != nil {
			contractCode = append([]byte{}, info.ContractInfo.Code...)
			for k, v := range info.ContractInfo.ContractStorage {
				contractStorage[k] = v
			}
		}
		alloc[info.Addr] = core.GenesisAccount{
			Balance: new(big.Int).Set(info.EthBalance),
			Nonce:   info.Nonce,
			Code:    contractCode,
			Storage: contractStorage,
		}
	}
	return alloc
}

func (b *BlockchainTestInfo) SetContract(name string, address common.Address) {
	b.Accounts[name] = &AccountInfo{
		PrivateKey: nil,
		Address:    address,
	}
}

func (b *BlockchainTestInfo) SetFullAccountInfo(name string, info *AccountInfo) {
	infoCopy := *info
	b.Accounts[name] = &infoCopy
}

func (b *BlockchainTestInfo) GetAddress(name string) common.Address {
	b.T.Helper()
	info, ok := b.Accounts[name]
	if !ok {
		b.T.Fatal("not found account: ", name)
	}
	return info.Address
}

func (b *BlockchainTestInfo) GetInfoWithPrivKey(name string) *AccountInfo {
	b.T.Helper()
	info, ok := b.Accounts[name]
	if !ok {
		b.T.Fatal("not found account: ", name)
	}
	if info.PrivateKey == nil {
		b.T.Fatal("no private key for account: ", name)
	}
	return info
}

func (b *BlockchainTestInfo) GetDefaultTransactOpts(name string, ctx context.Context) bind.TransactOpts {
	b.T.Helper()
	info := b.GetInfoWithPrivKey(name)
	return bind.TransactOpts{
		From: info.Address,
		Signer: func(address common.Address, tx *types.Transaction) (*types.Transaction, error) {
			if address != info.Address {
				return nil, errors.New("bad address")
			}
			signature, err := crypto.Sign(b.Signer.Hash(tx).Bytes(), info.PrivateKey)
			if err != nil {
				return nil, err
			}
			atomic.AddUint64(&info.Nonce, 1) // we don't set Nonce, but try to keep track..
			return tx.WithSignature(b.Signer, signature)
		},
		GasMargin: 2000, // adjust by 20%
		Context:   ctx,
	}
}

func (b *BlockchainTestInfo) GetDefaultCallOpts(name string, ctx context.Context) *bind.CallOpts {
	b.T.Helper()
	auth := b.GetDefaultTransactOpts(name, ctx)
	return &bind.CallOpts{
		From: auth.From,
	}
}

func (b *BlockchainTestInfo) SignTxAs(name string, data types.TxData) *types.Transaction {
	b.T.Helper()
	info := b.GetInfoWithPrivKey(name)
	tx := types.NewTx(data)
	tx, err := types.SignTx(tx, b.Signer, info.PrivateKey)
	if err != nil {
		b.T.Fatal(err)
	}
	return tx
}

func (b *BlockchainTestInfo) PrepareTx(from, to string, gas uint64, value *big.Int, data []byte) *types.Transaction {
	b.T.Helper()
	addr := b.GetAddress(to)
	return b.PrepareTxTo(from, &addr, gas, value, data)
}

func (b *BlockchainTestInfo) PrepareTxTo(
	from string, to *common.Address, gas uint64, value *big.Int, data []byte,
) *types.Transaction {
	b.T.Helper()
	info := b.GetInfoWithPrivKey(from)
	txNonce := atomic.AddUint64(&info.Nonce, 1) - 1
	txData := &types.DynamicFeeTx{
		To:        to,
		Gas:       gas,
		GasFeeCap: new(big.Int).Set(b.GasPrice),
		Value:     value,
		Nonce:     txNonce,
		Data:      data,
	}
	return b.SignTxAs(from, txData)
}

'''
'''--- system_tests/transfer_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"testing"
)

func TestTransfer(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	builder := NewNodeBuilder(ctx).DefaultConfig(t, false)
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User2")

	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)

	err := builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	bal, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("Owner"), nil)
	Require(t, err)
	fmt.Println("Owner balance is: ", bal)
	bal2, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)
	if bal2.Cmp(big.NewInt(1e12)) != 0 {
		Fatal(t, "Unexpected recipient balance: ", bal2)
	}
}

'''
'''--- system_tests/triedb_race_test.go ---
package arbtest

import (
	"context"
	"sync"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/arbitrum"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestTrieDBCommitRace(t *testing.T) {
	_ = testhelpers.InitTestLog(t, log.LvlError)
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.execConfig.RPC.MaxRecreateStateDepth = arbitrum.InfiniteMaxRecreateStateDepth
	builder.execConfig.Sequencer.MaxBlockSpeed = 0
	builder.execConfig.Sequencer.MaxTxDataSize = 150 // 1 test tx ~= 110
	builder.execConfig.Caching.Archive = true
	builder.execConfig.Caching.BlockCount = 127
	builder.execConfig.Caching.BlockAge = 0
	builder.execConfig.Caching.MaxNumberOfBlocksToSkipStateSaving = 127
	builder.execConfig.Caching.MaxAmountOfGasToSkipStateSaving = 0
	cleanup := builder.Build(t)
	defer cleanup()

	builder.L2Info.GenerateAccount("User2")
	bc := builder.L2.ExecNode.Backend.ArbInterface().BlockChain()

	var wg sync.WaitGroup
	quit := make(chan struct{})
	wg.Add(1)
	go func() {
		defer wg.Done()
		for {
			select {
			default:
				builder.L2.TransferBalance(t, "Faucet", "User2", common.Big1, builder.L2Info)
			case <-quit:
				return
			}
		}
	}()
	api := builder.L2.ExecNode.Backend.APIBackend()
	blockNumber := 1
	for i := 0; i < 5; i++ {
		var roots []common.Hash
		for len(roots) < 1024 {
			select {
			default:
				block, err := api.BlockByNumber(ctx, rpc.BlockNumber(blockNumber))
				if err == nil && block != nil {
					root := block.Root()
					if statedb, err := bc.StateAt(root); err == nil {
						err := statedb.Database().TrieDB().Reference(root, common.Hash{})
						Require(t, err)
						roots = append(roots, root)
					}
					blockNumber += 1
				}
			case <-quit:
				return
			}
		}
		t.Log("dereferencing...")
		for _, root := range roots {
			err := bc.TrieDB().Dereference(root)
			Require(t, err)
			time.Sleep(1)
		}
	}
	close(quit)
	wg.Wait()
}

'''
'''--- system_tests/twonodes_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbtest

import (
	"context"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/core/types"
)

func testTwoNodesSimple(t *testing.T, dasModeStr string) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	chainConfig, l1NodeConfigA, lifecycleManager, _, dasSignerKey := setupConfigWithDAS(t, ctx, dasModeStr)
	defer lifecycleManager.StopAndWaitUntil(time.Second)

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig = l1NodeConfigA
	builder.chainConfig = chainConfig
	builder.L2Info = nil
	cleanup := builder.Build(t)
	defer cleanup()

	authorizeDASKeyset(t, ctx, dasSignerKey, builder.L1Info, builder.L1.Client)
	l1NodeConfigBDataAvailability := l1NodeConfigA.DataAvailability
	l1NodeConfigBDataAvailability.RPCAggregator.Enable = false
	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{dasConfig: &l1NodeConfigBDataAvailability})
	defer cleanupB()

	builder.L2Info.GenerateAccount("User2")

	tx := builder.L2Info.PrepareTx("Owner", "User2", builder.L2Info.TransferGas, big.NewInt(1e12), nil)

	err := builder.L2.Client.SendTransaction(ctx, tx)
	Require(t, err)

	_, err = builder.L2.EnsureTxSucceeded(tx)
	Require(t, err)

	// give the inbox reader a bit of time to pick up the delayed message
	time.Sleep(time.Millisecond * 100)

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < 30; i++ {
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
		})
	}

	_, err = WaitForTx(ctx, testClientB.Client, tx.Hash(), time.Second*5)
	Require(t, err)

	l2balance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("User2"), nil)
	Require(t, err)

	if l2balance.Cmp(big.NewInt(1e12)) != 0 {
		Fatal(t, "Unexpected balance:", l2balance)
	}
}

func TestTwoNodesSimple(t *testing.T) {
	testTwoNodesSimple(t, "onchain")
}

func TestTwoNodesSimpleLocalDAS(t *testing.T) {
	testTwoNodesSimple(t, "files")
}

'''
'''--- system_tests/twonodeslong_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

// race detection makes things slow and miss timeouts
//go:build !race
// +build !race

package arbtest

import (
	"context"
	"math/big"
	"math/rand"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbos/l2pricing"
	"github.com/offchainlabs/nitro/arbutil"

	"github.com/ethereum/go-ethereum/core/txpool"
	"github.com/ethereum/go-ethereum/core/types"
)

func testTwoNodesLong(t *testing.T, dasModeStr string) {
	t.Parallel()
	largeLoops := 8
	avgL2MsgsPerLoop := 30
	avgDelayedMessagesPerLoop := 10
	avgTotalL1MessagesPerLoop := 12 // including delayed
	avgExtraBlocksPerLoop := 20
	finalPropagateLoops := 15

	fundsPerDelayed := big.NewInt(int64(100000000000000001))
	fundsPerDirect := big.NewInt(int64(200003))
	directTransfers := int64(0)
	delayedTransfers := int64(0)

	delayedTxs := make([]*types.Transaction, 0, largeLoops*avgDelayedMessagesPerLoop*2)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	chainConfig, l1NodeConfigA, lifecycleManager, _, dasSignerKey := setupConfigWithDAS(t, ctx, dasModeStr)
	defer lifecycleManager.StopAndWaitUntil(time.Second)

	builder := NewNodeBuilder(ctx).DefaultConfig(t, true)
	builder.nodeConfig = l1NodeConfigA
	builder.chainConfig = chainConfig
	builder.L2Info = nil
	builder.Build(t)
	defer requireClose(t, builder.L1.Stack)

	authorizeDASKeyset(t, ctx, dasSignerKey, builder.L1Info, builder.L1.Client)

	l1NodeConfigBDataAvailability := l1NodeConfigA.DataAvailability
	l1NodeConfigBDataAvailability.RPCAggregator.Enable = false
	testClientB, cleanupB := builder.Build2ndNode(t, &SecondNodeParams{dasConfig: &l1NodeConfigBDataAvailability})
	defer cleanupB()

	builder.L2Info.GenerateAccount("DelayedFaucet")
	builder.L2Info.GenerateAccount("DelayedReceiver")
	builder.L2Info.GenerateAccount("DirectReceiver")

	builder.L2Info.GenerateAccount("ErrorTxSender")

	builder.L2.SendWaitTestTransactions(t, []*types.Transaction{
		builder.L2Info.PrepareTx("Faucet", "ErrorTxSender", builder.L2Info.TransferGas, big.NewInt(l2pricing.InitialBaseFeeWei*int64(builder.L2Info.TransferGas)), nil),
	})

	delayedMsgsToSendMax := big.NewInt(int64(largeLoops * avgDelayedMessagesPerLoop * 10))
	delayedFaucetNeeds := new(big.Int).Mul(new(big.Int).Add(fundsPerDelayed, new(big.Int).SetUint64(l2pricing.InitialBaseFeeWei*100000)), delayedMsgsToSendMax)
	builder.L2.SendWaitTestTransactions(t, []*types.Transaction{
		builder.L2Info.PrepareTx("Faucet", "DelayedFaucet", builder.L2Info.TransferGas, delayedFaucetNeeds, nil),
	})
	delayedFaucetBalance, err := builder.L2.Client.BalanceAt(ctx, builder.L2Info.GetAddress("DelayedFaucet"), nil)
	Require(t, err)

	if delayedFaucetBalance.Cmp(delayedFaucetNeeds) != 0 {
		t.Fatalf("Unexpected balance, has %v, expects %v", delayedFaucetBalance, delayedFaucetNeeds)
	}
	t.Logf("DelayedFaucet has %v, per delayd: %v, baseprice: %v", delayedFaucetBalance, fundsPerDelayed, l2pricing.InitialBaseFeeWei)

	if avgTotalL1MessagesPerLoop < avgDelayedMessagesPerLoop {
		Fatal(t, "bad params, avgTotalL1MessagesPerLoop should include avgDelayedMessagesPerLoop")
	}
	for i := 0; i < largeLoops; i++ {
		l1TxsThisTime := rand.Int() % (avgTotalL1MessagesPerLoop * 2)
		wrappedL1Txs := make([]*txpool.Transaction, 0, l1TxsThisTime)
		for len(wrappedL1Txs) < l1TxsThisTime {
			randNum := rand.Int() % avgTotalL1MessagesPerLoop
			var l1tx *types.Transaction
			if randNum < avgDelayedMessagesPerLoop {
				delayedTx := builder.L2Info.PrepareTx("DelayedFaucet", "DelayedReceiver", 30001, fundsPerDelayed, nil)
				l1tx = WrapL2ForDelayed(t, delayedTx, builder.L1Info, "User", 100000)
				delayedTxs = append(delayedTxs, delayedTx)
				delayedTransfers++
			} else {
				l1tx = builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil)
			}
			wrappedL1Txs = append(wrappedL1Txs, &txpool.Transaction{Tx: l1tx})
		}

		// adding multiple messages in the same Add with local=true to get them in the same L1 block
		errs := builder.L1.L1Backend.TxPool().Add(wrappedL1Txs, true, false)
		for _, err := range errs {
			if err != nil {
				Fatal(t, err)
			}
		}
		l2TxsThisTime := rand.Int() % (avgL2MsgsPerLoop * 2)
		l2Txs := make([]*types.Transaction, 0, l2TxsThisTime)
		for len(l2Txs) < l2TxsThisTime {
			l2Txs = append(l2Txs, builder.L2Info.PrepareTx("Faucet", "DirectReceiver", builder.L2Info.TransferGas, fundsPerDirect, nil))
		}
		builder.L2.SendWaitTestTransactions(t, l2Txs)
		directTransfers += int64(l2TxsThisTime)
		if len(wrappedL1Txs) > 0 {
			_, err := builder.L1.EnsureTxSucceeded(wrappedL1Txs[len(wrappedL1Txs)-1].Tx)
			if err != nil {
				Fatal(t, err)
			}
		}
		// create bad tx on delayed inbox
		builder.L2Info.GetInfoWithPrivKey("ErrorTxSender").Nonce = 10
		builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
			WrapL2ForDelayed(t, builder.L2Info.PrepareTx("ErrorTxSender", "DelayedReceiver", 30002, delayedFaucetNeeds, nil), builder.L1Info, "User", 100000),
		})

		extrBlocksThisTime := rand.Int() % (avgExtraBlocksPerLoop * 2)
		for i := 0; i < extrBlocksThisTime; i++ {
			builder.L1.SendWaitTestTransactions(t, []*types.Transaction{
				builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil),
			})
		}
	}

	t.Log("Done sending", delayedTransfers, "delayed transfers", directTransfers, "direct transfers")
	if (delayedTransfers + directTransfers) == 0 {
		Fatal(t, "No transfers sent!")
	}

	// sending l1 messages creates l1 blocks.. make enough to get that delayed inbox message in
	for i := 0; i < finalPropagateLoops; i++ {
		var tx *types.Transaction
		for j := 0; j < 30; j++ {
			tx = builder.L1Info.PrepareTx("Faucet", "User", 30000, big.NewInt(1e12), nil)
			err := builder.L1.Client.SendTransaction(ctx, tx)
			if err != nil {
				Fatal(t, err)
			}
			_, err = builder.L1.EnsureTxSucceeded(tx)
			if err != nil {
				Fatal(t, err)
			}
		}
	}

	_, err = builder.L2.EnsureTxSucceededWithTimeout(delayedTxs[len(delayedTxs)-1], time.Second*10)
	Require(t, err, "Failed waiting for Tx on main node")

	_, err = testClientB.EnsureTxSucceededWithTimeout(delayedTxs[len(delayedTxs)-1], time.Second*30)
	Require(t, err, "Failed waiting for Tx on secondary node")
	delayedBalance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("DelayedReceiver"), nil)
	Require(t, err)
	directBalance, err := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("DirectReceiver"), nil)
	Require(t, err)
	delayedExpectd := new(big.Int).Mul(fundsPerDelayed, big.NewInt(delayedTransfers))
	directExpectd := new(big.Int).Mul(fundsPerDirect, big.NewInt(directTransfers))
	if (delayedBalance.Cmp(delayedExpectd) != 0) || (directBalance.Cmp(directExpectd) != 0) {
		t.Error("delayed balance", delayedBalance, "expected", delayedExpectd, "transfers", delayedTransfers)
		t.Error("direct balance", directBalance, "expected", directExpectd, "transfers", directTransfers)
		ownerBalance, _ := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("Owner"), nil)
		delayedFaucetBalance, _ := testClientB.Client.BalanceAt(ctx, builder.L2Info.GetAddress("DelayedFaucet"), nil)
		t.Error("owner balance", ownerBalance, "delayed faucet", delayedFaucetBalance)
		Fatal(t, "Unexpected balance")
	}

	builder.L2.ConsensusNode.StopAndWait()

	if testClientB.ConsensusNode.BlockValidator != nil {
		lastBlockHeader, err := testClientB.Client.HeaderByNumber(ctx, nil)
		Require(t, err)
		timeout := getDeadlineTimeout(t, time.Minute*30)
		// messageindex is same as block number here
		if !testClientB.ConsensusNode.BlockValidator.WaitForPos(t, ctx, arbutil.MessageIndex(lastBlockHeader.Number.Uint64()), timeout) {
			Fatal(t, "did not validate all blocks")
		}
	}
}

func TestTwoNodesLong(t *testing.T) {
	testTwoNodesLong(t, "onchain")
}

func TestTwoNodesLongLocalDAS(t *testing.T) {
	testTwoNodesLong(t, "files")
}

'''
'''--- system_tests/validation_mock_test.go ---
package arbtest

import (
	"bytes"
	"context"
	"math/big"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbnode"
	"github.com/offchainlabs/nitro/arbos/arbostypes"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/execution"
	"github.com/offchainlabs/nitro/staker"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/rpcclient"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_api"
	"github.com/offchainlabs/nitro/validator/server_arb"
)

type mockSpawner struct {
	ExecSpawned []uint64
	LaunchDelay time.Duration
}

var blockHashKey = common.HexToHash("0x11223344")
var sendRootKey = common.HexToHash("0x55667788")
var batchNumKey = common.HexToHash("0x99aabbcc")
var posInBatchKey = common.HexToHash("0xddeeff")

func globalstateFromTestPreimages(preimages map[arbutil.PreimageType]map[common.Hash][]byte) validator.GoGlobalState {
	keccakPreimages := preimages[arbutil.Keccak256PreimageType]
	return validator.GoGlobalState{
		Batch:      new(big.Int).SetBytes(keccakPreimages[batchNumKey]).Uint64(),
		PosInBatch: new(big.Int).SetBytes(keccakPreimages[posInBatchKey]).Uint64(),
		BlockHash:  common.BytesToHash(keccakPreimages[blockHashKey]),
		SendRoot:   common.BytesToHash(keccakPreimages[sendRootKey]),
	}
}

func globalstateToTestPreimages(gs validator.GoGlobalState) map[common.Hash][]byte {
	preimages := make(map[common.Hash][]byte)
	preimages[batchNumKey] = new(big.Int).SetUint64(gs.Batch).Bytes()
	preimages[posInBatchKey] = new(big.Int).SetUint64(gs.PosInBatch).Bytes()
	preimages[blockHashKey] = gs.BlockHash[:]
	preimages[sendRootKey] = gs.SendRoot[:]
	return preimages
}

func (s *mockSpawner) Launch(entry *validator.ValidationInput, moduleRoot common.Hash) validator.ValidationRun {
	run := &mockValRun{
		Promise: containers.NewPromise[validator.GoGlobalState](nil),
		root:    moduleRoot,
	}
	<-time.After(s.LaunchDelay)
	run.Produce(globalstateFromTestPreimages(entry.Preimages))
	return run
}

var mockWasmModuleRoot common.Hash = common.HexToHash("0xa5a5a5")

func (s *mockSpawner) Start(context.Context) error { return nil }
func (s *mockSpawner) Stop()                       {}
func (s *mockSpawner) Name() string                { return "mock" }
func (s *mockSpawner) Room() int                   { return 4 }

func (s *mockSpawner) CreateExecutionRun(wasmModuleRoot common.Hash, input *validator.ValidationInput) containers.PromiseInterface[validator.ExecutionRun] {
	s.ExecSpawned = append(s.ExecSpawned, input.Id)
	return containers.NewReadyPromise[validator.ExecutionRun](&mockExecRun{
		startState: input.StartState,
		endState:   globalstateFromTestPreimages(input.Preimages),
	}, nil)
}

func (s *mockSpawner) LatestWasmModuleRoot() containers.PromiseInterface[common.Hash] {
	return containers.NewReadyPromise[common.Hash](mockWasmModuleRoot, nil)
}

func (s *mockSpawner) WriteToFile(input *validator.ValidationInput, expOut validator.GoGlobalState, moduleRoot common.Hash) containers.PromiseInterface[struct{}] {
	return containers.NewReadyPromise[struct{}](struct{}{}, nil)
}

type mockValRun struct {
	containers.Promise[validator.GoGlobalState]
	root common.Hash
}

func (v *mockValRun) WasmModuleRoot() common.Hash { return v.root }
func (v *mockValRun) Close()                      {}

const mockExecLastPos uint64 = 100

type mockExecRun struct {
	startState validator.GoGlobalState
	endState   validator.GoGlobalState
}

func (r *mockExecRun) GetStepAt(position uint64) containers.PromiseInterface[*validator.MachineStepResult] {
	status := validator.MachineStatusRunning
	resState := r.startState
	if position >= mockExecLastPos {
		position = mockExecLastPos
		status = validator.MachineStatusFinished
		resState = r.endState
	}
	return containers.NewReadyPromise[*validator.MachineStepResult](&validator.MachineStepResult{
		Hash:        crypto.Keccak256Hash(new(big.Int).SetUint64(position).Bytes()),
		Position:    position,
		Status:      status,
		GlobalState: resState,
	}, nil)
}

func (r *mockExecRun) GetLastStep() containers.PromiseInterface[*validator.MachineStepResult] {
	return r.GetStepAt(mockExecLastPos)
}

var mockProof []byte = []byte("friendly jab at competitors")

func (r *mockExecRun) GetProofAt(uint64) containers.PromiseInterface[[]byte] {
	return containers.NewReadyPromise[[]byte](mockProof, nil)
}

func (r *mockExecRun) PrepareRange(uint64, uint64) containers.PromiseInterface[struct{}] {
	return containers.NewReadyPromise[struct{}](struct{}{}, nil)
}

func (r *mockExecRun) Close() {}

func createMockValidationNode(t *testing.T, ctx context.Context, config *server_arb.ArbitratorSpawnerConfig) (*mockSpawner, *node.Node) {
	stackConf := node.DefaultConfig
	stackConf.HTTPPort = 0
	stackConf.DataDir = ""
	stackConf.WSHost = "127.0.0.1"
	stackConf.WSPort = 0
	stackConf.WSModules = []string{server_api.Namespace}
	stackConf.P2P.NoDiscovery = true
	stackConf.P2P.ListenAddr = ""

	stack, err := node.New(&stackConf)
	Require(t, err)

	if config == nil {
		config = &server_arb.DefaultArbitratorSpawnerConfig
	}
	configFetcher := func() *server_arb.ArbitratorSpawnerConfig { return config }
	spawner := &mockSpawner{}
	serverAPI := server_api.NewExecutionServerAPI(spawner, spawner, configFetcher)

	valAPIs := []rpc.API{{
		Namespace:     server_api.Namespace,
		Version:       "1.0",
		Service:       serverAPI,
		Public:        true,
		Authenticated: false,
	}}
	stack.RegisterAPIs(valAPIs)

	err = stack.Start()
	Require(t, err)

	serverAPI.Start(ctx)

	go func() {
		<-ctx.Done()
		stack.Close()
		serverAPI.StopOnly()
	}()

	return spawner, stack
}

// mostly tests translation to/from json and running over network
func TestValidationServerAPI(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	_, validationDefault := createMockValidationNode(t, ctx, nil)
	client := server_api.NewExecutionClient(StaticFetcherFrom(t, &rpcclient.TestClientConfig), validationDefault)
	err := client.Start(ctx)
	Require(t, err)

	wasmRoot, err := client.LatestWasmModuleRoot().Await(ctx)
	Require(t, err)

	if wasmRoot != mockWasmModuleRoot {
		t.Error("unexpected mock wasmModuleRoot")
	}

	hash1 := common.HexToHash("0x11223344556677889900aabbccddeeff")
	hash2 := common.HexToHash("0x11111111122222223333333444444444")

	startState := validator.GoGlobalState{
		BlockHash:  hash1,
		SendRoot:   hash2,
		Batch:      300,
		PosInBatch: 3000,
	}
	endState := validator.GoGlobalState{
		BlockHash:  hash2,
		SendRoot:   hash1,
		Batch:      3000,
		PosInBatch: 300,
	}

	valInput := validator.ValidationInput{
		StartState: startState,
		Preimages: map[arbutil.PreimageType]map[common.Hash][]byte{
			arbutil.Keccak256PreimageType: globalstateToTestPreimages(endState),
		},
	}
	valRun := client.Launch(&valInput, wasmRoot)
	res, err := valRun.Await(ctx)
	Require(t, err)
	if res != endState {
		t.Error("unexpected mock validation run")
	}
	execRun, err := client.CreateExecutionRun(wasmRoot, &valInput).Await(ctx)
	Require(t, err)
	step0 := execRun.GetStepAt(0)
	step0Res, err := step0.Await(ctx)
	Require(t, err)
	if step0Res.GlobalState != startState {
		t.Error("unexpected globalstate on execution start")
	}
	lasteStep := execRun.GetLastStep()
	lasteStepRes, err := lasteStep.Await(ctx)
	Require(t, err)
	if lasteStepRes.GlobalState != endState {
		t.Error("unexpected globalstate on execution end")
	}
	proofPromise := execRun.GetProofAt(0)
	proof, err := proofPromise.Await(ctx)
	Require(t, err)
	if !bytes.Equal(proof, mockProof) {
		t.Error("mock proof not expected")
	}
}

func TestValidationClientRoom(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	mockSpawner, spawnerStack := createMockValidationNode(t, ctx, nil)
	client := server_api.NewExecutionClient(StaticFetcherFrom(t, &rpcclient.TestClientConfig), spawnerStack)
	err := client.Start(ctx)
	Require(t, err)

	wasmRoot, err := client.LatestWasmModuleRoot().Await(ctx)
	Require(t, err)

	if client.Room() != 4 {
		Fatal(t, "wrong initial room ", client.Room())
	}

	hash1 := common.HexToHash("0x11223344556677889900aabbccddeeff")
	hash2 := common.HexToHash("0x11111111122222223333333444444444")

	startState := validator.GoGlobalState{
		BlockHash:  hash1,
		SendRoot:   hash2,
		Batch:      300,
		PosInBatch: 3000,
	}
	endState := validator.GoGlobalState{
		BlockHash:  hash2,
		SendRoot:   hash1,
		Batch:      3000,
		PosInBatch: 300,
	}

	valInput := validator.ValidationInput{
		StartState: startState,
		Preimages: map[arbutil.PreimageType]map[common.Hash][]byte{
			arbutil.Keccak256PreimageType: globalstateToTestPreimages(endState),
		},
	}

	valRuns := make([]validator.ValidationRun, 0, 4)

	for i := 0; i < 4; i++ {
		valRun := client.Launch(&valInput, wasmRoot)
		valRuns = append(valRuns, valRun)
	}

	for i := range valRuns {
		_, err := valRuns[i].Await(ctx)
		Require(t, err)
	}

	if client.Room() != 4 {
		Fatal(t, "wrong room after launch", client.Room())
	}

	mockSpawner.LaunchDelay = time.Hour

	valRuns = make([]validator.ValidationRun, 0, 3)

	for i := 0; i < 4; i++ {
		valRun := client.Launch(&valInput, wasmRoot)
		valRuns = append(valRuns, valRun)
		room := client.Room()
		if room != 3-i {
			Fatal(t, "wrong room after launch ", room, " expected: ", 4-i)
		}
	}

	for i := range valRuns {
		valRuns[i].Cancel()
		_, err := valRuns[i].Await(ctx)
		if err == nil {
			Fatal(t, "no error returned after cancel i:", i)
		}
	}

	room := client.Room()
	if room != 4 {
		Fatal(t, "wrong room after canceling runs: ", room)
	}
}

func TestExecutionKeepAlive(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	_, validationDefault := createMockValidationNode(t, ctx, nil)
	shortTimeoutConfig := server_arb.DefaultArbitratorSpawnerConfig
	shortTimeoutConfig.ExecutionRunTimeout = time.Second
	_, validationShortTO := createMockValidationNode(t, ctx, &shortTimeoutConfig)
	configFetcher := StaticFetcherFrom(t, &rpcclient.TestClientConfig)

	clientDefault := server_api.NewExecutionClient(configFetcher, validationDefault)
	err := clientDefault.Start(ctx)
	Require(t, err)
	clientShortTO := server_api.NewExecutionClient(configFetcher, validationShortTO)
	err = clientShortTO.Start(ctx)
	Require(t, err)

	wasmRoot, err := clientDefault.LatestWasmModuleRoot().Await(ctx)
	Require(t, err)

	valInput := validator.ValidationInput{}
	runDefault, err := clientDefault.CreateExecutionRun(wasmRoot, &valInput).Await(ctx)
	Require(t, err)
	runShortTO, err := clientShortTO.CreateExecutionRun(wasmRoot, &valInput).Await(ctx)
	Require(t, err)
	<-time.After(time.Second * 10)
	stepDefault := runDefault.GetStepAt(0)
	stepTO := runShortTO.GetStepAt(0)

	_, err = stepDefault.Await(ctx)
	Require(t, err)
	_, err = stepTO.Await(ctx)
	if err == nil {
		t.Error("getStep should have timed out but didn't")
	}
}

type mockBlockRecorder struct {
	validator *staker.StatelessBlockValidator
	streamer  *arbnode.TransactionStreamer
}

func (m *mockBlockRecorder) RecordBlockCreation(
	ctx context.Context,
	pos arbutil.MessageIndex,
	msg *arbostypes.MessageWithMetadata,
) (*execution.RecordResult, error) {
	_, globalpos, err := m.validator.GlobalStatePositionsAtCount(pos + 1)
	if err != nil {
		return nil, err
	}
	res, err := m.streamer.ResultAtCount(pos + 1)
	if err != nil {
		return nil, err
	}
	globalState := validator.GoGlobalState{
		Batch:      globalpos.BatchNumber,
		PosInBatch: globalpos.PosInBatch,
		BlockHash:  res.BlockHash,
		SendRoot:   res.SendRoot,
	}
	return &execution.RecordResult{
		Pos:       pos,
		BlockHash: res.BlockHash,
		Preimages: globalstateToTestPreimages(globalState),
	}, nil
}

func (m *mockBlockRecorder) MarkValid(pos arbutil.MessageIndex, resultHash common.Hash) {}
func (m *mockBlockRecorder) PrepareForRecord(ctx context.Context, start, end arbutil.MessageIndex) error {
	return nil
}

func newMockRecorder(validator *staker.StatelessBlockValidator, streamer *arbnode.TransactionStreamer) *mockBlockRecorder {
	return &mockBlockRecorder{validator, streamer}
}

'''
'''--- system_tests/validator_reorg_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build validatorreorgtest
// +build validatorreorgtest

package arbtest

import "testing"

func TestBlockValidatorReorg(t *testing.T) {
	testSequencerInboxReaderImpl(t, true)
}

'''
'''--- system_tests/wrap_transaction_test.go ---
//
// Copyright 2021-2022, Offchain Labs, Inc. All rights reserved.
//

package arbtest

import (
	"context"
	"fmt"
	"math/big"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/solgen/go/precompilesgen"
	"github.com/offchainlabs/nitro/util/headerreader"
)

func GetPendingBlockNumber(ctx context.Context, client arbutil.L1Interface) (*big.Int, error) {
	// Attempt to get the block number from ArbSys, if it exists
	arbSys, err := precompilesgen.NewArbSys(common.BigToAddress(big.NewInt(100)), client)
	if err != nil {
		return arbutil.GetPendingCallBlockNumber(ctx, client)
	}
	blockNum, err := arbSys.ArbBlockNumber(&bind.CallOpts{Context: ctx})
	if err != nil {
		return arbutil.GetPendingCallBlockNumber(ctx, client)
	}
	// Arbitrum chains don't have miners, so they're one block behind non-Arbitrum chains.
	return blockNum.Add(blockNum, common.Big1), nil
}

// Will wait until txhash is in the blockchain and return its receipt
func WaitForTx(ctxinput context.Context, client arbutil.L1Interface, txhash common.Hash, timeout time.Duration) (*types.Receipt, error) {
	ctx, cancel := context.WithTimeout(ctxinput, timeout)
	defer cancel()

	chanHead, cancel := HeaderSubscribeWithRetry(ctx, client)
	defer cancel()

	checkInterval := timeout / 50
	if checkInterval > time.Second {
		checkInterval = time.Second
	}
	for {
		receipt, err := client.TransactionReceipt(ctx, txhash)
		if err == nil && receipt != nil {
			// For some reason, Geth has a weird property of giving out receipts and updating the latest block number
			// before calls will actually use the new block's state as pending. This leads to failures down the line,
			// as future calls/gas estimations will use a state before a transaction that is thought to have succeeded.
			// To prevent this, we do an eth_call to check what the pending state's block number is before returning the receipt.
			blockNumber, err := GetPendingBlockNumber(ctx, client)
			if err != nil {
				return nil, err
			}
			if blockNumber.Cmp(receipt.BlockNumber) > 0 {
				// The latest pending state contains the state of our transaction.
				return receipt, nil
			}
		}
		// Note: time.After won't free the timer until after it expires.
		// However, that's fine here, as checkInterval is at most a second.
		select {
		case <-chanHead:
		case <-time.After(checkInterval):
		case <-ctx.Done():
			return nil, ctx.Err()
		}
	}
}

func EnsureTxSucceeded(ctx context.Context, client arbutil.L1Interface, tx *types.Transaction) (*types.Receipt, error) {
	return EnsureTxSucceededWithTimeout(ctx, client, tx, time.Second*5)
}

func EnsureTxSucceededWithTimeout(ctx context.Context, client arbutil.L1Interface, tx *types.Transaction, timeout time.Duration) (*types.Receipt, error) {
	receipt, err := WaitForTx(ctx, client, tx.Hash(), timeout)
	if err != nil {
		return nil, fmt.Errorf("waitFoxTx (tx=%s) got: %w", tx.Hash().Hex(), err)
	}
	if receipt.Status == types.ReceiptStatusSuccessful && tx.ChainId().Cmp(simulatedChainID) == 0 {
		for {
			safeBlock, err := client.HeaderByNumber(ctx, big.NewInt(int64(rpc.SafeBlockNumber)))
			if err != nil {
				return receipt, err
			}
			if safeBlock.Number.Cmp(receipt.BlockNumber) >= 0 {
				break
			}
			select {
			case <-time.After(headerreader.TestConfig.Dangerous.WaitForTxApprovalSafePoll):
			case <-ctx.Done():
				return receipt, ctx.Err()
			}
		}
	}
	return receipt, arbutil.DetailTxError(ctx, client, tx, receipt)
}

func headerSubscribeMainLoop(chanOut chan<- *types.Header, ctx context.Context, client ethereum.ChainReader) {
	headerSubscription, err := client.SubscribeNewHead(ctx, chanOut)
	if err != nil {
		if ctx.Err() == nil {
			log.Error("failed subscribing to header", "err", err)
		}
		return
	}

	for {
		select {
		case err := <-headerSubscription.Err():
			if ctx.Err() == nil {
				return
			}
			log.Warn("error in subscription to L1 headers", "err", err)
			for {
				headerSubscription, err = client.SubscribeNewHead(ctx, chanOut)
				if err != nil {
					log.Warn("error re-subscribing to L1 headers", "err", err)
					select {
					case <-ctx.Done():
						return
					case <-time.After(time.Second):
					}
				} else {
					break
				}
			}
		case <-ctx.Done():
			headerSubscription.Unsubscribe()
			return
		}
	}
}

// returned channel will reconnect to client if disconnected, until context closed or cancel called
func HeaderSubscribeWithRetry(ctx context.Context, client ethereum.ChainReader) (<-chan *types.Header, context.CancelFunc) {
	chanOut := make(chan *types.Header)

	childCtx, cancelFunc := context.WithCancel(ctx)
	go headerSubscribeMainLoop(chanOut, childCtx, client)

	return chanOut, cancelFunc
}

'''
'''--- util/arbmath/bips.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbmath

import "math/big"

type Bips int64

const OneInBips Bips = 10000

func NaturalToBips(natural int64) Bips {
	return Bips(SaturatingMul(natural, int64(OneInBips)))
}

func PercentToBips(percentage int64) Bips {
	return Bips(SaturatingMul(percentage, 100))
}

func BigToBips(natural *big.Int) Bips {
	return Bips(natural.Uint64())
}

func BigMulByBips(value *big.Int, bips Bips) *big.Int {
	return BigMulByFrac(value, int64(bips), int64(OneInBips))
}

func IntMulByBips(value int64, bips Bips) int64 {
	return value * int64(bips) / int64(OneInBips)
}

func UintMulByBips(value uint64, bips Bips) uint64 {
	return value * uint64(bips) / uint64(OneInBips)
}

func SaturatingCastToBips(value uint64) Bips {
	return Bips(SaturatingCast(value))
}

'''
'''--- util/arbmath/bits.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbmath

import (
	"encoding/binary"

	"github.com/ethereum/go-ethereum/common"
)

type bytes32 = common.Hash

// FlipBit flips the nth bit in an ethereum word, starting from the left
func FlipBit(data bytes32, bit byte) bytes32 {
	data[bit/8] ^= 1 << (7 - bit%8)
	return data
}

// ConcatByteSlices unrolls a series of slices into a singular, concatenated slice
func ConcatByteSlices(slices ...[]byte) []byte {
	unrolled := []byte{}
	for _, slice := range slices {
		unrolled = append(unrolled, slice...)
	}
	return unrolled
}

// WordsForBytes returns the number of eth-words needed to store n bytes
func WordsForBytes(nbytes uint64) uint64 {
	return (nbytes + 31) / 32
}

// UintToBytes casts a uint64 to its big-endian representation
func UintToBytes(value uint64) []byte {
	result := make([]byte, 8)
	binary.BigEndian.PutUint64(result, value)
	return result
}

// Uint32ToBytes casts a uint32 to its big-endian representation
func Uint32ToBytes(value uint32) []byte {
	result := make([]byte, 4)
	binary.BigEndian.PutUint32(result, value)
	return result
}

'''
'''--- util/arbmath/math.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbmath

import (
	"math"
	"math/big"
	"math/bits"

	"github.com/ethereum/go-ethereum/params"
)

// NextPowerOf2 the smallest power of two greater than the input
func NextPowerOf2(value uint64) uint64 {
	return 1 << Log2ceil(value)
}

// NextOrCurrentPowerOf2 the smallest power of no less than the input
func NextOrCurrentPowerOf2(value uint64) uint64 {
	power := NextPowerOf2(value)
	if power == 2*value {
		power /= 2
	}
	return power
}

// Log2ceil the log2 of the int, rounded up
func Log2ceil(value uint64) uint64 {
	return uint64(64 - bits.LeadingZeros64(value))
}

type Signed interface {
	~int | ~int8 | ~int16 | ~int32 | ~int64
}

type Unsigned interface {
	~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 | ~uintptr
}

type Integer interface {
	Signed | Unsigned
}

type Float interface {
	~float32 | ~float64
}

// Ordered is anything that implements comparison operators such as `<` and `>`.
// Unfortunately, that doesn't include big ints.
type Ordered interface {
	Integer | Float
}

// MinInt the minimum of two ints
func MinInt[T Ordered](value, ceiling T) T {
	if value > ceiling {
		return ceiling
	}
	return value
}

// MaxInt the maximum of two ints
func MaxInt[T Ordered](value, floor T) T {
	if value < floor {
		return floor
	}
	return value
}

// UintToBig casts an int to a huge
func UintToBig(value uint64) *big.Int {
	return new(big.Int).SetUint64(value)
}

// FloatToBig casts a float to a huge
func FloatToBig(value float64) *big.Int {
	return new(big.Int).SetInt64(int64(value))
}

// UintToBigFloat casts a uint to a big float
func UintToBigFloat(value uint64) *big.Float {
	return new(big.Float).SetPrec(53).SetUint64(value)
}

// BigToUintSaturating casts a huge to a uint, saturating if out of bounds
func BigToUintSaturating(value *big.Int) uint64 {
	if value.Sign() < 0 {
		return 0
	}
	if !value.IsUint64() {
		return math.MaxUint64
	}
	return value.Uint64()
}

// BigToUintOrPanic casts a huge to a uint, panicking if out of bounds
func BigToUintOrPanic(value *big.Int) uint64 {
	if value.Sign() < 0 {
		panic("big.Int value is less than 0")
	}
	if !value.IsUint64() {
		panic("big.Int value exceeds the max Uint64")
	}
	return value.Uint64()
}

// UfracToBigFloat casts an rational to a big float
func UfracToBigFloat(numerator, denominator uint64) *big.Float {
	float := new(big.Float)
	float.Quo(UintToBigFloat(numerator), UintToBigFloat(denominator))
	return float
}

// BigEquals check huge equality
func BigEquals(first, second *big.Int) bool {
	return first.Cmp(second) == 0
}

// BigLessThan check if a huge is less than another
func BigLessThan(first, second *big.Int) bool {
	return first.Cmp(second) < 0
}

// BigGreaterThan check if a huge is greater than another
func BigGreaterThan(first, second *big.Int) bool {
	return first.Cmp(second) > 0
}

// BigMin returns a clone of the minimum of two big integers
func BigMin(first, second *big.Int) *big.Int {
	if BigLessThan(first, second) {
		return new(big.Int).Set(first)
	} else {
		return new(big.Int).Set(second)
	}
}

// BigMax returns a clone of the maximum of two big integers
func BigMax(first, second *big.Int) *big.Int {
	if BigGreaterThan(first, second) {
		return new(big.Int).Set(first)
	} else {
		return new(big.Int).Set(second)
	}
}

// BigAdd add a huge to another
func BigAdd(augend *big.Int, addend *big.Int) *big.Int {
	return new(big.Int).Add(augend, addend)
}

// BigSub subtract from a huge another
func BigSub(minuend *big.Int, subtrahend *big.Int) *big.Int {
	return new(big.Int).Sub(minuend, subtrahend)
}

// BigMul multiply a huge by another
func BigMul(multiplicand *big.Int, multiplier *big.Int) *big.Int {
	return new(big.Int).Mul(multiplicand, multiplier)
}

// BigDiv divide a huge by another
func BigDiv(dividend *big.Int, divisor *big.Int) *big.Int {
	return new(big.Int).Div(dividend, divisor)
}

// BigAbs absolute value of a huge
func BigAbs(value *big.Int) *big.Int {
	return new(big.Int).Abs(value)
}

// BigAddByUint add a uint to a huge
func BigAddByUint(augend *big.Int, addend uint64) *big.Int {
	return new(big.Int).Add(augend, UintToBig(addend))
}

// BigSub subtracts a uint from a huge
func BigSubByUint(minuend *big.Int, subtrahend uint64) *big.Int {
	return new(big.Int).Sub(minuend, UintToBig(subtrahend))
}

// BigMulByFrac multiply a huge by a rational
func BigMulByFrac(value *big.Int, numerator, denominator int64) *big.Int {
	value = new(big.Int).Set(value)
	value.Mul(value, big.NewInt(numerator))
	value.Div(value, big.NewInt(denominator))
	return value
}

// BigMulByUfrac multiply a huge by a rational whose components are non-negative
func BigMulByUfrac(value *big.Int, numerator, denominator uint64) *big.Int {
	value = new(big.Int).Set(value)
	value.Mul(value, new(big.Int).SetUint64(numerator))
	value.Div(value, new(big.Int).SetUint64(denominator))
	return value
}

// BigMulByInt multiply a huge by an integer
func BigMulByInt(multiplicand *big.Int, multiplier int64) *big.Int {
	return new(big.Int).Mul(multiplicand, big.NewInt(multiplier))
}

// BigMulByUint multiply a huge by a unsigned integer
func BigMulByUint(multiplicand *big.Int, multiplier uint64) *big.Int {
	return new(big.Int).Mul(multiplicand, new(big.Int).SetUint64(multiplier))
}

// BigDivByUint divide a huge by an unsigned integer
func BigDivByUint(dividend *big.Int, divisor uint64) *big.Int {
	return BigDiv(dividend, UintToBig(divisor))
}

// BigDivByInt divide a huge by an integer
func BigDivByInt(dividend *big.Int, divisor int64) *big.Int {
	return BigDiv(dividend, big.NewInt(divisor))
}

// BigAddFloat add two big floats together
func BigAddFloat(augend, addend *big.Float) *big.Float {
	return new(big.Float).Add(augend, addend)
}

// BigMulFloat multiply a big float by another
func BigMulFloat(multiplicand, multiplier *big.Float) *big.Float {
	return new(big.Float).Mul(multiplicand, multiplier)
}

// BigFloatMulByUint multiply a big float by an unsigned integer
func BigFloatMulByUint(multiplicand *big.Float, multiplier uint64) *big.Float {
	return new(big.Float).Mul(multiplicand, UintToBigFloat(multiplier))
}

// SaturatingAdd add two int64's without overflow
func SaturatingAdd(augend, addend int64) int64 {
	sum := augend + addend
	if addend > 0 && sum < augend {
		sum = math.MaxInt64
	}
	if addend < 0 && sum > augend {
		sum = math.MinInt64
	}
	return sum
}

// SaturatingUAdd add two uint64's without overflow
func SaturatingUAdd(augend uint64, addend uint64) uint64 {
	sum := augend + addend
	if sum < augend || sum < addend {
		sum = math.MaxUint64
	}
	return sum
}

// SaturatingSub subtract an int64 from another without overflow
func SaturatingSub(minuend, subtrahend int64) int64 {
	return SaturatingAdd(minuend, -subtrahend)
}

// SaturatingUSub subtract a uint64 from another without underflow
func SaturatingUSub(minuend uint64, subtrahend uint64) uint64 {
	if subtrahend >= minuend {
		return 0
	}
	return minuend - subtrahend
}

// SaturatingUMul multiply two uint64's without overflow
func SaturatingUMul(multiplicand uint64, multiplier uint64) uint64 {
	product := multiplicand * multiplier
	if multiplier != 0 && product/multiplier != multiplicand {
		product = math.MaxUint64
	}
	return product
}

// SaturatingMul multiply two int64's without over/underflow
func SaturatingMul(multiplicand int64, multiplier int64) int64 {
	product := multiplicand * multiplier
	if multiplier != 0 && product/multiplier != multiplicand {
		if (multiplicand > 0 && multiplier > 0) || (multiplicand < 0 && multiplier < 0) {
			product = math.MaxInt64
		} else {
			product = math.MinInt64
		}
	}
	return product
}

// SaturatingCast cast a uint64 to an int64, clipping to [0, 2^63-1]
func SaturatingCast(value uint64) int64 {
	if value > math.MaxInt64 {
		return math.MaxInt64
	}
	return int64(value)
}

// SaturatingUCast cast an int64 to a uint64, clipping to [0, 2^63-1]
func SaturatingUCast(value int64) uint64 {
	if value < 0 {
		return 0
	}
	return uint64(value)
}

func SaturatingCastToUint(value *big.Int) uint64 {
	if value.Sign() < 0 {
		return 0
	}
	if !value.IsUint64() {
		return math.MaxUint64
	}
	return value.Uint64()
}

// ApproxExpBasisPoints return the Maclaurin series approximation of e^x, where x is denominated in basis points.
// This quartic polynomial will underestimate e^x by about 5% as x approaches 20000 bips.
func ApproxExpBasisPoints(value Bips) Bips {
	input := value
	negative := value < 0
	if negative {
		input = -value
	}
	x := uint64(input)

	bips := uint64(OneInBips)
	res := bips + x/4
	res = bips + SaturatingUMul(res, x)/(3*bips)
	res = bips + SaturatingUMul(res, x)/(2*bips)
	res = bips + SaturatingUMul(res, x)/(1*bips)
	if negative {
		return Bips(SaturatingCast(bips * bips / res))
	} else {
		return Bips(SaturatingCast(res))
	}
}

// ApproxSquareRoot return the Newton's method approximation of sqrt(x)
// The error should be no more than 1 for values up to 2^63
func ApproxSquareRoot(value uint64) uint64 {

	if value == 0 {
		return 0
	}

	// ensure our starting approximation's square exceeds the value
	approx := value
	for SaturatingUMul(approx, approx)/2 > value {
		approx /= 2
	}

	for i := 0; i < 4; i++ {
		if approx > value/approx {
			diff := approx - value/approx
			approx = SaturatingUAdd(value/approx, diff/2)
		} else {
			diff := value/approx - approx
			approx = SaturatingUAdd(approx, diff/2)
		}
	}
	return approx
}

// SquareUint returns square of uint
func SquareUint(value uint64) uint64 {
	return value * value
}

// SquareFloat returns square of float
func SquareFloat(value float64) float64 {
	return value * value
}

// BalancePerEther returns balance per ether.
func BalancePerEther(balance *big.Int) float64 {
	balancePerEther, _ := new(big.Float).Quo(new(big.Float).SetInt(balance), new(big.Float).SetFloat64(params.Ether)).Float64()
	return balancePerEther
}

'''
'''--- util/arbmath/math_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package arbmath

import (
	"math"
	"math/rand"
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestMath(t *testing.T) {
	cases := []uint64{0, 1, 2, 3, 4, 7, 13, 28, 64}
	correctPower := []uint64{1, 2, 4, 4, 8, 8, 16, 32, 128}
	correctLog := []uint64{0, 1, 2, 2, 3, 3, 4, 5, 7}

	for i := 0; i < len(cases); i++ {
		calculated := NextPowerOf2(cases[i])
		if calculated != correctPower[i] {
			Fail(t, "expected power", correctPower[i], "but got", calculated)
		}
		calculated = Log2ceil(cases[i])
		if calculated != correctLog[i] {
			Fail(t, "expected log", correctLog[i], "but got", calculated)
		}
	}

	// try large random sqrts
	for i := 0; i < 100000; i++ {
		input := rand.Uint64() / 256
		approx := ApproxSquareRoot(input)
		correct := math.Sqrt(float64(input))
		diff := int(approx) - int(correct)
		if diff < -1 || diff > 1 {
			Fail(t, "sqrt approximation off by too much", diff, input, approx, correct)
		}
	}

	// try the first million sqrts
	for i := 0; i < 1000000; i++ {
		input := uint64(i)
		approx := ApproxSquareRoot(input)
		correct := math.Sqrt(float64(input))
		diff := int(approx) - int(correct)
		if diff < 0 || diff > 1 {
			Fail(t, "sqrt approximation off by too much", diff, input, approx, correct)
		}
	}

	// try powers of 2
	for i := 0; i < 63; i++ {
		input := uint64(1 << i)
		approx := ApproxSquareRoot(input)
		correct := math.Sqrt(float64(input))
		diff := int(approx) - int(correct)
		if diff != 0 {
			Fail(t, "incorrect", "2^", i, diff, approx, correct)
		}
	}
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- util/colors/colors.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package colors

import "fmt"

var Red = "\033[31;1m"
var Blue = "\033[34;1m"
var Yellow = "\033[33;1m"
var Pink = "\033[38;5;161;1m"
var Mint = "\033[38;5;48;1m"
var Grey = "\033[90m"

var Lime = "\033[38;5;119;1m"
var Lavender = "\033[38;5;183;1m"
var Maroon = "\033[38;5;124;1m"
var Orange = "\033[38;5;202;1m"

var Clear = "\033[0;0m"

func PrintBlue(args ...interface{}) {
	print(Blue)
	fmt.Print(args...)
	println(Clear)
}

func PrintGrey(args ...interface{}) {
	print(Grey)
	fmt.Print(args...)
	println(Clear)
}

func PrintMint(args ...interface{}) {
	print(Mint)
	fmt.Print(args...)
	println(Clear)
}

func PrintRed(args ...interface{}) {
	print(Red)
	fmt.Print(args...)
	println(Clear)
}

func PrintYellow(args ...interface{}) {
	print(Yellow)
	fmt.Print(args...)
	println(Clear)
}

'''
'''--- util/containers/lru.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package containers

import (
	"github.com/hashicorp/golang-lru/v2/simplelru"
)

// Not thread safe!
// Unlike simplelru, a zero or negative size means it has no capacity and is always empty.
type LruCache[K comparable, V any] struct {
	inner   *simplelru.LRU[K, V]
	onEvict func(key K, value V)
	size    int
}

func NewLruCache[K comparable, V any](size int) *LruCache[K, V] {
	return NewLruCacheWithOnEvict[K, V](size, nil)
}

func NewLruCacheWithOnEvict[K comparable, V any](size int, onEvict func(K, V)) *LruCache[K, V] {
	var inner *simplelru.LRU[K, V]
	if size > 0 {
		// Can't fail because newSize > 0
		inner, _ = simplelru.NewLRU(size, onEvict)
	}
	return &LruCache[K, V]{
		inner:   inner,
		onEvict: onEvict,
		size:    size,
	}
}

// Returns true if an item was evicted
func (c *LruCache[K, V]) Add(key K, value V) bool {
	if c.inner == nil {
		return true
	}
	return c.inner.Add(key, value)
}

func (c *LruCache[K, V]) Get(key K) (V, bool) {
	var empty V
	if c.inner == nil {
		return empty, false
	}
	return c.inner.Get(key)
}

func (c *LruCache[K, V]) Contains(key K) bool {
	if c.inner == nil {
		return false
	}
	return c.inner.Contains(key)
}

func (c *LruCache[K, V]) Remove(key K) {
	if c.inner == nil {
		return
	}
	c.inner.Remove(key)
}

func (c *LruCache[K, V]) GetOldest() (K, V, bool) {
	var emptyKey K
	var emptyValue V
	if c.inner == nil {
		return emptyKey, emptyValue, false
	}
	return c.inner.GetOldest()
}

func (c *LruCache[K, V]) RemoveOldest() {
	if c.inner == nil {
		return
	}
	c.inner.RemoveOldest()
}

func (c *LruCache[K, V]) Len() int {
	if c.inner == nil {
		return 0
	}
	return c.inner.Len()
}

func (c *LruCache[K, V]) Size() int {
	return c.size
}

func (c *LruCache[K, V]) Clear() {
	if c.inner == nil {
		return
	}
	c.inner.Purge()
}

func (c *LruCache[K, V]) Resize(newSize int) {
	if newSize <= 0 {
		if c.inner != nil && c.onEvict != nil {
			c.inner.Purge() // run the evict functions
		}
		c.inner = nil
	} else if c.inner == nil {
		// Can't fail because newSize > 0
		c.inner, _ = simplelru.NewLRU(newSize, c.onEvict)
	} else {
		c.inner.Resize(newSize)
	}
	c.size = newSize
}

'''
'''--- util/containers/promise.go ---
package containers

import (
	"context"
	"errors"
	"sync/atomic"
)

type PromiseInterface[R any] interface {
	Ready() bool
	ReadyChan() chan struct{}
	Await(ctx context.Context) (R, error)
	Current() (R, error) // doesn't wait
	Cancel()
}

var ErrNotReady error = errors.New("not ready")

type Promise[R any] struct {
	chanReady chan struct{}
	result    R
	err       error
	produced  uint32
	cancel    func()
}

func (p *Promise[R]) Ready() bool {
	select {
	case <-p.chanReady:
		return true
	default:
		return false
	}
}

func (p *Promise[R]) ReadyChan() chan struct{} {
	return p.chanReady
}

func (p *Promise[R]) Await(ctx context.Context) (R, error) {
	select {
	case <-p.chanReady:
		return p.result, p.err
	case <-ctx.Done():
		var empty R
		p.Cancel()
		return empty, ctx.Err()
	}
}

func (p *Promise[R]) Current() (R, error) {
	if !p.Ready() {
		var empty R
		return empty, ErrNotReady
	}
	return p.result, p.err
}

func (p *Promise[R]) Cancel() {
	if p.cancel == nil {
		return
	}
	if p.Ready() {
		return
	}
	p.cancel()
}

func (p *Promise[R]) ProduceErrorSafe(err error) error {
	if !atomic.CompareAndSwapUint32(&p.produced, 0, 1) {
		return errors.New("cannot produce two values")
	}
	p.err = err
	close(p.chanReady)
	return nil
}

func (p *Promise[R]) ProduceError(err error) {
	errSafe := p.ProduceErrorSafe(err)
	if errSafe != nil {
		panic(errSafe)
	}
}

func (p *Promise[R]) ProduceSafe(value R) error {
	if !atomic.CompareAndSwapUint32(&p.produced, 0, 1) {
		return errors.New("cannot produce two values")
	}
	p.result = value
	close(p.chanReady)
	return nil
}

func (p *Promise[R]) Produce(value R) {
	errSafe := p.ProduceSafe(value)
	if errSafe != nil {
		panic(errSafe)
	}
}

// cancel might be called multiple times while no value or error produced
// cancel will be called by Await if it's context is done
func NewPromise[R any](cancel func()) Promise[R] {
	return Promise[R]{
		chanReady: make(chan struct{}),
		cancel:    cancel,
	}
}

func NewReadyPromise[R any](val R, err error) PromiseInterface[R] {
	promise := NewPromise[R](nil)
	if err == nil {
		promise.Produce(val)
	} else {
		promise.ProduceError(err)
	}
	return &promise
}

'''
'''--- util/containers/promise_test.go ---
package containers

import (
	"context"
	"errors"
	"sync"
	"sync/atomic"
	"testing"
	"time"
)

func TestPromise(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	tempPromise := NewPromise[int](nil)

	tempPromise.Produce(1)
	res, err := tempPromise.Await(ctx)
	if res != 1 || err != nil {
		t.Fatal("unexpected Promise.Await")
	}
	res, err = tempPromise.Current()
	if res != 1 || err != nil {
		t.Fatal("unexpected Promise.Current when ready")
	}

	cancelCalled := int64(0)
	cancelFunc := func() { atomic.AddInt64(&cancelCalled, 1) }

	tempPromise = NewPromise[int](cancelFunc)
	res, err = tempPromise.Current()
	if res != 0 || !errors.Is(err, ErrNotReady) {
		t.Fatal("unexpected Promise.Current when not ready")
	}

	var wg sync.WaitGroup
	wg.Add(1)
	go func() {
		res, err = tempPromise.Await(ctx)
		wg.Done()
	}()
	tempPromise.Produce(2)
	wg.Wait()
	if res != 2 || err != nil {
		t.Fatal("unexpected Promise.Await in parallel")
	}
	res, err = tempPromise.Current()
	if res != 2 || err != nil {
		t.Fatal("unexpected Promise.Current 2nd time")
	}

	tempPromise = NewPromise[int](cancelFunc)

	errErrorProduced := errors.New("err produced")
	wg.Add(1)
	go func() {
		res, err = tempPromise.Await(ctx)
		wg.Done()
	}()
	tempPromise.ProduceError(errErrorProduced)
	wg.Wait()
	if res != 0 || !errors.Is(err, errErrorProduced) {
		t.Fatal("unexpected Promise.Await after setError")
	}
	res, err = tempPromise.Current()
	if res != 0 || !errors.Is(err, errErrorProduced) {
		t.Fatal("unexpected Promise.Current 2nd time")
	}

	if atomic.LoadInt64(&cancelCalled) != 0 {
		t.Fatal("cancel called by await/current when it shouldn't be")
	}

	tempPromise.Cancel()
	if atomic.LoadInt64(&cancelCalled) != 0 {
		t.Fatal("cancel called after error produced")
	}

	tempPromise = NewPromise[int](cancelFunc)
	shortCtx, shortCancel := context.WithTimeout(ctx, time.Millisecond*100)
	defer shortCancel()
	res, err = tempPromise.Await(shortCtx)
	if res != 0 || !errors.Is(err, context.DeadlineExceeded) {
		t.Fatal("unexpected Promise.Await with timeout")
	}
	if atomic.LoadInt64(&cancelCalled) != 1 {
		t.Fatal("cancel not called by await on timeout")
	}
	tempPromise.Cancel()
	if atomic.LoadInt64(&cancelCalled) != 2 {
		t.Fatal("cancel not called by promise.Cancel")
	}
}

'''
'''--- util/containers/queue.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package containers

// Queue of an arbitrary type backed by a slice which is shrinked when it grows too large
type Queue[T any] struct {
	slice []T
}

func (q *Queue[T]) Push(item T) {
	q.slice = append(q.slice, item)
}

// If cap(slice) >= len(slice)*shrinkRatio && cap(slice) >= shrinkMinimum,
// shrink the slice capacity back down to twice its length by re-allocating it.
const shrinkRatio = 16
const shrinkMinimum = 512

func (q *Queue[T]) shrink() {
	if cap(q.slice) >= len(q.slice)*shrinkRatio && cap(q.slice) >= shrinkMinimum {
		oldSlice := q.slice
		q.slice = make([]T, len(oldSlice), len(oldSlice)*2)
		copy(q.slice, oldSlice)
	}
}

func (q *Queue[T]) Pop() T {
	var empty T
	if len(q.slice) == 0 {
		return empty
	}
	item := q.slice[0]
	q.slice[0] = empty
	q.slice = q.slice[1:]
	q.shrink()
	return item
}

func (q *Queue[T]) Len() int {
	return len(q.slice)
}

'''
'''--- util/containers/queue_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package containers

import (
	"fmt"
	"math"
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

// This function calculates the number of elements needed to pop from the queue
// to cause a shrink, given a certain shrink ratio.
//
// A shrink occurs when the length * ratio <= capacity.
// Let length = l, ratio = r, and capacity = c.
//
// Then we need to find n such that
// (c-n) / (l-n) > r,
// because both the length and the capacity decrease by one when we assign
// `q.slice = q.slice[1:]`.
//
// Rearranging terms to solve for n, we get
// n > (r * l - c) / (r - 1).
//
// Take the ceiling of this value to find the number of elements needed to pop
// to force a shrink.
func calcNumElementsToPop(capacity, length, ratio int) int {
	return int(math.Ceil(float64((ratio*length)-capacity) / float64(ratio-1)))
}

func TestQueue(t *testing.T) {
	q := Queue[int]{}

	// Need enough elements that we can force a shrink.
	initNumElements := 10000
	for i := 0; i < initNumElements; i++ {
		q.Push(i)
	}

	// Save the capacity to calculate how many elements we need to pop.
	bigCap := cap(q.slice)
	if bigCap < initNumElements {
		testhelpers.FailImpl(t, fmt.Sprintf("Unexpected capacity %d<%d: ", bigCap, initNumElements))
	}

	// Pop elements up to the one that should cause shrink.
	popCount := calcNumElementsToPop(bigCap, initNumElements, shrinkRatio)
	for i := 0; i < popCount-1; i++ {
		got := q.Pop()
		if got != i {
			testhelpers.FailImpl(t, fmt.Sprintf("Unexpected element popped: want %d, got %d", i, got))
		}
	}

	// The next pop should cause the shrink.
	got := q.Pop()
	if got != popCount-1 {
		testhelpers.FailImpl(t, fmt.Sprintf("Unexpected element popped: want %d, got %d", popCount, got))
	}

	// After shrink, the capacity should be exactly twice the length.
	expectedNewCap := len(q.slice) * 2
	if cap(q.slice) != expectedNewCap {
		testhelpers.FailImpl(t, fmt.Sprintf("Unexpected post-shrink cap: want %d, got %d", expectedNewCap, cap(q.slice)))
	}

	// Pop the remaining elements.
	for i := popCount; i < initNumElements; i++ {
		got := q.Pop()
		if got != i {
			testhelpers.FailImpl(t, fmt.Sprintf("Unexpected element popped: want %d, got %d", i, got))
		}
	}

	// Assert that queue is empty.
	if len(q.slice) != 0 {
		testhelpers.FailImpl(t, fmt.Sprintf("Non-empty queue: len=%d", len(q.slice)))
	}

	// Pop on empty queue should return the default value of an int, which is 0.
	if got := q.Pop(); got != 0 {
		testhelpers.FailImpl(t, fmt.Sprintf("Unexpected element popped: want %d, got %d", 0, got))
	}
}

'''
'''--- util/containers/syncmap.go ---
package containers

import "sync"

type SyncMap[K any, V any] struct {
	internal sync.Map
}

func (m *SyncMap[K, V]) Load(key K) (V, bool) {
	val, found := m.internal.Load(key)
	if !found {
		var empty V
		return empty, false
	}
	return val.(V), true
}

func (m *SyncMap[K, V]) Store(key K, val V) {
	m.internal.Store(key, val)
}

func (m *SyncMap[K, V]) Delete(key K) {
	m.internal.Delete(key)
}

'''
'''--- util/contracts/address_verifier.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package contracts

import (
	"context"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/solgen/go/bridgegen"
)

type AddressVerifier struct {
	seqInboxCaller *bridgegen.SequencerInboxCaller
	cache          map[common.Address]bool
	cacheExpiry    time.Time
	mutex          sync.Mutex
}

// Note that we only cache positive instances, not negative ones. That's because we're willing to accept the
// consequences of a false positive (accepting a Store from a recently retired batch poster), but we don't want
// to accept the consequences of a false negative (rejecting a Store from a recently added batch poster).

var addressVerifierLifetime = time.Hour

func NewAddressVerifier(seqInboxCaller *bridgegen.SequencerInboxCaller) *AddressVerifier {
	return &AddressVerifier{
		seqInboxCaller: seqInboxCaller,
		cache:          make(map[common.Address]bool),
		cacheExpiry:    time.Now().Add(addressVerifierLifetime),
	}
}

func (av *AddressVerifier) IsBatchPosterOrSequencer(ctx context.Context, addr common.Address) (bool, error) {
	av.mutex.Lock()
	if time.Now().After(av.cacheExpiry) {
		if err := av.flushCache_locked(ctx); err != nil {
			av.mutex.Unlock()
			return false, err
		}
	}
	if av.cache[addr] {
		av.mutex.Unlock()
		return true, nil
	}
	av.mutex.Unlock()

	result, err := av.seqInboxCaller.IsBatchPoster(&bind.CallOpts{Context: ctx}, addr)
	if err != nil {
		return false, err
	}
	if !result {
		var err error
		result, err = av.seqInboxCaller.IsSequencer(&bind.CallOpts{Context: ctx}, addr)
		if err != nil {
			return false, err
		}
	}
	if result {
		av.mutex.Lock()
		av.cache[addr] = true
		av.mutex.Unlock()
		return true, nil
	}
	return result, nil
}

func (av *AddressVerifier) FlushCache(ctx context.Context) error {
	av.mutex.Lock()
	defer av.mutex.Unlock()
	return av.flushCache_locked(ctx)
}

func (av *AddressVerifier) flushCache_locked(ctx context.Context) error {
	av.cache = make(map[common.Address]bool)
	av.cacheExpiry = time.Now().Add(addressVerifierLifetime)
	return nil
}

func NewMockAddressVerifier(validAddr common.Address) *MockAddressVerifier {
	return &MockAddressVerifier{
		validAddr: validAddr,
	}
}

type MockAddressVerifier struct {
	validAddr common.Address
}

func (bpv *MockAddressVerifier) IsBatchPosterOrSequencer(_ context.Context, addr common.Address) (bool, error) {
	return addr == bpv.validAddr, nil
}

type AddressVerifierInterface interface {
	IsBatchPosterOrSequencer(ctx context.Context, addr common.Address) (bool, error)
}

'''
'''--- util/headerreader/header_reader.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package headerreader

import (
	"context"
	"errors"
	"fmt"
	"math/big"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum"
	"github.com/ethereum/go-ethereum/accounts/abi/bind"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	flag "github.com/spf13/pflag"
)

type ArbSysInterface interface {
	ArbBlockNumber(*bind.CallOpts) (*big.Int, error)
}

type HeaderReader struct {
	stopwaiter.StopWaiter
	config                ConfigFetcher
	client                arbutil.L1Interface
	isParentChainArbitrum bool
	arbSys                ArbSysInterface

	chanMutex sync.RWMutex
	// All fields below require the chanMutex
	outChannels                map[chan<- *types.Header]struct{}
	outChannelsBehind          map[chan<- *types.Header]struct{}
	lastBroadcastHash          common.Hash
	lastBroadcastHeader        *types.Header
	lastBroadcastErr           error
	lastPendingCallBlockNr     uint64
	requiresPendingCallUpdates int

	safe      cachedHeader
	finalized cachedHeader
}

type cachedHeader struct {
	mutex          sync.Mutex
	rpcBlockNum    *big.Int
	headWhenCached *types.Header
	header         *types.Header
}

type Config struct {
	Enable               bool            `koanf:"enable"`
	PollOnly             bool            `koanf:"poll-only" reload:"hot"`
	PollInterval         time.Duration   `koanf:"poll-interval" reload:"hot"`
	SubscribeErrInterval time.Duration   `koanf:"subscribe-err-interval" reload:"hot"`
	TxTimeout            time.Duration   `koanf:"tx-timeout" reload:"hot"`
	OldHeaderTimeout     time.Duration   `koanf:"old-header-timeout" reload:"hot"`
	UseFinalityData      bool            `koanf:"use-finality-data" reload:"hot"`
	Dangerous            DangerousConfig `koanf:"dangerous"`
}

type DangerousConfig struct {
	WaitForTxApprovalSafePoll time.Duration `koanf:"wait-for-tx-approval-safe-poll"`
}

type ConfigFetcher func() *Config

var DefaultConfig = Config{
	Enable:               true,
	PollOnly:             false,
	PollInterval:         15 * time.Second,
	SubscribeErrInterval: 5 * time.Minute,
	TxTimeout:            5 * time.Minute,
	OldHeaderTimeout:     5 * time.Minute,
	UseFinalityData:      true,
	Dangerous: DangerousConfig{
		WaitForTxApprovalSafePoll: 0,
	},
}

func AddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultConfig.Enable, "enable reader connection")
	f.Bool(prefix+".poll-only", DefaultConfig.PollOnly, "do not attempt to subscribe to header events")
	f.Bool(prefix+".use-finality-data", DefaultConfig.UseFinalityData, "use l1 data about finalized/safe blocks")
	f.Duration(prefix+".poll-interval", DefaultConfig.PollInterval, "interval when polling endpoint")
	f.Duration(prefix+".subscribe-err-interval", DefaultConfig.SubscribeErrInterval, "interval for subscribe error")
	f.Duration(prefix+".tx-timeout", DefaultConfig.TxTimeout, "timeout when waiting for a transaction")
	f.Duration(prefix+".old-header-timeout", DefaultConfig.OldHeaderTimeout, "warns if the latest l1 block is at least this old")
	AddDangerousOptions(prefix+".dangerous", f)
}

func AddDangerousOptions(prefix string, f *flag.FlagSet) {
	f.Duration(prefix+".wait-for-tx-approval-safe-poll", DefaultConfig.Dangerous.WaitForTxApprovalSafePoll, "Dangerous! only meant to be used by system tests")
}

var TestConfig = Config{
	Enable:           true,
	PollOnly:         false,
	PollInterval:     time.Millisecond * 10,
	TxTimeout:        time.Second * 5,
	OldHeaderTimeout: 5 * time.Minute,
	UseFinalityData:  false,
	Dangerous: DangerousConfig{
		WaitForTxApprovalSafePoll: time.Millisecond * 100,
	},
}

func New(ctx context.Context, client arbutil.L1Interface, config ConfigFetcher, arbSysPrecompile ArbSysInterface) (*HeaderReader, error) {
	isParentChainArbitrum := false
	var arbSys ArbSysInterface
	if arbSysPrecompile != nil {
		codeAt, err := client.CodeAt(ctx, types.ArbSysAddress, nil)
		if err != nil {
			return nil, err
		}
		if len(codeAt) != 0 {
			isParentChainArbitrum = true
			arbSys = arbSysPrecompile
		}
	}
	return &HeaderReader{
		client:                client,
		config:                config,
		isParentChainArbitrum: isParentChainArbitrum,
		arbSys:                arbSys,
		outChannels:           make(map[chan<- *types.Header]struct{}),
		outChannelsBehind:     make(map[chan<- *types.Header]struct{}),
		safe:                  cachedHeader{rpcBlockNum: big.NewInt(rpc.SafeBlockNumber.Int64())},
		finalized:             cachedHeader{rpcBlockNum: big.NewInt(rpc.FinalizedBlockNumber.Int64())},
	}, nil
}

func (s *HeaderReader) Config() *Config { return s.config() }

// Subscribe to block header updates.
// Subscribers are notified when there is a change.
// Channel could be missing headers and have duplicates.
// Listening to the channel will make sure listenere is notified when header changes.
// Warning: listeners must not modify the header or its number, as they're shared between listeners.
func (s *HeaderReader) Subscribe(requireBlockNrUpdates bool) (<-chan *types.Header, func()) {
	s.chanMutex.Lock()
	defer s.chanMutex.Unlock()

	if requireBlockNrUpdates {
		s.requiresPendingCallUpdates++
	}
	result := make(chan *types.Header)
	outchannel := (chan<- *types.Header)(result)
	s.outChannelsBehind[outchannel] = struct{}{}
	unsubscribeFunc := func() { s.unsubscribe(requireBlockNrUpdates, outchannel) }
	return result, unsubscribeFunc
}

func (s *HeaderReader) unsubscribe(requireBlockNrUpdates bool, from chan<- *types.Header) {
	s.chanMutex.Lock()
	defer s.chanMutex.Unlock()

	if requireBlockNrUpdates {
		s.requiresPendingCallUpdates--
	}

	if _, ok := s.outChannels[from]; ok {
		delete(s.outChannels, from)
		close(from)
	}
	if _, ok := s.outChannelsBehind[from]; ok {
		delete(s.outChannelsBehind, from)
		close(from)
	}
}

func (s *HeaderReader) closeAll() {
	s.chanMutex.Lock()
	defer s.chanMutex.Unlock()

	s.requiresPendingCallUpdates = 0

	for ch := range s.outChannels {
		delete(s.outChannels, ch)
		close(ch)
	}
	for ch := range s.outChannelsBehind {
		delete(s.outChannelsBehind, ch)
		close(ch)
	}
}

func (s *HeaderReader) possiblyBroadcast(h *types.Header) {
	s.chanMutex.Lock()
	defer s.chanMutex.Unlock()

	// Clear any previous errors
	s.lastBroadcastErr = nil

	headerHash := h.Hash()
	broadcastThis := false

	if headerHash != s.lastBroadcastHash {
		broadcastThis = true
		s.lastBroadcastHash = headerHash
		s.lastBroadcastHeader = h
	}

	if s.requiresPendingCallUpdates > 0 {
		pendingCallBlockNr, err := s.getPendingCallBlockNumber()
		if err == nil && pendingCallBlockNr.IsUint64() {
			pendingU64 := pendingCallBlockNr.Uint64()
			if pendingU64 > s.lastPendingCallBlockNr {
				broadcastThis = true
				s.lastPendingCallBlockNr = pendingU64
			}
		} else {
			log.Warn("GetPendingCallBlockNr: bad result", "err", err, "number", pendingCallBlockNr)
		}
	}

	if broadcastThis {
		for ch := range s.outChannels {
			select {
			case ch <- h:
			default:
				delete(s.outChannels, ch)
				s.outChannelsBehind[ch] = struct{}{}
			}
		}
	}

	for ch := range s.outChannelsBehind {
		select {
		case ch <- h:
			delete(s.outChannelsBehind, ch)
			s.outChannels[ch] = struct{}{}
		default:
		}
	}
}

func (s *HeaderReader) getPendingCallBlockNumber() (*big.Int, error) {
	if s.isParentChainArbitrum {
		return s.arbSys.ArbBlockNumber(&bind.CallOpts{Context: s.GetContext(), Pending: true})
	}
	return arbutil.GetPendingCallBlockNumber(s.GetContext(), s.client)
}

func (s *HeaderReader) setError(err error) {
	s.chanMutex.Lock()
	defer s.chanMutex.Unlock()
	s.lastBroadcastErr = err
}

func (s *HeaderReader) broadcastLoop(ctx context.Context) {
	var clientSubscription ethereum.Subscription = nil
	defer func() {
		if clientSubscription != nil {
			clientSubscription.Unsubscribe()
		}
	}()
	inputChannel := make(chan *types.Header)
	if err := ctx.Err(); err != nil {
		s.setError(fmt.Errorf("exiting at start of broadcastLoop: %w", err))
		return
	}
	nextSubscribeErr := time.Now().Add(-time.Second)
	var errChannel <-chan error
	pollOnlyOverride := false
	for {
		if clientSubscription != nil {
			errChannel = clientSubscription.Err()
		} else {
			errChannel = nil
		}
		timer := time.NewTimer(s.config().PollInterval)
		select {
		case h := <-inputChannel:
			log.Trace("got new header from L1", "number", h.Number, "hash", h.Hash(), "header", h)
			s.possiblyBroadcast(h)
			timer.Stop()
		case <-timer.C:
			h, err := s.client.HeaderByNumber(ctx, nil)
			if err != nil {
				s.setError(fmt.Errorf("failed reading HeaderByNumber: %w", err))
				if !errors.Is(err, context.Canceled) {
					log.Warn("failed reading header", "err", err)
				}
			} else {
				s.possiblyBroadcast(h)
			}
			if !(s.config().PollOnly || pollOnlyOverride) && clientSubscription == nil {
				clientSubscription, err = s.client.SubscribeNewHead(ctx, inputChannel)
				if err != nil {
					clientSubscription = nil
					if errors.Is(err, rpc.ErrNotificationsUnsupported) {
						pollOnlyOverride = true
					} else if time.Now().After(nextSubscribeErr) {
						s.setError(fmt.Errorf("failed subscribing to header: %w", err))
						log.Warn("failed subscribing to header", "err", err)
						nextSubscribeErr = time.Now().Add(s.config().SubscribeErrInterval)
					}
				}
			}
		case err := <-errChannel:
			if ctx.Err() != nil {
				s.setError(fmt.Errorf("exiting broadcastLoop: %w", ctx.Err()))
				return
			}
			clientSubscription = nil
			s.setError(fmt.Errorf("error in subscription to headers: %w", err))
			log.Warn("error in subscription to headers", "err", err)
			timer.Stop()
		case <-ctx.Done():
			timer.Stop()
			s.setError(fmt.Errorf("exiting broadcastLoop: %w", ctx.Err()))
			return
		}
		s.logIfHeaderIsOld()
	}
}

func (s *HeaderReader) logIfHeaderIsOld() {
	s.chanMutex.RLock()
	storedHeader := s.lastBroadcastHeader
	s.chanMutex.RUnlock()
	if storedHeader == nil {
		return
	}
	l1Timetamp := time.Unix(int64(storedHeader.Time), 0)
	headerTime := time.Since(l1Timetamp)
	if headerTime >= s.config().OldHeaderTimeout {
		s.setError(fmt.Errorf("latest header is at least %v old", headerTime))
		log.Error(
			"latest L1 block is old", "l1Block", storedHeader.Number,
			"l1Timestamp", l1Timetamp, "age", headerTime,
		)
	}
}

func (s *HeaderReader) WaitForTxApproval(ctxIn context.Context, tx *types.Transaction) (*types.Receipt, error) {
	headerchan, unsubscribe := s.Subscribe(true)
	defer unsubscribe()
	ctx, cancel := context.WithTimeout(ctxIn, s.config().TxTimeout)
	defer cancel()
	txHash := tx.Hash()
	waitForBlock := false
	waitForSafePoll := s.config().Dangerous.WaitForTxApprovalSafePoll
	for {
		if waitForBlock {
			select {
			case _, ok := <-headerchan:
				if !ok {
					return nil, fmt.Errorf("waiting for %v: channel closed", txHash)
				}
			case <-ctx.Done():
				return nil, ctx.Err()
			}
		}
		waitForBlock = true
		receipt, err := s.client.TransactionReceipt(ctx, txHash)
		if err != nil || receipt == nil {
			continue
		}
		if !receipt.BlockNumber.IsUint64() {
			continue
		}
		receiptBlockNr := receipt.BlockNumber.Uint64()
		callBlockNr := s.LastPendingCallBlockNr()
		if callBlockNr <= receiptBlockNr {
			continue
		}
		if waitForSafePoll != 0 {
			safeBlock, err := s.client.BlockByNumber(ctx, big.NewInt(int64(rpc.SafeBlockNumber)))
			if err != nil || safeBlock == nil {
				log.Warn("parent chain: failed getting safeblock", "err", err)
				continue
			}
			if safeBlock.NumberU64() < receiptBlockNr {
				log.Info("parent chain: waiting for safe block (see wait-for-tx-approval-safe-poll)", "waiting", receiptBlockNr, "safe", safeBlock.NumberU64())
				select {
				case <-time.After(time.Millisecond * 100):
				case <-ctx.Done():
					return nil, ctx.Err()
				}
				waitForBlock = false
				continue
			}
		}
		block, err := s.client.BlockByHash(ctx, receipt.BlockHash)
		if block != nil && err == nil {
			return receipt, arbutil.DetailTxError(ctx, s.client, tx, receipt)
		}
	}
}

func (s *HeaderReader) LastHeader(ctx context.Context) (*types.Header, error) {
	header, err := s.LastHeaderWithError()
	if err == nil && header != nil {
		return header, nil
	}
	return s.client.HeaderByNumber(ctx, nil)
}

func (s *HeaderReader) LastHeaderWithError() (*types.Header, error) {
	s.chanMutex.RLock()
	storedHeader := s.lastBroadcastHeader
	storedError := s.lastBroadcastErr
	s.chanMutex.RUnlock()
	if storedError != nil {
		return nil, storedError
	}
	return storedHeader, nil
}

func (s *HeaderReader) UpdatingPendingCallBlockNr() bool {
	s.chanMutex.RLock()
	defer s.chanMutex.RUnlock()
	return s.requiresPendingCallUpdates > 0
}

// LastPendingCallBlockNr returns the blockNumber currently used by pending calls.
// Note: This value is only updated if UpdatingPendingCallBlockNr returns true.
func (s *HeaderReader) LastPendingCallBlockNr() uint64 {
	s.chanMutex.RLock()
	defer s.chanMutex.RUnlock()
	return s.lastPendingCallBlockNr
}

var ErrBlockNumberNotSupported = errors.New("block number not supported")

func headerIndicatesFinalitySupport(header *types.Header) bool {
	if header.Difficulty.Sign() == 0 {
		// This is an Ethereum PoS chain
		return true
	}
	if types.DeserializeHeaderExtraInformation(header).ArbOSFormatVersion > 0 {
		// This is an Arbitrum chain
		return true
	}
	// This is probably an Ethereum PoW or Clique chain, which doesn't support finality
	return false
}

func HeadersEqual(ha, hb *types.Header) bool {
	if (ha == nil) != (hb == nil) {
		return false
	}
	return (ha == nil && hb == nil) || ha.Hash() == hb.Hash()
}

func (s *HeaderReader) getCached(ctx context.Context, c *cachedHeader) (*types.Header, error) {
	c.mutex.Lock()
	defer c.mutex.Unlock()
	currentHead, err := s.LastHeader(ctx)
	if err != nil {
		return nil, err
	}
	if HeadersEqual(currentHead, c.headWhenCached) {
		return c.header, nil
	}
	if !s.config().UseFinalityData || !headerIndicatesFinalitySupport(currentHead) {
		return nil, ErrBlockNumberNotSupported
	}
	header, err := s.client.HeaderByNumber(ctx, c.rpcBlockNum)
	if err != nil {
		return nil, err
	}
	c.header = header
	c.headWhenCached = currentHead
	return c.header, nil
}

func (s *HeaderReader) LatestSafeBlockHeader(ctx context.Context) (*types.Header, error) {
	header, err := s.getCached(ctx, &s.safe)
	if errors.Is(err, ErrBlockNumberNotSupported) {
		return nil, fmt.Errorf("%w: safe block not found", ErrBlockNumberNotSupported)
	}
	return header, err
}

func (s *HeaderReader) LatestSafeBlockNr(ctx context.Context) (uint64, error) {
	header, err := s.LatestSafeBlockHeader(ctx)
	if err != nil {
		return 0, err
	}
	return header.Number.Uint64(), nil
}

func (s *HeaderReader) LatestFinalizedBlockHeader(ctx context.Context) (*types.Header, error) {
	header, err := s.getCached(ctx, &s.finalized)
	if errors.Is(err, ErrBlockNumberNotSupported) {
		return nil, fmt.Errorf("%w: finalized block not found", ErrBlockNumberNotSupported)
	}
	return header, err
}

func (s *HeaderReader) LatestFinalizedBlockNr(ctx context.Context) (uint64, error) {
	header, err := s.LatestFinalizedBlockHeader(ctx)
	if err != nil {
		return 0, err
	}
	return header.Number.Uint64(), nil
}

func (s *HeaderReader) Client() arbutil.L1Interface {
	return s.client
}

func (s *HeaderReader) UseFinalityData() bool {
	return s.config().UseFinalityData
}

func (s *HeaderReader) IsParentChainArbitrum() bool {
	return s.isParentChainArbitrum
}

func (s *HeaderReader) Start(ctxIn context.Context) {
	s.StopWaiter.Start(ctxIn, s)
	s.LaunchThread(s.broadcastLoop)
}

func (s *HeaderReader) StopAndWait() {
	s.StopWaiter.StopAndWait()
	s.closeAll()
}

'''
'''--- util/jsonapi/preimages.go ---
// Copyright 2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package jsonapi

import (
	"bytes"
	"encoding/base64"
	"fmt"
	"io"

	"github.com/ethereum/go-ethereum/common"
)

type PreimagesMapJson struct {
	Map map[common.Hash][]byte
}

func NewPreimagesMapJson(inner map[common.Hash][]byte) *PreimagesMapJson {
	return &PreimagesMapJson{inner}
}

func (m *PreimagesMapJson) MarshalJSON() ([]byte, error) {
	encoding := base64.StdEncoding
	size := 2                                          // {}
	size += (5 + encoding.EncodedLen(32)) * len(m.Map) // "000..000":""
	if len(m.Map) > 0 {
		size += len(m.Map) - 1 // commas
	}
	for _, value := range m.Map {
		size += encoding.EncodedLen(len(value))
	}
	out := make([]byte, size)
	i := 0
	out[i] = '{'
	i++
	for key, value := range m.Map {
		if i > 1 {
			out[i] = ','
			i++
		}
		out[i] = '"'
		i++
		encoding.Encode(out[i:], key[:])
		i += encoding.EncodedLen(len(key))
		out[i] = '"'
		i++
		out[i] = ':'
		i++
		out[i] = '"'
		i++
		encoding.Encode(out[i:], value)
		i += encoding.EncodedLen(len(value))
		out[i] = '"'
		i++
	}
	out[i] = '}'
	i++
	if i != len(out) {
		return nil, fmt.Errorf("preimage map wrote %v bytes but expected to write %v", i, len(out))
	}
	return out, nil
}

func readNonWhitespace(data *[]byte) (byte, error) {
	c := byte('\t')
	for c == '\t' || c == '\n' || c == '\v' || c == '\f' || c == '\r' || c == ' ' {
		if len(*data) == 0 {
			return 0, io.ErrUnexpectedEOF
		}
		c = (*data)[0]
		*data = (*data)[1:]
	}
	return c, nil
}

func expectCharacter(data *[]byte, expected rune) error {
	got, err := readNonWhitespace(data)
	if err != nil {
		return fmt.Errorf("while looking for '%v' got %w", expected, err)
	}
	if rune(got) != expected {
		return fmt.Errorf("while looking for '%v' got '%v'", expected, rune(got))
	}
	return nil
}

func getStrLen(data []byte) (int, error) {
	// We don't allow strings to contain an escape sequence.
	// Searching for a backslash here would be duplicated work.
	// If the returned string length includes a backslash, base64 decoding will fail and error there.
	strLen := bytes.IndexByte(data, '"')
	if strLen == -1 {
		return 0, fmt.Errorf("%w: hit end of preimages map looking for end quote", io.ErrUnexpectedEOF)
	}
	return strLen, nil
}

func (m *PreimagesMapJson) UnmarshalJSON(data []byte) error {
	err := expectCharacter(&data, '{')
	if err != nil {
		return err
	}
	m.Map = make(map[common.Hash][]byte)
	encoding := base64.StdEncoding
	// Used to store base64 decoded data
	// Returned unmarshalled preimage slices will just be parts of this one
	buf := make([]byte, encoding.DecodedLen(len(data)))
	for {
		c, err := readNonWhitespace(&data)
		if err != nil {
			return fmt.Errorf("while looking for key in preimages map got %w", err)
		}
		if len(m.Map) == 0 && c == '}' {
			break
		} else if c != '"' {
			return fmt.Errorf("expected '\"' to begin key in preimages map but got '%v'", c)
		}
		strLen, err := getStrLen(data)
		if err != nil {
			return err
		}
		maxKeyLen := encoding.DecodedLen(strLen)
		if maxKeyLen > len(buf) {
			return fmt.Errorf("preimage key base64 possible length %v is greater than buffer size of %v", maxKeyLen, len(buf))
		}
		keyLen, err := encoding.Decode(buf, data[:strLen])
		if err != nil {
			return fmt.Errorf("error base64 decoding preimage key: %w", err)
		}
		var key common.Hash
		if keyLen != len(key) {
			return fmt.Errorf("expected preimage to be %v bytes long, but got %v bytes", len(key), keyLen)
		}
		copy(key[:], buf[:len(key)])
		// We don't need to advance buf here because we already copied the data we needed out of it
		data = data[strLen+1:]
		err = expectCharacter(&data, ':')
		if err != nil {
			return err
		}
		err = expectCharacter(&data, '"')
		if err != nil {
			return err
		}
		strLen, err = getStrLen(data)
		if err != nil {
			return err
		}
		maxValueLen := encoding.DecodedLen(strLen)
		if maxValueLen > len(buf) {
			return fmt.Errorf("preimage value base64 possible length %v is greater than buffer size of %v", maxValueLen, len(buf))
		}
		valueLen, err := encoding.Decode(buf, data[:strLen])
		if err != nil {
			return fmt.Errorf("error base64 decoding preimage value: %w", err)
		}
		m.Map[key] = buf[:valueLen]
		buf = buf[valueLen:]
		data = data[strLen+1:]
		c, err = readNonWhitespace(&data)
		if err != nil {
			return fmt.Errorf("after value in preimages map got %w", err)
		}
		if c == '}' {
			break
		} else if c != ',' {
			return fmt.Errorf("expected ',' or '}' after value in preimages map but got '%v'", c)
		}
	}
	return nil
}

'''
'''--- util/jsonapi/preimages_test.go ---
// Copyright 2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package jsonapi

import (
	"encoding/json"
	"fmt"
	"reflect"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func TestPreimagesMapJson(t *testing.T) {
	t.Parallel()
	for _, preimages := range []PreimagesMapJson{
		{},
		{make(map[common.Hash][]byte)},
		{map[common.Hash][]byte{
			{}: {},
		}},
		{map[common.Hash][]byte{
			{1}: {1},
			{2}: {1, 2},
			{3}: {1, 2, 3},
		}},
	} {
		t.Run(fmt.Sprintf("%v preimages", len(preimages.Map)), func(t *testing.T) {
			// These test cases are fast enough that t.Parallel() probably isn't worth it
			serialized, err := preimages.MarshalJSON()
			Require(t, err, "Failed to marshal preimagesj")

			// Make sure that `serialized` is a valid JSON map
			stringMap := make(map[string]string)
			err = json.Unmarshal(serialized, &stringMap)
			Require(t, err, "Failed to unmarshal preimages as string map")
			if len(stringMap) != len(preimages.Map) {
				t.Errorf("Got %v entries in string map but only had %v preimages", len(stringMap), len(preimages.Map))
			}

			var deserialized PreimagesMapJson
			err = deserialized.UnmarshalJSON(serialized)
			Require(t, err)

			if (len(preimages.Map) > 0 || len(deserialized.Map) > 0) && !reflect.DeepEqual(preimages, deserialized) {
				t.Errorf("Preimages map %v serialized to %v but then deserialized to different map %v", preimages, string(serialized), deserialized)
			}
		})
	}
}

'''
'''--- util/merkletree/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkletree

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- util/merkletree/merkleAccumulator_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkletree

import (
	"bytes"
	"encoding/binary"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/merkleAccumulator"
)

func TestEmptyAccumulator(t *testing.T) {
	acc := initializedMerkleAccumulatorForTesting()
	if root(t, acc) != (common.Hash{}) {
		Fail(t)
	}
	mt := NewEmptyMerkleTree()
	if root(t, acc) != mt.Hash() {
		Fail(t)
	}
	testAllSummarySizes(mt, t)
	testSerDe(mt, t)
}

func TestAccumulator1(t *testing.T) {
	acc := initializedMerkleAccumulatorForTesting()
	if root(t, acc) != (common.Hash{}) {
		Fail(t)
	}
	mt := NewEmptyMerkleTree()

	itemHash := pseudorandomForTesting(0)
	accAppend(t, acc, itemHash)
	if size(t, acc) != 1 {
		Fail(t)
	}
	mt = mt.Append(itemHash)
	if mt.Size() != 1 {
		t.Fatal(mt.Size())
	}
	if root(t, acc) != crypto.Keccak256Hash(itemHash.Bytes()) {
		Fail(t)
	}
	if root(t, acc) != mt.Hash() {
		Fail(t)
	}
	testAllSummarySizes(mt, t)
	testSerDe(mt, t)
}

func TestAccumulator3(t *testing.T) {
	acc := initializedMerkleAccumulatorForTesting()
	if root(t, acc) != (common.Hash{}) {
		Fail(t)
	}
	mt := NewEmptyMerkleTree()

	itemHash0 := pseudorandomForTesting(0)
	itemHash1 := pseudorandomForTesting(1)
	itemHash2 := pseudorandomForTesting(2)

	accAppend(t, acc, itemHash0)
	mt = mt.Append(itemHash0)
	accAppend(t, acc, itemHash1)
	mt = mt.Append(itemHash1)
	accAppend(t, acc, itemHash2)
	mt = mt.Append(itemHash2)

	if size(t, acc) != 3 {
		Fail(t)
	}
	if mt.Size() != 3 {
		Fail(t)
	}

	expectedHash := crypto.Keccak256(
		crypto.Keccak256(crypto.Keccak256(itemHash0.Bytes()), crypto.Keccak256(itemHash1.Bytes())),
		crypto.Keccak256(crypto.Keccak256(itemHash2.Bytes()), make([]byte, 32)),
	)
	if root(t, acc) != common.BytesToHash(expectedHash) {
		Fail(t)
	}
	if root(t, acc) != mt.Hash() {
		Fail(t)
	}
	testAllSummarySizes(mt, t)
	testSerDe(mt, t)
}

func TestAccumulator4(t *testing.T) {
	acc := initializedMerkleAccumulatorForTesting()
	if root(t, acc) != (common.Hash{}) {
		Fail(t)
	}
	mt := NewEmptyMerkleTree()

	itemHash0 := pseudorandomForTesting(0)
	itemHash1 := pseudorandomForTesting(1)
	itemHash2 := pseudorandomForTesting(2)
	itemHash3 := pseudorandomForTesting(3)

	accAppend(t, acc, itemHash0)
	mt = mt.Append(itemHash0)
	accAppend(t, acc, itemHash1)
	mt = mt.Append(itemHash1)
	accAppend(t, acc, itemHash2)
	mt = mt.Append(itemHash2)
	accAppend(t, acc, itemHash3)
	mt = mt.Append(itemHash3)

	if size(t, acc) != 4 {
		Fail(t)
	}
	if mt.Size() != 4 {
		Fail(t)
	}

	expectedHash := crypto.Keccak256(
		crypto.Keccak256(crypto.Keccak256(itemHash0.Bytes()), crypto.Keccak256(itemHash1.Bytes())),
		crypto.Keccak256(crypto.Keccak256(itemHash2.Bytes()), crypto.Keccak256(itemHash3.Bytes())),
	)
	if root(t, acc) != common.BytesToHash(expectedHash) {
		Fail(t)
	}
	if root(t, acc) != mt.Hash() {
		Fail(t)
	}
	testAllSummarySizes(mt, t)
	testSerDe(mt, t)
}

func testAllSummarySizes(tree MerkleTree, t *testing.T) {
	for i := uint64(1); i <= tree.Size(); i++ {
		sum := tree.SummarizeUpTo(i)
		if tree.Hash() != sum.Hash() {
			Fail(t)
		}
		if tree.Size() != sum.Size() {
			Fail(t)
		}
		if tree.Capacity() != sum.Capacity() {
			Fail(t)
		}
		testSerDe(sum, t)
	}
}

func testSerDe(tree MerkleTree, t *testing.T) {
	var wr bytes.Buffer
	if err := tree.Serialize(&wr); err != nil {
		Fail(t, err)
	}
	rd := bytes.NewReader(wr.Bytes())
	result, err := NewMerkleTreeFromReader(rd)
	if err != nil {
		Fail(t, err)
	}
	if tree.Hash() != result.Hash() {
		Fail(t)
	}
}

func pseudorandomForTesting(x uint64) common.Hash {
	var buf [8]byte
	binary.BigEndian.PutUint64(buf[:], x)
	return crypto.Keccak256Hash(buf[:])
}

func accAppend(t *testing.T, acc *merkleAccumulator.MerkleAccumulator, itemHash common.Hash) {
	t.Helper()
	_, err := acc.Append(itemHash)
	Require(t, err)
}

func root(t *testing.T, acc *merkleAccumulator.MerkleAccumulator) common.Hash {
	t.Helper()
	root, err := acc.Root()
	Require(t, err)
	return root
}

func size(t *testing.T, acc *merkleAccumulator.MerkleAccumulator) uint64 {
	t.Helper()
	size, err := acc.Size()
	Require(t, err)
	return size
}

'''
'''--- util/merkletree/merkleEventProof.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkletree

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbos/merkleAccumulator"
)

func NewMerkleTreeFromAccumulator(acc *merkleAccumulator.MerkleAccumulator) (MerkleTree, error) {
	partials, err := acc.GetPartials()
	if err != nil {
		return nil, err
	}
	if len(partials) == 0 {
		return NewEmptyMerkleTree(), nil
	}
	var tree MerkleTree
	capacity := uint64(1)
	for level, partial := range partials {
		if *partial != (common.Hash{}) {
			var thisLevel MerkleTree
			if level == 0 {
				thisLevel = NewMerkleLeaf(*partial)
			} else {
				thisLevel = NewSummaryMerkleTree(*partial, capacity)
			}
			if tree == nil {
				tree = thisLevel
			} else {
				for tree.Capacity() < capacity {
					tree = NewMerkleInternal(tree, NewMerkleEmpty(tree.Capacity()))
				}
				tree = NewMerkleInternal(thisLevel, tree)
			}
		}
		capacity *= 2
	}

	return tree, nil
}

func NewMerkleTreeFromEvents(
	events []merkleAccumulator.MerkleTreeNodeEvent, // latest event at each Level
) (MerkleTree, error) {
	acc, err := NewNonPersistentMerkleAccumulatorFromEvents(events)
	if err != nil {
		return nil, err
	}
	return NewMerkleTreeFromAccumulator(acc)
}

func NewNonPersistentMerkleAccumulatorFromEvents(
	events []merkleAccumulator.MerkleTreeNodeEvent,
) (*merkleAccumulator.MerkleAccumulator, error) {

	partials := make([]*common.Hash, len(events))
	zero := common.Hash{}
	for i := range partials {
		partials[i] = &zero
	}

	latestSeen := uint64(0)
	for i := len(events) - 1; i >= 0; i-- {
		event := events[i]
		if event.NumLeaves > latestSeen {
			latestSeen = event.NumLeaves
			partials[i] = &event.Hash
		}
	}
	return merkleAccumulator.NewNonpersistentMerkleAccumulatorFromPartials(partials)
}

'''
'''--- util/merkletree/merkleEventProof_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkletree

import (
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/burn"
	"github.com/offchainlabs/nitro/arbos/merkleAccumulator"
	"github.com/offchainlabs/nitro/arbos/storage"
)

func initializedMerkleAccumulatorForTesting() *merkleAccumulator.MerkleAccumulator {
	sto := storage.NewMemoryBacked(burn.NewSystemBurner(nil, false))
	merkleAccumulator.InitializeMerkleAccumulator(sto)
	return merkleAccumulator.OpenMerkleAccumulator(sto)
}

func TestProofForNext(t *testing.T) {
	leaves := make([]common.Hash, 13)
	for i := range leaves {
		leaves[i] = pseudorandomForTesting(uint64(i))
	}

	acc := initializedMerkleAccumulatorForTesting()
	for i, leaf := range leaves {
		proof, err := ProofFromAccumulator(acc, leaf)
		Require(t, err)
		if proof == nil {
			Fail(t, i)
		}
		if proof.LeafHash != crypto.Keccak256Hash(leaf.Bytes()) {
			Fail(t, i)
		}
		if !proof.IsCorrect() {
			Fail(t, proof)
		}
		_, err = acc.Append(leaf)
		Require(t, err)
		root, err := acc.Root()
		Require(t, err)
		if proof.RootHash != root {
			Fail(t, i)
		}
	}
}

func ProofFromAccumulator(acc *merkleAccumulator.MerkleAccumulator, nextHash common.Hash) (*MerkleProof, error) {
	origPartials, err := acc.GetPartials()
	if err != nil {
		return nil, err
	}
	partials := make([]common.Hash, len(origPartials))
	for i, orig := range origPartials {
		partials[i] = *orig
	}
	clone, err := acc.NonPersistentClone()
	if err != nil {
		return nil, err
	}
	_, err = clone.Append(nextHash)
	if err != nil {
		return nil, err
	}
	root, _ := clone.Root()
	size, err := acc.Size()
	if err != nil {
		return nil, err
	}

	return &MerkleProof{
		RootHash:  root,
		LeafHash:  crypto.Keccak256Hash(nextHash.Bytes()),
		LeafIndex: size,
		Proof:     partials,
	}, nil
}

'''
'''--- util/merkletree/merkleTree.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package merkletree

import (
	"errors"
	"io"
	"math/big"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/arbos/util"
)

type MerkleTree interface {
	Hash() common.Hash
	Size() uint64
	Capacity() uint64
	Append(common.Hash) MerkleTree
	SummarizeUpTo(num uint64) MerkleTree
	Serialize(wr io.Writer) error
}

const (
	SerializedLeaf byte = iota
	SerializedEmptySubtree
	SerializedInternalNode
	SerializedSubtreeSummary
)

type LevelAndLeaf struct {
	Level uint64
	Leaf  uint64
}

func NewLevelAndLeaf(level, leaf uint64) LevelAndLeaf {
	return LevelAndLeaf{
		Level: level,
		Leaf:  leaf,
	}
}

func (place LevelAndLeaf) ToBigInt() *big.Int {
	return new(big.Int).Add(
		new(big.Int).Lsh(big.NewInt(int64(place.Level)), 192),
		big.NewInt(int64(place.Leaf)),
	)
}

func NewEmptyMerkleTree() MerkleTree {
	return NewMerkleEmpty(0)
}

type merkleTreeLeaf struct {
	hash common.Hash
}

func NewMerkleLeaf(hash common.Hash) MerkleTree {
	return &merkleTreeLeaf{hash}
}

func newMerkleLeafFromReader(rd io.Reader) (MerkleTree, error) {
	hash, err := util.HashFromReader(rd)
	return NewMerkleLeaf(hash), err
}

func (leaf *merkleTreeLeaf) Hash() common.Hash {
	return crypto.Keccak256Hash(leaf.hash.Bytes())
}

func (leaf *merkleTreeLeaf) Size() uint64 {
	return 1
}

func (leaf *merkleTreeLeaf) Capacity() uint64 {
	return 1
}

func (leaf *merkleTreeLeaf) Append(newHash common.Hash) MerkleTree {
	return NewMerkleInternal(leaf, NewMerkleLeaf(newHash))
}

func (leaf *merkleTreeLeaf) SummarizeUpTo(num uint64) MerkleTree {
	return leaf
}

func (leaf *merkleTreeLeaf) Serialize(wr io.Writer) error {
	if _, err := wr.Write([]byte{SerializedLeaf}); err != nil {
		return err
	}
	_, err := wr.Write(leaf.hash.Bytes())
	return err
}

type merkleEmpty struct {
	capacity uint64
}

func NewMerkleEmpty(capacity uint64) MerkleTree {
	return &merkleEmpty{capacity}
}

func newMerkleEmptyFromReader(rd io.Reader) (MerkleTree, error) {
	capacity, err := util.Uint64FromReader(rd)
	return NewMerkleEmpty(capacity), err
}

func (me *merkleEmpty) Hash() common.Hash {
	return common.Hash{}
}

func (me *merkleEmpty) Size() uint64 {
	return 0
}

func (me *merkleEmpty) Capacity() uint64 {
	return me.capacity
}

func (me *merkleEmpty) Append(newHash common.Hash) MerkleTree {
	if me.capacity <= 1 {
		return NewMerkleLeaf(newHash)
	} else {
		halfSizeEmpty := NewMerkleEmpty(me.capacity / 2)
		return NewMerkleInternal(halfSizeEmpty.Append(newHash), halfSizeEmpty)
	}
}

func (me *merkleEmpty) SummarizeUpTo(num uint64) MerkleTree {
	return me
}

func (me *merkleEmpty) Serialize(wr io.Writer) error {
	if _, err := wr.Write([]byte{SerializedEmptySubtree}); err != nil {
		return err
	}
	return util.Uint64ToWriter(me.capacity, wr)
}

type merkleInternal struct {
	hash     common.Hash
	size     uint64
	capacity uint64
	left     MerkleTree
	right    MerkleTree
}

func NewMerkleInternal(left, right MerkleTree) MerkleTree {
	return &merkleInternal{
		crypto.Keccak256Hash(left.Hash().Bytes(), right.Hash().Bytes()),
		left.Size() + right.Size(),
		left.Capacity() + right.Capacity(),
		left,
		right,
	}
}

func newMerkleInternalFromReader(rd io.Reader) (MerkleTree, error) {
	left, err := NewMerkleTreeFromReader(rd)
	if err != nil {
		return nil, err
	}
	right, err := NewMerkleTreeFromReader(rd)
	if err != nil {
		return nil, err
	}
	return NewMerkleInternal(left, right), nil
}

func (mi *merkleInternal) Hash() common.Hash {
	return mi.hash
}

func (mi *merkleInternal) Size() uint64 {
	return mi.size
}

func (mi *merkleInternal) Capacity() uint64 {
	return mi.capacity
}

func (mi *merkleInternal) Append(newHash common.Hash) MerkleTree {
	if mi.size == mi.capacity {
		return NewMerkleInternal(mi, NewMerkleEmpty(mi.capacity).Append(newHash))
	} else if 2*mi.size < mi.capacity {
		return NewMerkleInternal(mi.left.Append(newHash), mi.right)
	} else {
		return NewMerkleInternal(mi.left, mi.right.Append(newHash))
	}
}

func (mi *merkleInternal) SummarizeUpTo(num uint64) MerkleTree {
	if num == mi.capacity {
		return summaryFromMerkleTree(mi)
	} else {
		leftSize := mi.left.Size()
		if num <= leftSize {
			return NewMerkleInternal(mi.left.SummarizeUpTo(num), mi.right)
		} else {
			return NewMerkleInternal(summaryFromMerkleTree(mi.left), mi.right.SummarizeUpTo(num-leftSize))
		}
	}
}

func (mi *merkleInternal) Serialize(wr io.Writer) error {
	if _, err := wr.Write([]byte{SerializedInternalNode}); err != nil {
		return err
	}
	if err := mi.left.Serialize(wr); err != nil {
		return err
	}
	return mi.right.Serialize(wr)
}

type merkleCompleteSubtreeSummary struct {
	hash     common.Hash
	capacity uint64
}

func NewSummaryMerkleTree(hash common.Hash, capacity uint64) MerkleTree {
	return &merkleCompleteSubtreeSummary{hash, capacity}
}

func summaryFromMerkleTree(subtree MerkleTree) MerkleTree {
	if subtree.Size() == 1 {
		return subtree
	}
	if subtree.Size() != subtree.Capacity() {
		panic("tried to summarize a non-full MerkleTree node")
	}
	return &merkleCompleteSubtreeSummary{subtree.Hash(), subtree.Capacity()}
}

func newMerkleSummaryFromReader(rd io.Reader) (MerkleTree, error) {
	capacity, err := util.Uint64FromReader(rd)
	if err != nil {
		return nil, err
	}
	hash, err := util.HashFromReader(rd)
	if err != nil {
		return nil, err
	}
	return &merkleCompleteSubtreeSummary{hash, capacity}, nil
}

func (sum *merkleCompleteSubtreeSummary) Hash() common.Hash {
	return sum.hash
}

func (sum *merkleCompleteSubtreeSummary) Size() uint64 {
	return sum.capacity
}

func (sum *merkleCompleteSubtreeSummary) Capacity() uint64 {
	return sum.capacity
}

func (sum *merkleCompleteSubtreeSummary) Append(newHash common.Hash) MerkleTree {
	return NewMerkleInternal(sum, NewMerkleEmpty(sum.capacity).Append(newHash))
}

func (sum *merkleCompleteSubtreeSummary) SummarizeUpTo(num uint64) MerkleTree {
	return sum
}

func (sum *merkleCompleteSubtreeSummary) Serialize(wr io.Writer) error {
	if _, err := wr.Write([]byte{SerializedSubtreeSummary}); err != nil {
		return err
	}
	if err := util.Uint64ToWriter(sum.capacity, wr); err != nil {
		return err
	}
	_, err := wr.Write(sum.hash.Bytes())
	return err
}

func NewMerkleTreeFromReader(rd io.Reader) (MerkleTree, error) {
	var typeBuf [1]byte
	if _, err := rd.Read(typeBuf[:]); err != nil {
		return nil, err
	}
	switch typeBuf[0] {
	case SerializedLeaf:
		return newMerkleLeafFromReader(rd)
	case SerializedInternalNode:
		return newMerkleInternalFromReader(rd)
	case SerializedEmptySubtree:
		return newMerkleEmptyFromReader(rd)
	case SerializedSubtreeSummary:
		return newMerkleSummaryFromReader(rd)
	default:
		return nil, errors.New("invalid node type in deserializing Merkle tree")
	}
}

type MerkleProof struct {
	RootHash  common.Hash
	LeafHash  common.Hash
	LeafIndex uint64
	Proof     []common.Hash
}

func (proof *MerkleProof) IsCorrect() bool {
	hash := proof.LeafHash
	index := proof.LeafIndex
	for _, hashFromProof := range proof.Proof {

		if index&1 == 0 {
			hash = crypto.Keccak256Hash(hash.Bytes(), hashFromProof.Bytes())
		} else {
			hash = crypto.Keccak256Hash(hashFromProof.Bytes(), hash.Bytes())
		}
		index = index / 2
	}
	if index != 0 {
		return false
	}
	return hash == proof.RootHash
}

'''
'''--- util/metricsutil/metricsutil.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package metricsutil

import (
	"regexp"
)

// Prometheus metric names must contain only chars [a-zA-Z0-9:_]
func CanonicalizeMetricName(metric string) string {
	invalidPromCharRegex := regexp.MustCompile(`[^a-zA-Z0-9:_]+`)
	return invalidPromCharRegex.ReplaceAllString(metric, "_")

}

'''
'''--- util/normalizeGas.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package util

import (
	"github.com/offchainlabs/nitro/arbos/l2pricing"
)

// NormalizeL2GasForL1GasInitial is for testing, adjusts an L2 gas amount that represents L1 gas spending, to compensate for
// the difference between the assumed L2 base fee and the actual initial L2 base fee.
func NormalizeL2GasForL1GasInitial(l2gas uint64, assumedL2Basefee uint64) uint64 {
	return l2gas * assumedL2Basefee / l2pricing.InitialBaseFeeWei
}

'''
'''--- util/pretty/pretty_printing.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package pretty

import (
	"fmt"

	"github.com/ethereum/go-ethereum/common"
)

func FirstFewBytes(b []byte) string {
	if len(b) < 9 {
		return fmt.Sprintf("[% x]", b)
	} else {
		return fmt.Sprintf("[% x ... ]", b[:8])
	}
}

func PrettyBytes(b []byte) string {
	hex := common.Bytes2Hex(b)
	if len(hex) > 24 {
		return fmt.Sprintf("%v...", hex[:24])
	}
	return hex
}

func PrettyHash(hash common.Hash) string {
	return FirstFewBytes(hash.Bytes())
}

func FirstFewChars(s string) string {
	if len(s) < 9 {
		return fmt.Sprintf("\"%s\"", s)
	} else {
		return fmt.Sprintf("\"%s...\"", s[:8])
	}
}

'''
'''--- util/redisutil/redis_coordinator.go ---
package redisutil

import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/go-redis/redis/v8"

	"github.com/ethereum/go-ethereum/log"

	"github.com/offchainlabs/nitro/arbutil"
)

const CHOSENSEQ_KEY string = "coordinator.chosen"                 // Never overwritten. Expires or released only
const MSG_COUNT_KEY string = "coordinator.msgCount"               // Only written by sequencer holding CHOSEN key
const PRIORITIES_KEY string = "coordinator.priorities"            // Read only
const WANTS_LOCKOUT_KEY_PREFIX string = "coordinator.liveliness." // Per server. Only written by self
const MESSAGE_KEY_PREFIX string = "coordinator.msg."              // Per Message. Only written by sequencer holding CHOSEN
const SIGNATURE_KEY_PREFIX string = "coordinator.msg.sig."        // Per Message. Only written by sequencer holding CHOSEN
const WANTS_LOCKOUT_VAL string = "OK"
const INVALID_VAL string = "INVALID"
const INVALID_URL string = "<?INVALID-URL?>"

type RedisCoordinator struct {
	Client redis.UniversalClient
}

func WantsLockoutKeyFor(url string) string { return WANTS_LOCKOUT_KEY_PREFIX + url }

func NewRedisCoordinator(redisUrl string) (*RedisCoordinator, error) {
	redisClient, err := RedisClientFromURL(redisUrl)
	if err != nil {
		return nil, err
	}

	return &RedisCoordinator{
		Client: redisClient,
	}, nil
}

// RecommendSequencerWantingLockout returns the top priority sequencer wanting the lockout
func (c *RedisCoordinator) RecommendSequencerWantingLockout(ctx context.Context) (string, error) {
	prioritiesString, err := c.Client.Get(ctx, PRIORITIES_KEY).Result()
	if err != nil {
		if errors.Is(err, redis.Nil) {
			err = errors.New("sequencer priorities unset")
		}
		return "", err
	}
	priorities := strings.Split(prioritiesString, ",")
	for _, url := range priorities {
		err := c.Client.Get(ctx, WantsLockoutKeyFor(url)).Err()
		if errors.Is(err, redis.Nil) { // wants lockout not set
			continue
		}
		if err != nil {
			return "", err
		}
		return url, nil
	}
	log.Error("no sequencer appears to want the lockout on redis", "priorities", prioritiesString)
	return "", nil
}

// CurrentChosenSequencer retrieves the current chosen sequencer holding the lock
func (c *RedisCoordinator) CurrentChosenSequencer(ctx context.Context) (string, error) {
	current, err := c.Client.Get(ctx, CHOSENSEQ_KEY).Result()
	if errors.Is(err, redis.Nil) {
		return "", nil
	}
	if err != nil {
		return "", err
	}
	return current, nil
}

// GetPriorities returns the priority list of sequencers
func (rc *RedisCoordinator) GetPriorities(ctx context.Context) ([]string, error) {
	prioritiesString, err := rc.Client.Get(ctx, PRIORITIES_KEY).Result()
	if err != nil {
		if errors.Is(err, redis.Nil) {
			err = errors.New("sequencer priorities unset")
		}
		return []string{}, err
	}
	prioritiesList := strings.Split(prioritiesString, ",")
	return prioritiesList, nil
}

// GetLiveliness returns a map whose keys are sequencers that have their liveliness set to OK
func (rc *RedisCoordinator) GetLiveliness(ctx context.Context) ([]string, error) {
	livelinessList, _, err := rc.Client.Scan(ctx, 0, WANTS_LOCKOUT_KEY_PREFIX+"*", 0).Result()
	if err != nil {
		return []string{}, err
	}
	for i, elem := range livelinessList {
		url := strings.TrimPrefix(elem, WANTS_LOCKOUT_KEY_PREFIX)
		livelinessList[i] = url
	}
	return livelinessList, nil
}

func MessageKeyFor(pos arbutil.MessageIndex) string {
	return fmt.Sprintf("%s%d", MESSAGE_KEY_PREFIX, pos)
}

func MessageSigKeyFor(pos arbutil.MessageIndex) string {
	return fmt.Sprintf("%s%d", SIGNATURE_KEY_PREFIX, pos)
}

'''
'''--- util/redisutil/redisutil.go ---
package redisutil

import "github.com/go-redis/redis/v8"

func RedisClientFromURL(url string) (redis.UniversalClient, error) {
	if url == "" {
		return nil, nil
	}
	redisOptions, err := redis.ParseURL(url)
	if err != nil {
		return nil, err
	}
	return redis.NewClient(redisOptions), nil
}

'''
'''--- util/redisutil/test_redis.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package redisutil

import (
	"context"
	"fmt"
	"os"
	"testing"

	"github.com/alicebob/miniredis/v2"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

// CreateTestRedis Provides external redis url, this is only done in TEST_REDIS env,
// else creates a new miniredis and returns its url.
func CreateTestRedis(ctx context.Context, t *testing.T) string {
	redisUrl := os.Getenv("TEST_REDIS")
	if redisUrl != "" {
		return redisUrl
	}
	redisServer, err := miniredis.Run()
	testhelpers.RequireImpl(t, err)
	go func() {
		<-ctx.Done()
		redisServer.Close()
	}()

	return fmt.Sprintf("redis://%s/0", redisServer.Addr())
}

'''
'''--- util/rpcclient/rpcclient.go ---
package rpcclient

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"regexp"
	"strings"
	"sync/atomic"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/rpc"

	"github.com/offchainlabs/nitro/util/signature"
)

type ClientConfig struct {
	URL            string        `koanf:"url"`
	JWTSecret      string        `koanf:"jwtsecret"`
	Timeout        time.Duration `koanf:"timeout" reload:"hot"`
	Retries        uint          `koanf:"retries" reload:"hot"`
	ConnectionWait time.Duration `koanf:"connection-wait"`
	ArgLogLimit    uint          `koanf:"arg-log-limit" reload:"hot"`
	RetryErrors    string        `koanf:"retry-errors" reload:"hot"`

	retryErrors *regexp.Regexp
}

func (c *ClientConfig) Validate() error {
	if c.RetryErrors == "" {
		c.retryErrors = nil
		return nil
	}
	var err error
	c.retryErrors, err = regexp.Compile(c.RetryErrors)
	return err
}

type ClientConfigFetcher func() *ClientConfig

var TestClientConfig = ClientConfig{
	URL:       "self",
	JWTSecret: "",
}

var DefaultClientConfig = ClientConfig{
	URL:         "self-auth",
	JWTSecret:   "",
	ArgLogLimit: 2048,
}

func RPCClientAddOptions(prefix string, f *flag.FlagSet, defaultConfig *ClientConfig) {
	f.String(prefix+".url", defaultConfig.URL, "url of server, use self for loopback websocket, self-auth for loopback with authentication")
	f.String(prefix+".jwtsecret", defaultConfig.JWTSecret, "path to file with jwtsecret for validation - ignored if url is self or self-auth")
	f.Duration(prefix+".connection-wait", defaultConfig.ConnectionWait, "how long to wait for initial connection")
	f.Duration(prefix+".timeout", defaultConfig.Timeout, "per-response timeout (0-disabled)")
	f.Uint(prefix+".arg-log-limit", defaultConfig.ArgLogLimit, "limit size of arguments in log entries")
	f.Uint(prefix+".retries", defaultConfig.Retries, "number of retries in case of failure(0 mean one attempt)")
	f.String(prefix+".retry-errors", defaultConfig.RetryErrors, "Errors matching this regular expression are automatically retried")
}

type RpcClient struct {
	config    ClientConfigFetcher
	client    *rpc.Client
	autoStack *node.Node
	logId     uint64
}

func NewRpcClient(config ClientConfigFetcher, stack *node.Node) *RpcClient {
	return &RpcClient{
		config:    config,
		autoStack: stack,
	}
}

func (c *RpcClient) Close() {
	if c.client != nil {
		c.client.Close()
	}
}

type limitedMarshal struct {
	limit int
	value any
}

func (m limitedMarshal) String() string {
	marshalled, err := json.Marshal(m.value)
	var str string
	if err != nil {
		str = "\"CANNOT MARSHALL: " + err.Error() + "\""
	} else {
		str = string(marshalled)
	}
	if m.limit == 0 || len(str) <= m.limit {
		return str
	}
	prefix := str[:m.limit/2-1]
	postfix := str[len(str)-m.limit/2+1:]
	return fmt.Sprintf("%v..%v", prefix, postfix)
}

type limitedArgumentsMarshal struct {
	limit int
	args  []any
}

func (m limitedArgumentsMarshal) String() string {
	res := "["
	for i, arg := range m.args {
		res += limitedMarshal{m.limit, arg}.String()
		if i < len(m.args)-1 {
			res += ", "
		}
	}
	res += "]"
	return res
}

func (c *RpcClient) CallContext(ctx_in context.Context, result interface{}, method string, args ...interface{}) error {
	if c.client == nil {
		return errors.New("not connected")
	}
	logId := atomic.AddUint64(&c.logId, 1)
	log.Trace("sending RPC request", "method", method, "logId", logId, "args", limitedArgumentsMarshal{int(c.config().ArgLogLimit), args})
	var err error
	for i := 0; i < int(c.config().Retries)+1; i++ {
		if ctx_in.Err() != nil {
			return ctx_in.Err()
		}
		var ctx context.Context
		var cancelCtx context.CancelFunc
		timeout := c.config().Timeout
		if timeout > 0 {
			ctx, cancelCtx = context.WithTimeout(ctx_in, timeout)
		} else {
			ctx, cancelCtx = context.WithCancel(ctx_in)
		}
		err = c.client.CallContext(ctx, result, method, args...)
		cancelCtx()
		logger := log.Trace
		limit := int(c.config().ArgLogLimit)
		if err != nil && err.Error() != "already known" {
			logger = log.Info
		}
		logger("rpc response", "method", method, "logId", logId, "err", err, "result", limitedMarshal{limit, result}, "attempt", i, "args", limitedArgumentsMarshal{limit, args})
		if err == nil {
			return nil
		}
		if errors.Is(err, context.DeadlineExceeded) {
			continue
		}
		retryErrs := c.config().retryErrors
		if retryErrs != nil && retryErrs.MatchString(err.Error()) {
			continue
		}
		return err
	}
	return err
}

func (c *RpcClient) BatchCallContext(ctx context.Context, b []rpc.BatchElem) error {
	return c.client.BatchCallContext(ctx, b)
}

func (c *RpcClient) EthSubscribe(ctx context.Context, channel interface{}, args ...interface{}) (*rpc.ClientSubscription, error) {
	return c.client.EthSubscribe(ctx, channel, args...)
}

func (c *RpcClient) Start(ctx_in context.Context) error {
	url := c.config().URL
	jwtPath := c.config().JWTSecret
	if url == "self" {
		if c.autoStack == nil {
			return errors.New("self not supported for this connection")
		}
		url = c.autoStack.WSEndpoint()
		jwtPath = ""
	} else if url == "self-auth" {
		if c.autoStack == nil {
			return errors.New("self-auth not supported for this connection")
		}
		url = c.autoStack.WSAuthEndpoint()
		jwtPath = c.autoStack.JWTPath()
	} else if url == "" {
		return errors.New("no url provided for this connection")
	}
	var jwt *common.Hash
	if jwtPath != "" {
		var err error
		jwt, err = signature.LoadSigningKey(jwtPath)
		if err != nil {
			return err
		}
	}
	connTimeout := time.After(c.config().ConnectionWait)
	for {
		var ctx context.Context
		var cancelCtx context.CancelFunc
		timeout := c.config().Timeout
		if timeout > 0 {
			ctx, cancelCtx = context.WithTimeout(ctx_in, timeout)
		} else {
			ctx, cancelCtx = context.WithCancel(ctx_in)
		}
		var err error
		var client *rpc.Client
		if jwt == nil {
			client, err = rpc.DialContext(ctx, url)
		} else {
			client, err = rpc.DialOptions(ctx, url, rpc.WithHTTPAuth(node.NewJWTAuth([32]byte(*jwt))))
		}
		cancelCtx()
		if err == nil {
			c.client = client
			return nil
		}
		if strings.Contains(err.Error(), "parse") ||
			strings.Contains(err.Error(), "malformed") {
			return fmt.Errorf("%w: url %s", err, url)
		}
		select {
		case <-connTimeout:
			return fmt.Errorf("timeout trying to connect lastError: %w", err)
		case <-time.After(time.Second):
		}
	}
}

'''
'''--- util/rpcclient/rpcclient_test.go ---
package rpcclient

import (
	"context"
	"errors"
	"sync/atomic"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/rpc"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestLogArgs(t *testing.T) {
	t.Parallel()

	args := []any{1, 2, 3, "hello, world"}
	str := limitedArgumentsMarshal{0, args}.String()
	if str != "[1, 2, 3, \"hello, world\"]" {
		Fail(t, "unexpected logs limit 0 got:", str)
	}

	str = limitedArgumentsMarshal{100, args}.String()
	if str != "[1, 2, 3, \"hello, world\"]" {
		Fail(t, "unexpected logs limit 100 got:", str)
	}

	str = limitedArgumentsMarshal{6, args}.String()
	if str != "[1, 2, 3, \"h..d\"]" {
		Fail(t, "unexpected logs limit 6 got:", str)
	}

}

func createTestNode(t *testing.T, ctx context.Context, stuckOrFailed int64) *node.Node {
	stackConf := node.DefaultConfig
	stackConf.HTTPPort = 0
	stackConf.DataDir = ""
	stackConf.WSHost = "127.0.0.1"
	stackConf.WSPort = 0
	stackConf.WSModules = []string{"test"}
	stackConf.P2P.NoDiscovery = true
	stackConf.P2P.ListenAddr = ""

	stack, err := node.New(&stackConf)
	Require(t, err)

	testAPIs := []rpc.API{{
		Namespace:     "test",
		Version:       "1.0",
		Service:       &testAPI{stuckOrFailed, stuckOrFailed},
		Public:        true,
		Authenticated: false,
	}}
	stack.RegisterAPIs(testAPIs)

	err = stack.Start()
	Require(t, err)

	go func() {
		<-ctx.Done()
		stack.Close()
	}()

	return stack
}

type testAPI struct {
	stuckCalls  int64
	failedCalls int64
}

func (t *testAPI) StuckAtFirst(ctx context.Context) error {
	stuckRemaining := atomic.AddInt64(&t.stuckCalls, -1) + 1
	if stuckRemaining <= 0 {
		return nil
	}
	<-ctx.Done()
	return errors.New("error")
}

func (t *testAPI) FailAtFirst(ctx context.Context) error {
	failedRemaining := atomic.AddInt64(&t.failedCalls, -1) + 1
	if failedRemaining <= 0 {
		return nil
	}
	return errors.New("error")
}

func TestRpcClientRetry(t *testing.T) {
	t.Parallel()
	ctx, cancel := context.WithTimeout(context.Background(), time.Minute*2)
	defer cancel()

	config := &ClientConfig{
		URL:         "self",
		Timeout:     time.Second * 5,
		Retries:     2,
		RetryErrors: "",
	}
	Require(t, config.Validate())
	configFetcher := func() *ClientConfig { return config }

	serverGood := createTestNode(t, ctx, 0)
	clientGood := NewRpcClient(configFetcher, serverGood)
	err := clientGood.Start(ctx)
	Require(t, err)
	err = clientGood.CallContext(ctx, nil, "test_failAtFirst")
	Require(t, err)
	err = clientGood.CallContext(ctx, nil, "test_stuckAtFirst")
	Require(t, err)

	serverBad := createTestNode(t, ctx, 1000)
	clientBad := NewRpcClient(configFetcher, serverBad)
	err = clientBad.Start(ctx)
	Require(t, err)
	err = clientBad.CallContext(ctx, nil, "test_failAtFirst")
	if err == nil {
		Fail(t, "no error for failAtFirst")
	}
	err = clientBad.CallContext(ctx, nil, "test_stuckAtFirst")
	if err == nil {
		Fail(t, "no error for stuckAtFirst")
	}

	serverRetry := createTestNode(t, ctx, 1)
	clientRetry := NewRpcClient(configFetcher, serverRetry)
	err = clientRetry.Start(ctx)
	Require(t, err)
	err = clientRetry.CallContext(ctx, nil, "test_failAtFirst")
	if err == nil {
		Fail(t, "no error for failAtFirst")
	}
	err = clientRetry.CallContext(ctx, nil, "test_stuckAtFirst")
	Require(t, err)

	retryConfig := &ClientConfig{
		URL:         "self",
		Timeout:     time.Second * 5,
		Retries:     2,
		RetryErrors: "er.*",
	}
	Require(t, retryConfig.Validate())
	retryErrConfigFetcher := func() *ClientConfig { return retryConfig }

	serverWorkWithRetry := createTestNode(t, ctx, 1)
	clientWorkWithRetry := NewRpcClient(retryErrConfigFetcher, serverWorkWithRetry)
	err = clientWorkWithRetry.Start(ctx)
	Require(t, err)
	err = clientWorkWithRetry.CallContext(ctx, nil, "test_failAtFirst")
	Require(t, err)

	clientFailsWithRetry := NewRpcClient(retryErrConfigFetcher, serverBad)
	err = clientFailsWithRetry.Start(ctx)
	Require(t, err)
	err = clientFailsWithRetry.CallContext(ctx, nil, "test_failAtFirst")
	if err == nil {
		Fail(t, "no error for failAtFirst")
	}

	noMatchconfig := &ClientConfig{
		URL:         "self",
		Timeout:     time.Second * 5,
		Retries:     2,
		RetryErrors: "b.*",
	}
	Require(t, config.Validate())
	noMatchFetcher := func() *ClientConfig { return noMatchconfig }
	serverWorkWithRetry2 := createTestNode(t, ctx, 1)
	clientNoMatch := NewRpcClient(noMatchFetcher, serverWorkWithRetry2)
	err = clientNoMatch.Start(ctx)
	Require(t, err)
	err = clientNoMatch.CallContext(ctx, nil, "test_failAtFirst")
	if err == nil {
		Fail(t, "no error for failAtFirst")
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- util/sharedmetrics/sharedmetrics.go ---
package sharedmetrics

import (
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/offchainlabs/nitro/arbutil"
)

var (
	latestSequenceNumberGauge  = metrics.NewRegisteredGauge("arb/sequencenumber/latest", nil)
	sequenceNumberInBlockGauge = metrics.NewRegisteredGauge("arb/sequencenumber/inblock", nil)
)

func UpdateSequenceNumberGauge(sequenceNumber arbutil.MessageIndex) {
	latestSequenceNumberGauge.Update(int64(sequenceNumber))
}
func UpdateSequenceNumberInBlockGauge(sequenceNumber arbutil.MessageIndex) {
	sequenceNumberInBlockGauge.Update(int64(sequenceNumber))
}

'''
'''--- util/signature/datasigner.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package signature

import (
	"crypto/ecdsa"

	"github.com/ethereum/go-ethereum/crypto"
)

type DataSignerFunc func([]byte) ([]byte, error)

func DataSignerFromPrivateKey(privateKey *ecdsa.PrivateKey) DataSignerFunc {
	return func(data []byte) ([]byte, error) {
		return crypto.Sign(data, privateKey)
	}
}

'''
'''--- util/signature/sign_verify.go ---
package signature

import (
	"context"
	"errors"

	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/util/contracts"
	flag "github.com/spf13/pflag"
)

type SignVerify struct {
	verifier   *Verifier
	signerFunc DataSignerFunc
	fallback   *SimpleHmac
	config     *SignVerifyConfig
}

type SignVerifyConfig struct {
	ECDSA             VerifierConfig   `koanf:"ecdsa"`
	SymmetricFallback bool             `koanf:"symmetric-fallback"`
	SymmetricSign     bool             `koanf:"symmetric-sign"`
	Symmetric         SimpleHmacConfig `koanf:"symmetric"`
}

func SignVerifyConfigAddOptions(prefix string, f *flag.FlagSet) {
	FeedVerifierConfigAddOptions(prefix+".ecdsa", f)
	f.Bool(prefix+".symmetric-fallback", DefaultSignVerifyConfig.SymmetricFallback, "if to fall back to symmetric hmac")
	f.Bool(prefix+".symmetric-sign", DefaultSignVerifyConfig.SymmetricSign, "if to sign with symmetric hmac")
	SimpleHmacConfigAddOptions(prefix+".symmetric", f)
}

var DefaultSignVerifyConfig = SignVerifyConfig{
	ECDSA:             DefultFeedVerifierConfig,
	SymmetricFallback: false,
	SymmetricSign:     false,
	Symmetric:         EmptySimpleHmacConfig,
}
var TestSignVerifyConfig = SignVerifyConfig{
	ECDSA: VerifierConfig{
		AcceptSequencer: true,
	},
	SymmetricFallback: false,
	SymmetricSign:     false,
	Symmetric:         TestSimpleHmacConfig,
}

func NewSignVerify(config *SignVerifyConfig, signerFunc DataSignerFunc, bpValidator contracts.AddressVerifierInterface) (*SignVerify, error) {
	var fallback *SimpleHmac
	if config.SymmetricFallback {
		var err error
		fallback, err = NewSimpleHmac(&config.Symmetric)
		if err != nil {
			return nil, err
		}
	}
	verifier, err := NewVerifier(&config.ECDSA, bpValidator)
	if err != nil {
		return nil, err
	}
	return &SignVerify{
		verifier:   verifier,
		signerFunc: signerFunc,
		fallback:   fallback,
		config:     config,
	}, nil
}

func (v *SignVerify) VerifySignature(ctx context.Context, signature []byte, data ...[]byte) error {
	ecdsaErr := v.verifier.verifyClosure(ctx, signature, crypto.Keccak256Hash(data...))
	if ecdsaErr == nil {
		return nil
	}
	if !v.config.SymmetricFallback {
		return ecdsaErr
	}
	return v.fallback.VerifySignature(signature, data...)
}

func (v *SignVerify) SignMessage(data ...[]byte) ([]byte, error) {
	if v.config.SymmetricSign {
		return v.fallback.SignMessage(data...)
	}
	if v.signerFunc == nil {
		if v.config.ECDSA.Dangerous.AcceptMissing {
			return make([]byte, 0), nil
		}
		return nil, errors.New("no private key. cannot sign messages")
	}
	return v.signerFunc(crypto.Keccak256Hash(data...).Bytes())
}

'''
'''--- util/signature/sign_verify_test.go ---
package signature

import (
	"context"
	"errors"
	"testing"

	"github.com/ethereum/go-ethereum/crypto"
)

func TestSignVerifyModes(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	signingAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := DataSignerFromPrivateKey(privateKey)

	config := TestSignVerifyConfig
	config.SymmetricFallback = false
	config.SymmetricSign = false
	config.ECDSA.AcceptSequencer = false
	config.ECDSA.AllowedAddresses = []string{signingAddr.Hex()}
	signVerifyECDSA, err := NewSignVerify(&config, dataSigner, nil)
	Require(t, err)

	configSymmetric := TestSignVerifyConfig
	configSymmetric.SymmetricFallback = true
	configSymmetric.SymmetricSign = true
	configSymmetric.ECDSA.AcceptSequencer = false
	signVerifySymmetric, err := NewSignVerify(&configSymmetric, nil, nil)
	Require(t, err)

	configFallback := TestSignVerifyConfig
	configFallback.SymmetricFallback = true
	configFallback.SymmetricSign = false
	configFallback.ECDSA.AllowedAddresses = []string{signingAddr.Hex()}
	configFallback.ECDSA.AcceptSequencer = false
	signVerifyFallback, err := NewSignVerify(&configFallback, dataSigner, nil)
	Require(t, err)

	data := []byte{0, 1, 2, 3, 4, 5, 6, 7}

	ecdsaSig, err := signVerifyECDSA.SignMessage(data)
	Require(t, err, "error signing data")

	err = signVerifyECDSA.VerifySignature(ctx, ecdsaSig, data)
	Require(t, err, "error verifying data")

	err = signVerifyFallback.VerifySignature(ctx, ecdsaSig, data)
	Require(t, err, "error verifying data")

	err = signVerifySymmetric.VerifySignature(ctx, ecdsaSig, data)
	if !errors.Is(err, ErrSignatureNotVerified) {
		t.Error("unexpected error", err)
	}

	symSig, err := signVerifySymmetric.SignMessage(data)
	Require(t, err, "error signing data")

	err = signVerifySymmetric.VerifySignature(ctx, symSig, data)
	Require(t, err, "error verifying data")

	err = signVerifyFallback.VerifySignature(ctx, symSig, data)
	Require(t, err, "error verifying data")

	err = signVerifyECDSA.VerifySignature(ctx, symSig, data)
	if !errors.Is(err, ErrSignatureNotVerified) {
		t.Error("unexpected error", err)
	}

	fallbackSig, err := signVerifyFallback.SignMessage(data)
	Require(t, err, "error signing data")

	err = signVerifyECDSA.VerifySignature(ctx, fallbackSig, data)
	Require(t, err, "error verifying data")
}

'''
'''--- util/signature/simple_hmac.go ---
package signature

import (
	"crypto/subtle"
	"errors"
	"fmt"
	"os"
	"regexp"
	"strings"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
)

type SimpleHmac struct {
	config                  *SimpleHmacConfig
	signingKey              *common.Hash
	fallbackVerificationKey *common.Hash
}

func NewSimpleHmac(config *SimpleHmacConfig) (*SimpleHmac, error) {
	signingKey, err := LoadSigningKey(config.SigningKey)
	if err != nil {
		return nil, err
	}
	fallbackVerificationKey, err := LoadSigningKey(config.FallbackVerificationKey)
	if err != nil {
		return nil, err
	}
	if signingKey == nil && fallbackVerificationKey != nil {
		return nil, errors.New("cannot have fallback-verification-key without signing-key")
	}
	if signingKey == nil && !config.Dangerous.DisableSignatureVerification {
		return nil, errors.New("signature verification is enabled but no key is present")
	}
	return &SimpleHmac{
		config:                  config,
		signingKey:              signingKey,
		fallbackVerificationKey: fallbackVerificationKey,
	}, nil
}

type SimpleHmacConfig struct {
	SigningKey              string                    `koanf:"signing-key"`
	FallbackVerificationKey string                    `koanf:"fallback-verification-key"`
	Dangerous               SimpleHmacDangerousConfig `koanf:"dangerous"`
}

type SimpleHmacDangerousConfig struct {
	DisableSignatureVerification bool `koanf:"disable-signature-verification"`
}

func SimpleHmacDangerousConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".disable-signature-verification", DefaultSimpleHmacDangerousConfig.DisableSignatureVerification, "disable message signature verification")
}

func SimpleHmacConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".signing-key", EmptySimpleHmacConfig.SigningKey, "a 32-byte (64-character) hex string used to sign messages, or a path to a file containing it")
	f.String(prefix+".fallback-verification-key", EmptySimpleHmacConfig.FallbackVerificationKey, "a fallback key used for message verification")
	SimpleHmacDangerousConfigAddOptions(prefix+".dangerous", f)
}

var DefaultSimpleHmacDangerousConfig = SimpleHmacDangerousConfig{
	DisableSignatureVerification: false,
}

var EmptySimpleHmacConfig = SimpleHmacConfig{
	SigningKey:              "",
	FallbackVerificationKey: "",
}

var TestSimpleHmacConfig = SimpleHmacConfig{
	SigningKey:              "b561f5d5d98debc783aa8a1472d67ec3bcd532a1c8d95e5cb23caa70c649f7c9",
	FallbackVerificationKey: "",
	Dangerous: SimpleHmacDangerousConfig{
		DisableSignatureVerification: false,
	},
}

var keyIsHexRegex = regexp.MustCompile("^(0x)?[a-fA-F0-9]{64}$")

func LoadSigningKey(keyConfig string) (*common.Hash, error) {
	if keyConfig == "" {
		return nil, nil
	}
	keyIsHex := keyIsHexRegex.Match([]byte(keyConfig))
	var keyString string
	if keyIsHex {
		keyString = keyConfig
	} else {
		contents, err := os.ReadFile(keyConfig)
		if err != nil {
			return nil, fmt.Errorf("failed to read signing key file: %w", err)
		}
		s := strings.TrimSpace(string(contents))
		if keyIsHexRegex.Match([]byte(s)) {
			keyString = s
		} else {
			return nil, errors.New("signing key file contents are not 32 bytes of hex")
		}
	}
	hash := common.HexToHash(keyString)
	return &hash, nil
}

func prependBytes(first []byte, rest ...[]byte) [][]byte {
	return append([][]byte{first}, rest...)
}

// On success, extracts the message from the message+signature data passed in, and returns it
func (h *SimpleHmac) VerifySignature(sig []byte, data ...[]byte) error {
	if h.config.Dangerous.DisableSignatureVerification {
		return nil
	}
	if len(sig) != 32 {
		return fmt.Errorf("%w: signature must be exactly 32 bytes", ErrSignatureNotVerified)
	}

	expectHmac := crypto.Keccak256Hash(prependBytes(h.signingKey[:], data...)...)
	if subtle.ConstantTimeCompare(expectHmac[:], sig) == 1 {
		return nil
	}

	if h.fallbackVerificationKey != nil {
		expectHmac = crypto.Keccak256Hash(prependBytes(h.fallbackVerificationKey[:], data...)...)
		if subtle.ConstantTimeCompare(expectHmac[:], sig) == 1 {
			return nil
		}
	}

	return ErrSignatureNotVerified
}

func (h *SimpleHmac) SignMessage(data ...[]byte) ([]byte, error) {
	var hmac [32]byte
	if h.signingKey != nil {
		hmac = crypto.Keccak256Hash(prependBytes(h.signingKey[:], data...)...)
	}
	return hmac[:], nil
}

'''
'''--- util/signature/verifier.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package signature

import (
	"context"
	"errors"
	"fmt"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/util/contracts"
)

type Verifier struct {
	config        *VerifierConfig
	authorizedMap map[common.Address]struct{}
	addrVerifier  contracts.AddressVerifierInterface
}

type VerifierConfig struct {
	AllowedAddresses []string                `koanf:"allowed-addresses"`
	AcceptSequencer  bool                    `koanf:"accept-sequencer"`
	Dangerous        DangerousVerifierConfig `koanf:"dangerous"`
}

type DangerousVerifierConfig struct {
	AcceptMissing bool `koanf:"accept-missing"`
}

var ErrSignatureNotVerified = errors.New("signature not verified")
var ErrMissingSignature = fmt.Errorf("%w: signature not found", ErrSignatureNotVerified)
var ErrSignerNotApproved = fmt.Errorf("%w: signer not approved", ErrSignatureNotVerified)

func FeedVerifierConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.StringSlice(prefix+".allowed-addresses", DefultFeedVerifierConfig.AllowedAddresses, "a list of allowed addresses")
	f.Bool(prefix+".accept-sequencer", DefultFeedVerifierConfig.AcceptSequencer, "accept verified message from sequencer")
	DangerousFeedVerifierConfigAddOptions(prefix+".dangerous", f)
}

func DangerousFeedVerifierConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".accept-missing", DefultFeedVerifierConfig.Dangerous.AcceptMissing, "accept empty as valid signature")
}

var DefultFeedVerifierConfig = VerifierConfig{
	AllowedAddresses: []string{},
	AcceptSequencer:  true,
	Dangerous: DangerousVerifierConfig{
		AcceptMissing: true,
	},
}

var TestingFeedVerifierConfig = VerifierConfig{
	AllowedAddresses: []string{},
	AcceptSequencer:  false,
	Dangerous: DangerousVerifierConfig{
		AcceptMissing: false,
	},
}

func NewVerifier(config *VerifierConfig, addrVerifier contracts.AddressVerifierInterface) (*Verifier, error) {
	authorizedMap := make(map[common.Address]struct{}, len(config.AllowedAddresses))
	for _, addrString := range config.AllowedAddresses {
		addr := common.HexToAddress(addrString)
		authorizedMap[addr] = struct{}{}
	}
	if addrVerifier == nil && !config.Dangerous.AcceptMissing && config.AcceptSequencer {
		return nil, errors.New("cannot read batch poster addresses")
	}
	return &Verifier{
		config:        config,
		authorizedMap: authorizedMap,
		addrVerifier:  addrVerifier,
	}, nil
}

func (v *Verifier) VerifyHash(ctx context.Context, signature []byte, hash common.Hash) error {
	return v.verifyClosure(ctx, signature, hash)
}

func (v *Verifier) VerifyData(ctx context.Context, signature []byte, data ...[]byte) error {
	return v.verifyClosure(ctx, signature, crypto.Keccak256Hash(data...))
}

func (v *Verifier) verifyClosure(ctx context.Context, sig []byte, hash common.Hash) error {
	if len(sig) == 0 {
		if v.config.Dangerous.AcceptMissing {
			// Signature missing and not required
			return nil
		}
		return ErrMissingSignature
	}

	sigPublicKey, err := crypto.SigToPub(hash.Bytes(), sig)
	if err != nil {
		// nolint:nilerr
		return ErrSignatureNotVerified
	}

	addr := crypto.PubkeyToAddress(*sigPublicKey)

	if _, exists := v.authorizedMap[addr]; exists {
		return nil
	}

	if v.config.Dangerous.AcceptMissing && v.addrVerifier == nil {
		return nil
	}

	if !v.config.AcceptSequencer || v.addrVerifier == nil {
		return ErrSignerNotApproved
	}

	batchPosterOrSequencer, err := v.addrVerifier.IsBatchPosterOrSequencer(ctx, addr)
	if err != nil {
		return err
	}

	if !batchPosterOrSequencer {
		return ErrSignerNotApproved
	}

	return nil
}

'''
'''--- util/signature/verifier_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package signature

import (
	"context"
	"errors"
	"testing"

	"github.com/ethereum/go-ethereum/crypto"

	"github.com/offchainlabs/nitro/util/contracts"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestVerifier(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	signingAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := DataSignerFromPrivateKey(privateKey)

	config := TestingFeedVerifierConfig
	config.AllowedAddresses = []string{signingAddr.Hex()}
	verifier, err := NewVerifier(&config, nil)
	Require(t, err)

	data := []byte{0, 1, 2, 3, 4, 5, 6, 7}
	hash := crypto.Keccak256Hash(data)

	signature, err := dataSigner(hash.Bytes())
	Require(t, err, "error signing data")

	err = verifier.VerifyData(ctx, signature, data)
	Require(t, err, "error verifying data")

	err = verifier.VerifyHash(ctx, signature, hash)
	Require(t, err, "error verifying data")

	badData := []byte{1, 1, 2, 3, 4, 5, 6, 7}
	err = verifier.VerifyData(ctx, signature, badData)
	if !errors.Is(err, ErrSignatureNotVerified) {
		t.Error("unexpected error", err)
	}
}

func TestMissingRequiredSignature(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := TestingFeedVerifierConfig
	config.Dangerous.AcceptMissing = false
	verifier, err := NewVerifier(&config, nil)
	Require(t, err)
	err = verifier.VerifyData(ctx, nil, nil)
	if !errors.Is(err, ErrMissingSignature) {
		t.Error("didn't fail when missing feed signature")
	}
}

func TestMissingSignatureAllowed(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	config := TestingFeedVerifierConfig
	config.Dangerous.AcceptMissing = true
	verifier, err := NewVerifier(&config, nil)
	Require(t, err)
	err = verifier.VerifyData(ctx, nil, nil)
	Require(t, err, "error verifying data")
}

func TestVerifierBatchPoster(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	privateKey, err := crypto.GenerateKey()
	Require(t, err)
	signingAddr := crypto.PubkeyToAddress(privateKey.PublicKey)
	dataSigner := DataSignerFromPrivateKey(privateKey)

	bpVerifier := contracts.NewMockAddressVerifier(signingAddr)
	config := TestingFeedVerifierConfig
	config.AcceptSequencer = true
	verifier, err := NewVerifier(&config, bpVerifier)
	Require(t, err)

	data := []byte{0, 1, 2, 3, 4, 5, 6, 7}
	hash := crypto.Keccak256Hash(data)

	signature, err := dataSigner(hash.Bytes())
	Require(t, err, "error signing data")

	err = verifier.VerifyData(ctx, signature, data)
	Require(t, err, "error verifying data")

	err = verifier.VerifyHash(ctx, signature, hash)
	Require(t, err, "error verifying data")

	badKey, err := crypto.GenerateKey()
	Require(t, err)
	badDataSigner := DataSignerFromPrivateKey(badKey)
	badSignature, err := badDataSigner(hash.Bytes())
	Require(t, err, "error signing data")

	err = verifier.VerifyData(ctx, badSignature, data)
	if !errors.Is(err, ErrSignerNotApproved) {
		t.Error("unexpected error", err)
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

'''
'''--- util/stopwaiter/stopwaiter.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package stopwaiter

import (
	"context"
	"errors"
	"reflect"
	"runtime"
	"strings"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/containers"
)

const stopDelayWarningTimeout = 30 * time.Second

type StopWaiterSafe struct {
	mutex     sync.Mutex // protects started, stopped, ctx, parentCtx, stopFunc
	started   bool
	stopped   bool
	ctx       context.Context
	parentCtx context.Context
	stopFunc  func()
	name      string
	waitChan  <-chan interface{}

	wg sync.WaitGroup
}

func (s *StopWaiterSafe) Started() bool {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	return s.started
}

func (s *StopWaiterSafe) Stopped() bool {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	return s.stopped
}

func (s *StopWaiterSafe) GetContextSafe() (context.Context, error) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	return s.getContext()
}

// this context is not cancelled even after someone calls Stop
func (s *StopWaiterSafe) GetParentContextSafe() (context.Context, error) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	return s.getParentContext()
}

// Only call this internally with the mutex held.
func (s *StopWaiterSafe) getContext() (context.Context, error) {
	if s.started {
		return s.ctx, nil
	}
	return nil, errors.New("not started")
}

// Only call this internally with the mutex held.
func (s *StopWaiterSafe) getParentContext() (context.Context, error) {
	if s.started {
		return s.parentCtx, nil
	}
	return nil, errors.New("not started")
}

func getParentName(parent any) string {
	// remove asterisk in case the type is a pointer
	return strings.Replace(reflect.TypeOf(parent).String(), "*", "", 1)
}

// start-after-start will error, start-after-stop will immediately cancel
func (s *StopWaiterSafe) Start(ctx context.Context, parent any) error {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	if s.started {
		return errors.New("start after start")
	}
	s.started = true
	s.name = getParentName(parent)
	s.parentCtx = ctx
	s.ctx, s.stopFunc = context.WithCancel(s.parentCtx)
	if s.stopped {
		s.stopFunc()
	}
	return nil
}

func (s *StopWaiterSafe) StopOnly() {
	_ = s.stopOnly()
}

// returns true if stop function was called
func (s *StopWaiterSafe) stopOnly() bool {
	stopWasCalled := false
	s.mutex.Lock()
	defer s.mutex.Unlock()
	if s.started && !s.stopped {
		s.stopFunc()
		stopWasCalled = true
	}
	s.stopped = true
	return stopWasCalled
}

// StopAndWait may be called multiple times, even before start.
func (s *StopWaiterSafe) StopAndWait() error {
	return s.stopAndWaitImpl(stopDelayWarningTimeout)
}

func getAllStackTraces() string {
	buf := make([]byte, 64*1024*1024)
	size := runtime.Stack(buf, true)
	builder := strings.Builder{}
	builder.Write(buf[0:size])
	return builder.String()
}

func (s *StopWaiterSafe) stopAndWaitImpl(warningTimeout time.Duration) error {
	if !s.stopOnly() {
		return nil
	}
	waitChan, err := s.GetWaitChannel()
	if err != nil {
		return err
	}
	timer := time.NewTimer(warningTimeout)

	select {
	case <-timer.C:
		traces := getAllStackTraces()
		log.Warn("taking too long to stop", "name", s.name, "delay[s]", warningTimeout.Seconds())
		log.Warn(traces)
	case <-waitChan:
		timer.Stop()
		return nil
	}
	<-waitChan
	return nil
}

func (s *StopWaiterSafe) GetWaitChannel() (<-chan interface{}, error) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	if s.waitChan == nil {
		ctx, err := s.getContext()
		if err != nil {
			return nil, err
		}
		waitChan := make(chan interface{})
		go func() {
			<-ctx.Done()
			s.wg.Wait()
			close(waitChan)
		}()
		s.waitChan = waitChan
	}
	return s.waitChan, nil
}

// If stop was already called, thread might silently not be launched
func (s *StopWaiterSafe) LaunchThreadSafe(foo func(context.Context)) error {
	ctx, err := s.GetContextSafe()
	if err != nil {
		return err
	}
	if s.Stopped() {
		return nil
	}
	s.wg.Add(1)
	go func() {
		foo(ctx)
		s.wg.Done()
	}()
	return nil
}

// This calls go foo() directly, with the benefit of being easily searchable.
// Callers may rely on the assumption that foo runs even if this is stopped.
func (s *StopWaiterSafe) LaunchUntrackedThread(foo func()) {
	go foo()
}

// CallIteratively calls function iteratively in a thread.
// input param return value is how long to wait before next invocation
func (s *StopWaiterSafe) CallIterativelySafe(foo func(context.Context) time.Duration) error {
	return s.LaunchThreadSafe(func(ctx context.Context) {
		for {
			interval := foo(ctx)
			if ctx.Err() != nil {
				return
			}
			if interval == time.Duration(0) {
				continue
			}
			timer := time.NewTimer(interval)
			select {
			case <-ctx.Done():
				timer.Stop()
				return
			case <-timer.C:
			}
		}
	})
}

type ThreadLauncher interface {
	GetContextSafe() (context.Context, error)
	LaunchThreadSafe(foo func(context.Context)) error
	LaunchUntrackedThread(foo func())
	Stopped() bool
}

// CallIterativelyWith calls function iteratively in a thread.
// The return value of foo is how long to wait before next invocation
// Anything sent to triggerChan parameter triggers call to happen immediately
func CallIterativelyWith[T any](
	s ThreadLauncher,
	foo func(context.Context, T) time.Duration,
	triggerChan <-chan T,
) error {
	return s.LaunchThreadSafe(func(ctx context.Context) {
		var defaultVal T
		var val T
		for {
			interval := foo(ctx, val)
			if ctx.Err() != nil {
				return
			}
			val = defaultVal
			if interval == time.Duration(0) {
				continue
			}
			timer := time.NewTimer(interval)
			select {
			case <-ctx.Done():
				timer.Stop()
				return
			case <-timer.C:
			case val = <-triggerChan:
			}
		}
	})
}

func LaunchPromiseThread[T any](
	s ThreadLauncher,
	foo func(context.Context) (T, error),
) containers.PromiseInterface[T] {
	ctx, err := s.GetContextSafe()
	if err != nil {
		promise := containers.NewPromise[T](nil)
		promise.ProduceError(err)
		return &promise
	}
	if s.Stopped() {
		promise := containers.NewPromise[T](nil)
		promise.ProduceError(errors.New("stopped"))
		return &promise
	}
	innerCtx, cancel := context.WithCancel(ctx)
	promise := containers.NewPromise[T](cancel)
	err = s.LaunchThreadSafe(func(context.Context) { // we don't use the param's context
		val, err := foo(innerCtx)
		if err != nil {
			promise.ProduceError(err)
		} else {
			promise.Produce(val)
		}
		cancel()
	})
	if err != nil {
		promise.ProduceError(err)
	}
	return &promise
}

func ChanRateLimiter[T any](s *StopWaiterSafe, inChan <-chan T, maxRateCallback func() time.Duration) (<-chan T, error) {
	outChan := make(chan T)
	err := s.LaunchThreadSafe(func(ctx context.Context) {
		nextAllowedTriggerTime := time.Now()
		for {
			select {
			case <-ctx.Done():
				close(outChan)
				return
			case data := <-inChan:
				now := time.Now()
				if now.After(nextAllowedTriggerTime) {
					outChan <- data
					nextAllowedTriggerTime = now.Add(maxRateCallback())
				}
			}
		}
	})
	if err != nil {
		close(outChan)
		return nil, err
	}

	return outChan, nil
}

// StopWaiter may panic on race conditions instead of returning errors
type StopWaiter struct {
	StopWaiterSafe
}

func (s *StopWaiter) Start(ctx context.Context, parent any) {
	if err := s.StopWaiterSafe.Start(ctx, parent); err != nil {
		panic(err)
	}
}

func (s *StopWaiter) StopAndWait() {
	if err := s.StopWaiterSafe.StopAndWait(); err != nil {
		panic(err)
	}
}

// If stop was already called, thread might silently not be launched
func (s *StopWaiter) LaunchThread(foo func(context.Context)) {
	if err := s.StopWaiterSafe.LaunchThreadSafe(foo); err != nil {
		panic(err)
	}
}

func (s *StopWaiter) CallIteratively(foo func(context.Context) time.Duration) {
	if err := s.StopWaiterSafe.CallIterativelySafe(foo); err != nil {
		panic(err)
	}
}

func (s *StopWaiter) GetContext() context.Context {
	ctx, err := s.StopWaiterSafe.GetContextSafe()
	if err != nil {
		panic(err)
	}
	return ctx
}

func (s *StopWaiter) GetParentContext() context.Context {
	ctx, err := s.StopWaiterSafe.GetParentContextSafe()
	if err != nil {
		panic(err)
	}
	return ctx
}

'''
'''--- util/stopwaiter/stopwaiter_promise_test.go ---
package stopwaiter

import (
	"context"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

type ClassA struct {
	StopWaiter
}

func (c *ClassA) Start(ctx context.Context) {
	c.StopWaiter.Start(ctx, c)
}

func (c *ClassA) shortFunc() (uint64, error) {
	return 42, nil
}

func (c *ClassA) longFunc(ctx context.Context, delay time.Duration) (uint64, error) {
	select {
	case <-time.After(delay):
		return 42, nil
	case <-ctx.Done():
		return 0, ctx.Err()
	}
}

func (c *ClassA) ShortFunc() containers.PromiseInterface[uint64] {
	return containers.NewReadyPromise[uint64](c.shortFunc())
}

func (c *ClassA) LongFunc(delay time.Duration) containers.PromiseInterface[uint64] {
	return LaunchPromiseThread[uint64](c, func(ctx context.Context) (uint64, error) {
		return c.longFunc(ctx, delay)
	})
}

type Caller struct {
	StopWaiter
	calee *ClassA
}

func (c *Caller) Start(ctx context.Context) {
	c.StopWaiter.Start(ctx, c)
}

func (c *Caller) ShortCaller() error {
	_, err := c.calee.ShortFunc().Await(c.GetContext())
	return err
}

func (c *Caller) LongCaller(delay time.Duration) error {
	_, err := c.calee.LongFunc(delay).Await(c.GetContext())
	return err
}

func TestStopWaiterPromise(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	classA := &ClassA{}
	caller := &Caller{
		calee: classA,
	}
	classA.Start(ctx)
	caller.Start(ctx)

	Require(t, caller.ShortCaller())
	Require(t, caller.LongCaller(time.Millisecond*200))

	go func() {
		<-time.After(time.Millisecond * 10)
		caller.StopAndWait()
	}()
	err := caller.LongCaller(time.Minute)
	if err == nil {
		t.Fatal("longcaller succeeded after caller stop")
	}

	callerB := &Caller{
		calee: classA,
	}
	callerB.Start(ctx)
	Require(t, callerB.LongCaller(time.Millisecond*200))

	go func() {
		<-time.After(time.Millisecond * 10)
		classA.StopAndWait()
	}()
	err = callerB.LongCaller(time.Minute)
	if err == nil {
		t.Fatal("longcaller succeeded after caller stop")
	}
}

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

'''
'''--- util/stopwaiter/stopwaiter_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package stopwaiter

import (
	"context"
	"testing"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

const testStopDelayWarningTimeout = 350 * time.Millisecond

type TestStruct struct{}

func TestStopWaiterStopAndWaitTimeoutShouldWarn(t *testing.T) {
	logHandler := testhelpers.InitTestLog(t, log.LvlTrace)
	sw := StopWaiter{}
	testCtx, cancel := context.WithCancel(context.Background())
	defer cancel()
	sw.Start(context.Background(), &TestStruct{})
	sw.LaunchThread(func(ctx context.Context) {
		<-testCtx.Done()
	})
	go func() {
		err := sw.stopAndWaitImpl(testStopDelayWarningTimeout)
		testhelpers.RequireImpl(t, err)
	}()
	time.Sleep(testStopDelayWarningTimeout + 100*time.Millisecond)
	if !logHandler.WasLogged("taking too long to stop") {
		testhelpers.FailImpl(t, "Failed to log about waiting long on StopAndWait")
	}
}

func TestStopWaiterStopAndWaitTimeoutShouldNotWarn(t *testing.T) {
	logHandler := testhelpers.InitTestLog(t, log.LvlTrace)
	sw := StopWaiter{}
	sw.Start(context.Background(), &TestStruct{})
	sw.LaunchThread(func(ctx context.Context) {
		<-ctx.Done()
	})
	sw.StopAndWait()
	if logHandler.WasLogged("taking too long to stop") {
		testhelpers.FailImpl(t, "Incorrectly logged about waiting long on StopAndWait")
	}
}

func TestStopWaiterStopAndWaitBeforeStart(t *testing.T) {
	sw := StopWaiter{}
	sw.StopAndWait()
}

func TestStopWaiterStopAndWaitAfterStop(t *testing.T) {
	sw := StopWaiter{}
	sw.Start(context.Background(), &TestStruct{})
	ctx := sw.GetContext()
	sw.StopOnly()
	<-ctx.Done()
	sw.StopAndWait()
}

func TestStopWaiterStopAndWaitMultipleTimes(t *testing.T) {
	sw := StopWaiter{}
	sw.StopAndWait()
	sw.StopAndWait()
	sw.StopAndWait()
	sw.Start(context.Background(), &TestStruct{})
	sw.StopAndWait()
	sw.StopAndWait()
	sw.StopAndWait()
}

'''
'''--- util/testhelpers/pseudorandom.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package testhelpers

import (
	"math/rand"
	"testing"

	"github.com/ethereum/go-ethereum/common"
)

type PseudoRandomDataSource struct {
	rand *rand.Rand
}

// NewPseudoRandomDataSource is the pseudorandom source that repeats on different executions
// T param is to make sure it's only used in testing
func NewPseudoRandomDataSource(_ *testing.T, seed int64) *PseudoRandomDataSource {
	return &PseudoRandomDataSource{
		rand: rand.New(rand.NewSource(seed)),
	}
}

func (r *PseudoRandomDataSource) GetHash() common.Hash {
	var outHash common.Hash
	r.rand.Read(outHash[:])
	return outHash
}

func (r *PseudoRandomDataSource) GetAddress() common.Address {
	return common.BytesToAddress(r.GetHash().Bytes()[:20])
}

func (r *PseudoRandomDataSource) GetUint64() uint64 {
	return r.rand.Uint64()
}

func (r *PseudoRandomDataSource) GetData(size int) []byte {
	outArray := make([]byte, size)
	r.rand.Read(outArray)
	return outArray
}

'''
'''--- util/testhelpers/testhelpers.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package testhelpers

import (
	"crypto/rand"
	"os"
	"regexp"
	"sync"
	"testing"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/colors"
)

// Fail a test should an error occur
func RequireImpl(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	if err != nil {
		t.Fatal(colors.Red, printables, err, colors.Clear)
	}
}

func FailImpl(t *testing.T, printables ...interface{}) {
	t.Helper()
	t.Fatal(colors.Red, printables, colors.Clear)
}

func RandomizeSlice(slice []byte) []byte {
	_, err := rand.Read(slice)
	if err != nil {
		panic(err)
	}
	return slice
}

func RandomAddress() common.Address {
	var address common.Address
	RandomizeSlice(address[:])
	return address
}

type LogHandler struct {
	mutex         sync.Mutex
	t             *testing.T
	records       []log.Record
	streamHandler log.Handler
}

func (h *LogHandler) Log(record *log.Record) error {
	if err := h.streamHandler.Log(record); err != nil {
		return err
	}
	h.mutex.Lock()
	defer h.mutex.Unlock()
	h.records = append(h.records, *record)
	return nil
}

func (h *LogHandler) WasLogged(pattern string) bool {
	re, err := regexp.Compile(pattern)
	RequireImpl(h.t, err)
	h.mutex.Lock()
	defer h.mutex.Unlock()
	for _, record := range h.records {
		if re.MatchString(record.Msg) {
			return true
		}
	}
	return false
}

func newLogHandler(t *testing.T) *LogHandler {
	return &LogHandler{
		t:             t,
		records:       make([]log.Record, 0),
		streamHandler: log.StreamHandler(os.Stderr, log.TerminalFormat(false)),
	}
}

func InitTestLog(t *testing.T, level log.Lvl) *LogHandler {
	handler := newLogHandler(t)
	glogger := log.NewGlogHandler(handler)
	glogger.Verbosity(level)
	log.Root().SetHandler(glogger)
	return handler
}

'''
'''--- validator/execution_state.go ---
package validator

import (
	"encoding/binary"
	"fmt"
	"math"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/offchainlabs/nitro/solgen/go/challengegen"
	"github.com/offchainlabs/nitro/solgen/go/rollupgen"
)

type GoGlobalState struct {
	BlockHash  common.Hash
	SendRoot   common.Hash
	Batch      uint64
	PosInBatch uint64
}

type MachineStatus uint8

const (
	MachineStatusRunning  MachineStatus = 0
	MachineStatusFinished MachineStatus = 1
	MachineStatusErrored  MachineStatus = 2
	MachineStatusTooFar   MachineStatus = 3
)

type ExecutionState struct {
	GlobalState   GoGlobalState
	MachineStatus MachineStatus
}

func u64ToBe(x uint64) []byte {
	data := make([]byte, 8)
	binary.BigEndian.PutUint64(data, x)
	return data
}

func (s GoGlobalState) Hash() common.Hash {
	data := []byte("Global state:")
	data = append(data, s.BlockHash.Bytes()...)
	data = append(data, s.SendRoot.Bytes()...)
	data = append(data, u64ToBe(s.Batch)...)
	data = append(data, u64ToBe(s.PosInBatch)...)
	return crypto.Keccak256Hash(data)
}

func (s GoGlobalState) AsSolidityStruct() challengegen.GlobalState {
	return challengegen.GlobalState{
		Bytes32Vals: [2][32]byte{s.BlockHash, s.SendRoot},
		U64Vals:     [2]uint64{s.Batch, s.PosInBatch},
	}
}

func NewExecutionStateFromSolidity(eth rollupgen.ExecutionState) *ExecutionState {
	return &ExecutionState{
		GlobalState:   GoGlobalStateFromSolidity(challengegen.GlobalState(eth.GlobalState)),
		MachineStatus: MachineStatus(eth.MachineStatus),
	}
}

func GoGlobalStateFromSolidity(gs challengegen.GlobalState) GoGlobalState {
	return GoGlobalState{
		BlockHash:  gs.Bytes32Vals[0],
		SendRoot:   gs.Bytes32Vals[1],
		Batch:      gs.U64Vals[0],
		PosInBatch: gs.U64Vals[1],
	}
}

func (s *ExecutionState) AsSolidityStruct() rollupgen.ExecutionState {
	return rollupgen.ExecutionState{
		GlobalState:   rollupgen.GlobalState(s.GlobalState.AsSolidityStruct()),
		MachineStatus: uint8(s.MachineStatus),
	}
}

func (s *ExecutionState) BlockStateHash() common.Hash {
	if s.MachineStatus == MachineStatusFinished {
		return crypto.Keccak256Hash([]byte("Block state:"), s.GlobalState.Hash().Bytes())
	} else if s.MachineStatus == MachineStatusErrored {
		return crypto.Keccak256Hash([]byte("Block state, errored:"), s.GlobalState.Hash().Bytes())
	} else if s.MachineStatus == MachineStatusTooFar {
		return crypto.Keccak256Hash([]byte("Block state, too far:"))
	} else {
		panic(fmt.Sprintf("invalid machine status %v", s.MachineStatus))
	}
}

// RequiredBatches determines the batch count required to reach the execution state.
// If the machine errored or the state is after the beginning of the batch,
// the current batch is required to reach the state.
// That's because if the machine errored, it might've read the current batch before erroring,
// and if it's in the middle of a batch, it had to read prior parts of the batch to get there.
// However, if the machine finished successfully and the new state is the start of the batch,
// it hasn't read the batch yet, as it just finished the last batch.
//
// This logic is replicated in Solidity in a few places; search for RequiredBatches to find them.
func (s *ExecutionState) RequiredBatches() uint64 {
	count := s.GlobalState.Batch
	if (s.MachineStatus == MachineStatusErrored || s.GlobalState.PosInBatch > 0) && count < math.MaxUint64 {
		// The current batch was read
		count++
	}
	return count
}

type MachineStepResult struct {
	Hash        common.Hash
	Position    uint64
	Status      MachineStatus
	GlobalState GoGlobalState
}

'''
'''--- validator/interface.go ---
package validator

import (
	"context"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/util/containers"
)

type ValidationSpawner interface {
	Launch(entry *ValidationInput, moduleRoot common.Hash) ValidationRun
	Start(context.Context) error
	Stop()
	Name() string
	Room() int
}

type ValidationRun interface {
	containers.PromiseInterface[GoGlobalState]
	WasmModuleRoot() common.Hash
}

type ExecutionSpawner interface {
	ValidationSpawner
	CreateExecutionRun(wasmModuleRoot common.Hash, input *ValidationInput) containers.PromiseInterface[ExecutionRun]
	LatestWasmModuleRoot() containers.PromiseInterface[common.Hash]
	WriteToFile(input *ValidationInput, expOut GoGlobalState, moduleRoot common.Hash) containers.PromiseInterface[struct{}]
}

type ExecutionRun interface {
	GetStepAt(uint64) containers.PromiseInterface[*MachineStepResult]
	GetLastStep() containers.PromiseInterface[*MachineStepResult]
	GetProofAt(uint64) containers.PromiseInterface[[]byte]
	PrepareRange(uint64, uint64) containers.PromiseInterface[struct{}]
	Close()
}

'''
'''--- validator/server_api/json.go ---
// Copyright 2023, Offchain Labs, Inc.
// For license information, see https://github.com/OffchainLabs/nitro/blob/master/LICENSE

package server_api

import (
	"encoding/base64"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/jsonapi"
	"github.com/offchainlabs/nitro/validator"
)

type BatchInfoJson struct {
	Number  uint64
	DataB64 string
}

type ValidationInputJson struct {
	Id            uint64
	HasDelayedMsg bool
	DelayedMsgNr  uint64
	PreimagesB64  map[arbutil.PreimageType]*jsonapi.PreimagesMapJson
	BatchInfo     []BatchInfoJson
	DelayedMsgB64 string
	StartState    validator.GoGlobalState
}

func ValidationInputToJson(entry *validator.ValidationInput) *ValidationInputJson {
	jsonPreimagesMap := make(map[arbutil.PreimageType]*jsonapi.PreimagesMapJson)
	for ty, preimages := range entry.Preimages {
		jsonPreimagesMap[ty] = jsonapi.NewPreimagesMapJson(preimages)
	}
	res := &ValidationInputJson{
		Id:            entry.Id,
		HasDelayedMsg: entry.HasDelayedMsg,
		DelayedMsgNr:  entry.DelayedMsgNr,
		DelayedMsgB64: base64.StdEncoding.EncodeToString(entry.DelayedMsg),
		StartState:    entry.StartState,
		PreimagesB64:  jsonPreimagesMap,
	}
	for _, binfo := range entry.BatchInfo {
		encData := base64.StdEncoding.EncodeToString(binfo.Data)
		res.BatchInfo = append(res.BatchInfo, BatchInfoJson{binfo.Number, encData})
	}
	return res
}

func ValidationInputFromJson(entry *ValidationInputJson) (*validator.ValidationInput, error) {
	preimages := make(map[arbutil.PreimageType]map[common.Hash][]byte)
	for ty, jsonPreimages := range entry.PreimagesB64 {
		preimages[ty] = jsonPreimages.Map
	}
	valInput := &validator.ValidationInput{
		Id:            entry.Id,
		HasDelayedMsg: entry.HasDelayedMsg,
		DelayedMsgNr:  entry.DelayedMsgNr,
		StartState:    entry.StartState,
		Preimages:     preimages,
	}
	delayed, err := base64.StdEncoding.DecodeString(entry.DelayedMsgB64)
	if err != nil {
		return nil, err
	}
	valInput.DelayedMsg = delayed
	for _, binfo := range entry.BatchInfo {
		data, err := base64.StdEncoding.DecodeString(binfo.DataB64)
		if err != nil {
			return nil, err
		}
		decInfo := validator.BatchInfo{
			Number: binfo.Number,
			Data:   data,
		}
		valInput.BatchInfo = append(valInput.BatchInfo, decInfo)
	}
	return valInput, nil
}

type MachineStepResultJson struct {
	Hash        common.Hash
	Position    uint64
	Status      uint8
	GlobalState validator.GoGlobalState
}

func MachineStepResultToJson(result *validator.MachineStepResult) *MachineStepResultJson {
	return &MachineStepResultJson{
		Hash:        result.Hash,
		Position:    result.Position,
		Status:      uint8(result.Status),
		GlobalState: result.GlobalState,
	}
}

func MachineStepResultFromJson(resultJson *MachineStepResultJson) (*validator.MachineStepResult, error) {

	return &validator.MachineStepResult{
		Hash:        resultJson.Hash,
		Position:    resultJson.Position,
		Status:      validator.MachineStatus(resultJson.Status),
		GlobalState: resultJson.GlobalState,
	}, nil
}

'''
'''--- validator/server_api/valiation_api.go ---
package server_api

import (
	"context"
	"encoding/base64"
	"errors"
	"math/rand"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_arb"
)

const Namespace string = "validation"

type ValidationServerAPI struct {
	spawner validator.ValidationSpawner
}

func (a *ValidationServerAPI) Name() string {
	return a.spawner.Name()
}

func (a *ValidationServerAPI) Room() int {
	return a.spawner.Room()
}

func (a *ValidationServerAPI) Validate(ctx context.Context, entry *ValidationInputJson, moduleRoot common.Hash) (validator.GoGlobalState, error) {
	valInput, err := ValidationInputFromJson(entry)
	if err != nil {
		return validator.GoGlobalState{}, err
	}
	valRun := a.spawner.Launch(valInput, moduleRoot)
	return valRun.Await(ctx)
}

func NewValidationServerAPI(spawner validator.ValidationSpawner) *ValidationServerAPI {
	return &ValidationServerAPI{spawner}
}

type execRunEntry struct {
	run      validator.ExecutionRun
	accessed time.Time
}

type ExecServerAPI struct {
	stopwaiter.StopWaiter
	ValidationServerAPI
	execSpawner validator.ExecutionSpawner

	config server_arb.ArbitratorSpawnerConfigFecher

	runIdLock sync.Mutex
	nextId    uint64
	runs      map[uint64]*execRunEntry
}

func NewExecutionServerAPI(valSpawner validator.ValidationSpawner, execution validator.ExecutionSpawner, config server_arb.ArbitratorSpawnerConfigFecher) *ExecServerAPI {
	return &ExecServerAPI{
		ValidationServerAPI: *NewValidationServerAPI(valSpawner),
		execSpawner:         execution,
		nextId:              rand.Uint64(), // good-enough to aver reusing ids after reboot
		runs:                make(map[uint64]*execRunEntry),
		config:              config,
	}
}

func (a *ExecServerAPI) CreateExecutionRun(ctx context.Context, wasmModuleRoot common.Hash, jsonInput *ValidationInputJson) (uint64, error) {
	input, err := ValidationInputFromJson(jsonInput)
	if err != nil {
		return 0, err
	}
	execRun, err := a.execSpawner.CreateExecutionRun(wasmModuleRoot, input).Await(ctx)
	if err != nil {
		return 0, err
	}
	a.runIdLock.Lock()
	defer a.runIdLock.Unlock()
	newId := a.nextId
	a.nextId++
	a.runs[newId] = &execRunEntry{execRun, time.Now()}
	return newId, nil
}

func (a *ExecServerAPI) LatestWasmModuleRoot(ctx context.Context) (common.Hash, error) {
	return a.execSpawner.LatestWasmModuleRoot().Await(ctx)
}

func (a *ExecServerAPI) removeOldRuns(ctx context.Context) time.Duration {
	oldestKept := time.Now().Add(-1 * a.config().ExecutionRunTimeout)
	a.runIdLock.Lock()
	defer a.runIdLock.Unlock()
	for id, entry := range a.runs {
		if entry.accessed.Before(oldestKept) {
			delete(a.runs, id)
		}
	}
	return a.config().ExecutionRunTimeout / 5
}

func (a *ExecServerAPI) Start(ctx_in context.Context) {
	a.StopWaiter.Start(ctx_in, a)
	a.CallIteratively(a.removeOldRuns)
}

func (a *ExecServerAPI) WriteToFile(ctx context.Context, jsonInput *ValidationInputJson, expOut validator.GoGlobalState, moduleRoot common.Hash) error {
	input, err := ValidationInputFromJson(jsonInput)
	if err != nil {
		return err
	}
	_, err = a.execSpawner.WriteToFile(input, expOut, moduleRoot).Await(ctx)
	return err
}

var errRunNotFound error = errors.New("run not found")

func (a *ExecServerAPI) getRun(id uint64) (validator.ExecutionRun, error) {
	a.runIdLock.Lock()
	defer a.runIdLock.Unlock()
	entry := a.runs[id]
	if entry == nil {
		return nil, errRunNotFound
	}
	entry.accessed = time.Now()
	return entry.run, nil
}

func (a *ExecServerAPI) GetStepAt(ctx context.Context, execid uint64, position uint64) (*MachineStepResultJson, error) {
	run, err := a.getRun(execid)
	if err != nil {
		return nil, err
	}
	step := run.GetStepAt(position)
	res, err := step.Await(ctx)
	if err != nil {
		return nil, err
	}
	return MachineStepResultToJson(res), nil
}

func (a *ExecServerAPI) GetProofAt(ctx context.Context, execid uint64, position uint64) (string, error) {
	run, err := a.getRun(execid)
	if err != nil {
		return "", err
	}
	promise := run.GetProofAt(position)
	res, err := promise.Await(ctx)
	if err != nil {
		return "", err
	}
	return base64.StdEncoding.EncodeToString(res), nil
}

func (a *ExecServerAPI) PrepareRange(ctx context.Context, execid uint64, start, end uint64) error {
	run, err := a.getRun(execid)
	if err != nil {
		return err
	}
	_, err = run.PrepareRange(start, end).Await(ctx)
	return err
}

func (a *ExecServerAPI) ExecKeepAlive(ctx context.Context, execid uint64) error {
	_, err := a.getRun(execid)
	if err != nil {
		return err
	}
	return nil
}

func (a *ExecServerAPI) CloseExec(execid uint64) {
	a.runIdLock.Lock()
	defer a.runIdLock.Unlock()
	run, found := a.runs[execid]
	if !found {
		return
	}
	run.run.Close()
	delete(a.runs, execid)
}

'''
'''--- validator/server_api/validation_client.go ---
package server_api

import (
	"context"
	"encoding/base64"
	"errors"
	"sync/atomic"
	"time"

	"github.com/offchainlabs/nitro/validator"

	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/rpcclient"
	"github.com/offchainlabs/nitro/util/stopwaiter"

	"github.com/offchainlabs/nitro/validator/server_common"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
)

type ValidationClient struct {
	stopwaiter.StopWaiter
	client *rpcclient.RpcClient
	name   string
	room   int32
}

func NewValidationClient(config rpcclient.ClientConfigFetcher, stack *node.Node) *ValidationClient {
	return &ValidationClient{
		client: rpcclient.NewRpcClient(config, stack),
	}
}

func (c *ValidationClient) Launch(entry *validator.ValidationInput, moduleRoot common.Hash) validator.ValidationRun {
	atomic.AddInt32(&c.room, -1)
	promise := stopwaiter.LaunchPromiseThread[validator.GoGlobalState](c, func(ctx context.Context) (validator.GoGlobalState, error) {
		input := ValidationInputToJson(entry)
		var res validator.GoGlobalState
		err := c.client.CallContext(ctx, &res, Namespace+"_validate", input, moduleRoot)
		atomic.AddInt32(&c.room, 1)
		return res, err
	})
	return server_common.NewValRun(promise, moduleRoot)
}

func (c *ValidationClient) Start(ctx_in context.Context) error {
	c.StopWaiter.Start(ctx_in, c)
	ctx := c.GetContext()
	err := c.client.Start(ctx)
	if err != nil {
		return err
	}
	var name string
	err = c.client.CallContext(ctx, &name, Namespace+"_name")
	if err != nil {
		return err
	}
	if len(name) == 0 {
		return errors.New("couldn't read name from server")
	}
	var room int
	err = c.client.CallContext(c.GetContext(), &room, Namespace+"_room")
	if err != nil {
		return err
	}
	if room < 2 {
		log.Warn("validation server not enough room, overriding to 2", "name", name, "room", room)
		room = 2
	} else {
		log.Info("connected to validation server", "name", name, "room", room)
	}
	atomic.StoreInt32(&c.room, int32(room))
	c.name = name
	return nil
}

func (c *ValidationClient) Stop() {
	c.StopWaiter.StopOnly()
	if c.client != nil {
		c.client.Close()
	}
}

func (c *ValidationClient) Name() string {
	if c.Started() {
		return c.name
	}
	return "(not started)"
}

func (c *ValidationClient) Room() int {
	room32 := atomic.LoadInt32(&c.room)
	if room32 < 0 {
		return 0
	}
	return int(room32)
}

type ExecutionClient struct {
	ValidationClient
}

func NewExecutionClient(config rpcclient.ClientConfigFetcher, stack *node.Node) *ExecutionClient {
	return &ExecutionClient{
		ValidationClient: *NewValidationClient(config, stack),
	}
}

func (c *ExecutionClient) CreateExecutionRun(wasmModuleRoot common.Hash, input *validator.ValidationInput) containers.PromiseInterface[validator.ExecutionRun] {
	return stopwaiter.LaunchPromiseThread[validator.ExecutionRun](c, func(ctx context.Context) (validator.ExecutionRun, error) {
		var res uint64
		err := c.client.CallContext(ctx, &res, Namespace+"_createExecutionRun", wasmModuleRoot, ValidationInputToJson(input))
		if err != nil {
			return nil, err
		}
		run := &ExecutionClientRun{
			client: c,
			id:     res,
		}
		run.Start(c.GetContext()) // note: not this temporary thread's context!
		return run, nil
	})
}

type ExecutionClientRun struct {
	stopwaiter.StopWaiter
	client *ExecutionClient
	id     uint64
}

func (c *ExecutionClient) LatestWasmModuleRoot() containers.PromiseInterface[common.Hash] {
	return stopwaiter.LaunchPromiseThread[common.Hash](c, func(ctx context.Context) (common.Hash, error) {
		var res common.Hash
		err := c.client.CallContext(ctx, &res, Namespace+"_latestWasmModuleRoot")
		if err != nil {
			return common.Hash{}, err
		}
		return res, nil
	})
}

func (c *ExecutionClient) WriteToFile(input *validator.ValidationInput, expOut validator.GoGlobalState, moduleRoot common.Hash) containers.PromiseInterface[struct{}] {
	jsonInput := ValidationInputToJson(input)
	return stopwaiter.LaunchPromiseThread[struct{}](c, func(ctx context.Context) (struct{}, error) {
		err := c.client.CallContext(ctx, nil, Namespace+"_writeToFile", jsonInput, expOut, moduleRoot)
		return struct{}{}, err
	})
}

func (r *ExecutionClientRun) SendKeepAlive(ctx context.Context) time.Duration {
	err := r.client.client.CallContext(ctx, nil, Namespace+"_execKeepAlive", r.id)
	if err != nil {
		log.Error("execution run keepalive failed", "err", err)
	}
	return time.Minute // TODO: configurable
}

func (r *ExecutionClientRun) Start(ctx_in context.Context) {
	r.StopWaiter.Start(ctx_in, r)
	r.CallIteratively(r.SendKeepAlive)
}

func (r *ExecutionClientRun) GetStepAt(pos uint64) containers.PromiseInterface[*validator.MachineStepResult] {
	return stopwaiter.LaunchPromiseThread[*validator.MachineStepResult](r, func(ctx context.Context) (*validator.MachineStepResult, error) {
		var resJson MachineStepResultJson
		err := r.client.client.CallContext(ctx, &resJson, Namespace+"_getStepAt", r.id, pos)
		if err != nil {
			return nil, err
		}
		res, err := MachineStepResultFromJson(&resJson)
		if err != nil {
			return nil, err
		}
		return res, err
	})
}

func (r *ExecutionClientRun) GetProofAt(pos uint64) containers.PromiseInterface[[]byte] {
	return stopwaiter.LaunchPromiseThread[[]byte](r, func(ctx context.Context) ([]byte, error) {
		var resString string
		err := r.client.client.CallContext(ctx, &resString, Namespace+"_getProofAt", r.id, pos)
		if err != nil {
			return nil, err
		}
		return base64.StdEncoding.DecodeString(resString)
	})
}

func (r *ExecutionClientRun) GetLastStep() containers.PromiseInterface[*validator.MachineStepResult] {
	return r.GetStepAt(^uint64(0))
}

func (r *ExecutionClientRun) PrepareRange(start, end uint64) containers.PromiseInterface[struct{}] {
	return stopwaiter.LaunchPromiseThread[struct{}](r, func(ctx context.Context) (struct{}, error) {
		err := r.client.client.CallContext(ctx, nil, Namespace+"_prepareRange", r.id, start, end)
		if err != nil && ctx.Err() == nil {
			log.Warn("prepare execution got error", "err", err)
		}
		return struct{}{}, err
	})
}

func (r *ExecutionClientRun) Close() {
	r.StopOnly()
	r.LaunchUntrackedThread(func() {
		err := r.client.client.CallContext(r.GetParentContext(), nil, Namespace+"_closeExec", r.id)
		if err != nil {
			log.Warn("closing execution client run got error", "err", err, "client", r.client.Name(), "id", r.id)
		}
	})
}

'''
'''--- validator/server_arb/execution_run.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

import (
	"context"
	"fmt"
	"sync"

	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
)

type executionRun struct {
	stopwaiter.StopWaiter
	cache *MachineCache
	close sync.Once
}

// NewExecutionChallengeBackend creates a backend with the given arguments.
// Note: machineCache may be nil, but if present, it must not have a restricted range.
func NewExecutionRun(
	ctxIn context.Context,
	initialMachineGetter func(context.Context) (MachineInterface, error),
	config *MachineCacheConfig,
) (*executionRun, error) {
	exec := &executionRun{}
	exec.Start(ctxIn, exec)
	exec.cache = NewMachineCache(exec.GetContext(), initialMachineGetter, config)
	return exec, nil
}

func (e *executionRun) Close() {
	go e.close.Do(func() {
		e.StopAndWait()
		if e.cache != nil {
			e.cache.Destroy(e.GetParentContext())
		}
	})
}

func (e *executionRun) PrepareRange(start uint64, end uint64) containers.PromiseInterface[struct{}] {
	return stopwaiter.LaunchPromiseThread[struct{}](e, func(ctx context.Context) (struct{}, error) {
		err := e.cache.SetRange(ctx, start, end)
		return struct{}{}, err
	})
}

func (e *executionRun) GetStepAt(position uint64) containers.PromiseInterface[*validator.MachineStepResult] {
	return stopwaiter.LaunchPromiseThread[*validator.MachineStepResult](e, func(ctx context.Context) (*validator.MachineStepResult, error) {
		var machine MachineInterface
		var err error
		if position == ^uint64(0) {
			machine, err = e.cache.GetFinalMachine(ctx)
		} else {
			// todo cache last machine
			machine, err = e.cache.GetMachineAt(ctx, position)
		}
		if err != nil {
			return nil, err
		}
		machineStep := machine.GetStepCount()
		if position != machineStep {
			machineRunning := machine.IsRunning()
			if machineRunning || machineStep > position {
				return nil, fmt.Errorf("machine is in wrong position want: %d, got: %d", position, machine.GetStepCount())
			}

		}
		result := &validator.MachineStepResult{
			Position:    machineStep,
			Status:      validator.MachineStatus(machine.Status()),
			GlobalState: machine.GetGlobalState(),
			Hash:        machine.Hash(),
		}
		return result, nil
	})
}

func (e *executionRun) GetProofAt(position uint64) containers.PromiseInterface[[]byte] {
	return stopwaiter.LaunchPromiseThread[[]byte](e, func(ctx context.Context) ([]byte, error) {
		machine, err := e.cache.GetMachineAt(ctx, position)
		if err != nil {
			return nil, err
		}
		return machine.ProveNextStep(), nil
	})
}

func (e *executionRun) GetLastStep() containers.PromiseInterface[*validator.MachineStepResult] {
	return e.GetStepAt(^uint64(0))
}

'''
'''--- validator/server_arb/machine.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

/*
#cgo CFLAGS: -g -Wall -I../../target/include/
#include "arbitrator.h"

ResolvedPreimage preimageResolverC(size_t context, uint8_t preimageType, const uint8_t* hash);
*/
import "C"
import (
	"context"
	"errors"
	"fmt"
	"runtime"
	"sync"
	"sync/atomic"
	"unsafe"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/validator"
)

type MachineInterface interface {
	CloneMachineInterface() MachineInterface
	GetStepCount() uint64
	IsRunning() bool
	ValidForStep(uint64) bool
	Status() uint8
	Step(context.Context, uint64) error
	Hash() common.Hash
	GetGlobalState() validator.GoGlobalState
	ProveNextStep() []byte
	Freeze()
	Destroy()
}

// ArbitratorMachine holds an arbitrator machine pointer, and manages its lifetime
type ArbitratorMachine struct {
	mutex     sync.Mutex // needed because go finalizers don't synchronize (meaning they aren't thread safe)
	ptr       *C.struct_Machine
	contextId *int64 // has a finalizer attached to remove the preimage resolver from the global map
	frozen    bool   // does not allow anything that changes machine state, not cloned with the machine
}

// Assert that ArbitratorMachine implements MachineInterface
var _ MachineInterface = (*ArbitratorMachine)(nil)

var preimageResolvers containers.SyncMap[int64, GoPreimageResolver]
var lastPreimageResolverId int64 // atomic

// Any future calls to this machine will result in a panic
func (m *ArbitratorMachine) Destroy() {
	m.mutex.Lock()
	defer m.mutex.Unlock()
	if m.ptr != nil {
		C.arbitrator_free_machine(m.ptr)
		m.ptr = nil
		// We no longer need a finalizer
		runtime.SetFinalizer(m, nil)
	}
	m.contextId = nil
}

func freeContextId(context *int64) {
	preimageResolvers.Delete(*context)
}

func machineFromPointer(ptr *C.struct_Machine) *ArbitratorMachine {
	if ptr == nil {
		return nil
	}
	mach := &ArbitratorMachine{ptr: ptr}
	C.arbitrator_set_preimage_resolver(ptr, (*[0]byte)(C.preimageResolverC))
	runtime.SetFinalizer(mach, (*ArbitratorMachine).Destroy)
	return mach
}

func LoadSimpleMachine(wasm string, libraries []string) (*ArbitratorMachine, error) {
	cWasm := C.CString(wasm)
	cLibraries := CreateCStringList(libraries)
	mach := C.arbitrator_load_machine(cWasm, cLibraries, C.long(len(libraries)))
	C.free(unsafe.Pointer(cWasm))
	FreeCStringList(cLibraries, len(libraries))
	if mach == nil {
		return nil, fmt.Errorf("failed to load simple machine at path %v", wasm)
	}
	return machineFromPointer(mach), nil
}

func (m *ArbitratorMachine) Freeze() {
	m.frozen = true
}

// Even if origin is frozen - clone is not
func (m *ArbitratorMachine) Clone() *ArbitratorMachine {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	newMach := machineFromPointer(C.arbitrator_clone_machine(m.ptr))
	newMach.contextId = m.contextId
	return newMach
}

func (m *ArbitratorMachine) CloneMachineInterface() MachineInterface {
	return m.Clone()
}

func (m *ArbitratorMachine) SetGlobalState(globalState validator.GoGlobalState) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	if m.frozen {
		return errors.New("machine frozen")
	}
	cGlobalState := GlobalStateToC(globalState)
	C.arbitrator_set_global_state(m.ptr, cGlobalState)
	return nil
}

func (m *ArbitratorMachine) GetGlobalState() validator.GoGlobalState {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	cGlobalState := C.arbitrator_global_state(m.ptr)
	return GlobalStateFromC(cGlobalState)
}

func (m *ArbitratorMachine) GetStepCount() uint64 {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	return uint64(C.arbitrator_get_num_steps(m.ptr))
}

func (m *ArbitratorMachine) IsRunning() bool {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	return C.arbitrator_get_status(m.ptr) == C.ARBITRATOR_MACHINE_STATUS_RUNNING
}

func (m *ArbitratorMachine) IsErrored() bool {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	return C.arbitrator_get_status(m.ptr) == C.ARBITRATOR_MACHINE_STATUS_ERRORED
}

func (m *ArbitratorMachine) Status() uint8 {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	return uint8(C.arbitrator_get_status(m.ptr))
}

func (m *ArbitratorMachine) ValidForStep(requestedStep uint64) bool {
	haveStep := m.GetStepCount()
	if haveStep > requestedStep {
		return false
	} else if haveStep == requestedStep {
		return true
	} else { // haveStep < requestedStep
		// if the machine is halted, its state persists for future steps
		return !m.IsRunning()
	}
}

func manageConditionByte(ctx context.Context) (*C.uint8_t, func()) {
	var zero C.uint8_t
	conditionByte := &zero

	doneEarlyChan := make(chan struct{})

	go (func() {
		defer runtime.KeepAlive(conditionByte)
		select {
		case <-ctx.Done():
			C.atomic_u8_store(conditionByte, 1)
		case <-doneEarlyChan:
		}
	})()

	cancel := func() {
		runtime.KeepAlive(conditionByte)
		close(doneEarlyChan)
	}

	return conditionByte, cancel
}

func (m *ArbitratorMachine) Step(ctx context.Context, count uint64) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	if m.frozen {
		return errors.New("machine frozen")
	}
	conditionByte, cancel := manageConditionByte(ctx)
	defer cancel()

	err := C.arbitrator_step(m.ptr, C.uint64_t(count), conditionByte)
	if err != nil {
		errString := C.GoString(err)
		C.free(unsafe.Pointer(err))
		return errors.New(errString)
	}

	return ctx.Err()
}

func (m *ArbitratorMachine) StepUntilHostIo(ctx context.Context) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	if m.frozen {
		return errors.New("machine frozen")
	}

	conditionByte, cancel := manageConditionByte(ctx)
	defer cancel()

	C.arbitrator_step_until_host_io(m.ptr, conditionByte)

	return ctx.Err()
}

func (m *ArbitratorMachine) Hash() (hash common.Hash) {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	bytes := C.arbitrator_hash(m.ptr)
	for i, b := range bytes.bytes {
		hash[i] = byte(b)
	}
	return
}

func (m *ArbitratorMachine) GetModuleRoot() (hash common.Hash) {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	bytes := C.arbitrator_module_root(m.ptr)
	for i, b := range bytes.bytes {
		hash[i] = byte(b)
	}
	return
}
func (m *ArbitratorMachine) ProveNextStep() []byte {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	rustProof := C.arbitrator_gen_proof(m.ptr)
	proofBytes := C.GoBytes(unsafe.Pointer(rustProof.ptr), C.int(rustProof.len))
	C.arbitrator_free_proof(rustProof)

	return proofBytes
}

func (m *ArbitratorMachine) SerializeState(path string) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	cPath := C.CString(path)
	status := C.arbitrator_serialize_state(m.ptr, cPath)
	C.free(unsafe.Pointer(cPath))

	if status != 0 {
		return errors.New("failed to serialize machine state")
	} else {
		return nil
	}
}

func (m *ArbitratorMachine) DeserializeAndReplaceState(path string) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	if m.frozen {
		return errors.New("machine frozen")
	}

	cPath := C.CString(path)
	status := C.arbitrator_deserialize_and_replace_state(m.ptr, cPath)
	C.free(unsafe.Pointer(cPath))

	if status != 0 {
		return errors.New("failed to deserialize machine state")
	} else {
		return nil
	}
}

func (m *ArbitratorMachine) AddSequencerInboxMessage(index uint64, data []byte) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	if m.frozen {
		return errors.New("machine frozen")
	}
	cbyte := CreateCByteArray(data)
	status := C.arbitrator_add_inbox_message(m.ptr, C.uint64_t(0), C.uint64_t(index), cbyte)
	DestroyCByteArray(cbyte)
	if status != 0 {
		return errors.New("failed to add sequencer inbox message")
	} else {
		return nil
	}
}

func (m *ArbitratorMachine) AddDelayedInboxMessage(index uint64, data []byte) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()

	if m.frozen {
		return errors.New("machine frozen")
	}

	cbyte := CreateCByteArray(data)
	status := C.arbitrator_add_inbox_message(m.ptr, C.uint64_t(1), C.uint64_t(index), cbyte)
	DestroyCByteArray(cbyte)
	if status != 0 {
		return errors.New("failed to add sequencer inbox message")
	} else {
		return nil
	}
}

type GoPreimageResolver = func(arbutil.PreimageType, common.Hash) ([]byte, error)

//export preimageResolver
func preimageResolver(context C.size_t, ty C.uint8_t, ptr unsafe.Pointer) C.ResolvedPreimage {
	var hash common.Hash
	input := (*[1 << 30]byte)(ptr)[:32]
	copy(hash[:], input)
	resolver, ok := preimageResolvers.Load(int64(context))
	if !ok {
		return C.ResolvedPreimage{
			len: -1,
		}
	}
	preimage, err := resolver(arbutil.PreimageType(ty), hash)
	if err != nil {
		log.Error("preimage resolution failed", "err", err)
		return C.ResolvedPreimage{
			len: -1,
		}
	}
	return C.ResolvedPreimage{
		ptr: (*C.uint8_t)(C.CBytes(preimage)),
		len: (C.ptrdiff_t)(len(preimage)),
	}
}

func (m *ArbitratorMachine) SetPreimageResolver(resolver GoPreimageResolver) error {
	defer runtime.KeepAlive(m)
	m.mutex.Lock()
	defer m.mutex.Unlock()
	if m.frozen {
		return errors.New("machine frozen")
	}
	id := atomic.AddInt64(&lastPreimageResolverId, 1)
	preimageResolvers.Store(id, resolver)
	m.contextId = &id
	runtime.SetFinalizer(m.contextId, freeContextId)
	C.arbitrator_set_context(m.ptr, C.uint64_t(id))
	return nil
}

'''
'''--- validator/server_arb/machine_cache.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

import (
	"context"
	"errors"
	"fmt"
	"sync"

	flag "github.com/spf13/pflag"
)

// MachineCache manages a list of machines at various step counts.
// Aims to speed the retrieval of a machine at a given step count.
type MachineCache struct {
	buildingLock chan struct{}
	err          error

	zeroStepMachine     MachineInterface
	finalMachine        MachineInterface
	finalMachineStep    uint64
	machines            []MachineInterface
	firstMachineStep    uint64
	machineStepInterval uint64
	config              *MachineCacheConfig

	lastMachine     MachineInterface
	lastMachineLock sync.Mutex
}

type MachineCacheConfig struct {
	CachedChallengeMachines int    `koanf:"cached-challenge-machines"`
	InitialSteps            uint64 `koanf:"initial-steps"`
}

var DefaultMachineCacheConfig = MachineCacheConfig{
	CachedChallengeMachines: 4,
	InitialSteps:            100000,
}

func MachineCacheConfigConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Uint64(prefix+".initial-steps", DefaultMachineCacheConfig.InitialSteps, "initial steps between machines")
	f.Int(prefix+".cached-challenge-machines", DefaultMachineCacheConfig.CachedChallengeMachines, "how many machines to store in cache while working on a challenge (should be even)")
}

// `initialMachine` won't be mutated by this function.
func NewMachineCache(ctx context.Context, initialMachineGetter func(context.Context) (MachineInterface, error), config *MachineCacheConfig) *MachineCache {
	cache := &MachineCache{
		buildingLock: make(chan struct{}, 1), // locked on init
		config:       config,
	}
	go func() {
		zeroStepMachine, err := initialMachineGetter(ctx)
		if err == nil && zeroStepMachine.GetStepCount() != 0 {
			zeroStepMachine.Destroy()
			err = errors.New("initialMachine not at step count 0")
		}
		if err != nil {
			cache.unlockBuild(err)
			return
		}
		zeroStepMachine.Freeze()
		cache.zeroStepMachine = zeroStepMachine
		cache.machines = []MachineInterface{}
		cache.machineStepInterval = config.InitialSteps
		cache.finalMachineStep = ^uint64(0)
		for {
			err = cache.populateCache(ctx)
			if err != nil {
				cache.unlockBuild(err)
				return
			}
			if !cache.machines[len(cache.machines)-1].IsRunning() {
				break
			}
			compressedMachines := []MachineInterface{}
			for i, mach := range cache.machines {
				if i%2 == 1 {
					compressedMachines = append(compressedMachines, mach)
				} else {
					mach.Destroy()
				}
			}
			cache.machines = compressedMachines
			cache.firstMachineStep += cache.machineStepInterval
			cache.machineStepInterval *= 2
		}
		lastMachine := cache.machines[len(cache.machines)-1]
		cache.machines = cache.machines[:len(cache.machines)-1]
		cache.finalMachine = lastMachine
		cache.finalMachineStep = lastMachine.GetStepCount()
		cache.unlockBuild(nil)
	}()
	return cache
}

func (c *MachineCache) Destroy(ctx context.Context) {
	err := c.lockBuild(ctx)
	if err != nil {
		return
	}
	c.unlockBuild(errors.New("machine cache destroyed"))
}

func (c *MachineCache) destroyWithLock() {
	if c.zeroStepMachine != nil {
		c.zeroStepMachine.Destroy()
		c.zeroStepMachine = nil
	}
	if c.finalMachine != nil {
		c.finalMachine.Destroy()
		c.finalMachine = nil
	}
	for _, mach := range c.machines {
		mach.Destroy()
	}
}

func (c *MachineCache) lockBuild(ctx context.Context) error {
	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-c.buildingLock:
	}
	err := c.err
	if err != nil {
		c.buildingLock <- struct{}{}
	}
	return err
}

func (c *MachineCache) unlockBuild(err error) {
	c.err = err
	if err != nil {
		c.destroyWithLock()
	}
	c.buildingLock <- struct{}{}
}

func (c *MachineCache) setRangeLocked(ctx context.Context, start uint64, end uint64) error {
	newInterval := (end - start) / uint64(c.config.CachedChallengeMachines)
	if newInterval == 0 {
		newInterval = 2
	}
	if start == 0 {
		start = newInterval / 2
	}
	if end >= c.finalMachineStep {
		end = c.finalMachineStep - newInterval/2
	}
	newInterval = (end - start) / uint64(c.config.CachedChallengeMachines)
	if newInterval == 0 {
		newInterval = 1
	}
	if start == c.firstMachineStep && newInterval == c.machineStepInterval {
		return nil
	}
	closestIndex, closest := c.getClosestMachine(start)
	closestStep := closest.GetStepCount()
	if closestStep > start {
		return fmt.Errorf("initial machine step too large %d > %d", closestStep, start)
	}
	for i, mach := range c.machines {
		if i != closestIndex {
			mach.Destroy()
		}
	}
	var initial MachineInterface
	if closestStep < start {
		initial = closest.CloneMachineInterface()
		err := initial.Step(ctx, start-closestStep)
		if err != nil {
			return err
		}
		if closest != c.zeroStepMachine && closest != c.finalMachine {
			closest.Destroy()
		}
		initial.Freeze()
	} else {
		initial = closest
	}
	c.machines = []MachineInterface{initial}
	c.firstMachineStep = start
	c.machineStepInterval = newInterval
	return c.populateCache(ctx)
}

func (c *MachineCache) SetRange(ctx context.Context, start uint64, end uint64) error {
	err := c.lockBuild(ctx)
	if err != nil {
		return err
	}
	err = c.setRangeLocked(ctx, start, end)
	c.unlockBuild(err)
	return err
}

func (c *MachineCache) populateCache(ctx context.Context) error {
	var nextMachine MachineInterface
	if len(c.machines) == 0 {
		nextMachine = c.zeroStepMachine
		c.firstMachineStep = c.machineStepInterval
	} else {
		nextMachine = c.machines[len(c.machines)-1]
	}
	for {
		if !nextMachine.IsRunning() {
			break
		}
		if nextMachine.GetStepCount()+c.machineStepInterval >= c.finalMachineStep {
			break
		}
		if len(c.machines) >= c.config.CachedChallengeMachines {
			break
		}
		nextMachine = nextMachine.CloneMachineInterface()
		err := nextMachine.Step(ctx, c.machineStepInterval)
		if err != nil {
			return err
		}
		nextMachine.Freeze()
		c.machines = append(c.machines, nextMachine)
	}
	return nil
}

// Warning: don't mutate the result of this!
func (c *MachineCache) getClosestMachine(stepCount uint64) (int, MachineInterface) {
	if stepCount < c.firstMachineStep {
		return -1, c.zeroStepMachine
	}
	if stepCount >= c.finalMachineStep {
		return len(c.machines), c.finalMachine
	}
	stepsFromStart := stepCount - c.firstMachineStep
	var index int
	if c.machineStepInterval == 0 || stepsFromStart > c.machineStepInterval*uint64(len(c.machines)-1) {
		index = len(c.machines) - 1
	} else {
		index = int(stepsFromStart / c.machineStepInterval)
	}
	return index, c.machines[index]
}

func (c *MachineCache) getLastMachine() MachineInterface {
	c.lastMachineLock.Lock()
	defer c.lastMachineLock.Unlock()
	last := c.lastMachine
	c.lastMachine = nil
	return last
}

func (c *MachineCache) setLastMachine(machine MachineInterface) {
	c.lastMachineLock.Lock()
	prevLast := c.lastMachine
	c.lastMachine = machine
	c.lastMachineLock.Unlock()
	if prevLast != nil {
		prevLast.Destroy()
	}
}

// GetMachineAt a given step count, optionally using a passed in machine if that's the best option.
func (c *MachineCache) GetMachineAt(ctx context.Context, stepCount uint64) (MachineInterface, error) {
	err := c.lockBuild(ctx)
	if err != nil {
		return nil, err
	}
	_, closestMachine := c.getClosestMachine(stepCount)
	lastMachine := c.getLastMachine()
	if lastMachine != nil && lastMachine.GetStepCount() >= closestMachine.GetStepCount() && lastMachine.GetStepCount() <= stepCount {
		closestMachine = lastMachine
	} else {
		closestMachine = closestMachine.CloneMachineInterface()
	}
	c.unlockBuild(nil)

	err = closestMachine.Step(ctx, stepCount-closestMachine.GetStepCount())
	if err != nil {
		return nil, err
	}
	if !closestMachine.ValidForStep(stepCount) {
		return nil, fmt.Errorf("internal error: got machine with wrong step count %v looking for step count %v", closestMachine.GetStepCount(), stepCount)
	}
	c.setLastMachine(closestMachine)
	return closestMachine, nil
}

func (c *MachineCache) GetFinalMachine(ctx context.Context) (MachineInterface, error) {
	err := c.lockBuild(ctx)
	if err != nil {
		return nil, err
	}
	defer c.unlockBuild(nil)
	return c.finalMachine, nil
}

'''
'''--- validator/server_arb/machine_loader.go ---
package server_arb

import (
	"context"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator/server_common"
)

type ArbitratorMachineConfig struct {
	WavmBinaryPath       string
	UntilHostIoStatePath string
}

var DefaultArbitratorMachineConfig = ArbitratorMachineConfig{
	WavmBinaryPath:       "machine.wavm.br",
	UntilHostIoStatePath: "until-host-io-state.bin",
}

type arbMachines struct {
	zeroStep *ArbitratorMachine
	hostIo   *ArbitratorMachine
}

type ArbMachineLoader struct {
	server_common.MachineLoader[arbMachines]
}

func NewArbMachineLoader(config *ArbitratorMachineConfig, locator *server_common.MachineLocator) *ArbMachineLoader {
	createMachineFunc := func(ctx context.Context, moduleRoot common.Hash) (*arbMachines, error) {
		return createArbMachine(ctx, locator, config, moduleRoot)
	}
	return &ArbMachineLoader{
		MachineLoader: *server_common.NewMachineLoader[arbMachines](locator, createMachineFunc),
	}
}

func (a *ArbMachineLoader) GetHostIoMachine(ctx context.Context, moduleRoot common.Hash) (*ArbitratorMachine, error) {
	machines, err := a.GetMachine(ctx, moduleRoot)
	if err != nil {
		return nil, err
	}
	return machines.hostIo, nil
}

func (a *ArbMachineLoader) GetZeroStepMachine(ctx context.Context, moduleRoot common.Hash) (*ArbitratorMachine, error) {
	machines, err := a.GetMachine(ctx, moduleRoot)
	if err != nil {
		return nil, err
	}
	return machines.zeroStep, nil
}

'''
'''--- validator/server_arb/mock_machine.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

import (
	"context"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator"
)

type IncorrectMachine struct {
	inner         *ArbitratorMachine
	incorrectStep uint64
	stepCount     uint64
}

var badGlobalState = validator.GoGlobalState{Batch: 0xbadbadbadbad, PosInBatch: 0xbadbadbadbad}

var _ MachineInterface = (*IncorrectMachine)(nil)

func NewIncorrectMachine(inner *ArbitratorMachine, incorrectStep uint64) *IncorrectMachine {
	return &IncorrectMachine{
		inner:         inner.Clone(),
		incorrectStep: incorrectStep,
	}
}

func (m *IncorrectMachine) CloneMachineInterface() MachineInterface {
	return &IncorrectMachine{
		inner:         m.inner.Clone(),
		incorrectStep: m.incorrectStep,
		stepCount:     m.stepCount,
	}
}

func (m *IncorrectMachine) GetGlobalState() validator.GoGlobalState {
	if m.GetStepCount() >= m.incorrectStep {
		return badGlobalState
	}
	return m.inner.GetGlobalState()
}

func (m *IncorrectMachine) GetStepCount() uint64 {
	if !m.IsRunning() {
		endStep := m.incorrectStep
		if endStep < m.inner.GetStepCount() {
			endStep = m.inner.GetStepCount()
		}
		return endStep
	}
	return m.stepCount
}

func (m *IncorrectMachine) IsRunning() bool {
	return m.inner.IsRunning() || m.stepCount < m.incorrectStep
}

func (m *IncorrectMachine) ValidForStep(step uint64) bool {
	return m.inner.ValidForStep(step)
}

func (m *IncorrectMachine) Step(ctx context.Context, count uint64) error {
	err := m.inner.Step(ctx, count)
	if err != nil {
		return err
	}
	prevStepCount := m.stepCount
	m.stepCount += count
	if m.stepCount < prevStepCount {
		// saturate on overflow instead of wrapping
		m.stepCount = ^uint64(0)
	}
	return nil
}

func (m *IncorrectMachine) Status() uint8 {
	return m.inner.Status()
}

func (m *IncorrectMachine) Hash() common.Hash {
	if m.GetStepCount() >= m.incorrectStep {
		if m.inner.IsErrored() {
			return common.HexToHash("0xbad00000bad00000bad00000bad00000")
		}
		beforeGs := m.inner.GetGlobalState()
		if beforeGs != badGlobalState {
			if err := m.inner.SetGlobalState(badGlobalState); err != nil {
				panic(err)
			}
		}
	}
	return m.inner.Hash()
}

func (m *IncorrectMachine) ProveNextStep() []byte {
	return m.inner.ProveNextStep()
}

func (m *IncorrectMachine) Freeze() {
	m.inner.Freeze()
}

func (m *IncorrectMachine) Destroy() {
	m.inner.Destroy()
}

'''
'''--- validator/server_arb/nitro_machine.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

/*
#cgo CFLAGS: -g -Wall -I../../target/include/
#include "arbitrator.h"
#include <stdlib.h>
*/
import "C"
import (
	"context"
	"errors"
	"fmt"
	"os"
	"path/filepath"
	"unsafe"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/validator/server_common"
)

func createArbMachine(ctx context.Context, locator *server_common.MachineLocator, config *ArbitratorMachineConfig, moduleRoot common.Hash) (*arbMachines, error) {
	binPath := filepath.Join(locator.GetMachinePath(moduleRoot), config.WavmBinaryPath)
	cBinPath := C.CString(binPath)
	defer C.free(unsafe.Pointer(cBinPath))
	log.Info("creating nitro machine", "binpath", binPath)
	baseMachine := C.arbitrator_load_wavm_binary(cBinPath)
	if baseMachine == nil {
		return nil, errors.New("failed to load base machine")
	}
	machine := machineFromPointer(baseMachine)
	machineModuleRoot := machine.GetModuleRoot()
	if machineModuleRoot != moduleRoot {
		return nil, fmt.Errorf("attempting to load module root %v got machine with module root %v", moduleRoot, machineModuleRoot)
	}
	result := &arbMachines{
		zeroStep: machine,
	}
	result.zeroStep.Freeze()
	machine = result.zeroStep.Clone()

	// We try to store/load state before first host_io to a file.
	// We will chicken out of that if something fails, but still try to calculate the machine
	statePath := filepath.Join(locator.GetMachinePath(moduleRoot), config.UntilHostIoStatePath)
	_, err := os.Stat(statePath)
	if err == nil {
		log.Info("found cached machine until host io state", "moduleRoot", moduleRoot)

		err := machine.DeserializeAndReplaceState(statePath)
		if err != nil {
			// Safe as if DeserializeAndReplaceState returns an error it will not have mutated the machine
			log.Warn("failed to load machine until host io state; will reexecute", "err", err)
		} else {
			result.hostIo = machine
			result.hostIo.Freeze()
			return result, nil
		}
	} else if errors.Is(err, os.ErrNotExist) {
		log.Info("didn't find cached machine until host io state", "path", statePath)
	} else {
		log.Warn("error checking if machine until host io state is cached", "path", statePath, "err", err)
	}

	if err := machine.StepUntilHostIo(ctx); err != nil {
		return nil, err
	}

	if machine.IsErrored() {
		return nil, errors.New("machine entered errored state while caching execution up to host io")
	}

	result.hostIo = machine
	result.hostIo.Freeze()
	return result, nil
}

'''
'''--- validator/server_arb/preimage_resolver.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

/*
#cgo CFLAGS: -g -Wall -I../../target/include/
#include "arbitrator.h"

extern ResolvedPreimage preimageResolver(size_t context, uint8_t preimageType, const uint8_t* hash);

ResolvedPreimage preimageResolverC(size_t context, uint8_t preimageType, const uint8_t* hash) {
  return preimageResolver(context, preimageType, hash);
}
*/
import "C"

'''
'''--- validator/server_arb/prover_interface.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_arb

/*
#cgo CFLAGS: -g -Wall -I../../target/include/
#cgo LDFLAGS: ${SRCDIR}/../../target/lib/libprover.a -ldl -lm
#include "arbitrator.h"
#include <stdlib.h>

char **PrepareStringList(intptr_t num) {
	char** res = malloc(sizeof(char*) * num);
	if (! res) {
		return 0;
	}
	return res;
}

void AddToStringList(char** list, int index, char* val) {
	list[index] = val;
}
*/
import "C"
import (
	"unsafe"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator"
)

func CreateCByteArray(input []byte) C.CByteArray {
	return C.CByteArray{
		// Warning: CBytes uses malloc internally, so this must be freed later
		ptr: (*C.uint8_t)(C.CBytes(input)),
		len: C.uintptr_t(len(input)),
	}
}

func DestroyCByteArray(cbyte C.CByteArray) {
	C.free(unsafe.Pointer(cbyte.ptr))
}

func GlobalStateToC(gsIn validator.GoGlobalState) C.GlobalState {
	gs := C.GlobalState{}
	gs.u64_vals[0] = C.uint64_t(gsIn.Batch)
	gs.u64_vals[1] = C.uint64_t(gsIn.PosInBatch)
	for i, b := range gsIn.BlockHash {
		gs.bytes32_vals[0].bytes[i] = C.uint8_t(b)
	}
	for i, b := range gsIn.SendRoot {
		gs.bytes32_vals[1].bytes[i] = C.uint8_t(b)
	}
	return gs
}

func GlobalStateFromC(gs C.GlobalState) validator.GoGlobalState {
	var blockHash common.Hash
	for i := range blockHash {
		blockHash[i] = byte(gs.bytes32_vals[0].bytes[i])
	}
	var sendRoot common.Hash
	for i := range sendRoot {
		sendRoot[i] = byte(gs.bytes32_vals[1].bytes[i])
	}
	return validator.GoGlobalState{
		Batch:      uint64(gs.u64_vals[0]),
		PosInBatch: uint64(gs.u64_vals[1]),
		BlockHash:  blockHash,
		SendRoot:   sendRoot,
	}
}

// CreateCStringList creates a list of strings, does take ownership, should be freed
func CreateCStringList(input []string) **C.char {
	res := C.PrepareStringList(C.intptr_t(len(input)))
	for i, str := range input {
		C.AddToStringList(res, C.int(i), C.CString(str))
	}
	return res
}

func FreeCStringList(arrPtr **C.char, size int) {
	arr := (*[1 << 30]*C.char)(unsafe.Pointer(arrPtr))[:size:size]
	for _, ptr := range arr {
		C.free(unsafe.Pointer(ptr))
	}
	C.free(unsafe.Pointer(arrPtr))
}

'''
'''--- validator/server_arb/validator_spawner.go ---
package server_arb

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"os"
	"path/filepath"
	"runtime"
	"sync/atomic"
	"time"

	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_common"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
)

type ArbitratorSpawnerConfig struct {
	Workers             int                `koanf:"workers" reload:"hot"`
	OutputPath          string             `koanf:"output-path" reload:"hot"`
	Execution           MachineCacheConfig `koanf:"execution" reload:"hot"` // hot reloading for new executions only
	ExecutionRunTimeout time.Duration      `koanf:"execution-run-timeout" reload:"hot"`
}

type ArbitratorSpawnerConfigFecher func() *ArbitratorSpawnerConfig

var DefaultArbitratorSpawnerConfig = ArbitratorSpawnerConfig{
	Workers:             0,
	OutputPath:          "./target/output",
	Execution:           DefaultMachineCacheConfig,
	ExecutionRunTimeout: time.Minute * 15,
}

func ArbitratorSpawnerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int(prefix+".workers", DefaultArbitratorSpawnerConfig.Workers, "number of concurrent validation threads")
	f.Duration(prefix+".execution-run-timeout", DefaultArbitratorSpawnerConfig.ExecutionRunTimeout, "timeout before discarding execution run")
	f.String(prefix+".output-path", DefaultArbitratorSpawnerConfig.OutputPath, "path to write machines to")
	MachineCacheConfigConfigAddOptions(prefix+".execution", f)
}

func DefaultArbitratorSpawnerConfigFetcher() *ArbitratorSpawnerConfig {
	return &DefaultArbitratorSpawnerConfig
}

type ArbitratorSpawner struct {
	stopwaiter.StopWaiter
	count         int32
	locator       *server_common.MachineLocator
	machineLoader *ArbMachineLoader
	config        ArbitratorSpawnerConfigFecher
}

func NewArbitratorSpawner(locator *server_common.MachineLocator, config ArbitratorSpawnerConfigFecher) (*ArbitratorSpawner, error) {
	// TODO: preload machines
	spawner := &ArbitratorSpawner{
		locator:       locator,
		machineLoader: NewArbMachineLoader(&DefaultArbitratorMachineConfig, locator),
		config:        config,
	}
	return spawner, nil
}

func (s *ArbitratorSpawner) Start(ctx_in context.Context) error {
	s.StopWaiter.Start(ctx_in, s)
	return nil
}

func (s *ArbitratorSpawner) LatestWasmModuleRoot() containers.PromiseInterface[common.Hash] {
	return containers.NewReadyPromise(s.locator.LatestWasmModuleRoot(), nil)
}

func (s *ArbitratorSpawner) Name() string {
	return "arbitrator"
}

func (v *ArbitratorSpawner) loadEntryToMachine(ctx context.Context, entry *validator.ValidationInput, mach *ArbitratorMachine) error {
	resolver := func(ty arbutil.PreimageType, hash common.Hash) ([]byte, error) {
		// Check if it's a known preimage
		if preimage, ok := entry.Preimages[ty][hash]; ok {
			return preimage, nil
		}
		return nil, errors.New("preimage not found")
	}
	if err := mach.SetPreimageResolver(resolver); err != nil {
		return err
	}
	err := mach.SetGlobalState(entry.StartState)
	if err != nil {
		log.Error("error while setting global state for proving", "err", err, "gsStart", entry.StartState)
		return fmt.Errorf("error while setting global state for proving: %w", err)
	}
	for _, batch := range entry.BatchInfo {
		err = mach.AddSequencerInboxMessage(batch.Number, batch.Data)
		if err != nil {
			log.Error(
				"error while trying to add sequencer msg for proving",
				"err", err, "seq", entry.StartState.Batch, "blockNr", entry.Id,
			)
			return fmt.Errorf("error while trying to add sequencer msg for proving: %w", err)
		}
	}
	if entry.HasDelayedMsg {
		err = mach.AddDelayedInboxMessage(entry.DelayedMsgNr, entry.DelayedMsg)
		if err != nil {
			log.Error(
				"error while trying to add delayed msg for proving",
				"err", err, "seq", entry.DelayedMsgNr, "blockNr", entry.Id,
			)
			return fmt.Errorf("error while trying to add delayed msg for proving: %w", err)
		}
	}
	return nil
}

func (v *ArbitratorSpawner) execute(
	ctx context.Context, entry *validator.ValidationInput, moduleRoot common.Hash,
) (validator.GoGlobalState, error) {
	basemachine, err := v.machineLoader.GetHostIoMachine(ctx, moduleRoot)
	if err != nil {
		return validator.GoGlobalState{}, fmt.Errorf("unabled to get WASM machine: %w", err)
	}

	mach := basemachine.Clone()
	defer mach.Destroy()
	err = v.loadEntryToMachine(ctx, entry, mach)
	if err != nil {
		return validator.GoGlobalState{}, err
	}
	var steps uint64
	for mach.IsRunning() {
		var count uint64 = 500000000
		err = mach.Step(ctx, count)
		if steps > 0 {
			log.Debug("validation", "moduleRoot", moduleRoot, "block", entry.Id, "steps", steps)
		}
		if err != nil {
			return validator.GoGlobalState{}, fmt.Errorf("machine execution failed with error: %w", err)
		}
		steps += count
	}
	if mach.IsErrored() {
		log.Error("machine entered errored state during attempted validation", "block", entry.Id)
		return validator.GoGlobalState{}, errors.New("machine entered errored state during attempted validation")
	}
	return mach.GetGlobalState(), nil
}

func (v *ArbitratorSpawner) Launch(entry *validator.ValidationInput, moduleRoot common.Hash) validator.ValidationRun {
	atomic.AddInt32(&v.count, 1)
	promise := stopwaiter.LaunchPromiseThread[validator.GoGlobalState](v, func(ctx context.Context) (validator.GoGlobalState, error) {
		defer atomic.AddInt32(&v.count, -1)
		return v.execute(ctx, entry, moduleRoot)
	})
	return server_common.NewValRun(promise, moduleRoot)
}

func (v *ArbitratorSpawner) Room() int {
	avail := v.config().Workers
	if avail == 0 {
		avail = runtime.NumCPU()
	}
	return avail
}

var launchTime = time.Now().Format("2006_01_02__15_04")

//nolint:gosec
func (v *ArbitratorSpawner) writeToFile(ctx context.Context, input *validator.ValidationInput, expOut validator.GoGlobalState, moduleRoot common.Hash) error {
	outDirPath := filepath.Join(v.locator.RootPath(), v.config().OutputPath, launchTime, fmt.Sprintf("block_%d", input.Id))
	err := os.MkdirAll(outDirPath, 0755)
	if err != nil {
		return err
	}
	if ctx.Err() != nil {
		return ctx.Err()
	}

	rootPathAssign := ""
	if executable, err := os.Executable(); err == nil {
		rootPathAssign = "ROOTPATH=\"" + filepath.Dir(executable) + "\"\n"
	}
	cmdFile, err := os.OpenFile(filepath.Join(outDirPath, "run-prover.sh"), os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0755)
	if err != nil {
		return err
	}
	defer cmdFile.Close()
	_, err = cmdFile.WriteString("#!/bin/bash\n" +
		fmt.Sprintf("# expected output: batch %d, postion %d, hash %s\n", expOut.Batch, expOut.PosInBatch, expOut.BlockHash) +
		"MACHPATH=\"" + v.locator.GetMachinePath(moduleRoot) + "\"\n" +
		rootPathAssign +
		"if (( $# > 1 )); then\n" +
		"	if [[ $1 == \"-m\" ]]; then\n" +
		"		MACHPATH=$2\n" +
		"		shift\n" +
		"		shift\n" +
		"	fi\n" +
		"fi\n" +
		"${ROOTPATH}/bin/prover ${MACHPATH}/replay.wasm")
	if err != nil {
		return err
	}
	if ctx.Err() != nil {
		return ctx.Err()
	}

	libraries := []string{"soft-float.wasm", "wasi_stub.wasm", "go_stub.wasm", "host_io.wasm", "brotli.wasm"}
	for _, module := range libraries {
		_, err = cmdFile.WriteString(" -l " + "${MACHPATH}/" + module)
		if err != nil {
			return err
		}
	}
	_, err = cmdFile.WriteString(fmt.Sprintf(" --inbox-position %d --position-within-message %d --last-block-hash %s", input.StartState.Batch, input.StartState.PosInBatch, input.StartState.BlockHash))
	if err != nil {
		return err
	}

	for _, msg := range input.BatchInfo {
		if ctx.Err() != nil {
			return ctx.Err()
		}
		sequencerFileName := fmt.Sprintf("sequencer_%d.bin", msg.Number)
		err = os.WriteFile(filepath.Join(outDirPath, sequencerFileName), msg.Data, 0644)
		if err != nil {
			return err
		}
		_, err = cmdFile.WriteString(" --inbox " + sequencerFileName)
		if err != nil {
			return err
		}
	}

	preimageFile, err := os.Create(filepath.Join(outDirPath, "preimages.bin"))
	if err != nil {
		return err
	}
	defer preimageFile.Close()
	for ty, preimages := range input.Preimages {
		_, err = preimageFile.Write([]byte{byte(ty)})
		if err != nil {
			return err
		}
		for _, data := range preimages {
			if ctx.Err() != nil {
				return ctx.Err()
			}
			lenbytes := make([]byte, 8)
			binary.LittleEndian.PutUint64(lenbytes, uint64(len(data)))
			_, err := preimageFile.Write(lenbytes)
			if err != nil {
				return err
			}
			_, err = preimageFile.Write(data)
			if err != nil {
				return err
			}
		}
	}

	_, err = cmdFile.WriteString(" --preimages preimages.bin")
	if err != nil {
		return err
	}

	if input.HasDelayedMsg {
		if ctx.Err() != nil {
			return ctx.Err()
		}
		_, err = cmdFile.WriteString(fmt.Sprintf(" --delayed-inbox-position %d", input.DelayedMsgNr))
		if err != nil {
			return err
		}
		filename := fmt.Sprintf("delayed_%d.bin", input.DelayedMsgNr)
		err = os.WriteFile(filepath.Join(outDirPath, filename), input.DelayedMsg, 0644)
		if err != nil {
			return err
		}
		_, err = cmdFile.WriteString(fmt.Sprintf(" --delayed-inbox %s", filename))
		if err != nil {
			return err
		}
	}

	_, err = cmdFile.WriteString(" \"$@\"\n")
	if err != nil {
		return err
	}
	return nil
}

func (v *ArbitratorSpawner) WriteToFile(input *validator.ValidationInput, expOut validator.GoGlobalState, moduleRoot common.Hash) containers.PromiseInterface[struct{}] {
	return stopwaiter.LaunchPromiseThread[struct{}](v, func(ctx context.Context) (struct{}, error) {
		err := v.writeToFile(ctx, input, expOut, moduleRoot)
		return struct{}{}, err
	})
}

func (v *ArbitratorSpawner) CreateExecutionRun(wasmModuleRoot common.Hash, input *validator.ValidationInput) containers.PromiseInterface[validator.ExecutionRun] {
	getMachine := func(ctx context.Context) (MachineInterface, error) {
		initialFrozenMachine, err := v.machineLoader.GetZeroStepMachine(ctx, wasmModuleRoot)
		if err != nil {
			return nil, err
		}
		machine := initialFrozenMachine.Clone()
		err = v.loadEntryToMachine(ctx, input, machine)
		if err != nil {
			machine.Destroy()
			return nil, err
		}
		return machine, nil
	}
	currentExecConfig := v.config().Execution
	return stopwaiter.LaunchPromiseThread[validator.ExecutionRun](v, func(ctx context.Context) (validator.ExecutionRun, error) {
		return NewExecutionRun(v.GetContext(), getMachine, &currentExecConfig)
	})
}

func (v *ArbitratorSpawner) Stop() {
	v.StopOnly()
}

'''
'''--- validator/server_common/machine_loader.go ---
package server_common

import (
	"context"
	"sync"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/util/containers"
)

type MachineStatus[M any] struct {
	containers.Promise[*M]
}

func newMachineStatus[M any]() *MachineStatus[M] {
	return &MachineStatus[M]{
		Promise: containers.NewPromise[*M](nil),
	}
}

type MachineLoader[M any] struct {
	mapMutex      sync.Mutex
	machines      map[common.Hash]*MachineStatus[M]
	locator       *MachineLocator
	createMachine func(ctx context.Context, moduleRoot common.Hash) (*M, error)
}

func NewMachineLoader[M any](
	locator *MachineLocator,
	createMachine func(ctx context.Context, moduleRoot common.Hash) (*M, error),
) *MachineLoader[M] {

	return &MachineLoader[M]{
		machines:      make(map[common.Hash]*MachineStatus[M]),
		locator:       locator,
		createMachine: createMachine,
	}
}

func (l *MachineLoader[M]) GetMachine(ctx context.Context, moduleRoot common.Hash) (*M, error) {
	if moduleRoot == (common.Hash{}) {
		moduleRoot = l.locator.LatestWasmModuleRoot()
		if (moduleRoot == common.Hash{}) {
			return nil, ErrMachineNotFound
		}
	}
	l.mapMutex.Lock()
	status := l.machines[moduleRoot]
	if status == nil {
		status = newMachineStatus[M]()
		l.machines[moduleRoot] = status
		go func() {
			machine, err := l.createMachine(context.Background(), moduleRoot)
			if err != nil {
				status.ProduceError(err)
				return
			}
			status.Produce(machine)
		}()
	}
	l.mapMutex.Unlock()
	return status.Await(ctx)
}

func (l *MachineLoader[M]) ForEachReadyMachine(runme func(*M)) {
	l.mapMutex.Lock()
	defer l.mapMutex.Unlock()
	for _, stat := range l.machines {
		if stat.Ready() {
			machine, err := stat.Current()
			if err != nil {
				runme(machine)
			}
		}
	}
}

'''
'''--- validator/server_common/machine_locator.go ---
package server_common

import (
	"errors"
	"os"
	"path/filepath"
	"runtime"
	"strings"

	"github.com/ethereum/go-ethereum/common"
)

type MachineLocator struct {
	rootPath string
	latest   common.Hash
}

var ErrMachineNotFound = errors.New("machine not found")

func NewMachineLocator(rootPath string) (*MachineLocator, error) {
	var places []string

	if rootPath != "" {
		places = append(places, rootPath)
	} else {
		// Check the project dir: <project>/arbnode/node.go => ../../target/machines
		_, thisFile, _, ok := runtime.Caller(0)
		if !ok {
			panic("failed to find root path")
		}
		projectDir := filepath.Dir(filepath.Dir(filepath.Dir(thisFile)))
		projectPath := filepath.Join(filepath.Join(projectDir, "target"), "machines")
		places = append(places, projectPath)

		// Check the working directory: ./machines and ./target/machines
		workDir, err := os.Getwd()
		if err != nil {
			return nil, err
		}
		workPath1 := filepath.Join(workDir, "machines")
		workPath2 := filepath.Join(filepath.Join(workDir, "target"), "machines")
		places = append(places, workPath1)
		places = append(places, workPath2)

		// Check above the executable: <binary> => ../../machines
		execfile, err := os.Executable()
		if err != nil {
			return nil, err
		}
		execPath := filepath.Join(filepath.Dir(filepath.Dir(execfile)), "machines")
		places = append(places, execPath)
	}

	for _, place := range places {
		if _, err := os.Stat(place); err == nil {
			var latestModuleRoot common.Hash
			latestModuleRootPath := filepath.Join(place, "latest", "module-root.txt")
			fileBytes, err := os.ReadFile(latestModuleRootPath)
			if err == nil {
				s := strings.TrimSpace(string(fileBytes))
				latestModuleRoot = common.HexToHash(s)
			}
			return &MachineLocator{place, latestModuleRoot}, nil
		}
	}
	return nil, ErrMachineNotFound
}

func (l MachineLocator) GetMachinePath(moduleRoot common.Hash) string {
	if moduleRoot == (common.Hash{}) || moduleRoot == l.latest {
		return filepath.Join(l.rootPath, "latest")
	} else {
		return filepath.Join(l.rootPath, moduleRoot.String())
	}
}

func (l MachineLocator) LatestWasmModuleRoot() common.Hash {
	return l.latest
}

func (l MachineLocator) RootPath() string {
	return l.rootPath
}

'''
'''--- validator/server_common/valrun.go ---
package server_common

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/util/containers"
	"github.com/offchainlabs/nitro/validator"
)

type ValRun struct {
	containers.PromiseInterface[validator.GoGlobalState]
	root common.Hash
}

func (r *ValRun) WasmModuleRoot() common.Hash {
	return r.root
}

func NewValRun(promise containers.PromiseInterface[validator.GoGlobalState], root common.Hash) *ValRun {
	return &ValRun{
		PromiseInterface: promise,
		root:             root,
	}
}

'''
'''--- validator/server_jit/jit_machine.go ---
// Copyright 2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package server_jit

import (
	"context"
	"encoding/binary"
	"errors"
	"fmt"
	"io"
	"net"
	"os"
	"os/exec"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/util/arbmath"
	"github.com/offchainlabs/nitro/validator"
)

type JitMachine struct {
	binary  string
	process *exec.Cmd
	stdin   io.WriteCloser
}

func createJitMachine(jitBinary string, binaryPath string, cranelift bool, moduleRoot common.Hash, fatalErrChan chan error) (*JitMachine, error) {
	invocation := []string{"--binary", binaryPath, "--forks"}
	if cranelift {
		invocation = append(invocation, "--cranelift")
	}
	process := exec.Command(jitBinary, invocation...)
	stdin, err := process.StdinPipe()
	if err != nil {
		return nil, err
	}
	process.Stdout = os.Stdout
	process.Stderr = os.Stderr
	go func() {
		if err := process.Run(); err != nil {
			fatalErrChan <- fmt.Errorf("lost jit block validator process: %w", err)
		}
	}()

	machine := &JitMachine{
		binary:  binaryPath,
		process: process,
		stdin:   stdin,
	}
	return machine, nil
}

func (machine *JitMachine) close() {
	_, err := machine.stdin.Write([]byte("\n"))
	if err != nil {
		log.Error("error closing jit machine", "error", err)
	}
}

func (machine *JitMachine) prove(
	ctxIn context.Context, entry *validator.ValidationInput,
) (validator.GoGlobalState, error) {
	ctx, cancel := context.WithCancel(ctxIn)
	defer cancel() // ensure our cleanup functions run when we're done
	state := validator.GoGlobalState{}

	timeout := time.Now().Add(60 * time.Second)
	tcp, err := net.ListenTCP("tcp4", &net.TCPAddr{
		IP: []byte{127, 0, 0, 1},
	})
	if err != nil {
		return state, err
	}
	if err := tcp.SetDeadline(timeout); err != nil {
		return state, err
	}
	go func() {
		<-ctx.Done()
		err := tcp.Close()
		if err != nil {
			log.Warn("error closing JIT validation TCP listener", "err", err)
		}
	}()
	address := fmt.Sprintf("%v\n", tcp.Addr().String())

	// Tell the spawner process about the new tcp port
	if _, err := machine.stdin.Write([]byte(address)); err != nil {
		return state, err
	}

	// Wait for the forked process to connect
	conn, err := tcp.Accept()
	if err != nil {
		return state, err
	}
	go func() {
		<-ctx.Done()
		err := conn.Close()
		if err != nil && !errors.Is(err, net.ErrClosed) {
			log.Warn("error closing JIT validation TCP connection", "err", err)
		}
	}()
	if err := conn.SetReadDeadline(timeout); err != nil {
		return state, err
	}
	if err := conn.SetWriteDeadline(timeout); err != nil {
		return state, err
	}

	writeExact := func(data []byte) error {
		_, err := conn.Write(data)
		return err
	}
	writeUint8 := func(data uint8) error {
		return writeExact([]byte{data})
	}
	writeUint64 := func(data uint64) error {
		return writeExact(arbmath.UintToBytes(data))
	}
	writeBytes := func(data []byte) error {
		if err := writeUint64(uint64(len(data))); err != nil {
			return err
		}
		return writeExact(data)
	}

	// send global state
	if err := writeUint64(entry.StartState.Batch); err != nil {
		return state, err
	}
	if err := writeUint64(entry.StartState.PosInBatch); err != nil {
		return state, err
	}
	if err := writeExact(entry.StartState.BlockHash[:]); err != nil {
		return state, err
	}
	if err := writeExact(entry.StartState.SendRoot[:]); err != nil {
		return state, err
	}

	const successByte = 0x0
	const failureByte = 0x1
	const anotherByte = 0x3
	const readyByte = 0x4

	success := []byte{successByte}
	another := []byte{anotherByte}
	ready := []byte{readyByte}

	// send inbox
	for _, batch := range entry.BatchInfo {
		if err := writeExact(another); err != nil {
			return state, err
		}
		if err := writeUint64(batch.Number); err != nil {
			return state, err
		}
		if err := writeBytes(batch.Data); err != nil {
			return state, err
		}
	}
	if err := writeExact(success); err != nil {
		return state, err
	}

	// send delayed inbox
	if entry.HasDelayedMsg {
		if err := writeExact(another); err != nil {
			return state, err
		}
		if err := writeUint64(entry.DelayedMsgNr); err != nil {
			return state, err
		}
		if err := writeBytes(entry.DelayedMsg); err != nil {
			return state, err
		}
	}
	if err := writeExact(success); err != nil {
		return state, err
	}

	// send known preimages
	preimageTypes := entry.Preimages
	if err := writeUint64(uint64(len(preimageTypes))); err != nil {
		return state, err
	}
	for ty, preimages := range preimageTypes {
		if err := writeUint8(uint8(ty)); err != nil {
			return state, err
		}
		if err := writeUint64(uint64(len(preimages))); err != nil {
			return state, err
		}
		for hash, preimage := range preimages {
			if err := writeExact(hash[:]); err != nil {
				return state, err
			}
			if err := writeBytes(preimage); err != nil {
				return state, err
			}
		}
	}

	// signal that we are done sending global state
	if err := writeExact(ready); err != nil {
		return state, err
	}

	read := func(count uint64) ([]byte, error) {
		slice := make([]byte, count)
		_, err := io.ReadFull(conn, slice)
		if err != nil {
			return nil, err
		}
		return slice, nil
	}
	readUint64 := func() (uint64, error) {
		slice, err := read(8)
		if err != nil {
			return 0, err
		}
		return binary.BigEndian.Uint64(slice), nil
	}
	readHash := func() (common.Hash, error) {
		slice, err := read(32)
		if err != nil {
			return common.Hash{}, err
		}
		return common.BytesToHash(slice), nil
	}

	for {
		kind, err := read(1)
		if err != nil {
			return state, err
		}
		switch kind[0] {
		case failureByte:
			length, err := readUint64()
			if err != nil {
				return state, err
			}
			message, err := read(length)
			if err != nil {
				return state, err
			}
			log.Error("Jit Machine Failure", "message", string(message))
			return state, errors.New(string(message))
		case successByte:
			if state.Batch, err = readUint64(); err != nil {
				return state, err
			}
			if state.PosInBatch, err = readUint64(); err != nil {
				return state, err
			}
			if state.BlockHash, err = readHash(); err != nil {
				return state, err
			}
			state.SendRoot, err = readHash()
			return state, err
		default:
			message := "inter-process communication failure"
			log.Error("Jit Machine Failure", "message", message)
			return state, errors.New("inter-process communication failure")
		}
	}
}

'''
'''--- validator/server_jit/machine_loader.go ---
package server_jit

import (
	"context"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"strings"

	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/validator/server_common"
)

type JitMachineConfig struct {
	ProverBinPath string
	JitCranelift  bool
}

var DefaultJitMachineConfig = JitMachineConfig{
	JitCranelift:  true,
	ProverBinPath: "replay.wasm",
}

func getJitPath() (string, error) {
	var jitBinary string
	executable, err := os.Executable()
	if err == nil {
		if strings.Contains(filepath.Base(executable), "test") {
			_, thisfile, _, _ := runtime.Caller(0)
			projectDir := filepath.Dir(filepath.Dir(filepath.Dir(thisfile)))
			jitBinary = filepath.Join(projectDir, "target", "bin", "jit")
		} else {
			jitBinary = filepath.Join(filepath.Dir(executable), "jit")
		}
		_, err = os.Stat(jitBinary)
	}
	if err != nil {
		var lookPathErr error
		jitBinary, lookPathErr = exec.LookPath("jit")
		if lookPathErr == nil {
			return jitBinary, nil
		}
	}
	return jitBinary, err
}

type JitMachineLoader struct {
	server_common.MachineLoader[JitMachine]
	stopped bool
}

func NewJitMachineLoader(config *JitMachineConfig, locator *server_common.MachineLocator, fatalErrChan chan error) (*JitMachineLoader, error) {
	jitPath, err := getJitPath()
	if err != nil {
		return nil, err
	}
	createMachineThreadFunc := func(ctx context.Context, moduleRoot common.Hash) (*JitMachine, error) {
		binPath := filepath.Join(locator.GetMachinePath(moduleRoot), config.ProverBinPath)
		return createJitMachine(jitPath, binPath, config.JitCranelift, moduleRoot, fatalErrChan)
	}
	return &JitMachineLoader{
		MachineLoader: *server_common.NewMachineLoader[JitMachine](locator, createMachineThreadFunc),
	}, nil
}

func (j *JitMachineLoader) Stop() {
	if j.stopped {
		return
	}
	j.ForEachReadyMachine(func(machine *JitMachine) { machine.close() })
	j.stopped = true
}

'''
'''--- validator/server_jit/spawner.go ---
package server_jit

import (
	"context"
	"fmt"
	"runtime"
	"sync/atomic"

	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/common"

	"github.com/offchainlabs/nitro/util/stopwaiter"
	"github.com/offchainlabs/nitro/validator"
	"github.com/offchainlabs/nitro/validator/server_common"
)

type JitSpawnerConfig struct {
	Workers   int  `koanf:"workers" reload:"hot"`
	Cranelift bool `koanf:"cranelift"`
}

type JitSpawnerConfigFecher func() *JitSpawnerConfig

var DefaultJitSpawnerConfig = JitSpawnerConfig{
	Workers:   0,
	Cranelift: true,
}

func JitSpawnerConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Int(prefix+".workers", DefaultJitSpawnerConfig.Workers, "number of concurrent validation threads")
	f.Bool(prefix+".cranelift", DefaultJitSpawnerConfig.Cranelift, "use Cranelift instead of LLVM when validating blocks using the jit-accelerated block validator")
}

type JitSpawner struct {
	stopwaiter.StopWaiter
	count         int32
	locator       *server_common.MachineLocator
	machineLoader *JitMachineLoader
	config        JitSpawnerConfigFecher
}

func NewJitSpawner(locator *server_common.MachineLocator, config JitSpawnerConfigFecher, fatalErrChan chan error) (*JitSpawner, error) {
	// TODO - preload machines
	machineConfig := DefaultJitMachineConfig
	machineConfig.JitCranelift = config().Cranelift
	loader, err := NewJitMachineLoader(&machineConfig, locator, fatalErrChan)
	if err != nil {
		return nil, err
	}
	spawner := &JitSpawner{
		locator:       locator,
		machineLoader: loader,
		config:        config,
	}
	return spawner, nil
}

func (v *JitSpawner) Start(ctx_in context.Context) error {
	v.StopWaiter.Start(ctx_in, v)
	return nil
}

func (v *JitSpawner) execute(
	ctx context.Context, entry *validator.ValidationInput, moduleRoot common.Hash,
) (validator.GoGlobalState, error) {
	machine, err := v.machineLoader.GetMachine(ctx, moduleRoot)
	if err != nil {
		return validator.GoGlobalState{}, fmt.Errorf("unabled to get WASM machine: %w", err)
	}

	state, err := machine.prove(ctx, entry)
	return state, err
}

func (s *JitSpawner) Name() string {
	if s.config().Cranelift {
		return "jit-cranelift"
	}
	return "jit"
}

func (v *JitSpawner) Launch(entry *validator.ValidationInput, moduleRoot common.Hash) validator.ValidationRun {
	atomic.AddInt32(&v.count, 1)
	promise := stopwaiter.LaunchPromiseThread[validator.GoGlobalState](v, func(ctx context.Context) (validator.GoGlobalState, error) {
		defer atomic.AddInt32(&v.count, -1)
		return v.execute(ctx, entry, moduleRoot)
	})
	return server_common.NewValRun(promise, moduleRoot)
}

func (v *JitSpawner) Room() int {
	avail := v.config().Workers
	if avail == 0 {
		avail = runtime.NumCPU()
	}
	return avail
}

func (v *JitSpawner) Stop() {
	v.StopOnly()
	v.machineLoader.Stop()
}

'''
'''--- validator/validation_entry.go ---
package validator

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbutil"
)

type BatchInfo struct {
	Number uint64
	Data   []byte
}

type ValidationInput struct {
	Id            uint64
	HasDelayedMsg bool
	DelayedMsgNr  uint64
	Preimages     map[arbutil.PreimageType]map[common.Hash][]byte
	BatchInfo     []BatchInfo
	DelayedMsg    []byte
	StartState    GoGlobalState
}

'''
'''--- validator/valnode/valnode.go ---
package valnode

import (
	"context"

	"github.com/offchainlabs/nitro/validator"

	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/rpc"
	flag "github.com/spf13/pflag"

	"github.com/offchainlabs/nitro/validator/server_api"
	"github.com/offchainlabs/nitro/validator/server_arb"
	"github.com/offchainlabs/nitro/validator/server_common"
	"github.com/offchainlabs/nitro/validator/server_jit"
)

type WasmConfig struct {
	RootPath string `koanf:"root-path"`
}

func WasmConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.String(prefix+".root-path", DefaultWasmConfig.RootPath, "path to machine folders, each containing wasm files (machine.wavm.br, replay.wasm)")
}

var DefaultWasmConfig = WasmConfig{
	RootPath: "",
}

type Config struct {
	UseJit     bool                               `koanf:"use-jit"`
	ApiAuth    bool                               `koanf:"api-auth"`
	ApiPublic  bool                               `koanf:"api-public"`
	Arbitrator server_arb.ArbitratorSpawnerConfig `koanf:"arbitrator" reload:"hot"`
	Jit        server_jit.JitSpawnerConfig        `koanf:"jit" reload:"hot"`
	Wasm       WasmConfig                         `koanf:"wasm"`
}

type ValidationConfigFetcher func() *Config

var DefaultValidationConfig = Config{
	UseJit:     true,
	Jit:        server_jit.DefaultJitSpawnerConfig,
	ApiAuth:    true,
	ApiPublic:  false,
	Arbitrator: server_arb.DefaultArbitratorSpawnerConfig,
	Wasm:       DefaultWasmConfig,
}

var TestValidationConfig = Config{
	UseJit:     true,
	Jit:        server_jit.DefaultJitSpawnerConfig,
	ApiAuth:    false,
	ApiPublic:  true,
	Arbitrator: server_arb.DefaultArbitratorSpawnerConfig,
	Wasm:       DefaultWasmConfig,
}

func ValidationConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".use-jit", DefaultValidationConfig.UseJit, "use jit for validation")
	f.Bool(prefix+".api-auth", DefaultValidationConfig.ApiAuth, "validate is an authenticated API")
	f.Bool(prefix+".api-public", DefaultValidationConfig.ApiPublic, "validate is a public API")
	server_arb.ArbitratorSpawnerConfigAddOptions(prefix+".arbitrator", f)
	server_jit.JitSpawnerConfigAddOptions(prefix+".jit", f)
	WasmConfigAddOptions(prefix+".wasm", f)
}

type ValidationNode struct {
	config     ValidationConfigFetcher
	arbSpawner *server_arb.ArbitratorSpawner
	jitSpawner *server_jit.JitSpawner
}

func EnsureValidationExposedViaAuthRPC(stackConf *node.Config) {
	found := false
	for _, module := range stackConf.AuthModules {
		if module == server_api.Namespace {
			found = true
			break
		}
	}
	if !found {
		stackConf.AuthModules = append(stackConf.AuthModules, server_api.Namespace)
	}
}

func CreateValidationNode(configFetcher ValidationConfigFetcher, stack *node.Node, fatalErrChan chan error) (*ValidationNode, error) {
	config := configFetcher()
	locator, err := server_common.NewMachineLocator(config.Wasm.RootPath)
	if err != nil {
		return nil, err
	}
	arbConfigFetcher := func() *server_arb.ArbitratorSpawnerConfig {
		return &configFetcher().Arbitrator
	}
	arbSpawner, err := server_arb.NewArbitratorSpawner(locator, arbConfigFetcher)
	if err != nil {
		return nil, err
	}
	var serverAPI *server_api.ExecServerAPI
	var jitSpawner *server_jit.JitSpawner
	if config.UseJit {
		jitConfigFetcher := func() *server_jit.JitSpawnerConfig { return &configFetcher().Jit }
		var err error
		jitSpawner, err = server_jit.NewJitSpawner(locator, jitConfigFetcher, fatalErrChan)
		if err != nil {
			return nil, err
		}
		serverAPI = server_api.NewExecutionServerAPI(jitSpawner, arbSpawner, arbConfigFetcher)
	} else {
		serverAPI = server_api.NewExecutionServerAPI(arbSpawner, arbSpawner, arbConfigFetcher)
	}
	valAPIs := []rpc.API{{
		Namespace:     server_api.Namespace,
		Version:       "1.0",
		Service:       serverAPI,
		Public:        config.ApiPublic,
		Authenticated: config.ApiAuth,
	}}
	stack.RegisterAPIs(valAPIs)

	return &ValidationNode{configFetcher, arbSpawner, jitSpawner}, nil
}

func (v *ValidationNode) Start(ctx context.Context) error {
	if err := v.arbSpawner.Start(ctx); err != nil {
		return err
	}
	if v.jitSpawner != nil {
		if err := v.jitSpawner.Start(ctx); err != nil {
			return err
		}
	}
	return nil
}

func (v *ValidationNode) GetExec() validator.ExecutionSpawner {
	return v.arbSpawner
}

'''
'''--- wavmio/higher.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build js
// +build js

package wavmio

import (
	"github.com/ethereum/go-ethereum/common"
	"github.com/offchainlabs/nitro/arbutil"
)

const INITIAL_CAPACITY = 128
const QUERY_SIZE = 32

// bytes32
const IDX_LAST_BLOCKHASH = 0
const IDX_SEND_ROOT = 1

// u64
const IDX_INBOX_POSITION = 0
const IDX_POSITION_WITHIN_MESSAGE = 1

func readBuffer(f func(uint32, []byte) uint32) []byte {
	buf := make([]byte, 0, INITIAL_CAPACITY)
	offset := 0
	for {
		if len(buf) < offset+QUERY_SIZE {
			buf = append(buf, make([]byte, offset+QUERY_SIZE-len(buf))...)
		}
		read := f(uint32(offset), buf[offset:(offset+QUERY_SIZE)])
		offset += int(read)
		if read < QUERY_SIZE {
			buf = buf[:offset]
			return buf
		}
	}
}

func StubInit() {}

func StubFinal() {}

func GetLastBlockHash() (hash common.Hash) {
	getGlobalStateBytes32(IDX_LAST_BLOCKHASH, hash[:])
	return
}

func ReadInboxMessage(msgNum uint64) []byte {
	return readBuffer(func(offset uint32, buf []byte) uint32 {
		return readInboxMessage(msgNum, offset, buf)
	})
}

func ReadDelayedInboxMessage(seqNum uint64) []byte {
	return readBuffer(func(offset uint32, buf []byte) uint32 {
		return readDelayedInboxMessage(seqNum, offset, buf)
	})
}

func AdvanceInboxMessage() {
	pos := getGlobalStateU64(IDX_INBOX_POSITION)
	setGlobalStateU64(IDX_INBOX_POSITION, pos+1)
}

func ResolveTypedPreimage(ty arbutil.PreimageType, hash common.Hash) ([]byte, error) {
	return readBuffer(func(offset uint32, buf []byte) uint32 {
		return resolveTypedPreimage(uint8(ty), hash[:], offset, buf)
	}), nil
}

func SetLastBlockHash(hash [32]byte) {
	setGlobalStateBytes32(IDX_LAST_BLOCKHASH, hash[:])
}

// Note: if a GetSendRoot is ever modified, the validator will need to fill in the previous send root, which it currently does not.
func SetSendRoot(hash [32]byte) {
	setGlobalStateBytes32(IDX_SEND_ROOT, hash[:])
}

func GetPositionWithinMessage() uint64 {
	return getGlobalStateU64(IDX_POSITION_WITHIN_MESSAGE)
}

func SetPositionWithinMessage(pos uint64) {
	setGlobalStateU64(IDX_POSITION_WITHIN_MESSAGE, pos)
}

func GetInboxPosition() uint64 {
	return getGlobalStateU64(IDX_INBOX_POSITION)
}

'''
'''--- wavmio/raw.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build js
// +build js

package wavmio

func getGlobalStateBytes32(idx uint64, output []byte)
func setGlobalStateBytes32(idx uint64, val []byte)
func getGlobalStateU64(idx uint64) uint64
func setGlobalStateU64(idx uint64, val uint64)
func readInboxMessage(msgNum uint64, offset uint32, output []byte) uint32
func readDelayedInboxMessage(seqNum uint64, offset uint32, output []byte) uint32
func resolveTypedPreimage(ty uint8, hash []byte, offset uint32, output []byte) uint32

'''
'''--- wavmio/stub.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

//go:build !js
// +build !js

package wavmio

import (
	"encoding/binary"
	"errors"
	"flag"
	"fmt"
	"io"
	"os"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/log"
	"github.com/offchainlabs/nitro/arbutil"
)

// source for arrayFlags: https://stackoverflow.com/questions/28322997/how-to-get-a-list-of-values-into-a-flag-in-golang
type arrayFlags []string

func (i *arrayFlags) String() string {
	return "my string representation"
}

func (i *arrayFlags) Set(value string) error {
	*i = append(*i, value)
	return nil
}

var (
	seqMsg             []byte
	seqMsgPos          uint64
	posWithinMsg       uint64
	delayedMsgs        [][]byte
	delayedMsgFirstPos uint64
	lastBlockHash      common.Hash
	preimages          map[common.Hash][]byte
	seqAdvanced        uint64
)

func parsePreimageBytes(path string) {
	file, err := os.Open(path)
	if err != nil {
		panic(err)
	}
	for {
		lenBuf := make([]byte, 8)
		read, err := file.Read(lenBuf)
		if err == io.EOF {
			return
		}
		if err != nil {
			panic(err)
		}
		if read != len(lenBuf) {
			panic(fmt.Sprintf("missing bytes reading len got %d", read))
		}
		fieldSize := int(binary.LittleEndian.Uint64(lenBuf))
		dataBuf := make([]byte, fieldSize)
		read, err = file.Read(dataBuf)
		if err != nil {
			panic(err)
		}
		if read != fieldSize {
			panic("missing bytes reading data")
		}
		hash := crypto.Keccak256Hash(dataBuf)
		preimages[hash] = dataBuf
	}
}

func StubInit() {
	preimages = make(map[common.Hash][]byte)
	var delayedMsgPath arrayFlags
	seqMsgPosFlag := flag.Int("inbox-position", 0, "position for sequencer inbox message")
	posWithinMsgFlag := flag.Int("position-within-message", 0, "position inside sequencer inbox message")
	delayedPositionFlag := flag.Int("delayed-inbox-position", 0, "position for first delayed inbox message")
	lastBlockFlag := flag.String("last-block-hash", "0000000000000000000000000000000000000000000000000000000000000000", "lastBlockHash")
	flag.Var(&delayedMsgPath, "delayed-inbox", "delayed inbox messages (multiple values)")
	inboxPath := flag.String("inbox", "", "file to load sequencer message")
	preimagesPath := flag.String("preimages", "", "file to load preimages from")
	flag.Parse()

	seqMsgPos = uint64(*seqMsgPosFlag)
	posWithinMsg = uint64(*posWithinMsgFlag)
	delayedMsgFirstPos = uint64(*delayedPositionFlag)
	lastBlockHash = common.HexToHash(*lastBlockFlag)
	for _, path := range delayedMsgPath {
		msg, err := os.ReadFile(path)
		if err != nil {
			panic(err)
		}
		delayedMsgs = append(delayedMsgs, msg)
	}
	if *inboxPath != "" {
		msg, err := os.ReadFile(*inboxPath)
		if err != nil {
			panic(err)
		}
		seqMsg = msg
	}
	if *preimagesPath != "" {
		parsePreimageBytes(*preimagesPath)
	}
}

func StubFinal() {
	log.Info("End state", "lastblockHash", lastBlockHash, "InboxPosition", seqMsgPos+seqAdvanced, "positionWithinMessage", posWithinMsg)
}

func GetLastBlockHash() (hash common.Hash) {
	return lastBlockHash
}

func ReadInboxMessage(msgNum uint64) []byte {
	if msgNum != seqMsgPos {
		panic(fmt.Sprintf("trying to read bad msg %d", msgNum))
	}
	return seqMsg
}

func ReadDelayedInboxMessage(seqNum uint64) []byte {
	if seqNum < delayedMsgFirstPos || (int(seqNum-delayedMsgFirstPos) > len(delayedMsgs)) {
		panic(fmt.Sprintf("trying to read bad delayed msg %d", seqNum))
	}
	return delayedMsgs[seqNum-delayedMsgFirstPos]
}

func AdvanceInboxMessage() {
	seqAdvanced++
}

func ResolveTypedPreimage(ty arbutil.PreimageType, hash common.Hash) ([]byte, error) {
	val, ok := preimages[hash]
	if !ok {
		return []byte{}, errors.New("preimage not found")
	}
	return val, nil
}

func SetLastBlockHash(hash [32]byte) {
	lastBlockHash = hash
}

func SetSendRoot(hash [32]byte) {
}

func GetPositionWithinMessage() uint64 {
	return posWithinMsg
}

func SetPositionWithinMessage(pos uint64) {
	posWithinMsg = pos
}

func GetInboxPosition() uint64 {
	return seqMsgPos
}

'''
'''--- wsbroadcastserver/clientconnection.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"context"
	"fmt"
	"math/rand"
	"net"
	"sync"
	"sync/atomic"
	"time"

	"github.com/offchainlabs/nitro/arbutil"

	"github.com/gobwas/ws"
	"github.com/gobwas/ws/wsflate"
	"github.com/mailru/easygo/netpoll"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

// ClientConnection represents client connection.
type ClientConnection struct {
	stopwaiter.StopWaiter

	ioMutex  sync.Mutex
	conn     net.Conn
	creation time.Time
	clientIp net.IP

	desc            *netpoll.Desc
	Name            string
	clientManager   *ClientManager
	requestedSeqNum arbutil.MessageIndex

	lastHeardUnix int64
	out           chan []byte

	compression bool
	flateReader *wsflate.Reader

	delay time.Duration
}

func NewClientConnection(
	conn net.Conn,
	desc *netpoll.Desc,
	clientManager *ClientManager,
	requestedSeqNum arbutil.MessageIndex,
	connectingIP net.IP,
	compression bool,
	delay time.Duration,
) *ClientConnection {
	return &ClientConnection{
		conn:            conn,
		clientIp:        connectingIP,
		desc:            desc,
		creation:        time.Now(),
		Name:            fmt.Sprintf("%s@%s-%d", connectingIP, conn.RemoteAddr(), rand.Intn(10)),
		clientManager:   clientManager,
		requestedSeqNum: requestedSeqNum,
		lastHeardUnix:   time.Now().Unix(),
		out:             make(chan []byte, clientManager.config().MaxSendQueue),
		compression:     compression,
		flateReader:     NewFlateReader(),
		delay:           delay,
	}
}

func (cc *ClientConnection) Age() time.Duration {
	return time.Since(cc.creation)
}

func (cc *ClientConnection) Compression() bool {
	return cc.compression
}

func (cc *ClientConnection) Start(parentCtx context.Context) {
	cc.StopWaiter.Start(parentCtx, cc)
	cc.LaunchThread(func(ctx context.Context) {
		if cc.delay != 0 {
			var delayQueue [][]byte
			t := time.NewTimer(cc.delay)
			done := false
			for !done {
				select {
				case <-ctx.Done():
					return
				case data := <-cc.out:
					delayQueue = append(delayQueue, data)
				case <-t.C:
					for _, data := range delayQueue {
						err := cc.writeRaw(data)
						if err != nil {
							logWarn(err, "error writing data to client")
							cc.clientManager.Remove(cc)
							return
						}
					}
					done = true
				}
			}
		}

		for {
			select {
			case <-ctx.Done():
				return
			case data := <-cc.out:
				err := cc.writeRaw(data)
				if err != nil {
					logWarn(err, "error writing data to client")
					cc.clientManager.Remove(cc)
					return
				}
			}
		}
	})
}

func (cc *ClientConnection) StopOnly() {
	// Ignore errors from conn.Close since we are just shutting down
	_ = cc.conn.Close()
	if cc.Started() {
		cc.StopWaiter.StopOnly()
	}
}

func (cc *ClientConnection) RequestedSeqNum() arbutil.MessageIndex {
	return cc.requestedSeqNum
}

func (cc *ClientConnection) GetLastHeard() time.Time {
	return time.Unix(atomic.LoadInt64(&cc.lastHeardUnix), 0)
}

// Receive reads next message from client's underlying connection.
// It blocks until full message received.
func (cc *ClientConnection) Receive(ctx context.Context, timeout time.Duration) ([]byte, ws.OpCode, error) {
	msg, op, err := cc.readRequest(ctx, timeout)
	if err != nil {
		_ = cc.conn.Close()
		return nil, op, err
	}

	return msg, op, err
}

// readRequests reads json-rpc request from connection.
func (cc *ClientConnection) readRequest(ctx context.Context, timeout time.Duration) ([]byte, ws.OpCode, error) {
	cc.ioMutex.Lock()
	defer cc.ioMutex.Unlock()

	atomic.StoreInt64(&cc.lastHeardUnix, time.Now().Unix())

	var data []byte
	var opCode ws.OpCode
	var err error
	data, opCode, err = ReadData(ctx, cc.conn, nil, timeout, ws.StateServerSide, cc.compression, cc.flateReader)
	return data, opCode, err
}

func (cc *ClientConnection) Write(x interface{}) error {
	cc.ioMutex.Lock()
	defer cc.ioMutex.Unlock()

	notCompressed, compressed, err := serializeMessage(cc.clientManager, x, !cc.compression, cc.compression)
	if err != nil {
		return err
	}

	if cc.compression {
		cc.out <- compressed.Bytes()
	} else {
		cc.out <- notCompressed.Bytes()
	}
	return nil
}

func (cc *ClientConnection) writeRaw(p []byte) error {
	cc.ioMutex.Lock()
	defer cc.ioMutex.Unlock()

	_, err := cc.conn.Write(p)

	return err
}

func (cc *ClientConnection) Ping() error {
	cc.ioMutex.Lock()
	defer cc.ioMutex.Unlock()
	_, err := cc.conn.Write(ws.CompiledPing)
	if err != nil {
		return err
	}

	return nil
}

'''
'''--- wsbroadcastserver/clientmanager.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"bytes"
	"compress/flate"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net"
	"strings"
	"sync/atomic"
	"time"

	"github.com/gobwas/ws"
	"github.com/gobwas/ws-examples/src/gopool"
	"github.com/gobwas/ws/wsflate"
	"github.com/gobwas/ws/wsutil"
	"github.com/mailru/easygo/netpoll"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"

	"github.com/offchainlabs/nitro/arbutil"
	"github.com/offchainlabs/nitro/util/stopwaiter"
)

var (
	clientsCurrentGauge               = metrics.NewRegisteredGauge("arb/feed/clients/current", nil)
	clientsConnectCount               = metrics.NewRegisteredCounter("arb/feed/clients/connect", nil)
	clientsDisconnectCount            = metrics.NewRegisteredCounter("arb/feed/clients/disconnect", nil)
	clientsTotalSuccessCounter        = metrics.NewRegisteredCounter("arb/feed/clients/success", nil)
	clientsTotalFailedRegisterCounter = metrics.NewRegisteredCounter("arb/feed/clients/failed/register", nil)
	clientsTotalFailedUpgradeCounter  = metrics.NewRegisteredCounter("arb/feed/clients/failed/upgrade", nil)
	clientsTotalFailedWorkerCounter   = metrics.NewRegisteredCounter("arb/feed/clients/failed/worker", nil)
	clientsDurationHistogram          = metrics.NewRegisteredHistogram("arb/feed/clients/duration", nil, metrics.NewBoundedHistogramSample())
)

// CatchupBuffer is a Protocol-specific client catch-up logic can be injected using this interface
type CatchupBuffer interface {
	OnRegisterClient(*ClientConnection) (error, int, time.Duration)
	OnDoBroadcast(interface{}) error
	GetMessageCount() int
}

// ClientManager manages client connections
type ClientManager struct {
	stopwaiter.StopWaiter

	clientPtrMap  map[*ClientConnection]bool
	clientCount   int32
	pool          *gopool.Pool
	poller        netpoll.Poller
	broadcastChan chan interface{}
	clientAction  chan ClientConnectionAction
	config        BroadcasterConfigFetcher
	catchupBuffer CatchupBuffer
	flateWriter   *flate.Writer

	connectionLimiter *ConnectionLimiter
}

type ClientConnectionAction struct {
	cc     *ClientConnection
	create bool
}

func NewClientManager(poller netpoll.Poller, configFetcher BroadcasterConfigFetcher, catchupBuffer CatchupBuffer) *ClientManager {
	config := configFetcher()
	return &ClientManager{
		poller:            poller,
		pool:              gopool.NewPool(config.Workers, config.Queue, 1),
		clientPtrMap:      make(map[*ClientConnection]bool),
		broadcastChan:     make(chan interface{}, 1),
		clientAction:      make(chan ClientConnectionAction, 128),
		config:            configFetcher,
		catchupBuffer:     catchupBuffer,
		connectionLimiter: NewConnectionLimiter(func() *ConnectionLimiterConfig { return &configFetcher().ConnectionLimits }),
	}
}

func (cm *ClientManager) registerClient(ctx context.Context, clientConnection *ClientConnection) error {
	defer func() {
		if r := recover(); r != nil {
			log.Error("Recovered in registerClient", "recover", r)
		}
	}()

	if cm.config().ConnectionLimits.Enable && !cm.connectionLimiter.Register(clientConnection.clientIp) {
		return fmt.Errorf("Connection limited %s", clientConnection.clientIp)
	}

	clientsCurrentGauge.Inc(1)
	clientsConnectCount.Inc(1)

	atomic.AddInt32(&cm.clientCount, 1)
	err, sent, elapsed := cm.catchupBuffer.OnRegisterClient(clientConnection)
	if err != nil {
		clientsTotalFailedRegisterCounter.Inc(1)
		if cm.config().ConnectionLimits.Enable {
			cm.connectionLimiter.Release(clientConnection.clientIp)
		}
		return err
	}
	if cm.config().LogConnect {
		log.Info("client registered", "client", clientConnection.Name, "requestedSeqNum", clientConnection.RequestedSeqNum(), "sentCount", sent, "elapsed", elapsed)
	}

	clientConnection.Start(ctx)
	cm.clientPtrMap[clientConnection] = true
	clientsTotalSuccessCounter.Inc(1)

	return nil
}

// Register registers new connection as a Client.
func (cm *ClientManager) Register(
	conn net.Conn,
	desc *netpoll.Desc,
	requestedSeqNum arbutil.MessageIndex,
	connectingIP net.IP,
	compression bool,
) *ClientConnection {
	createClient := ClientConnectionAction{
		NewClientConnection(conn, desc, cm, requestedSeqNum, connectingIP, compression, cm.config().ClientDelay),
		true,
	}
	cm.clientAction <- createClient

	return createClient.cc
}

// removeAll removes all clients after main ClientManager thread exits
func (cm *ClientManager) removeAll() {
	// Only called after main ClientManager thread exits, so remove client directly
	for client := range cm.clientPtrMap {
		cm.removeClientImpl(client)
	}
}

func (cm *ClientManager) removeClientImpl(clientConnection *ClientConnection) {
	clientConnection.StopOnly()

	err := cm.poller.Stop(clientConnection.desc)
	if err != nil {
		log.Warn("Failed to stop poller", "err", err)
	}

	err = clientConnection.conn.Close()
	if err != nil && !strings.Contains(err.Error(), "use of closed network connection") {
		log.Warn("Failed to close client connection", "err", err)
	}

	if cm.config().LogDisconnect {
		log.Info("client removed", "client", clientConnection.Name, "age", clientConnection.Age())
	}

	clientsDurationHistogram.Update(clientConnection.Age().Microseconds())
	clientsCurrentGauge.Dec(1)
	clientsDisconnectCount.Inc(1)
	atomic.AddInt32(&cm.clientCount, -1)
}

func (cm *ClientManager) removeClient(clientConnection *ClientConnection) {
	if !cm.clientPtrMap[clientConnection] {
		return
	}

	cm.removeClientImpl(clientConnection)
	if cm.config().ConnectionLimits.Enable {
		cm.connectionLimiter.Release(clientConnection.clientIp)
	}

	delete(cm.clientPtrMap, clientConnection)
}

func (cm *ClientManager) Remove(clientConnection *ClientConnection) {
	cm.clientAction <- ClientConnectionAction{
		clientConnection,
		false,
	}
}

func (cm *ClientManager) ClientCount() int32 {
	return atomic.LoadInt32(&cm.clientCount)
}

// Broadcast sends batch item to all clients.
func (cm *ClientManager) Broadcast(bm interface{}) {
	if cm.Stopped() {
		// This should only occur if a reorg occurs after the broadcast server is stopped,
		// with the sequencer enabled but not the sequencer coordinator.
		// In this case we should proceed without broadcasting the message.
		return
	}
	cm.broadcastChan <- bm
}

func (cm *ClientManager) doBroadcast(bm interface{}) ([]*ClientConnection, error) {
	if err := cm.catchupBuffer.OnDoBroadcast(bm); err != nil {
		return nil, err
	}
	config := cm.config()
	//                                        /-> wsutil.Writer -> not compressed msg buffer
	// bm -> json.Encoder -> io.MultiWriter -|
	//                                        \-> cm.flateWriter -> wsutil.Writer -> compressed msg buffer

	notCompressed, compressed, err := serializeMessage(cm, bm, !config.RequireCompression, config.EnableCompression)
	if err != nil {
		return nil, err
	}

	sendQueueTooLargeCount := 0
	clientDeleteList := make([]*ClientConnection, 0, len(cm.clientPtrMap))
	for client := range cm.clientPtrMap {
		var data []byte
		if client.Compression() {
			if config.EnableCompression {
				data = compressed.Bytes()
			} else {
				log.Warn("disconnecting because client has enabled compression, but compression support is disabled", "client", client.Name)
				clientDeleteList = append(clientDeleteList, client)
				continue
			}
		} else {
			if !config.RequireCompression {
				data = notCompressed.Bytes()
			} else {
				log.Warn("disconnecting because client has disabled compression, but compression support is required", "client", client.Name)
				clientDeleteList = append(clientDeleteList, client)
				continue
			}
		}
		select {
		case client.out <- data:
		default:
			// Queue for client too backed up, disconnect instead of blocking on channel send
			sendQueueTooLargeCount++
			clientDeleteList = append(clientDeleteList, client)
		}
	}

	if sendQueueTooLargeCount > 0 {
		if sendQueueTooLargeCount < 10 {
			log.Warn("disconnecting clients because send queue too large", "count", sendQueueTooLargeCount)
		} else {
			log.Error("disconnecting clients because send queue too large", "count", sendQueueTooLargeCount)
		}
	}

	return clientDeleteList, nil
}

func serializeMessage(cm *ClientManager, bm interface{}, enableNonCompressedOutput, enableCompressedOutput bool) (bytes.Buffer, bytes.Buffer, error) {
	var notCompressed bytes.Buffer
	var compressed bytes.Buffer
	writers := []io.Writer{}
	var notCompressedWriter *wsutil.Writer
	var compressedWriter *wsutil.Writer
	if enableNonCompressedOutput {
		notCompressedWriter = wsutil.NewWriter(&notCompressed, ws.StateServerSide, ws.OpText)
		writers = append(writers, notCompressedWriter)
	}
	if enableCompressedOutput {
		if cm.flateWriter == nil {
			var err error
			cm.flateWriter, err = flate.NewWriterDict(nil, DeflateCompressionLevel, GetStaticCompressorDictionary())
			if err != nil {
				return bytes.Buffer{}, bytes.Buffer{}, fmt.Errorf("unable to create flate writer: %w", err)
			}
		}
		compressedWriter = wsutil.NewWriter(&compressed, ws.StateServerSide|ws.StateExtended, ws.OpText)
		var msg wsflate.MessageState
		msg.SetCompressed(true)
		compressedWriter.SetExtensions(&msg)
		cm.flateWriter.Reset(compressedWriter)
		writers = append(writers, cm.flateWriter)
	}

	multiWriter := io.MultiWriter(writers...)
	encoder := json.NewEncoder(multiWriter)
	if err := encoder.Encode(bm); err != nil {
		return bytes.Buffer{}, bytes.Buffer{}, fmt.Errorf("unable to encode message: %w", err)
	}
	if notCompressedWriter != nil {
		if err := notCompressedWriter.Flush(); err != nil {
			return bytes.Buffer{}, bytes.Buffer{}, fmt.Errorf("unable to flush message: %w", err)
		}
	}
	if compressedWriter != nil {
		if err := cm.flateWriter.Close(); err != nil {
			return bytes.Buffer{}, bytes.Buffer{}, fmt.Errorf("unable to close flate writer: %w", err)
		}
		if err := compressedWriter.Flush(); err != nil {
			return bytes.Buffer{}, bytes.Buffer{}, fmt.Errorf("unable to flush message: %w", err)
		}
	}
	return notCompressed, compressed, nil
}

// verifyClients should be called every cm.config.ClientPingInterval
func (cm *ClientManager) verifyClients() []*ClientConnection {
	clientConnectionCount := len(cm.clientPtrMap)

	// Create list of clients to remove
	clientDeleteList := make([]*ClientConnection, 0, clientConnectionCount)

	// Send ping to all connected clients
	log.Debug("pinging clients", "count", len(cm.clientPtrMap))
	for client := range cm.clientPtrMap {
		diff := time.Since(client.GetLastHeard())
		if diff > cm.config().ClientTimeout {
			log.Debug("disconnecting because connection timed out", "client", client.Name)
			clientDeleteList = append(clientDeleteList, client)
		} else {
			err := client.Ping()
			if err != nil {
				log.Debug("disconnecting because error pinging client", "client", client.Name)
				clientDeleteList = append(clientDeleteList, client)
			}
		}
	}

	return clientDeleteList
}

func (cm *ClientManager) Start(parentCtx context.Context) {
	cm.StopWaiter.Start(parentCtx, cm)

	cm.LaunchThread(func(ctx context.Context) {
		defer cm.removeAll()

		// Ping needs to occur regularly regardless of other traffic
		pingTimer := time.NewTimer(cm.config().Ping)
		var clientDeleteList []*ClientConnection
		defer pingTimer.Stop()
		for {
			select {
			case <-ctx.Done():
				return
			case clientAction := <-cm.clientAction:
				if clientAction.create {
					err := cm.registerClient(ctx, clientAction.cc)
					if err != nil {
						// Log message already output in registerClient
						cm.removeClientImpl(clientAction.cc)
					}
				} else {
					cm.removeClient(clientAction.cc)
				}
			case bm := <-cm.broadcastChan:
				var err error
				clientDeleteList, err = cm.doBroadcast(bm)
				logError(err, "failed to do broadcast")
			case <-pingTimer.C:
				clientDeleteList = cm.verifyClients()
				pingTimer.Reset(cm.config().Ping)
			}

			if len(clientDeleteList) > 0 {
				for _, client := range clientDeleteList {
					cm.removeClient(client)
				}
				clientDeleteList = nil
			}
		}
	})
}

'''
'''--- wsbroadcastserver/connectionlimiter.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"net"
	"sync"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	flag "github.com/spf13/pflag"
)

var (
	clientsLimitedCounter = metrics.NewRegisteredCounter("arb/feed/clients/limited", nil)
)

type ConnectionLimiterConfig struct {
	Enable                  bool          `koanf:"enable" reload:"hot"`
	PerIpLimit              int           `koanf:"per-ip-limit" reload:"hot"`
	PerIpv6Cidr48Limit      int           `koanf:"per-ipv6-cidr-48-limit" reload:"hot"`
	PerIpv6Cidr64Limit      int           `koanf:"per-ipv6-cidr-64-limit" reload:"hot"`
	ReconnectCooldownPeriod time.Duration `koanf:"reconnect-cooldown-period" reload:"hot"`
}

var DefaultConnectionLimiterConfig = ConnectionLimiterConfig{
	Enable:                  false,
	PerIpLimit:              5,
	PerIpv6Cidr48Limit:      20,
	PerIpv6Cidr64Limit:      10,
	ReconnectCooldownPeriod: 0,
}

func ConnectionLimiterConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultConnectionLimiterConfig.Enable, "enable broadcaster per-client connection limiting")
	f.Int(prefix+".per-ip-limit", DefaultConnectionLimiterConfig.PerIpLimit, "limit clients, as identified by IPv4/v6 address, to this many connections to this relay")
	f.Int(prefix+".per-ipv6-cidr-48-limit", DefaultConnectionLimiterConfig.PerIpv6Cidr48Limit, "limit ipv6 clients, as identified by IPv6 address masked with /48, to this many connections to this relay")
	f.Int(prefix+".per-ipv6-cidr-64-limit", DefaultConnectionLimiterConfig.PerIpv6Cidr64Limit, "limit ipv6 clients, as identified by IPv6 address masked with /64, to this many connections to this relay")
	f.Duration(prefix+".reconnect-cooldown-period", DefaultConnectionLimiterConfig.ReconnectCooldownPeriod, "time to wait after a relay client disconnects before the disconnect is registered with respect to the limit for this client")
}

type ConnectionLimiterConfigFetcher func() *ConnectionLimiterConfig

type ConnectionLimiter struct {
	sync.RWMutex

	ipConnectionCounts map[string]int
	config             ConnectionLimiterConfigFetcher
}

func NewConnectionLimiter(configFetcher ConnectionLimiterConfigFetcher) *ConnectionLimiter {
	return &ConnectionLimiter{
		ipConnectionCounts: make(map[string]int),
		config:             configFetcher,
	}
}

func (l *ConnectionLimiter) IsAllowed(ip net.IP) bool {
	l.RLock()
	defer l.RUnlock()
	return l.isAllowedImpl(ip)
}

func isIpv6(ip net.IP) bool {
	// This seems to be the canonical way to distinguish IPv4 from IPv6 in Go
	// https://stackoverflow.com/questions/22751035/golang-distinguish-ipv4-ipv6
	// We don't care about the case where it is an IPv4 address in IPv6
	// representation, we'll just treat that as IPv4.
	return ip.To4() == nil
}

type ipStringAndLimit struct {
	ipString string
	limit    int
}

func (l *ConnectionLimiter) getIpStringsAndLimits(ip net.IP) []ipStringAndLimit {
	var result []ipStringAndLimit
	if ip == nil || ip.IsPrivate() || ip.IsLoopback() {
		log.Warn("Ignoring private, looback, or unparseable IP. Please check relay and network configuration to ensure client IP addresses are detected correctly", "ip", ip)
		return result
	}

	config := l.config()
	result = append(result, ipStringAndLimit{string(ip), config.PerIpLimit})

	if isIpv6(ip) {
		ipv6Slash48 := ip.Mask(net.CIDRMask(48, 128))
		if ipv6Slash48 == nil {
			log.Warn("Error taking /48 mask of ipv6 client address", "ip", ip)
		} else {
			result = append(result, ipStringAndLimit{string(ipv6Slash48) + "/48", config.PerIpv6Cidr48Limit})
		}

		ipv6Slash64 := ip.Mask(net.CIDRMask(64, 128))
		if ipv6Slash64 == nil {
			log.Warn("Error taking /64 mask of ipv6 client address", "ip", ip)
		} else {
			result = append(result, ipStringAndLimit{string(ipv6Slash64) + "/64", config.PerIpv6Cidr64Limit})
		}
	}
	return result
}

func (l *ConnectionLimiter) isAllowedImpl(ip net.IP) bool {
	for _, item := range l.getIpStringsAndLimits(ip) {
		if res := l.ipConnectionCounts[item.ipString]; res >= item.limit {
			clientsLimitedCounter.Inc(1)
			return false
		}
	}

	return true
}

func (l *ConnectionLimiter) updateUsage(ip net.IP, increment bool) {
	if ip == nil {
		return
	}

	updateAmount := -1
	if increment {
		updateAmount = 1
	}

	for _, item := range l.getIpStringsAndLimits(ip) {
		l.ipConnectionCounts[item.ipString] += updateAmount
		if l.ipConnectionCounts[item.ipString] < 0 {
			log.Error("BUG: Unbalanced ConnectionLimiter.updateUsage(..., false) calls", "ip", item.ipString)
			l.ipConnectionCounts[item.ipString] = 0
		} else if l.ipConnectionCounts[item.ipString] > item.limit {
			log.Error("BUG: Unbalanced ConnectionLimiter.updateUsage(..., true) calls", "ip", item.ipString)
			l.ipConnectionCounts[item.ipString] = item.limit
		}
	}
}

func (l *ConnectionLimiter) Register(ip net.IP) bool {
	l.Lock()
	defer l.Unlock()

	// First check if allowed without modifying counts so that we don't need to roll back partial counts.
	if !l.isAllowedImpl(ip) {
		return false
	}

	l.updateUsage(ip, true)

	return true
}

func (l *ConnectionLimiter) Release(ip net.IP) {
	p := l.config().ReconnectCooldownPeriod
	if p > 0 {
		go func() {
			time.Sleep(p)
			l.Lock()
			defer l.Unlock()
			l.updateUsage(ip, false)
		}()
	} else {
		l.Lock()
		defer l.Unlock()
		l.updateUsage(ip, false)
	}
}

'''
'''--- wsbroadcastserver/connectionlimiter_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"net"
	"runtime"
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestIpv4BasicConnectionLimiting(t *testing.T) {
	configFetcher := func() *ConnectionLimiterConfig {
		return &ConnectionLimiterConfig{
			Enable:             true,
			PerIpLimit:         3,
			PerIpv6Cidr48Limit: 1,
			PerIpv6Cidr64Limit: 1,
		}
	}
	l := NewConnectionLimiter(configFetcher)

	ip1 := net.ParseIP("1.2.3.4")

	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))

	l.Release(ip1)
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))
	Expect(t, !l.Register(ip1))

	l.Release(ip1)
	l.Release(ip1)
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))
	Expect(t, !l.Register(ip1))

	l.Release(ip1)
	l.Release(ip1)
	l.Release(ip1)
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))
	Expect(t, !l.Register(ip1))
}

func TestTooManyReleases(t *testing.T) {
	configFetcher := func() *ConnectionLimiterConfig {
		return &ConnectionLimiterConfig{
			Enable:             true,
			PerIpLimit:         3,
			PerIpv6Cidr48Limit: 1,
			PerIpv6Cidr64Limit: 1,
		}
	}
	l := NewConnectionLimiter(configFetcher)

	ip1 := net.ParseIP("1.2.3.4")

	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))

	// Make sure the count doesn't go negative and allow too many connections.
	l.Release(ip1)
	l.Release(ip1)
	l.Release(ip1)
	l.Release(ip1)
	l.Release(ip1)
	l.Release(ip1)

	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.IsAllowed(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, !l.IsAllowed(ip1))
	Expect(t, !l.Register(ip1))
}

func TestIpv6Masks(t *testing.T) {
	configFetcher := func() *ConnectionLimiterConfig {
		return &ConnectionLimiterConfig{
			Enable:             true,
			PerIpLimit:         3,
			PerIpv6Cidr48Limit: 5,
			PerIpv6Cidr64Limit: 2,
		}
	}
	l := NewConnectionLimiter(configFetcher)

	ip1 := net.ParseIP("1:2:3:4:5:5:5:5")
	ip2 := net.ParseIP("1:2:3:4:6:6:6:6")
	ip3 := net.ParseIP("1:2:3:4:7:7:7:7")

	ip4 := net.ParseIP("1:2:3:5:5:5:5:5")
	ip5 := net.ParseIP("1:2:3:6:6:6:6:6")
	ip6 := net.ParseIP("1:2:3:7:7:7:7:7")

	ip7 := net.ParseIP("1:2:4:5:5:5:5:5")
	ip8 := net.ParseIP("1:2:4:6:6:6:6:6")
	ip9 := net.ParseIP("1:2:4:7:7:7:7:7")

	// /64 limit blocks
	Expect(t, l.Register(ip1))  // 1:2:3:4/64 1, 1:2:3/48 1
	Expect(t, l.Register(ip2))  // 1:2:3:4/64 2, 1:2:3/48 2
	Expect(t, !l.Register(ip2)) // 1:2:3:4/64 2*, 1:2:3/48 2
	Expect(t, !l.Register(ip3)) // 1:2:3:4/64 2*, 1:2:3/48 2

	// /48 limit blocks
	Expect(t, l.Register(ip4))  // 1:2:3:5/64 1, 1:2:3/48 3
	Expect(t, l.Register(ip5))  // 1:2:3:6/64 1, 1:2:3/48 4
	Expect(t, l.Register(ip4))  // 1:2:3:5/64 2, 1:2:3/48 5
	Expect(t, !l.Register(ip5)) // 1:2:3:6/64 1, 1:2:3/48 5*

	// /64 limit blocks after releasing from the /48 that would've blocked
	l.Release(ip1)              // 1:2:3:4/64 1, 1:2:3/48 4
	Expect(t, l.Register(ip5))  // 1:2:3:6/64 2, 1:2:3/48 5
	l.Release(ip2)              // 1:2:3:4/64 0, 1:2:3/48 4
	Expect(t, !l.Register(ip5)) // 1:2:3:6/64 2*, 1:2:3/48 4

	// /48 limit blocks a new /64 IP
	Expect(t, l.Register(ip6))  // 1:2:3:7/64 1, 1:2:3/48 5
	Expect(t, !l.Register(ip6)) // 1:2:3:7/64 1, 1:2:3/48 5*

	// IPs in different range to above have separate counts
	Expect(t, l.Register(ip7))  // 1:2:4:5/64 1, 1:2:4/48 1
	Expect(t, l.Register(ip7))  // 1:2:4:5/64 2, 1:2:4/48 2
	Expect(t, !l.Register(ip7)) // 1:2:4:5/64 2*, 1:2:4/48 2
	Expect(t, l.Register(ip8))  // 1:2:4:6/64 1, 1:2:4/48 3
	Expect(t, l.Register(ip8))  // 1:2:4:6/64 2, 1:2:4/48 4
	Expect(t, !l.Register(ip8)) // 1:2:4:6/64 2*, 1:2:4/48 4
	Expect(t, l.Register(ip9))  // 1:2:4:7/64 1, 1:2:4/48 5
	Expect(t, !l.Register(ip9)) // 1:2:4:7/64 1, 1:2:4/48 5

}

func TestPrivateAddresses(t *testing.T) {
	configFetcher := func() *ConnectionLimiterConfig {
		return &ConnectionLimiterConfig{
			Enable:             true,
			PerIpLimit:         1,
			PerIpv6Cidr48Limit: 1,
			PerIpv6Cidr64Limit: 1,
		}
	}
	l := NewConnectionLimiter(configFetcher)
	ip1 := net.ParseIP("fc00:0:0:0:0:0:0:1")
	ip2 := net.ParseIP("fc00:0:0:0:1:0:0:1")

	ip3 := net.ParseIP("10.0.0.1")
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))
	Expect(t, l.Register(ip1))

	Expect(t, l.Register(ip2))
	Expect(t, l.Register(ip2))
	Expect(t, l.Register(ip2))

	Expect(t, l.Register(ip3))
	Expect(t, l.Register(ip3))
	Expect(t, l.Register(ip3))
}

func Expect(t *testing.T, res bool, text ...interface{}) {
	t.Helper()
	if !res {
		buf := make([]byte, 1<<16)
		stackSize := runtime.Stack(buf, false)
		testhelpers.FailImpl(t, string(buf[0:stackSize]))
	}
}

func Require(t *testing.T, err error, text ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, text...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

'''
'''--- wsbroadcastserver/dictionary.go ---
package wsbroadcastserver

import "compress/flate"

const DeflateCompressionLevel = flate.BestCompression

// The static dictionary was created by appending manually created dictionary to a dictionary generated with dictator tool (https://github.com/vkrasnov/dictator)
// * the dictator tool was used with default parameters, except for threshold which was set to 0.05% and compression level to 9
// * the input for the generator consisted of 61512 preprocessed messages gathered from public feed with wscat
// * the preprocessing substituted sequential numeric fields with pseudorandom values to minimize overfitting
func GetStaticCompressorDictionary() []byte {
	return []byte(`ADDUAtcQABTAABSAAEBCAoAEABUAABAOABAUACshAFYAACAKAAOAAHoSABAa/8AAAmJaAAAYAoAFABLAAAK/AASAEAAc3,"rACJA2,"ru4AgAoAgANCAAsAAATEtAOAAAQKgALXEAAB9AKAgAAAU6,"rABAe1,"rAWB4AEsAAAJAASDQ0,"rAVkAAFQAACQQA+AEQCAQAJAuA7u4AKABAGAQ5,"rACAIABAYADARAgAMAAAGgICA7,"rDu77A+gACACYloAFAiAFAcABzPUADgAEAW42AEABIBuAEAP/8AL68IAFAaABKABKgAgAwAQAaAAAAOuBAAIgAF9eEAGGoADu7gAEgAIAADgAgAAGAdzWUAKSxABkAGABAAGABAABAAAAEAAFAAEJoAAbANBuAFACR5AAAVgAGABwABAMAAAAKAYACgABYIAAIAQAwAADABgAQACgACADYAPAAA4AOjUpRAgAAIAqMA5U8BZoABIAFADgAAUAA7uAEeGjACQALAACADAAAH0AAgABAPj1gAAADAEABAPQkAKjAACcQ/AGAAoAHM9QArAAGACowAABhqAAQAD0JAcz1/pHKAAUAABwAgAKBvAAAgAC+vCABvwjrAAhpWEzBuA0AUAABoAgABASABwAAAAMNBECV6nsAACcQABAIABfXhAEAD4ACAoABAfQAAABYAASwAiAAQAKTG8zQSARgABOAYAEAoACBY0V4XYoAwAGgADAQAAMAYAC1xBAkACoA0AD4BIAHAgAJAG0AAgADAAYABYAHAKI3r14AkYTnKgAEcN5N+CAADQAANkZXYAABZIgxZFAAAADwAC7gALXmIPSAAAQCAoABiAAIABgASAD0JABASAOAQABaAIQAQAQgAACAFjRXhdigA9CQAABCaAABxr9SY0ACrSZH+AACBa8xB6AMAABgAWCIADEzCmjBgA//wAYABqAACGlYTAA5IqooZ8QADExvM0EABvBbWdOyAEAABAAMgyAbAANAwABa8xB6QADAAQABafqYnuuCGlYTNBAFAWAAojevXgACgAWAAPt8BfQ8rTGAFgAOQE5Fqv0},"signatureASADADMsADAEA1},"signatureAIACgAUxvoGihWXACoCkDEnMvAKI3r17/ARWORgkT0AAQAABAgAAMAUAADwAQACgAABACAMALQAEABoAAQSIQ6KAMywAMABoADnRyYW5zZmVydG8ueHl6AsaK8LsUxAOTwIQkpAZN8qtbsAEAAiAKo9n6OrkwrmNbAB0AxhKqWxTXUOAAGwsBnkLPj48OOEu4i+Bg7N6E+5VAHTHZNQbd9u7T+dx2rGTmwCxRolK/Aga49BeTw0N0p1oQEJKC3Yaf9tRACRhOcqAf5ASK/BwD55+H2iP6SaUDog581MAEjHetvV0nvbOaUOidaHT50hvTq4AAB3SUKdPZBHqWayikiVADJbrJ8bYAACAQAIWtLS6nz5xlyaIbTShd0hCeL/K4AAbABAAoAAQEAgADAQEAAMAADDB4RmVlRHluYW1pYwAACAAP+XCmGgSxyhSDSkP13kUz6921zADe3ewbDwKPaDF2XpISpo9ZkLoJ5AAQAH0CAnfW1TPgC5B2Eh5MaYaWckS0X9+wAElVuaXN3YXBWMwAD0FR7r+huch92SyCQ6GLG674wYEABoABBVdVUpiKOoBQS7rrExFnT8/UCtSwA89kS5/sYCs3qMaUnl9Ve4piKuQAQAIAEAEAAQAgq9JRH2KB+O9lb0NVvNSQVI/urEADAD7BUH1QAAnEADOXyS3qV6cun30tU6RG0o9yM2vb/AYAO7uAAEMgABQAIKvSUR9igfjvZW9DVbzUkFSP7qxD3OLoZC9eSRPkj6IynEJZwWwLEngAoAAAN4Lazp2QAIAIACQ5XAaZCZFijXCkSPVHiyhxfb0Rf/AAGABAq2r1ujOPoL1LZmYp/ZKkNKUqSpAAUABYABZede1RuOOQU9+mCJRS+RDpIAFKQANdPUlXVV5RM990ORf9SFSAALVdIAN4Lazp2Ab4KjwZ3A+NdFtyqiY9izeh/xiUYAAqApAxJzL4ADkiqihnAJGE5yoAG4AUAGwLajLDQl+uNV6F1uIx9i0eZdQYAKfOPF5skfWgUV1EKJj30I95rVlxAAAwAP+XCmGgSxyhSDSkP13kUz6921zIADO8ceRzdjD6pLWqzI5kRn9MOH/IUNXhV4AEj4AINa9v3iAWp0DXd+teAnNUAbhv/eF2tWGnv2BosQlRcnZADAAcCiawDB8N8AMAA5ACcUAAD6pxkA//ADAAO7uAkYTnKAXgAD/lwphoEscoUg0pD9d5FM+vdtcyADdlAGPVOVl2/yTn3xEoW4WP6qzMf/AAAD/lwphoEscoUg0pD9d5FM+vdtcyAAQAACAs0i4eyPVl34pSObzbKB+HslNcygACnOzOcP64nvt98ty2dAAsI/ImWCAFuJc82TOzDBVkBKDNbjzJ5ddBWAQAEgAKAAIKvSUR9igfjvZW9DVbzUkFSP7qx{"version":9,"confirmedSequenceNumberMessage":{"sAHdJQp09kEepZrKKSJUAMlusnxtADXT1JV1VeUTPfdDkX/UhUgAC1XSADekD4nEiiKHagpQt3fLCBSlWWsMAeUphNY1oRVlPlNwdsColK1tIFK3/ABvgqPBncD410W3KqJj2LN6H/GJRgAAUpPGyla4lBBAuNGPVX36gs9SAhYA2TtNC95pDBRPnyerIEHNRSXSCrQAMAQAABQBaWIT8MZSNWd8q7MyhQ96QDUnhoEgAcADZO00L3mkMFE+fJ6sgQc1FJdIKtAAIKvSUR9igfjvZW9DVbzUkFSP7qx/AG+EKg4f/8xJI9BLVZ9NKai/xs2gTrWw08RgdRwkAsXFzJEJyAJwUSAfdy+jvCYxYOoJuxbOGmuPwPqzaoAKWtxUhPmZf799QFuapip9iIg8NFAG5I8o79l7fnA6Dd8lKtG+LIm2nCsGj8N42qlSun8WPEoRYo9VpN9SOz76AOq0DN1iubhNjVSmCb9F2m7/oQYZ8},"signatureAABAQBkVBIWuv/+7I6lNbtx+8kngx0FlQAD//Ylj79H8alBkiEldlR1SY5iNJQAkIxNlNNJJHZfHtwiod0Jg5fFndT/{"version":3,"confirmedSequenceNumberMessage":{"s4},"signatureAEgAWAGUJhbGFuY2VyVjIAIAP+XCmGgSxyhSDSkP13kUz6921zIAGNSpWyq3E8eJc1sdZcPp2ijME5kAAt0wZ7sleWOhblY6MPXVyMUkzjz/AASoEqo8rwAgAAEAgAQABvhjoIxb4eXr7H1b0U9xQn0ehPPdAxTA97IpHlsgCsjHw7kloAQABEvMFpTAOqN/uGJin4KWfdSfwdhBtfkTCF2/AIgAAEAUDAMAAtcI7zczXsf6TOFOH+lOdEj8kPsALGivC7FAwAEAIBWANyK2RbSIeY12mcx41eEXTL5AHXBQ0YPbj4i9Dnf+UfiXJzLctLo/AAgAF/XEoC2OFFXspG5li8iFT/J55AojevXv/AMABABIAMAAAApLEAAAQADAAaACA5FAjtN8AR7e3r9jvvDqLS0HG9+I9xVD7G+/AABESUBLfAAADAVPCAkuMlYTGVTdV8BGR96LLnqakABU8ICS4yVhMZVN1XwEZH3osuepqAH9cdky8FPlmm4iDfKFJDMoXwxYHAQAQAC65B/tLcTkNxc0A5rgbfarONYGTAAAIKvSUR9igfjvZW9DVbzUkFSP7qxACKxyMEiegAACAgq9JRH2KB+O9lb0NVvNSQVI/urEAEgAwAACABAZQADUtgnWq4+DCQE2faPbO4IS1vrPd4QLkXpLZ8orAB5SmE1jWhFWU+U3B2wKiUrW0gUrQAAMAE29uZUluY2hWNUZlZUR5bmFtaWMAKAY7fVAJEWuQB32},"signature{"version":2,"confirmedSequenceNumberMessage":{"sA32luBRWVeSLpwGMgp9yI8aeEVv/ABgACwABYiv2fm5XR7gYZv4LhReKk8f4bjAEB9Aza7GVJX6XAVFxaQFIkIU41lPMNj/Aq0mR/yICgbE7KJ/+XCmGgSxyhSDSkP13kUz6921zIERERElTuslR3to+4Xtkp9zqWBYACbZCA4ePJOsM31XIxvp9CLoM935QA{"version":1,"confirmedSequenceNumberMessage":{"sAD//Ylj79H8alBkiEldlR1SY5iNJAbwW1nTs7gAAAKKFT7vZs+9m8X1HKE5/iZuVCTMADU8HwvHj7vCRbPJPvd7Hcxbd9KgJxABZede1RuOOQU9+mCJRS+RDpIAFKABLkor9fKd1x/Ts3ywAt+YIliq73P/EDk8IJdc8XejUTBxvBEPeWLNZ9oACOG8m/BAWSIMWRWAPvYSeYAf5vDzC1usVnARbjcZgJo/AOmB4qmknIKiL+XkTljIUjyNYieSAAAWAAFSMMLCvPObZbxpjr8BfD7ZklRef/ADEaB5sAjUXnsPU9PxN89FP3ZYBcA8xo+QZmHMVCAkOVwGmQmRYo1wpEj1R4socX29EX/AIALXCO83M17H+kzhTh/pTnRI/JD7AM5fJLepXpy6ffS1TpEbSj3Iza9vAABAAQAxGgebAI1F57D1PT8TfPRT92WAXA15NgUtHgltSMge85GPn9Y4QQhIALWXdrDcKM+Sq7Ql1unEWo/8pO7kACBRImhU8w7AhpWEzQAOVH+rTVzq/SniZTyxnmro7ZyFibC0nE5oAXTjMcsKf/OrWK/Jc41fiwAAAEAP//8ACIABAB0eRC3TSZRoGVjwxgoOOrkEg9CdwAcAIx2QGsLadAjqlu6XlqjEAB/kBIr8HAPnn4faI/pJpQOiDnzUwAEAIKvSUR9igfjvZW9DVbzUkFSP7qxABoCE/G94ZQABAfALNIuHsj1Zd+KUjm82ygfh7JTXMoC7gCYtAq+JB6qyr1N2unANhxiC/knUABIx3rb1dJ72zmlDonWh0+dIb06uAAAwABA5ZJCegrs6S3j7e4fGOAVfAWGFWQASMd629XSe9s5pQ6J1odPnSG9OrAFACQaEAMz7vou/DieyDam/2GfwcZEgaTn5Pj8hzxNi6xcVW3QCd95Wh9rpqDqFkJ/XHalZROe4ALfeM0QLcXEVmpcYy+dICGzs3ZaF/42ZSgzDPdb0gZ1HMiYld5Kv4jDsPGf+GgTkHuE+A81eBEZIe1JP3Eivas8mwHTazb37a1dLQsgUaAPl/TfdRF6eMGloNu4FK+SRYU5+0ARw3k34IAcRIbz3pk9L4p/F6Z8hyYJDRo7Z0AAgAP+XCmGgSxyhSDSkP13kUz6921zIAFgALyolQ7dqQWZUn3qrLnW+8K78Ww8AAIA4AWNFeF2K{"version":7,"confirmedSequenceNumberMessage":{"sAAUAcBqVcHoCkKyLkLNxno7lshA2CIMAAgALNIuHsj1Zd+KUjm82ygfh7JTXMABQAFgrEA5PAhCSAMMus2+Ib2OP/9g230TBJAdM/jWE/AABsC2oyw0JfrjVehdbiMfYtHmXUG/AEAC"signature":nulADAAABAQAABgAYAH+QEivwcA+efh9oj+kmlA6IOfNT/ACzSLh7I9WXfilI5vNsoH4eyU1zKAFTwgJLjJWExlU3VfARkfeiy56mpJzA14Q8QZJnvrOOF26BxNefMjl{"version":8,"confirmedSequenceNumberMessage":{"sACw1QLpOO1fTfLmgf5uQZ/yljHWK6AagACKXrr04N4ehYN0Nn3FQgUh2k0LjAAJxAMBgERERElTuslR3to+4Xtkp9zqWBYIAQAAAF0ImTqTM/Q8l0xC7igySsut8ILACD/lwphoEscoUg0pD9d5FM+vdtcyAKAA4ADHT+THFVEOwvjGHXDTl7MgQ/VavADAAEAAQEAAIT8b3hlAtf47wu9MNMwjOSLf8vyxsb+Jo47/AESVPtsRLrAvtKFTnb5BkMJfAFkiDFkVAGizRlgz+3KnDs30heDkx72GZfxFAACE/G94ZQAWACCr0lEfYoH472VvQ1W81JBUj+6sQAIOYloCUhYWhD1n9Tdbn1eGSVNWnkdwl8/SAuIQhehPpAABV9KG/xlXPVe0yXyM4od7oT3VN8A9q7TL8R03VcXEF26XqVyaPRusANEMgz9DBeEFOmS8c4xVA4H0gQTKAIAAEAHSAIk7CuY8ozuN0DICUCi9Im/NSAABihBVWWwAL+i9ov5rWDcPPsc7wRzDrf6JRQkACZDrKON4ZZuTop1G/0HwjcYxbdmAAAOQE5FqvAHgBJeD8AigCFGT4AAD0AzVQAPHEAAmmZkQIQQihvCOpzNUNbWPlMJjbAAkAAAmACajPrPUT+z1eOfWVLIYI6YWz3G7ADAAEBAQA7gAP+XCmGgSxyhSDSkP13kUz6921zAJrFJ5AT7f7HTFwpdvyDGtBSdALAlQFuNq204BUXNc7TmSp/pU4WvQADNrsZUlfpcBUXFpAUiQhTjWU8w2P/C28epj6Vm//ofq9WpKx7n937ar8AIcM+N1dnI6xQD39bmpHU/TWF6UADeC2s6dkAJjp0oh0ODnpaoAFprUcdwu/d4jAGAAQAx0rKuMCjQPWF0AjLUh1k0lVBcaACVStx0SBY0tNJ4xFmFO05swXro0ADPEt0011l9Bv+Y+F0rlIPZ1ffZUAAP+XCmGgSxyhSDSkP13kUz6921zIAKJuhcBwvCIzw+vizcFJGMxy4qDkACAOA9zi6GQvXkkT5I+iMpxCWcFsCxJ4{"version":0,"confirmedSequenceNumberMessage":{"sAD9CGvHzVxIHcychevkeKHAtp/LuQAAJeZtZ4TPZWo8UW+NF8l6O6XgVzgADpgeKppJyCoi/l5E5YyFI8jWInkAI4byb8EA9uk+sohljeXi6YL5nSs3iyKVnRUAPWtfzeC6KZ7/6KXaE4nz5/MeBvhC7ACCR+876cyqwM2g9E2EO/fZbiHL0A0CalPeZU0OuylKwRI/1NyKC1Bf0AIAAQABAEAD8WhpusHaix60G7SLJDX5xDjWtCgAcACQABkiw287x2LKMAtKRrshAvhLFoSrAAFbU9J3r4YPUcPmhD+AdQBwJruzoijYukRZWKdaBwTVZr8siAuQHEUru+KQAAgAgAFAgq9JRH2KB+O9lb0NVvNSQVI/urEAGwLajLDQl+uNV6F1uIx9i0eZdQb/A=="},"delayedMADAAEAAQAAMFg5P6Wf4rae1jZxXgmfrvsHnpjAOu6Rn7LayEjkXgDMYnOrifKEurfAABuAAAooVPu9mz72bxfUcoTn+Jm5UJMABTm94Nfb0za3kUiqdCiDGYu/YDQgAAEAICAgq9JRH2KB+O9lb0NVvNSQVI/urEAH/xzLaL26dBZQ9VIYtTCYs0qTPl/ADGAC+wACzQACswAAGkBBgq9JRH2KB+O9lb0NVvNSQVI/urHQ4w2wAKCGCjLsAKAGRUEha6//7sjqU1u3H7ySeDHQWVBTm94Nfb0za3kUiqdCiDGYu/YDQ"timestampAAJDlcBpkJkWKNcKRI9UeLKHF9vRF/A3gtrOnZAbCwGeQs+Pjw44S7iL4GDs3oT7lUAQACAACuOSh5oiiySE2bH4Cl0LcID+ecIAAPrjnsCXMMoPFCYqY20tfFU5NTdSBAAVgAKtJkf4AgAIA2DG6+AiA5HHnAWAD4nXuchk9Ym79TqCEFEHYis16qQAAjhvJvwQAzLAiATBPSHaQdLx1jNLgPMkHKctK3ADYMbr4AERERElT7bES6wL7ShU52+QZDCX0APl8cHAk7w3T53oIJFVaRrYiv7UA/6},"signatureAC3TBnuyV5Y6FuVjow9dXIxSTOPP/AA/Qhrx81cSB3MnIXr5HihwLafy7kADAAABAAEAABWQAFZAABAFAAwACqVK09MK93tg2TmuNW5mBt6aTaZ1g/Arli0tPy5IFITp077175656e6AhPxveGUAPQkAAFpYhPwxlI1Z3yrszKFD3pANSeGjuAIaVhM0ABQAAKJrAMHw3wAwADkAJxQAAPqnGQAKAifyGhNNeCrXtzPa079pf6i+N4tArgAGAMpjCZphW4lCePsTuZxCIFhPiegFAAQxERYdwuskWg9Ru3kxDB6A0BKbQABvFe6SWKzevzVtt6tge7JVoAxv3AU5veDX29M2t5FIqnQogxmLv2A0IACAAwAxniIt5GKslZuvKuyEhpeuwrvXcAADlkkJ6CuzpLePt7h8Y4BV8BYYVZAQAFgACAABAZgyCUcqe5hdiHOTTyjyV4aV/9n4AU5veDX29M2t5FIqnQogxmLv2A0ADAAABAQEAADIMAKgKQMScy+Aca/UmNASVW5pc3dhcFYzAnBRIB93L6O8JjFg6gm7Fs4aa4/A+rNq/5cKYaBLHKFINKQ/XeRTPr3bXMgA5PAhCSkAUACCr0lEfYoH472VvQ1W81JBUj+6sQABAAFgAM5fJLepXpy6ffS1TpEbSj3Iza9v/7},"signatureA5ZJCegrs6S3j7e4fGOAVfAWGFWAoAfl4WjWiQXe4U7d71xZZYDiJ6y08AKAAOJjNTmGpGOBRMQeRM66ydCnbsqzAADA/5cKYaBLHKFINKQ/XeRTPr3bXMADGeIi3kYqyVm68q7ISGl67Cu9dwA5},"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:{"kind":9,"senARElAS3wACAyuajp9+dnaMqScQA9PkPhYWsf/ACC1wjvNzNex/pM4U4f6U50SPyQ+yAALXCO83M17H+kzhTh/pTnRI/JD7AIAGkrL60MAPkJFvh6lLacjLzZCjnY09PM8KPpaFEcOFag+EKg4f/8xJI9BLVZ9NKai/xs2gTrWw08RgdRwkAsXFzJEJyABos0ZYM/typw7N9IXg5Me9hmX8Rf/Ag3zFstBFvjZlE1zrbcrw902iWbt0PCyFIQBoXji4aCwxjL1Xx4bOyw9gvQe5HFrtMAPD12Eza/BQVgbuHV6T7iAtJxOaAF04zHLCn/zq1ivyXONX4uAgASS4WBg/AQAgq9JRH2KB+O9lb0NVvNSQVI/urEABgAAKADALyolQ7dqQWZUn3qrLnW+8K78Ww8572","blockA47tanGk3anX93aUbjz6TsQ3ga5v/AJ3abvPZGcm8iIXVVgmZo2QEMejm/AMAAE29uZUluY2hWNUZlZUR5bmFtaWAMbZc7MbsTXKuoPPBXTANHvXY+zFAGABgADAE:"0xa4b1e63cb4901e327597bc35d36fe8a23e4c253f","blockAGRW5ZaIAORx5wFoAIAADAFiK/Z+bldHuBhm/guFF4qTx/huM6e672","blockCuCr0lEfYoH472VvQ1W81JBUj+6sQAB9P+XCmGgSxyhSDSkP13kUz6921zICOG8m/BAIEkuFgYPwAbwAJ3zyoBAPuIp8ONbW1K3DbBcRulL3TJXjMGAAZS0nwPcncc5cdv1ADt1htAasbZf/AF0ImTqTM/Q8l0xC7igySsut8ILAwWDk/pZ/itp7WNnFeCZ+u+weemMABvAAEAwC8qJUO3akFmVJ96qy51vvCu/FsPAIAACijYukRZWKdaBwTVZr8siAuQLkuVysKPtebQwd/tK6AA+8BAq43zYVrDKcAAGQ0NZSrm1bpkIe/pvIzXbJMLR8XAKoAAHAAJy9XQfdDOzGYWH8k9fJAA2hALoSIiIiIo2LpEWVinWgcE1Wa/LI/ALXCO83M17H+kzhTh/pTnRI/JD7AFOb3g19vTNreRSKp0KIMZi79gNCAACihU+72bPvZvF9RyhOf4mblQkzsUEBgq9JRH2KB+O9lb0NVvNSQVI/urEABC4afU0IOYloCUq16ZB83Bz9DMa5ve9rFZcqRFuD+AuIQ6XcAIAABos0ZYM/typw7N9IXg5Me9hmX8RQAAqj2fo6uTCuY1sAHQDGEqpbFNdQ4AAnBRIB93L6O8JjFg6gm7Fs4aa4/A+rNqAQAAEAAD/lwphoEscoUg0pD9d5FM+vdtcyADjX6kxoANgxuvABa8xB6QAA//2JY+/R/GpQZIhJXZUdUmOYjSUAB1VWkVlZjzcCvdff9iM6MXwVbT3f/AERJQEt8:{"kind":13,"senAuhIiA2hAAnL1dB90M7MZhYfyT18kADaEAAgq9JRH2KB+O9lb0NVvNSQVI/urEAG8V7pJYrN6/NW23q2B7slWgDG/fARElAS3AiATBPSHaQdLx1jNLgPMkHKctK3AB+AyWylIdckeoGKCbCxXDjj5Yoo/AUAcBqVcHoCkKyLkLNxno7lshA2CIMijYukRZWKdaBwTVZr8siAuQMEuVysKGRUEha6//7sjqU1u3H7ySeDHQWVAAEAAHVVaRWVmPNwK919/2IzoxfBVtPd/ABbZ3bNnFF2j8kVyten6PXEpjMRMf/ACQjE2U00kkdl8e3CKh3QmDl8Wd1P/B7AiMAkaftASMAcvcAagBNYKjU5x1Zm4EEJQ8ABdCJk6kzP0PJdMQu4oMkrLrfCCwA2hAAnL1dB90M7MZhYfyT18kADaAaLNGWDP7cqcOzfSF4OTHvYZl/EX/ABsLAZ5Cz4+PDjhLuIvgYOzehPuVQAAAgq9JRH2KB+O9lb0NVvNSQVI/urAdVVpFZWY83Ar3X3/YjOjF8FW093/Af5ASK/BwD55+H2iP6SaUDog581P/AM2uxlSV+lwFRcWkBSJCFONZTzDY/AYjg3OdaN0PhEED2437Baft7Vu+YAAgACABACAM7xx5HN2MPqktarMjmRGf0w4f8hQ1eFXgABlLSfA9ydxzlx2/UAO3WG0Bqxtl/AONfqTGgA3vHA3tm+x/GhZwgZgzJA8CeyXv8AASTvXas6AW4lzzZM7MMFWQEoM1uPMnl10FYAFgAORQI7TfABAWACBJLhYGD8AAC8qJUO3akFmVJ96qy51vvCu/FsPA41+pMaAALX+O8LvTDTMIzki3/L8sbG/iaOO/APjZlKDMM91vSBnUcyJiV3kq/iMOw8Z/4aBOQe4T4DzV4ERkh7Uk/cSK9qzybAdNrNvftrV0tCyBRrigAEk712rOgAN7xwN7ZvsfxoWcIGYMyQPAnsl7/AAMAvKiVDt2pBZlSfeqsudb7wrvxbDwAIAAgAEAYAAEEiEOigsRead":1AD5fHBwJO8N0+d6CCRVWka2Ir+1AP/AQAA+Qi6+QE6lN7MDAnDtfbpLvQYQSXVZIpm41KY4aA0Zg/IrzBEZFKfSKd44D0D5NNLzV+bbwz7880jjGQvf7kBAh7uALZwOND8AUQAAcpAxzgC/JABIACAGUtJ8D3J3HOXHb9QA7dYbQGrG2X/AJCMTZTTSSR2Xx7cIqHdCYOXxZ3U/Af/HMtovbp0FlD1Uhi1MJizSpM+UAAMBZede1RuOOQU9+mCJRS+RDpIAFKQAdHkQt00mUaBlY8MYKDjq5BIPQncA0BaG9RrhcXAKUg/vs8vPmEQJuRQAAQABALcsF3V6OPkC2pAwAFl517VG445BT36YIlFL5EOkgAUpADAWXnXtUbjjkFPfpgiUUvkQ6SABSkADlj/8nf9EPCqU8hsSnUKYkeMuwY/ABESUBLfAEAIA+XxwcCTvDdPneggkVVpGtiK/tQD/ABJO9dqzoAd2enhQvP4TZCvGp8sthfH4IQgpAB/kBIr8HAPnn4faI/pJpQOiDnzU/AIAD/lwphoEscoUg0pD9d5FM+vdtcyAAD4eZRwGpVwegKQrIuQs3GejuWyEDYIg+GgjT7g32pLfoKn8gp2PxxoJuYXYyPmVa9k8yMYgn0hEtS4AFO/gzpdbE3aiI9pwiyIyfNWpBYU/AWvMQekAAJiK5REozb4+ZRNc623K8PdNolm7dDwshSEAaF44uGgsMYy9V8eGzssPYL0HuRxa7TADw9dhM2vwUFYG7h1ek+4wAC8qJUO3akFmVJ96qy51vvCu/FsPAewIjAJGn7QEjAHL3AGoATWCo1OcdWZuBBCUPABBIhDooAD4m5R/XHZMvBT5ZpuIg3yhSQzKF8MWB/hjoN3yUq0b4sibacKwaPw3jaqVK6fxY8ShFij1Wk31I7PvAHsCIwCRp+0BIwBy9wBqAE1gqNTnHVmbgQQlDwAomsAwfDfADAAOQAnFAAA+qcZADaEACcvV0H3QzsxmFh/JPXyQANoQA5FAjtN8ACBrj0F5PDQ3SnWhAQkoLdhp/21HACAAIABAGAAORQI7TfADkBORarwAXQiZOpMz9DyXTELuKDJKy63wgsAJDlcBpkJkWKNcKRI9UeLKHF9vRF/AIGuPQXk8NDdKdaEBCSgt2Gn/bUcAAAABAALyolQ7dqQWZUn3qrLnW+8K78Ww8ijYukRZWKdaBwTVZr8sj/AIgEwT0h2kHS8dYzS4DzJBynLStwAD9CGvHzVxIHcychevkeKHAtp/LuQg6b3tXySkFo5PO/ROACmMmTsiN2qtjFjH3alxilTL6oK5AqAAbxXuklis3r81bberYHuyVaAMb98AvKiVDt2pBZlSfeqsudb7wrvxbDwAO7uACIBME9IdpB0vHWM0uA8yQcpy0rcACAAQAD7Xm0MHf7SugAPvAQKuN82FawynADu7AEEiEOiAEAAgAD9CGvHzVxIHcychevkeKHAtp/LusRead":A+15tDB3+0roAD7wECrjfNhWsMpwAPtebQwd/tK6AA+8BAq43zYVrDKcAIADkBORarAG8FtZ07ADO8ceRzdjD6pLWqzI5kRn9MOH/IUNXhV4aBpOfk+PyHPE2LrFxVbdAJ33laH2umoOoWQn9cdqVlE57iAFrzEHpAANoQAJy9XQfdDOzGYWH8k9fJAA2hABAAEAB4ASXg/AIoAhRk+AAA9AM1UADxxAAC6EiIiIiKNi6RFlYp1oHBNVmvyyP/A7uu4AAC0rqqxAlj0AAGizRlgz+3KnDs30heDkx72GZfxF/AD8WhpusHaix60G7SLJDX5xDjWtCABAAIAEAAQ:{"kind":12,"senABsC2oyw0JfrjVehdbiMfYtHmXUG/A/FoabrB2osetBu0iyQ1+cQ41rQoADlR/q01c6v0p4mU8sZ5q6O2chYmwAQAABA/Qhrx81cSB3MnIXr5HihwLafy7kABSFwmzzX8H4pcivguiiozg6AbbwwAza7GVJX6XAVFxaQFIkIU41lPMNgAQAACAEABYAbwADUtgnWq4+DCQE2faPbO4IS1vrPdAQSIQ6KAPxaGm6wdqLHrQbtIskNfnEONa0KAgAOQE5FqvACA5ATkWq8AGACgADkUCO03wAP0Ia8fNXEgdzJyF6+R4ocC2n8u5ACAAmAFIXCbPNfwfilyK+C6KKjODoBtvDAgFvJ9vLDqfIXmm48gAMU7XBXIGAKZDYfoI9PMqDaDIH3LLVZg3r83YAD/lwphoEscoUg0pD9d5FM+vdtcyAAUhcJs81/B+KXIr4LooqM4OgG28MAzvHHkc3Yw+qS1qsyOZEZ/TDh/yFDV4VeAAIKvSUR9igfjvZW9DVbzUkFSP7qxAA/5cKYaBLHKFINKQ/XeRTPr3bXMgACCr0lEfYoH472VvQ1W81JBUj+6sQACCr0lEfYoH472VvQ1W81JBUj+6sA/5cKYaBLHKFINKQ/XeRTPr3bXM"restId":null,"baseFeeL1AP+XCmGgSxyhSDSkP13kUz6921zIAgq9JRH2KB+O9lb0NVvNSQVI/urEAD/lwphoEscoUg0pD9d5FM+vdtcysReadAIKvSUR9igfjvZW9DVbzUkFSP7qxl},"l2Msg":"AwAe":{"head:"0xa4b072","block:{"kind":3,"sen073657175656e6estId":null,"baseFeeL10,"message":{"message":{"header":{"kind":13,"sender":"0xa4b
1,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 2,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 3,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 4,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 5,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 6,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 7,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 8,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 9,"message":{"message":{"header":{"kind":13,"sender":"0xa4b 0,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 1,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 2,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 3,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 4,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 5,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 6,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 7,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 8,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 9,"message":{"message":{"header":{"kind":12,"sender":"0xa4b 0,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 1,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 2,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 9,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 4,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 5,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 6,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 7,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 8,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 9,"message":{"message":{"header":{"kind":9,"sender":"0xa4b 0,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 1,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 2,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 3,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 4,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 5,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 6,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 7,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 8,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 9,"message":{"message":{"header":{"kind":3,"sender":"0xa4b 1,"timestamp": 2,"timestamp": 3,"timestamp": 4,"timestamp": 5,"timestamp": 6,"timestamp": 7,"timestamp": 8,"timestamp": 9,"timestamp": 0,"requestId":null,"baseFeeL1":null},"l2Msg":" 1,"requestId":null,"baseFeeL1":null},"l2Msg":" 2,"requestId":null,"baseFeeL1":null},"l2Msg":" 3,"requestId":null,"baseFeeL1":null},"l2Msg":" 4,"requestId":null,"baseFeeL1":null},"l2Msg":" 5,"requestId":null,"baseFeeL1":null},"l2Msg":" 6,"requestId":null,"baseFeeL1":null},"l2Msg":" 7,"requestId":null,"baseFeeL1":null},"l2Msg":" 8,"requestId":null,"baseFeeL1":null},"l2Msg":" 9,"requestId":null,"baseFeeL1":null},"l2Msg":" 1,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 2,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 3,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 4,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 5,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 6,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 7,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 8,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 9,"requestId":"0x00000000000000000000000000000000000000000000000000000000000 {"version":1,"confirmedSequenceNumberMessage":{"sequenceNumber":
{"version":1,"messages":[{"sequenceNumber":
","blockNumber":
0,"timestamp":
0,"requestId":"0x00000000000000000000000000000000000000000000000000000000000
","baseFeeL1":null},"l2Msg":"
=="},"delayedMessagesRead":
},"signature":null}]}
`)
}

'''
'''--- wsbroadcastserver/utils.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"compress/flate"
	"context"
	"errors"
	"io"
	"net"
	"strings"
	"time"

	"github.com/ethereum/go-ethereum/log"
	"github.com/gobwas/ws"
	"github.com/gobwas/ws/wsflate"
	"github.com/gobwas/ws/wsutil"
)

func init() {
	// We use a custom dictionary, so our compression isn't compatible with other websocket clients.
	wsflate.ExtensionNameBytes = append([]byte("Arbitrum-"), wsflate.ExtensionNameBytes...)
}

type chainedReader struct {
	readers []io.Reader
}

func logError(err error, msg string) {
	if err != nil && !strings.Contains(err.Error(), "use of closed network connection") {
		log.Error(msg, "err", err)
	}
}

func logWarn(err error, msg string) {
	if err != nil && !strings.Contains(err.Error(), "use of closed network connection") {
		log.Warn(msg, "err", err)
	}
}

func (cr *chainedReader) Read(b []byte) (n int, err error) {
	for len(cr.readers) > 0 {
		n, err = cr.readers[0].Read(b)
		if errors.Is(err, io.EOF) {
			cr.readers = cr.readers[1:]
			if n == 0 {
				continue // EOF and empty, skip to next
			} else {
				// The Read interface specifies some data can be returned along with an EOF.
				if len(cr.readers) != 1 {
					// If this isn't the last reader, return the data without the EOF since this
					// may not be the end of all the readers.
					return n, nil
				}
				return
			}
		}
		break
	}
	return
}

func (cr *chainedReader) add(r io.Reader) *chainedReader {
	if r != nil {
		cr.readers = append(cr.readers, r)
	}
	return cr
}

func NewFlateReader() *wsflate.Reader {
	return wsflate.NewReader(nil, func(r io.Reader) wsflate.Decompressor {
		return flate.NewReaderDict(r, GetStaticCompressorDictionary())
	})
}

func ReadData(ctx context.Context, conn net.Conn, earlyFrameData io.Reader, timeout time.Duration, state ws.State, compression bool, flateReader *wsflate.Reader) ([]byte, ws.OpCode, error) {
	if compression {
		state |= ws.StateExtended
	}
	controlHandler := wsutil.ControlFrameHandler(conn, state)
	var msg wsflate.MessageState
	reader := wsutil.Reader{
		Source:          (&chainedReader{}).add(earlyFrameData).add(conn),
		State:           state,
		CheckUTF8:       !compression,
		SkipHeaderCheck: false,
		OnIntermediate:  controlHandler,
		Extensions:      []wsutil.RecvExtension{&msg},
	}

	err := conn.SetReadDeadline(time.Now().Add(timeout))
	if err != nil {
		return nil, 0, err
	}

	// Remove timeout when leaving this function
	defer func() {
		err := conn.SetReadDeadline(time.Time{})
		logError(err, "error removing read deadline")
	}()

	for {
		select {
		case <-ctx.Done():
			return nil, 0, nil
		default:
		}

		// Control packet may be returned even if err set
		header, err := reader.NextFrame()
		if header.OpCode.IsControl() {
			// Control packet may be returned even if err set
			if err2 := controlHandler(header, &reader); err2 != nil {
				return nil, 0, err2
			}

			// Discard any data after control packet
			if err2 := reader.Discard(); err2 != nil {
				return nil, 0, err2
			}

			return nil, 0, nil
		}
		if err != nil {
			return nil, 0, err
		}

		if header.OpCode != ws.OpText &&
			header.OpCode != ws.OpBinary {
			if err := reader.Discard(); err != nil {
				return nil, 0, err
			}
			continue
		}
		var data []byte
		if msg.IsCompressed() {
			if !compression {
				return nil, 0, errors.New("Received compressed frame even though compression is disabled")
			}
			flateReader.Reset(&reader)
			data, err = io.ReadAll(flateReader)
		} else {
			data, err = io.ReadAll(&reader)
		}

		return data, header.OpCode, err
	}
}

'''
'''--- wsbroadcastserver/wsbroadcastserver.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package wsbroadcastserver

import (
	"context"
	"errors"
	"fmt"
	"net"
	"net/http"
	"net/textproto"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/gobwas/httphead"
	"github.com/gobwas/ws"
	"github.com/gobwas/ws-examples/src/gopool"
	"github.com/gobwas/ws/wsflate"
	"github.com/mailru/easygo/netpoll"
	flag "github.com/spf13/pflag"

	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/offchainlabs/nitro/arbutil"
)

var (
	HTTPHeaderCloudflareConnectingIP  = textproto.CanonicalMIMEHeaderKey("CF-Connecting-IP")
	HTTPHeaderFeedServerVersion       = textproto.CanonicalMIMEHeaderKey("Arbitrum-Feed-Server-Version")
	HTTPHeaderFeedClientVersion       = textproto.CanonicalMIMEHeaderKey("Arbitrum-Feed-Client-Version")
	HTTPHeaderRequestedSequenceNumber = textproto.CanonicalMIMEHeaderKey("Arbitrum-Requested-Sequence-Number")
	HTTPHeaderChainId                 = textproto.CanonicalMIMEHeaderKey("Arbitrum-Chain-Id")
	upgradeToWSTimer                  = metrics.NewRegisteredTimer("arb/feed/clients/upgrade/duration", nil)
	startWithHeaderTimer              = metrics.NewRegisteredTimer("arb/feed/clients/start/duration", nil)
)

const (
	FeedServerVersion = 2
	FeedClientVersion = 2
	LivenessProbeURI  = "livenessprobe"
)

type BroadcasterConfig struct {
	Enable             bool                    `koanf:"enable"`
	Signed             bool                    `koanf:"signed"`
	Addr               string                  `koanf:"addr"`
	ReadTimeout        time.Duration           `koanf:"read-timeout" reload:"hot"`      // reloaded value will affect all clients (next time the timeout is checked)
	WriteTimeout       time.Duration           `koanf:"write-timeout" reload:"hot"`     // reloading will affect only new connections
	HandshakeTimeout   time.Duration           `koanf:"handshake-timeout" reload:"hot"` // reloading will affect only new connections
	Port               string                  `koanf:"port"`
	Ping               time.Duration           `koanf:"ping" reload:"hot"`           // reloaded value will change future ping intervals
	ClientTimeout      time.Duration           `koanf:"client-timeout" reload:"hot"` // reloaded value will affect all clients (next time the timeout is checked)
	Queue              int                     `koanf:"queue"`
	Workers            int                     `koanf:"workers"`
	MaxSendQueue       int                     `koanf:"max-send-queue" reload:"hot"`  // reloaded value will affect only new connections
	RequireVersion     bool                    `koanf:"require-version" reload:"hot"` // reloaded value will affect only future upgrades to websocket
	DisableSigning     bool                    `koanf:"disable-signing"`
	LogConnect         bool                    `koanf:"log-connect"`
	LogDisconnect      bool                    `koanf:"log-disconnect"`
	EnableCompression  bool                    `koanf:"enable-compression" reload:"hot"`  // if reloaded to false will cause disconnection of clients with enabled compression on next broadcast
	RequireCompression bool                    `koanf:"require-compression" reload:"hot"` // if reloaded to true will cause disconnection of clients with disabled compression on next broadcast
	LimitCatchup       bool                    `koanf:"limit-catchup" reload:"hot"`
	MaxCatchup         int                     `koanf:"max-catchup" reload:"hot"`
	ConnectionLimits   ConnectionLimiterConfig `koanf:"connection-limits" reload:"hot"`
	ClientDelay        time.Duration           `koanf:"client-delay" reload:"hot"`
}

func (bc *BroadcasterConfig) Validate() error {
	if !bc.EnableCompression && bc.RequireCompression {
		return errors.New("require-compression cannot be true while enable-compression is false")
	}
	return nil
}

type BroadcasterConfigFetcher func() *BroadcasterConfig

func BroadcasterConfigAddOptions(prefix string, f *flag.FlagSet) {
	f.Bool(prefix+".enable", DefaultBroadcasterConfig.Enable, "enable broadcaster")
	f.Bool(prefix+".signed", DefaultBroadcasterConfig.Signed, "sign broadcast messages")
	f.String(prefix+".addr", DefaultBroadcasterConfig.Addr, "address to bind the relay feed output to")
	f.Duration(prefix+".read-timeout", DefaultBroadcasterConfig.ReadTimeout, "duration to wait before timing out reading data (i.e. pings) from clients")
	f.Duration(prefix+".write-timeout", DefaultBroadcasterConfig.WriteTimeout, "duration to wait before timing out writing data to clients")
	f.Duration(prefix+".handshake-timeout", DefaultBroadcasterConfig.HandshakeTimeout, "duration to wait before timing out HTTP to WS upgrade")
	f.String(prefix+".port", DefaultBroadcasterConfig.Port, "port to bind the relay feed output to")
	f.Duration(prefix+".ping", DefaultBroadcasterConfig.Ping, "duration for ping interval")
	f.Duration(prefix+".client-timeout", DefaultBroadcasterConfig.ClientTimeout, "duration to wait before timing out connections to client")
	f.Int(prefix+".queue", DefaultBroadcasterConfig.Queue, "queue size for HTTP to WS upgrade")
	f.Int(prefix+".workers", DefaultBroadcasterConfig.Workers, "number of threads to reserve for HTTP to WS upgrade")
	f.Int(prefix+".max-send-queue", DefaultBroadcasterConfig.MaxSendQueue, "maximum number of messages allowed to accumulate before client is disconnected")
	f.Bool(prefix+".require-version", DefaultBroadcasterConfig.RequireVersion, "don't connect if client version not present")
	f.Bool(prefix+".disable-signing", DefaultBroadcasterConfig.DisableSigning, "don't sign feed messages")
	f.Bool(prefix+".log-connect", DefaultBroadcasterConfig.LogConnect, "log every client connect")
	f.Bool(prefix+".log-disconnect", DefaultBroadcasterConfig.LogDisconnect, "log every client disconnect")
	f.Bool(prefix+".enable-compression", DefaultBroadcasterConfig.EnableCompression, "enable per message deflate compression support")
	f.Bool(prefix+".require-compression", DefaultBroadcasterConfig.RequireCompression, "require clients to use compression")
	f.Bool(prefix+".limit-catchup", DefaultBroadcasterConfig.LimitCatchup, "only supply catchup buffer if requested sequence number is reasonable")
	f.Int(prefix+".max-catchup", DefaultBroadcasterConfig.MaxCatchup, "the maximum size of the catchup buffer (-1 means unlimited)")
	ConnectionLimiterConfigAddOptions(prefix+".connection-limits", f)
	f.Duration(prefix+".client-delay", DefaultBroadcasterConfig.ClientDelay, "delay the first messages sent to each client by this amount")
}

var DefaultBroadcasterConfig = BroadcasterConfig{
	Enable:             false,
	Signed:             false,
	Addr:               "",
	ReadTimeout:        time.Second,
	WriteTimeout:       2 * time.Second,
	HandshakeTimeout:   time.Second,
	Port:               "9642",
	Ping:               5 * time.Second,
	ClientTimeout:      15 * time.Second,
	Queue:              100,
	Workers:            100,
	MaxSendQueue:       4096,
	RequireVersion:     false,
	DisableSigning:     true,
	LogConnect:         false,
	LogDisconnect:      false,
	EnableCompression:  true,
	RequireCompression: false,
	LimitCatchup:       false,
	MaxCatchup:         -1,
	ConnectionLimits:   DefaultConnectionLimiterConfig,
	ClientDelay:        0,
}

var DefaultTestBroadcasterConfig = BroadcasterConfig{
	Enable:             false,
	Signed:             false,
	Addr:               "0.0.0.0",
	ReadTimeout:        2 * time.Second,
	WriteTimeout:       2 * time.Second,
	HandshakeTimeout:   2 * time.Second,
	Port:               "0",
	Ping:               5 * time.Second,
	ClientTimeout:      15 * time.Second,
	Queue:              1,
	Workers:            100,
	MaxSendQueue:       4096,
	RequireVersion:     false,
	DisableSigning:     false,
	LogConnect:         false,
	LogDisconnect:      false,
	EnableCompression:  true,
	RequireCompression: false,
	LimitCatchup:       false,
	MaxCatchup:         -1,
	ConnectionLimits:   DefaultConnectionLimiterConfig,
	ClientDelay:        0,
}

type WSBroadcastServer struct {
	startMutex sync.Mutex
	poller     netpoll.Poller

	acceptDescMutex sync.Mutex
	acceptDesc      *netpoll.Desc

	listener      net.Listener
	config        BroadcasterConfigFetcher
	started       bool
	clientManager *ClientManager
	catchupBuffer CatchupBuffer
	chainId       uint64
	fatalErrChan  chan error
}

func NewWSBroadcastServer(config BroadcasterConfigFetcher, catchupBuffer CatchupBuffer, chainId uint64, fatalErrChan chan error) *WSBroadcastServer {
	return &WSBroadcastServer{
		config:        config,
		started:       false,
		catchupBuffer: catchupBuffer,
		chainId:       chainId,
		fatalErrChan:  fatalErrChan,
	}
}

func (s *WSBroadcastServer) Initialize() error {
	if s.poller != nil {
		return errors.New("broadcast server already initialized")
	}

	var err error
	s.poller, err = netpoll.New(nil)
	if err != nil {
		log.Error("unable to initialize netpoll for monitoring client connection events", "err", err)
		return err
	}

	// Make pool of X size, Y sized work queue and one pre-spawned
	// goroutine.
	s.clientManager = NewClientManager(s.poller, s.config, s.catchupBuffer)

	return nil
}

func (s *WSBroadcastServer) Start(ctx context.Context) error {
	// Prepare handshake header writer from http.Header mapping.
	header := ws.HandshakeHeaderHTTP(http.Header{
		HTTPHeaderFeedServerVersion: []string{strconv.Itoa(FeedServerVersion)},
		HTTPHeaderChainId:           []string{strconv.FormatUint(s.chainId, 10)},
	})

	startTime := time.Now()
	err := s.StartWithHeader(ctx, header)
	elapsed := time.Since(startTime)
	startWithHeaderTimer.Update(elapsed)
	return err
}

func (s *WSBroadcastServer) StartWithHeader(ctx context.Context, header ws.HandshakeHeader) error {
	s.startMutex.Lock()
	defer s.startMutex.Unlock()
	if s.started {
		return errors.New("broadcast server already started")
	}

	s.clientManager.Start(ctx)

	// handle incoming connection requests.
	// It upgrades TCP connection to WebSocket, registers netpoll listener on
	// it and stores it as a Client connection in ClientManager instance.
	//
	// Called below in accept() loop.
	handle := func(conn net.Conn) {
		config := s.config()
		// Set read and write deadlines for the handshake/upgrade
		err := conn.SetReadDeadline(time.Now().Add(config.HandshakeTimeout))
		if err != nil {
			log.Warn("error setting handshake read deadline", "err", err)
			_ = conn.Close()
			return
		}
		err = conn.SetWriteDeadline(time.Now().Add(config.HandshakeTimeout))
		if err != nil {
			log.Warn("error setting handshake write deadline", "err", err)
			_ = conn.Close()
			return
		}

		var compress *wsflate.Extension
		var negotiate func(httphead.Option) (httphead.Option, error)
		if config.EnableCompression {
			compress = &wsflate.Extension{
				Parameters: wsflate.DefaultParameters, // TODO
			}
			negotiate = compress.Negotiate
		}
		var feedClientVersionSeen bool
		var connectingIP net.IP
		var requestedSeqNum arbutil.MessageIndex
		upgrader := ws.Upgrader{
			OnRequest: func(uri []byte) error {
				if strings.Contains(string(uri), LivenessProbeURI) {
					return ws.RejectConnectionError(
						ws.RejectionStatus(http.StatusOK),
					)
				}
				return nil
			},
			OnHeader: func(key []byte, value []byte) error {
				headerName := textproto.CanonicalMIMEHeaderKey(string(key))
				if headerName == HTTPHeaderFeedClientVersion {
					feedClientVersion, err := strconv.ParseUint(string(value), 0, 64)
					if err != nil {
						return ws.RejectConnectionError(
							ws.RejectionStatus(http.StatusBadRequest),
							ws.RejectionReason(fmt.Sprintf("Malformed HTTP header %s", HTTPHeaderFeedClientVersion)),
						)
					}
					if feedClientVersion < FeedClientVersion {
						return ws.RejectConnectionError(
							ws.RejectionStatus(http.StatusBadRequest),
							ws.RejectionReason(fmt.Sprintf("Feed Client version too old: %d, expected %d", feedClientVersion, FeedClientVersion)),
						)
					}
					feedClientVersionSeen = true
				} else if headerName == HTTPHeaderRequestedSequenceNumber {
					num, err := strconv.ParseUint(string(value), 0, 64)
					if err != nil {
						return ws.RejectConnectionError(
							ws.RejectionStatus(http.StatusBadRequest),
							ws.RejectionReason(fmt.Sprintf("Malformed HTTP header %s", HTTPHeaderRequestedSequenceNumber)),
						)
					}
					requestedSeqNum = arbutil.MessageIndex(num)
				} else if headerName == HTTPHeaderCloudflareConnectingIP {
					connectingIP = net.ParseIP(string(value))
					log.Trace("Client IP parsed from header", "ip", connectingIP, "header", headerName, "value", string(value))
				}

				return nil
			},
			OnBeforeUpgrade: func() (ws.HandshakeHeader, error) {
				if config.RequireVersion && !feedClientVersionSeen {
					return nil, ws.RejectConnectionError(
						ws.RejectionStatus(http.StatusBadRequest),
						ws.RejectionReason(fmt.Sprintf("Missing HTTP header %s", HTTPHeaderFeedClientVersion)),
					)
				}
				if connectingIP == nil {
					if addr, ok := conn.RemoteAddr().(*net.TCPAddr); ok {
						connectingIP = addr.IP
						log.Trace("Client IP taken from socket", "ip", connectingIP, "remoteAddr", conn.RemoteAddr())
					} else {
						log.Warn("No client IP could be determined from socket", "remoteAddr", conn.RemoteAddr())
					}
				}

				if config.ConnectionLimits.Enable && !s.clientManager.connectionLimiter.IsAllowed(connectingIP) {
					return nil, ws.RejectConnectionError(
						ws.RejectionStatus(http.StatusTooManyRequests),
						ws.RejectionReason("Too many open feed connections."),
					)
				}

				return header, nil
			},
			Negotiate: negotiate,
		}

		// Zero-copy upgrade to WebSocket connection.
		startTime := time.Now()
		_, err = upgrader.Upgrade(conn)
		elapsed := time.Since(startTime)
		upgradeToWSTimer.Update(elapsed)

		if err != nil {
			if err.Error() != "" {
				// Only log if liveness probe was not called
				log.Debug("websocket upgrade error", "connectingIP", connectingIP, "err", err)
				clientsTotalFailedUpgradeCounter.Inc(1)
			}
			_ = conn.Close()
			return
		}

		compressionAccepted := false
		if compress != nil {
			_, compressionAccepted = compress.Accepted()
		}
		if config.RequireCompression && !compressionAccepted {
			log.Warn("client did not accept required compression, disconnecting", "connectingIP", connectingIP)
			_ = conn.Close()
			return
		}
		// Unset our handshake/upgrade deadlines
		err = conn.SetReadDeadline(time.Time{})
		if err != nil {
			log.Warn("error unsetting read deadline", "connectingIP", connectingIP, "err", err)
			_ = conn.Close()
			return
		}
		err = conn.SetWriteDeadline(time.Time{})
		if err != nil {
			log.Warn("error unsetting write deadline", "connectingIP", connectingIP, "err", err)
			_ = conn.Close()
			return
		}

		// Create netpoll event descriptor to handle only read events.
		desc, err := netpoll.HandleRead(conn)
		if err != nil {
			log.Warn("error in HandleRead", "connectingIP", connectingIP, "err", err)
			_ = conn.Close()
			return
		}

		// Register incoming client in clientManager.
		safeConn := writeDeadliner{conn, config.WriteTimeout}

		client := s.clientManager.Register(safeConn, desc, requestedSeqNum, connectingIP, compressionAccepted)

		// Subscribe to events about conn.
		err = s.poller.Start(desc, func(ev netpoll.Event) {
			if ev&(netpoll.EventReadHup|netpoll.EventHup) != 0 {
				// ReadHup or Hup received, means the client has close the connection
				// remove it from the clientManager registry.
				log.Debug("Hup received", "age", client.Age(), "client", client.Name)
				s.clientManager.Remove(client)
				return
			}

			if ev > 1 {
				log.Debug("event greater than 1 received", "client", client.Name, "event", int(ev))
			}

			// receive client messages, close on error
			s.clientManager.pool.Schedule(func() {
				// Ignore any messages sent from client, close on any error
				if _, _, err := client.Receive(ctx, s.config().ReadTimeout); err != nil {
					s.clientManager.Remove(client)
					return
				}
			})
		})

		if err != nil {
			log.Warn("error starting client connection poller", "err", err)
		}
	}

	// Create tcp server for relay connections
	config := s.config()
	ln, err := net.Listen("tcp", config.Addr+":"+config.Port)
	if err != nil {
		log.Error("error calling net.Listen", "err", err)
		return err
	}

	s.listener = ln

	log.Info("arbitrum websocket broadcast server is listening", "address", ln.Addr().String())

	// Create netpoll descriptor for the listener.
	// We use OneShot here to synchronously manage the rate that new connections are accepted
	acceptDesc, err := netpoll.HandleListener(ln, netpoll.EventRead|netpoll.EventOneShot)
	if err != nil {
		log.Error("error calling HandleListener", "err", err)
		return err
	}
	s.acceptDesc = acceptDesc

	// acceptErrChan blocks until connection accepted or error occurred
	// OneShot is used, so reusing a single channel is fine
	acceptErrChan := make(chan error, 1)

	// Subscribe to events about listener.
	err = s.poller.Start(acceptDesc, func(e netpoll.Event) {
		select {
		case <-ctx.Done():
			return
		default:
		}
		// We do not want to accept incoming connection when goroutine pool is
		// busy. So if there are no free goroutines during 1ms we want to
		// cooldown the server and do not receive connection for some short
		// time.
		err := s.clientManager.pool.ScheduleTimeout(time.Millisecond, func() {
			conn, err := ln.Accept()
			if err != nil {
				acceptErrChan <- err
				return
			}

			acceptErrChan <- nil
			handle(conn)
		})
		if err == nil {
			err = <-acceptErrChan
		}
		if err != nil {
			if errors.Is(err, gopool.ErrScheduleTimeout) {
				log.Warn("broadcast poller timed out waiting for available worker", "err", err)
				clientsTotalFailedWorkerCounter.Inc(1)
			} else if errors.Is(err, netpoll.ErrNotRegistered) {
				log.Error("broadcast poller unable to register file descriptor", "err", err)
			} else {
				var netError net.Error
				isNetError := errors.As(err, &netError)
				if (!isNetError || !netError.Timeout()) && !strings.Contains(err.Error(), "timed out") {
					log.Error("broadcast poller error", "err", err)
				}
			}

			// cooldown
			delay := 5 * time.Millisecond
			log.Info("accept error", "delay", delay.String(), "err", err)
			time.Sleep(delay)
		}

		s.acceptDescMutex.Lock()
		if s.acceptDesc == nil {
			// Already shutting down
			s.acceptDescMutex.Unlock()
			return
		}
		err = s.poller.Resume(s.acceptDesc)
		s.acceptDescMutex.Unlock()
		if err != nil {
			log.Warn("error in poller.Resume", "err", err)
			s.fatalErrChan <- fmt.Errorf("error in poller.Resume: %w", err)
			return
		}
	})
	if err != nil {
		log.Warn("error in starting broadcaster poller", "err", err)
		return err
	}

	s.started = true

	return nil
}

func (s *WSBroadcastServer) ListenerAddr() net.Addr {
	return s.listener.Addr()
}

func (s *WSBroadcastServer) StopAndWait() {
	err := s.listener.Close()
	if err != nil {
		log.Warn("error in listener.Close", "err", err)
	}

	err = s.poller.Stop(s.acceptDesc)
	if err != nil {
		log.Warn("error in poller.Stop", "err", err)
	}

	s.acceptDescMutex.Lock()
	err = s.acceptDesc.Close()
	s.acceptDesc = nil
	s.acceptDescMutex.Unlock()
	if err != nil {
		log.Warn("error in acceptDesc.Close", "err", err)
	}

	s.clientManager.StopAndWait()
	s.started = false
}

func (s *WSBroadcastServer) Started() bool {
	return s.started
}

// Broadcast sends batch item to all clients.
func (s *WSBroadcastServer) Broadcast(bm interface{}) {
	s.clientManager.Broadcast(bm)
}

func (s *WSBroadcastServer) ClientCount() int32 {
	return s.clientManager.ClientCount()
}

// writeDeadliner is a wrapper around net.Conn that sets write deadlines before
// every Write() call.
type writeDeadliner struct {
	net.Conn
	t time.Duration
}

func (d writeDeadliner) Write(p []byte) (int, error) {
	if err := d.Conn.SetWriteDeadline(time.Now().Add(d.t)); err != nil {
		return 0, err
	}
	return d.Conn.Write(p)
}

'''
'''--- zeroheavy/common_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package zeroheavy

import (
	"testing"

	"github.com/offchainlabs/nitro/util/testhelpers"
)

func Require(t *testing.T, err error, printables ...interface{}) {
	t.Helper()
	testhelpers.RequireImpl(t, err, printables...)
}

func Fail(t *testing.T, printables ...interface{}) {
	t.Helper()
	testhelpers.FailImpl(t, printables...)
}

func ShowError(t *testing.T, err error) {
	t.Helper()
	if err != nil {
		t.Error(err)
	}
}

'''
'''--- zeroheavy/zeroheavy.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package zeroheavy

import (
	"errors"
	"io"
)

type ZeroheavyEncoder struct {
	inner              io.Reader
	buffer             byte
	bitsReadFromBuffer uint8
	nowInPadding       bool
	atEof              bool
}

func NewZeroheavyEncoder(inner io.Reader) *ZeroheavyEncoder {
	return &ZeroheavyEncoder{inner, 0, 8, false, false}
}

func (enc *ZeroheavyEncoder) nextInputBit() (bool, error) {
	if enc.nowInPadding {
		return true, nil
	}
	if enc.bitsReadFromBuffer == 8 {
		var buf [1]byte
		_, err := enc.inner.Read(buf[:])
		if errors.Is(err, io.EOF) {
			// we're in padding mode now; we'll emit a false, then as many trues as needed
			enc.nowInPadding = true
			return false, nil
		}
		if err != nil {
			return false, err
		}
		enc.bitsReadFromBuffer = 0
		enc.buffer = buf[0]
	}
	ret := (enc.buffer & (1 << (7 - enc.bitsReadFromBuffer))) != 0
	enc.bitsReadFromBuffer++
	return ret, nil
}

func (enc *ZeroheavyEncoder) readOne() (byte, error) {
	if enc.atEof {
		return 0, io.EOF
	}
	b, err := enc.readOneImpl()
	if err != nil {
		return b, err
	}
	if enc.nowInPadding {
		// our input is at EOF, and we have consumed some padding, so this should be the last byte produced
		enc.atEof = true
	}
	return b, nil
}

func (enc *ZeroheavyEncoder) readOneImpl() (byte, error) {
	firstBit, err := enc.nextInputBit()
	if err != nil {
		return 0, err
	}
	if !firstBit {
		secondBit, err := enc.nextInputBit()
		if err != nil {
			return 0, err
		}
		if !secondBit {
			return 0, nil
		}
		ret := byte(1)
		for i := 0; i < 6; i++ {
			nextBit, err := enc.nextInputBit()
			if err != nil {
				return 0, err
			}
			ret <<= 1
			if nextBit {
				ret++
			}
		}
		if ret == 64 {
			return 1, nil
		}
		ret = (ret << 1) & 0x7f
		nextBit, err := enc.nextInputBit()
		if err != nil {
			return 0, err
		}
		if nextBit {
			ret++
		}
		return ret, nil
	}
	ret := byte(1) // first bit is 1
	for i := 0; i < 7; i++ {
		ret <<= 1
		nextBit, err := enc.nextInputBit()
		if err != nil {
			return 0, err
		}
		if nextBit {
			ret += 1
		}
	}
	return ret, nil
}

func (enc *ZeroheavyEncoder) Read(p []byte) (int, error) {
	for i := range p {
		b, err := enc.readOne()
		if err != nil {
			return i, err
		}
		p[i] = b
	}
	return len(p), nil
}

type ZeroheavyDecoder struct {
	source          io.Reader
	buffer          []bool
	eofAfterBuffer  bool
	deferredZero    bool
	numDeferredOnes uint
}

func NewZeroheavyDecoder(source io.Reader) *ZeroheavyDecoder {
	return &ZeroheavyDecoder{source, []bool{}, false, false, 0}
}

func (br *ZeroheavyDecoder) readOne() (byte, error) {
	ret := byte(0)
	for i := 0; i < 8; i++ {
		b, err := br.nextBit()
		if err != nil {
			return 0, err
		}
		ret <<= 1
		if b {
			ret |= 1
		}
	}
	return ret, nil
}

func (br *ZeroheavyDecoder) Read(p []byte) (int, error) {
	for i := range p {
		b, err := br.readOne()
		if err != nil {
			return i, err
		}
		p[i] = b
	}
	return len(p), nil
}

func (br *ZeroheavyDecoder) pushBit(b bool) {
	if br.deferredZero {
		if b {
			br.numDeferredOnes++
		} else {
			br.buffer = append(br.buffer, false)
			for br.numDeferredOnes > 0 {
				br.buffer = append(br.buffer, true)
				br.numDeferredOnes--
			}
		}
	} else {
		if b {
			br.buffer = append(br.buffer, true)
		} else {
			br.deferredZero = true
		}
	}
}

func (br *ZeroheavyDecoder) push7Bits(b byte) {
	for i := 0; i < 7; i++ {
		br.pushBit(b&(1<<(6-i)) != 0)
	}
}

func (br *ZeroheavyDecoder) nextBit() (bool, error) {
	for len(br.buffer) == 0 {
		if br.eofAfterBuffer {
			return false, io.EOF
		}
		br.eofAfterBuffer = br.refill()
	}
	ret := br.buffer[0]
	br.buffer = br.buffer[1:]
	return ret, nil
}

func (br *ZeroheavyDecoder) refill() bool {
	var buf [1]byte
	_, err := io.ReadFull(br.source, buf[:])
	if err != nil {
		return true
	}
	b := buf[0]
	if b == 0 {
		br.pushBit(false)
		br.pushBit(false)
	} else if b == 1 {
		br.pushBit(false)
		br.pushBit(true)
		for i := 0; i < 6; i++ {
			br.pushBit(false)
		}
	} else if b < 0x80 {
		br.pushBit(false)
		br.pushBit(true)
		br.push7Bits(b)
	} else {
		br.pushBit(true)
		br.push7Bits(b & 0x7f)
	}
	return false
}

'''
'''--- zeroheavy/zeroheavy_test.go ---
// Copyright 2021-2022, Offchain Labs, Inc.
// For license information, see https://github.com/nitro/blob/master/LICENSE

package zeroheavy

import (
	"bytes"
	"errors"
	"io"
	"math/rand"
	"os"
	"testing"
	"time"

	"github.com/offchainlabs/nitro/arbcompress"
	"github.com/offchainlabs/nitro/util/colors"
	"github.com/offchainlabs/nitro/util/testhelpers"
)

func TestZeroheavyNullInput(t *testing.T) {
	inBuf := []byte{}
	source := bytes.NewReader(inBuf)
	enc := NewZeroheavyEncoder(source)
	dec := NewZeroheavyDecoder(enc)

	var buf [256]byte
	n, err := dec.Read(buf[:])
	if !errors.Is(err, io.EOF) {
		Fail(t)
	}
	if n != 0 {
		Fail(t, n, buf[0])
	}
}

func TestZeroHeavyOneByte(t *testing.T) {
	for i := 0; i < 256; i++ {
		inBuf := []byte{byte(i)}
		source := bytes.NewReader(inBuf)
		enc := NewZeroheavyEncoder(source)
		dec := NewZeroheavyDecoder(enc)

		buf, err := io.ReadAll(dec)
		ShowError(t, err)

		if len(buf) != 1 {
			Fail(t, i, len(buf))
		}
		if buf[0] != byte(i) {
			Fail(t, buf[0], i)
		}
	}
}

func l1Cost(data []byte) int {
	cost := 4 * len(data)
	for _, b := range data {
		if b != 0 {
			cost += 12
		}
	}
	return cost
}

func TestZeroHeavyRandomDataRandom(t *testing.T) {
	rand.Seed(time.Now().UTC().UnixNano())

	trials := 1024
	avg := 0.0
	best := 0.0
	worst := 100.0

	for i := 0; i < trials; i++ {
		size := 1 + rand.Uint64()%4096
		inBuf := testhelpers.RandomizeSlice(make([]byte, size))
		enc := NewZeroheavyEncoder(bytes.NewReader(inBuf))
		encoded, err := io.ReadAll(enc)
		ShowError(t, err)

		improvement := 100.0 * float64(l1Cost(inBuf)-l1Cost(encoded)) / float64(l1Cost(inBuf))
		if improvement > best {
			best = improvement
			colors.PrintGrey("best  ", len(encoded), "/", size, "\t", l1Cost(encoded), "/", l1Cost(inBuf))
		}
		if improvement < worst {
			worst = improvement
			colors.PrintGrey("worst ", len(encoded), "/", size, "\t", l1Cost(encoded), "/", l1Cost(inBuf))
		}

		avg += improvement / float64(trials)

		dec := NewZeroheavyDecoder(bytes.NewReader(encoded))
		res, err := io.ReadAll(dec)
		ShowError(t, err)
		if !bytes.Equal(inBuf, res) {
			Fail(t, size, inBuf)
		}
	}

	colors.PrintBlue("avg   improvement ", avg)
	colors.PrintBlue("best  improvement ", best)
	colors.PrintBlue("worst improvement ", worst)
}

func TestZeroHeavyRandomDataBrotli(t *testing.T) {
	rand.Seed(time.Now().UTC().UnixNano())

	trials := 256
	avg := 0.0
	best := 0.0
	worst := 100.0

	for i := 0; i < trials; i++ {

		// Compress a low-entropy input
		size := 100 + rand.Uint64()%2048
		randomBytes := testhelpers.RandomizeSlice(make([]byte, size))
		for i := range randomBytes {
			randomBytes[i] /= 8
		}
		for i, b := range randomBytes {
			if b < 0x14 {
				randomBytes[i] = 0
			}
		}
		input, err := arbcompress.CompressWell(randomBytes)
		Require(t, err)

		ShowError(t, err)
		enc := NewZeroheavyEncoder(bytes.NewReader(input))
		encoded, err := io.ReadAll(enc)
		ShowError(t, err)

		improvement := 100.0 * float64(l1Cost(input)-l1Cost(encoded)) / float64(l1Cost(input))
		if improvement > best {
			best = improvement
			colors.PrintGrey("best  ", len(encoded), "/", size, "\t", l1Cost(encoded), "/", l1Cost(input))
		}
		if improvement < worst {
			worst = improvement
			colors.PrintGrey("worst ", len(encoded), "/", size, "\t", l1Cost(encoded), "/", l1Cost(input))
		}

		avg += improvement / float64(trials)

		dec := NewZeroheavyDecoder(bytes.NewReader(encoded))
		res, err := io.ReadAll(dec)
		ShowError(t, err)
		if !bytes.Equal(input, res) {
			Fail(t, size, input)
		}
	}

	colors.PrintBlue("avg   improvement ", avg)
	colors.PrintBlue("best  improvement ", best)
	colors.PrintBlue("worst improvement ", worst)
}

func TestZeroHeavyAndBrotli(t *testing.T) {
	inData, err := os.ReadFile("../go.sum")
	ShowError(t, err)

	bout, err := arbcompress.CompressWell(inData)
	ShowError(t, err)

	zhout, err := io.ReadAll(NewZeroheavyDecoder(NewZeroheavyEncoder(bytes.NewReader(bout))))
	ShowError(t, err)

	res, err := arbcompress.Decompress(zhout, len(inData))
	ShowError(t, err)

	if !bytes.Equal(inData, res) {
		Fail(t)
	}
}

'''